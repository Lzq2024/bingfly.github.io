{"meta":{"title":"冰的技术专栏","subtitle":"学习、分享知识的快乐","description":"","author":"lzq","url":"http://bingfly.top","root":"/"},"pages":[{"title":"一个平平无奇的码农","date":"2024-06-14T21:24:01.000Z","updated":"2024-12-14T21:34:01.303Z","comments":true,"path":"about/index.html","permalink":"http://bingfly.top/about/index.html","excerpt":"","text":"2024.06.21开始我的学习之旅！"},{"title":"404","date":"2024-12-12T14:50:20.000Z","updated":"2024-12-12T14:50:20.453Z","comments":true,"path":"404/index.html","permalink":"http://bingfly.top/404/index.html","excerpt":"","text":""},{"title":"search","date":"2024-12-12T14:49:18.000Z","updated":"2024-12-12T14:49:18.701Z","comments":true,"path":"search/index.html","permalink":"http://bingfly.top/search/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"总结/Linux运维常见面试题","date":"2024-12-14T22:02:47.174Z","updated":"2021-01-27T08:33:28.000Z","comments":true,"path":"2024/12/15/总结/Linux运维常见面试题/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/Linux%E8%BF%90%E7%BB%B4%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"Linux运维常见面试题 1、什么是运维？什么是游戏运维？1）运维是指大型组织已经建立好的网络软硬件的维护，就是要保证业务的上线与运作的正常，在他运转的过程中，对他进行维护，他集合了网络、系统、数据库、开发、安全、监控于一身的技术运维又包括很多种，有DBA运维、网站运维、虚拟化运维、监控运维、游戏运维等等2）游戏运维又有分工，分为开发运维、应用运维（业务运维）和系统运维开发运维：是给应用运维开发运维工具和运维平台的应用运维：是给业务上线、维护和做故障排除的，用开发运维开发出来的工具给业务上线、维护、做故障排查系统运维：是给应用运维提供业务上的基础设施，比如：系统、网络、监控、硬件等等总结：开发运维和系统运维给应用运维提供了“工具”和“基础设施”上的支撑开发运维、应用运维和系统运维他们的工作是环环相扣的—————————————————————————————————————————————————————————————————————————————————— 2、在工作中，运维人员经常需要跟运营人员打交道，请问运营人员是做什么工作的？游戏运营要做的一个事情除了协调工作以外还需要与各平台沟通，做好开服的时间、开服数、用户导量、活动等计划———————————————————————————————————————————————————————————————————————————————————————— 3、现在给你三百台服务器，你怎么对他们进行管理？管理3百台服务器的方式：1）设定跳板机，使用统一账号登录，便于安全与登录的考量。2）使用saltstark、ansiable、puppet进行系统的统一调度与配置的统一管理。3）建立简单的服务器的系统、配置、应用的cmdb信息管理。便于查阅每台服务器上的各种信息记录———————————————————————————————————————————————————————————————————————————————————————————————————————————— 4、简述raid0 raid1 raid5 三种工作模式的工作原理及特点RAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据还有一个大功能，多块盘放在一起可以有冗余（备份）RAID整合方式有很多，常用的：0 1 5 10RAID 0，可以是一块盘和N个盘组合其优点读写快，是RAID中最好的缺点：没有冗余，一块坏了数据就全没有了RAID 1，只能2块盘，盘的大小可以不一样，以小的为准10G+10G只有10G，另一个做备份。它有100%的冗余，缺点：浪费资源，成本高RAID 5 ，3块盘，容量计算10*（n-1）,损失一块盘特点，读写性能一般，读还好一点，写不好冗余从好到坏：RAID1 RAID10 RAID 5 RAID0性能从好到坏：RAID0 RAID10 RAID5 RAID1成本从低到高：RAID0 RAID5 RAID1 RAID10单台服务器：很重要盘不多，系统盘，RAID1数据库服务器：主库：RAID10 从库 RAID5\\RAID0（为了维护成本，RAID10）WEB服务器，如果没有太多的数据的话，RAID5,RAID0（单盘）有多台，监控、应用服务器，RAID0 RAID5我们会根据数据的存储和访问的需求，去匹配对应的RAID级别————————————————————————————————————————————————————————————————————————————————————————————————————————— 5、LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？LVS： 是基于四层的转发HAproxy： 是基于四层和七层的转发，是专业的代理服务器Nginx： 是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发区别： LVS由于是基于四层的转发所以只能做端口的转发而基于URL的、基于目录的这种转发LVS就做不了工作选择：HAproxy和Nginx由于可以做七层的转发，所以URL和目录的转发都可以做在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大选择HAproxy或者Nginx足已，由于HAproxy由是专业的代理服务器配置简单，所以中小型企业推荐使用HAproxy———————————————————————————————————————————————————————————————————————————————————————————————————————————————— 6、Squid、Varinsh和Nginx有什么区别，工作中你怎么选择？Squid、Varinsh和Nginx都是代理服务器什么是代理服务器：能当替用户去访问公网，并且能把访问到的数据缓存到服务器本地，等用户下次再访问相同的资源的时候，代理服务器直接从本地回应给用户，当本地没有的时候，我代替你去访问公网，我接收你的请求，我先在我自已的本地缓存找，如果我本地缓存有，我直接从我本地的缓存里回复你如果我在我本地没有找到你要访问的缓存的数据，那么代理服务器就会代替你去访问公网区别：1）Nginx本来是反向代理&#x2F;web服务器，用了插件可以做做这个副业但是本身不支持特性挺多，只能缓存静态文件2）从这些功能上。varnish和squid是专业的cache服务，而nginx这些是第三方模块完成3）varnish本身的技术上优势要高于squid，它采用了可视化页面缓存技术在内存的利用上，Varnish比Squid具有优势，性能要比Squid高。还有强大的通过Varnish管理端口，可以使用正则表达式快速、批量地清除部分缓存它是内存缓存，速度一流，但是内存缓存也限制了其容量，缓存页面和图片一般是挺好的4）squid的优势在于完整的庞大的cache技术资料，和很多的应用生产环境工作中选择：要做cache服务的话，我们肯定是要选择专业的cache服务，优先选择squid或者varnish。———————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 7、Tomcat和Resin有什么区别，工作中你怎么选择？区别：Tomcat用户数多，可参考文档多，Resin用户数少，可考虑文档少最主要区别则是Tomcat是标准的java容器，不过性能方面比resin的要差一些但稳定性和java程序的兼容性，应该是比resin的要好工作中选择：现在大公司都是用resin，追求性能；而中小型公司都是用Tomcat，追求稳定和程序的兼容———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 8、什么是中间件？什么是jdk？中间件介绍：中间件是一种独立的系统软件或服务程序，分布式应用软件借助这种软件在不同的技术之间共享资源中间件位于客户机&#x2F; 服务器的操作系统之上，管理计算机资源和网络通讯是连接两个独立应用程序或独立系统的软件。相连接的系统，即使它们具有不同的接口但通过中间件相互之间仍能交换信息。执行中间件的一个关键途径是信息传递通过中间件，应用程序可以工作于多平台或OS环境。jdk：jdk是Java的开发工具包它是一种用于构建在 Java 平台上发布的应用程序、applet 和组件的开发环境———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 9、讲述一下Tomcat8005、8009、8080三个端口的含义？8005&#x3D;&#x3D;》 关闭时使用8009&#x3D;&#x3D;》 为AJP端口，即容器使用，如Apache能通过AJP协议访问Tomcat的8009端口8080&#x3D;&#x3D;》 一般应用使用—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 10、什么叫CDN？即内容分发网络其目的是通过在现有的Internet中增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 11、什么叫网站灰度发布？灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式AB test就是一种灰度发布方式，让一部用户继续用A，一部分用户开始用B如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面 来灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 12、简述DNS进行域名解析的过程？用户要访问 www.baidu.com，会先找本机的host文件，再找本地设置的DNS服务器，如果也没有的话，就去网络中找根服务器，根服务器反馈结果，说只能提供一级域名服务器.cn，就去找一级域名服务器，一级域名服务器说只能提供二级域名服务器.com.cn,就去找二级域名服务器，二级域服务器只能提供三级域名服务器.baidu.com.cn，就去找三级域名服务器，三级域名服务器正好有这个网站www.baidu.com，然后发给请求的服务器，保存一份之后，再发给客户端 ———————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 13、RabbitMQ是什么东西？RabbitMQ也就是消息队列中间件，消息中间件是在消息的传息过程中保存消息的容器消息中间件再将消息从它的源中到它的目标中标时充当中间人的作用队列的主要目的是提供路由并保证消息的传递；如果发送消息时接收者不可用消息队列不会保留消息，直到可以成功地传递为止，当然，消息队列保存消息也是有期限地———————————————————————————————————————————————————————————————————————————————————————— 14、讲一下Keepalived的工作原理？在一个虚拟路由器中，只有作为MASTER的VRRP(虚拟路由冗余协议)路由器会一直发送VRRP通告信息,BACKUP不会抢占MASTER，除非它的优先级更高。当MASTER不可用时(BACKUP收不到通告信息)多台BACKUP中优先级最高的这台会被抢占为MASTER。这种抢占是非常快速的(&lt;1s)，以保证服务的连续性由于安全性考虑，VRRP包使用了加密协议进行加密。BACKUP不会发送通告信息，只会接收通告信息—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 15、讲述一下LVS三种模式的工作过程？LVS 有三种负载均衡的模式，分别是VS&#x2F;NAT（nat 模式） VS&#x2F;DR(路由模式) VS&#x2F;TUN（隧道模式）一、NAT模式（VS-NAT）原理：就是把客户端发来的数据包的IP头的目的地址，在负载均衡器上换成其中一台RS的IP地址并发至此RS来处理,RS处理完后把数据交给负载均衡器,负载均衡器再把数据包原IP地址改为自己的IP将目的地址改为客户端IP地址即可期间,无论是进来的流量,还是出去的流量,都必须经过负载均衡器优点：集群中的物理服务器可以使用任何支持TCP&#x2F;IP操作系统，只有负载均衡器需要一个合法的IP地址缺点：扩展性有限。当服务器节点（普通PC服务器）增长过多时,负载均衡器将成为整个系统的瓶颈因为所有的请求包和应答包的流向都经过负载均衡器。当服务器节点过多时大量的数据包都交汇在负载均衡器那，速度就会变慢！二、IP隧道模式（VS-TUN）原理：首先要知道，互联网上的大多Internet服务的请求包很短小，而应答包通常很大那么隧道模式就是，把客户端发来的数据包，封装一个新的IP头标记(仅目的IP)发给RSRS收到后,先把数据包的头解开,还原数据包,处理后,直接返回给客户端,不需要再经过负载均衡器。注意,由于RS需要对负载均衡器发过来的数据包进行还原,所以说必须支持IPTUNNEL协议，所以,在RS的内核中,必须编译支持IPTUNNEL这个选项优点：负载均衡器只负责将请求包分发给后端节点服务器，而RS将应答包直接发给用户所以，减少了负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量这种方式，一台负载均衡器能够为很多RS进行分发。而且跑在公网上就能进行不同地域的分发。缺点：隧道模式的RS节点需要合法IP，这种方式需要所有的服务器支持”IP Tunneling”(IP Encapsulation)协议，服务器可能只局限在部分Linux系统上三、直接路由模式（VS-DR）原理：负载均衡器和RS都使用同一个IP对外服务但只有DR对ARP请求进行响应所有RS对本身这个IP的ARP请求保持静默也就是说,网关会把对这个服务IP的请求全部定向给DR而DR收到数据包后根据调度算法,找出对应的RS,把目的MAC地址改为RS的MAC（因为IP一致）并将请求分发给这台RS这时RS收到这个数据包,处理完成之后，由于IP一致，可以直接将数据返给客户则等于直接从客户端收到这个数据包无异,处理后直接返回给客户端由于负载均衡器要对二层包头进行改换,所以负载均衡器和RS之间必须在一个广播域也可以简单的理解为在同一台交换机上优点：和TUN（隧道模式）一样，负载均衡器也只是分发请求，应答包通过单独的路由方法返回给客户端与VS-TUN相比，VS-DR这种实现方式不需要隧道结构，因此可以使用大多数操作系统做为物理服务器。缺点：（不能说缺点，只能说是不足）要求负载均衡器的网卡必须与物理网卡在一个物理段上。———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 16、mysql的innodb如何定位锁问题，mysql如何减少主从复制延迟？mysql的innodb如何定位锁问题:在使用 show engine innodb status检查引擎状态时，发现了死锁问题在5.5中，information_schema 库中增加了三个关于锁的表（MEMORY引擎）innodb_trx ## 当前运行的所有事务innodb_locks ## 当前出现的锁innodb_lock_waits ## 锁等待的对应关系mysql如何减少主从复制延迟:如果延迟比较大，就先确认以下几个因素：从库硬件比主库差，导致复制延迟 主从复制单线程，如果主库写并发太大，来不及传送到从库 就会导致延迟。更高版本的mysql可以支持多线程复制慢SQL语句过多 网络延迟 master负载主库读写压力大，导致复制延迟，架构的前端要加buffer及缓存层slave负载一般的做法是，使用多台slave来分摊读请求，再从这些slave中取一台专用的服务器只作为备份用，不进行其他任何操作.另外， 2个可以减少延迟的参数:–slave-net-timeout&#x3D;seconds 单位为秒 默认设置为 3600秒#参数含义：当slave从主数据库读取log数据失败后，等待多久重新建立连接并获取数据–master-connect-retry&#x3D;seconds 单位为秒 默认设置为 60秒#参数含义：当重新建立主从连接时，如果连接建立失败，间隔多久后重试通常配置以上2个参数可以减少网络问题导致的主从数据同步延迟MySQL数据库主从同步延迟解决方案最简单的减少slave同步延时的方案就是在架构上做优化，尽量让主库的DDL快速执行还有就是主库是写，对数据安全性较高，比如sync_binlog&#x3D;1，innodb_flush_log_at_trx_commit&#x3D; 1 之类的设置，而slave则不需要这么高的数据安全，完全可以讲sync_binlog设置为0或者关闭binloginnodb_flushlog也可以设置为0来提高sql的执行效率。另外就是使用比主库更好的硬件设备作为slave—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 17、如何重置mysql root密码？一、 在已知MYSQL数据库的ROOT用户密码的情况下，修改密码的方法：1、 在SHELL环境下，使用mysqladmin命令设置：mysqladmin –u root –p password “新密码” 回车后要求输入旧密码2、 在mysql&gt;环境中,使用update命令，直接更新mysql库user表的数据：Update mysql.user set password&#x3D;password(‘新密码’) where user&#x3D;’root’;flush privileges;注意：mysql语句要以分号”；”结束3、 在mysql&gt;环境中，使用grant命令，修改root用户的授权权限。grant all on . to root@’localhost’ identified by ‘新密码’；二、 如查忘记了mysql数据库的ROOT用户的密码，又如何做呢？方法如下：1、 关闭当前运行的mysqld服务程序：service mysqld stop（要先将mysqld添加为系统服务）2、 使用mysqld_safe脚本以安全模式（不加载授权表）启动mysqld 服务&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin&#x2F;mysqld_safe –skip-grant-table &amp;3、 使用空密码的root用户登录数据库，重新设置ROOT用户的密码＃mysql -u rootMysql&gt; Update mysql.user set password&#x3D;password(‘新密码’) where user&#x3D;’root’;Mysql&gt; flush privileges;—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 18、lvs&#x2F;nginx&#x2F;haproxy优缺点Nginx的优点是：1、工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一Nginx单凭这点可利用的场合就远多于LVS了。2、Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一相反LVS对网络稳定性依赖比较大，这点本人深有体会；3、Nginx安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来LVS的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。4、可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。5、Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。6、Nginx不仅仅是一款优秀的负载均衡器&#x2F;反向代理软件，它同时也是功能强大的Web应用服务器LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。7、Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可考虑用其作为反向代理加速器8、Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有lighttpd了不过lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃9、Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多Nginx的缺点是：1、Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点2、对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测不支持Session的直接保持，但能通过ip_hash来解决LVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)LVS的优点是：1、抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低2、配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西所以并不需要太多接触，大大减少了人为出错的几率3、工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案如LVS+Keepalived，不过我们在项目实施中用得最多的还是LVS&#x2F;DR+Keepalived4、无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会收到大流量的影响。5、应用范围较广，因为LVS工作在4层，所以它几乎可对所有应用做负载均衡，包括http、数据库、在线聊天室等LVS的缺点是：1、软件本身不支持正则表达式处理，不能做动静分离而现在许多网站在这方面都有较强的需求，这个是Nginx&#x2F;HAProxy+Keepalived的优势所在2、如果是网站应用比较庞大的话，LVS&#x2F;DR+Keepalived实施起来就比较复杂了特别后面有Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了相对而言，Nginx&#x2F;HAProxy+Keepalived就简单多了。HAProxy的特点是：1、HAProxy也是支持虚拟主机的。2、HAProxy的优点能够补充Nginx的一些缺点，比如支持Session的保持，Cookie的引导同时支持通过获取指定的url来检测后端服务器的状态3、HAProxy跟LVS类似，本身就只是一款负载均衡软件单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的4、HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡对后端的MySQL节点进行检测和负载均衡，大家可以用LVS+Keepalived对MySQL主从做负载均衡5、HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：①roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；② static-rr，表示根据权重，建议关注；③leastconn，表示最少连接者先处理，建议关注；④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似我们用其作为解决session问题的一种方法，建议关注；⑤ri，表示根据请求的URI；⑥rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；⑦hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；⑧rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 19、mysql数据备份工具mysqldump工具mysqldump是mysql自带的备份工具，目录在bin目录下面：&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;bin&#x2F;mysqldump支持基于innodb的热备份，但是由于是逻辑备份，所以速度不是很快，适合备份数据比较小的场景Mysqldump完全备份+二进制日志可以实现基于时间点的恢复。基于LVM快照备份在物理备份中，有基于文件系统的物理备份（LVM的快照），也可以直接用tar之类的命令对整个数据库目录进行打包备份，但是这些只能进行泠备份，不同的存储引擎备份的也不一样，myisam自动备份到表级别而innodb不开启独立表空间的话只能备份整个数据库。tar包备份percona提供的xtrabackup工具支持innodb的物理热备份，支持完全备份，增量备份，而且速度非常快，支持innodb存储引起的数据在不同数据库之间迁移，支持复制模式下的从机备份恢复备份恢复，为了让xtrabackup支持更多的功能扩展可以设立独立表空间，打开 innodb_file_per_table功能，启用之后可以支持单独的表备份—————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 20、keepalive的工作原理和如何做到健康检查keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成一个路由器组这个组里面有一个master和多个backup，master上面有一个对外提供服务的vip（该路由器所在局域网内其他机器的默认路由为该vip），master会发组播，当backup收不到vrrp包时就认为master宕掉了这时就需要根据VRRP的优先级来选举一个backup当master。这样就可以保证路由器的高可用了keepalived主要有三个模块，分别是core、check和vrrp。core模块为keepalived的核心，负责主进程的启动、维护及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查方式，vrrp模块是来实现VRRP协议的Keepalived健康检查方式配置HTTP_GET|SSL_GETHTTP_GET | SSL_GET{url {path &#x2F;# HTTP&#x2F;SSL 检查的url可以是多个digest # HTTP&#x2F;SSL 检查后的摘要信息用工具genhash生成status_code 200# HTTP&#x2F;SSL 检查返回的状态码}connect_port 80 # 连接端口bindtoconnect_timeout 3 # 连接超时时间nb_get_retry 3 # 重连次数delay_before_retry 2 #连接间隔时间}———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 21、统计ip访问情况，要求分析nginx访问日志，找出访问页面数量在前十位的ipcat access.log | awk ‘{print $1}’ | uniq -c | sort -rn | head -10———————————————————————————————————————————————————————————————————————————————————————————————————————— 22、使用tcpdump监听主机为192.168.1.1，tcp端口为80的数据，同时将输出结果保存输出到tcpdump.logtcpdump ‘host 192.168.1.1 and port 80’ &gt; tcpdump.log—————————————————————————————————————————————————————————————————————————————————————————————————————————— 23、如何将本地80 端口的请求转发到8080 端口，当前主机IP 为192.168.2.1iptables -A PREROUTING -d 192.168.2.1 -p tcp -m tcp -dport 80 -j DNAT-to-destination 192.168.2.1:8080———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 24、简述raid0 raid1 raid5 三种工作模式的工作原理及特点RAID 0：带区卷，连续以位或字节为单位分割数据，并行读&#x2F;写于多个磁盘上，因此具有很高的数据传输率但它没有数据冗余，RAID 0 只是单纯地提高性能，并没有为数据的可靠性提供保证而且其中的一个磁盘失效将影响到所有数据。因此，RAID 0 不能应用于数据安全性要求高的场合RAID 1：镜像卷，它是通过磁盘数据镜像实现数据冗余，在成对的独立磁盘上产生互为备份的数据不能提升写数据效率。当原始数据繁忙时，可直接从镜像拷贝中读取数据，因此RAID1 可以提高读取性能RAID 1 是磁盘阵列中单位成本最高的，镜像卷可用容量为总容量的1&#x2F;2，但提供了很高的数据安全性和可用性当一个磁盘失效时，系统可以自动切换到镜像磁盘上读写，而不需要重组失效的数据RAID5：至少由3块硬盘组成，分布式奇偶校验的独立磁盘结构，它的奇偶校验码存在于所有磁盘上任何一个硬盘损坏，都可以根据其它硬盘上的校验位来重建损坏的数据（最多允许1块硬盘损坏）所以raid5可以实现数据冗余，确保数据的安全性，同时raid5也可以提升数据的读写性能———————————————————————————————————————————————————————————————————————————————————————————————————————————— 25、你对现在运维工程师的理解和以及对其工作的认识运维工程师在公司当中责任重大，需要保证时刻为公司及客户提供最高、最快、最稳定、最安全的服务运维工程师的一个小小的失误，很有可能会对公司及客户造成重大损失因此运维工程师的工作需要严谨及富有创新精神———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 26、实时抓取并显示当前系统中tcp 80端口的网络数据信息，请写出完整操作命令tcpdump -nn tcp port 80 ———————————————————————————————————————————————————————————————————————————————————— 27、如何优化 Linux系统（可以不说太具体）？不用root，添加普通用户，通过sudo授权管理更改默认的远程连接SSH服务端口及禁止root用户远程连接定时自动更新服务器时间配置国内yum源关闭selinux及iptables（iptables工作场景如果有外网IP一定要打开，高并发除外）调整文件描述符的数量精简开机启动服务（crond rsyslog network sshd）内核参数优化（&#x2F;etc&#x2F;sysctl.conf）更改字符集，支持中文，但建议还是用英文字符集，防止乱码锁定关键系统文件清空&#x2F;etc&#x2F;issue，去除系统及内核版本登录前的屏幕显示 —————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 28、Linux系统中病毒怎么解决1）最简单有效的方法就是重装系统2）要查的话就是找到病毒文件然后删除中毒之后一般机器cpu、内存使用率会比较高机器向外发包等异常情况，排查方法简单介绍下top 命令找到cpu使用率最高的进程一般病毒文件命名都比较乱，可以用 ps aux 找到病毒文件位置rm -f 命令删除病毒文件检查计划任务、开机启动项和病毒文件目录有无其他可以文件等3）由于即使删除病毒文件不排除有潜伏病毒，所以最好是把机器备份数据之后重装一下———————————————————————————————————————————————————————————————————————————————————————————————————————————— 29、发现一个病毒文件你删了他又自动创建怎么解决公司的内网某台linux服务器流量莫名其妙的剧增,用iftop查看有连接外网的情况针对这种情况一般重点查看netstat连接的外网ip和端口。用lsof -p pid可以查看到具体是那些进程，哪些文件经查勘发现&#x2F;root下有相关的配置conf.n hhe两个可疑文件，rm -rf后不到一分钟就自动生成了由此推断是某个母进程产生的这些文件。所以找到母进程就是找到罪魁祸首查杀病毒最好断掉外网访问，还好是内网服务器，可以通过内网访问断了内网，病毒就失去外联的能力，杀掉它就容易的多怎么找到呢，找了半天也没有看到蛛丝马迹，没办法只有ps axu一个个排查方法是查看可以的用户和和系统相似而又不是的冒牌货，果然，看到了如下进程可疑看不到图片就是&#x2F;usr&#x2F;bin&#x2F;.sshd于是我杀掉所有.sshd相关的进程，然后直接删掉.sshd这个可执行文件然后才删掉了文章开头提到的自动复活的文件总结一下，遇到这种问题，如果不是太严重，尽量不要重装系统一般就是先断外网，然后利用iftop,ps,netstat,chattr,lsof,pstree这些工具顺藤摸瓜一般都能找到元凶。但是如果遇到诸如此类的问题&#x2F;boot&#x2F;efi&#x2F;EFI&#x2F;redhat&#x2F;grub.efi: Heuristics.Broken.Executable FOUND，个人觉得就要重装系统了———————————————————————————————————————————————————————————————————————————————————————————————————————— 30、说说TCP&#x2F;IP的七层模型应用层 (Application)：网络服务与最终用户的一个接口。协议有：HTTP FTP TFTP SMTP SNMP DNS TELNET HTTPS POP3 DHCP表示层（Presentation Layer）：数据的表示、安全、压缩。（在五层模型里面已经合并到了应用层）格式有，JPEG、ASCll、DECOIC、加密格式等会话层（Session Layer）：建立、管理、终止会话。（在五层模型里面已经合并到了应用层）对应主机进程，指本地主机与远程主机正在进行的会话传输层 (Transport)：定义传输数据的协议端口号，以及流控和差错校验。协议有：TCP UDP，数据包一旦离开网卡即进入网络传输层网络层 (Network)：进行逻辑地址寻址，实现不同网络之间的路径选择。协议有：ICMP IGMP IP（IPV4 IPV6） ARP RARP数据链路层 (Link)：建立逻辑连接、进行硬件地址寻址、差错校验等功能。（由底层网络定义协议）将比特组合成字节进而组合成帧，用MAC地址访问介质，错误发现但不能纠正物理层（Physical Layer）：是计算机网络OSI模型中最低的一层物理层规定:为传输数据所需要的物理链路创建、维持、拆除而提供具有机械的，电子的，功能的和规范的特性简单的说，物理层确保原始的数据可在各种物理媒体上传输。局域网与广域网皆属第1、2层物理层是OSI的第一层，它虽然处于最底层，却是整个开放系统的基础物理层为设备之间的数据通信提供传输媒体及互连设备，为数据传输提供可靠的环境如果您想要用尽量少的词来记住这个第一层，那就是“信号和介质”—————————————————————————————————————————————————————————————————————————————————————————————————————————————— 31、你常用的Nginx模块，用来做什么rewrite模块，实现重写功能access模块：来源控制ssl模块：安全加密ngx_http_gzip_module：网络传输压缩模块ngx_http_proxy_module 模块实现代理ngx_http_upstream_module模块实现定义后端服务器列表ngx_cache_purge实现缓存清除功能———————————————————————————————————————————————————————————————————————————————————————————————————————————— 32、请列出你了解的web服务器负载架构NginxHaproxyKeepalivedLVS—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 33、查看http的并发请求数与其TCP连接状态netstat -n | awk ‘&#x2F;^tcp&#x2F; {++S[$NF]} END {for(a in S) print a, S[a]}’还有ulimit -n 查看linux系统打开最大的文件描述符，这里默认1024不修改这里web服务器修改再大也没用，若要用就修改很几个办法，这里说其中一个：修改&#x2F;etc&#x2F;security&#x2F;limits.confsoft nofile 10240hard nofile 10240重启后生效———————————————————————————————————————————————————————————————————————————————————————————————————————— 34、用tcpdump嗅探80端口的访问看看谁最高1tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F&quot;.&quot; &#x27;&#123;print $1&quot;.&quot;$2&quot;.&quot;$3&quot;.&quot;$4&#125;&#x27; | sort | uniq -c | sort -nr |head -5 https://blog.csdn.net/lgh1117/article/details/80213397 35、光标下插入一行:o36、复制5行：5yy37、删除10行：10dd38、替换：:%s&#x2F;jingfeng&#x2F;jfedu.net&#x2F;g—————————————————————————————————————————————————————————————————————————————————————————————————— 39、查找linux系统下以txt结尾，30天没有修改的文件大小大于20K同时具有执行权限的文件并备份到&#x2F;data&#x2F;backup&#x2F;目录下。答：find &#x2F; -name *txt -mtime +30 -type f -size +20k -perma&#x3D; x -exec cp {} &#x2F;data&#x2F;backup&#x2F; ;———————————————————————————————————————————————————————————————————————————————————————————————————————— 40、当前test.txt所属的用户为root，组为abc，请将test.txt使拥有者为abc，组为root，写出命令。答：chown abc:root test.txt—————————————————————————————————————————————————————————————————————————————————————————————————— 41、如何修改Linux启动级别为字符模式并永久生效，如何临时、永久关闭selinux及防火墙，请分别写出操作方法。答：更改字符模式：修改&#x2F;etc&#x2F;inittab一行为id:3:initdefault:临时关闭selinnuxsetenforce0临时关闭防火墙iptables-F永久关闭selinux修改&#x2F;etc&#x2F;selinux&#x2F;config一行为SELINUX&#x3D;permissive永久关闭防火墙 iptables -F; &#x2F;etc&#x2F;init.d&#x2F;iptablessave——————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 41、.每次开机在&#x2F;tmp目录下创建一个当天的日期文件夹(提示：当前日期表示的方法为：date+%Y%m%d)答：echo “mkdir&#x2F;tmp&#x2F; date+%Y%m%d” &gt;&gt; &#x2F;etc&#x2F;rc.d&#x2F;rc.local—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 43、.如何查看文件内容，命令有哪些？查看文件第1行到3行，查看文件最后一行。答：查看文件内容：vim、cat、head、tail查看第1到行：head -3 file查看最后一行：tail -1 file—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 44、.查看linux服务器IP的命令，同时只显示包含ip所在的行打印出来。答：以eth0为例只打印所在的行：ifconfig eth0 | grep “inetaddr:”只打印ip：ifconfig eth0 | grep “inetaddr:” | awk -F: ‘{print$2}’ | awk -F ’ ’ ‘{print$1}’———————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————— 45、.将普通用户test加入root组的命令是？答：usermod -G root test 46、linux如何挂在windows下的共享目录mount.cifs &#x2F;&#x2F;192.168.1.3&#x2F;server &#x2F;mnt&#x2F;server -o user&#x3D;administrator,pass&#x3D;123456linux 下的server需要自己手动建一个 后面的user与pass 是windows主机的账号和密码 注意空格 和逗号 47、查看http的并发请求数与其TCP连接状态netstat -n | awk ‘&#x2F;^tcp&#x2F; {++b[$NF]} END {for(a in b) print a, b[a]}’还有ulimit -n 查看linux系统最大的tcp连接，这里默认1024，不修改这里web服务器修改再大也没用。若要用就修改很几个办法，这里说其中一个：修改&#x2F;etc&#x2F;security&#x2F;limits.conf* soft nofile 10240* hard nofile 10240重启后生效 48、查看当前系统每个IP的连接数1netstat -n | awk &#x27;/^tcp/ &#123;print $5&#125;&#x27;| awk -F: &#x27;&#123;print $1&#125;&#x27; | sort | uniq -c | sort -rn 49、查看&#x2F;var&#x2F;log中以-开头的日志文件的行数1ls /var/log/ -lR| grep &quot;^-&quot; |wc -l 50、shell下32位随机密码生成1cat /dev/urandom | head -1 | md5sum | head -c 32 &gt;&gt; /pass 将生成的32位随机数 保存到&#x2F;pass文件里了 51、统计出apache的access.log中访问量最多的5个I1Pcat access_log | awk ’&#123;print $1&#125;’ | sort | uniq -c | sort -n -r | head -5 52、如何查看二进制文件的内容我们一般通过hexdump命令 来查看二进制文件的内容。hexdump -C XXX(文件名) -C是参数 不同的参数有不同的意义-C 是比较规范的 十六进制和ASCII码显示-c 是单字节字符显示-b 单字节八进制显示-o 是双字节八进制显示-d 是双字节十进制显示-x 是双字节十六进制显示等等等等 53、ps aux 中的VSZ代表什么意思，RSS代表什么意思12VSZ:虚拟内存集,进程占用的虚拟内存空间RSS:物理内存集,进程战用实际物理内存空间 54、检测并修复&#x2F;dev&#x2F;hda5fsck用来检查和维护不一致的文件系统。若系统掉电或磁盘发生问题 可利用fsck命令对文件系统进行检查,用法：fsck -P &#x2F;dev&#x2F;hda5","categories":[],"tags":[]},{"title":"","slug":"总结/Jenkins详细教程","date":"2024-12-14T22:02:47.169Z","updated":"2021-02-22T09:10:06.000Z","comments":true,"path":"2024/12/15/总结/Jenkins详细教程/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/Jenkins%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/","excerpt":"","text":"Jenkins详细教程一、jenkins是什么？​ Jenkins是一个开源的、提供友好操作界面的持续集成(CI)工具，起源于Hudson（Hudson是商用的），主要用于持续、自动的构建&#x2F;测试软件项目、监控外部任务的运行（这个比较抽象，暂且写上，不做解释）。Jenkins用Java语言编写，可在Tomcat等流行的servlet容器中运行，也可独立运行。通常与版本管理工具(SCM)、构建工具结合使用。常用的版本控制工具有SVN、GIT，构建工具有Maven、Ant、Gradle。 二、CI&#x2F;CD是什么？​ CI(Continuous integration，中文意思是持续集成)是一种软件开发时间。持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。借用网络图片对CI加以理解。 CI ​ CD(Continuous Delivery， 中文意思持续交付)是在持续集成的基础上，将集成后的代码部署到更贴近真实运行环境(类生产环境)中。比如，我们完成单元测试后，可以把代码部署到连接数据库的Staging环境中更多的测试。如果代码没有问题，可以继续手动部署到生产环境。下图反应的是CI&#x2F;CD 的大概工作模式。 CI&#x2F;CD 三、使用Jenkins进行PHP代码(单元)测试、打包。​ Jenkins是一个强大的CI工具，虽然本身使用Java开发，但也能用来做其他语言开发的项目CI。下面讲解如何使用Jenkins创建一个构建任务。 登录Jenkins， 点击左侧的新建，创建新的构建任务。 跳转到如下界面。任务名称可以自行设定，但需要全局唯一。输入名称后选择构建一个自由风格的软件项目(其他选项不作介绍)。并点击下方的确定按钮即创建了一个构建任务。之后会自动跳转到该job的配置页面。 新建自由风格的软件项目 下图是构建任务设置界面，可以看到上方的几个选项**”General”, “源码管理”， “构建触发器”，”构建环境”， “构建”， “构建后操作”**。下面逐一介绍。 1.GeneralGeneral是构建任务的一些基本配置。名称，描述之类的。 General 项目名称: 是刚才创建构建任务步骤设置的，当然在这里也可以更改。 描述: 对构建任务的描述。 丢弃旧的构建： 服务器资源是有限的，有时候保存了太多的历史构建，会导致Jenkins速度变慢，并且服务器硬盘资源也会被占满。当然下方的”保持构建天数” 和 保持构建的最大个数是可以自定义的，需要根据实际情况确定一个合理的值。 其他几个选项在这里不做介绍，有兴趣的可以查看Jenkins”帮助信息”， 会有一个大概的介绍。不过这些”帮助信息”都是英文的。 点击右方的这些”问号”查看”帮助信息” 2.源码管理源码管理就是配置你代码的存放位置。 源码管理 Git: 支持主流的github 和gitlab代码仓库。因我们的研发团队使用的是gitlab，所以下面我只会对该项进行介绍。 Repository URL：仓库地址 Credentials：凭证。可以使用HTTP方式的用户名密码，也可以是RSA文件。 但要通过后面的”ADD”按钮添加凭证。 Branches to build：构建的分支。*&#x2F;master表示master分支，也可以设置为其他分支。 源码浏览器：你所使用的代码仓库管理工具，如github, gitlab. URL：填入上方的仓库地址即可。 Version: 8.7 这个是我们gitlab服务器的版本。 Subversion：就是SVN，这里不作介绍。 **** 3.构建触发器构建触发器，顾名思义，就是构建任务的触发器。 触发远程构建(例如，使用脚本): 该选项会提供一个接口，可以用来在代码层面触发构建。这里不做介绍，后期可能会用到。 Build after other projects are built： 该选项意思是”在其他projects构建后构建”。这里不作介绍，后期可能会用到该选项。 Build periodically： 周期性的构建。很好理解，就是每隔一段时间进行构建。日程表类似 linux crontab书写格式。如下图的设置，表示每隔30分钟进行一次构建。 周期构建 Build when a change is pushed to GitLab：当有更改push到gitlab代码仓库，即触发构建。后面会有一个触发构建的地址，一般被称为webhooks。需要将这个地址配置到gitlab中，webhooks如何配置后面介绍。这个是常用的构建触发器。 Poll SCM：该选项是配合上面这个选项使用的。当代码仓库发生改动，jenkins并不知道。需要配置这个选项，周期性的去检查代码仓库是否发生改动。 十分钟检查一次 4.构建环境构建环境就是构建之前的一些准备工作，如指定构建工具(在这里我使用ant)。 构建环境中的构建工具 With Ant：选择这个工具，并指定ant版本和jdk版本。这两个工具的版本我都事先在服务器上安装，并且在jenkins全局工具中配置好了。 其他选项不作介绍，同样可以查看”帮助信息” 获得使用帮助。 5.构建​ 选择下方的增加构建步骤。 增加构建步骤 可以选择的项很多。这里就介绍”Invoke Ant” 和”Execute shell”. Eexcute shell： 执行shell命令，该工具是针对linux环境的，windows环境也有对应的工 具”Execute Windows batch command”。 在构建之前，可能我们需要执行一些命令，比如压缩包的解压之类的。为了演示，我就简单的执行 “echo $RANDOM” 这样的linux shell下生产随机数命令。 Invoke Ant：Ant是一款java项目构建工具，当然也能用来构建php。 Ant Version： 选择Ant版本。这个ant版本是安装在jenkins服务器上的版本，并且需要在jenkins”系统工具”中设置好。 Targets：要执行的操作，一行一个操作任务。以上图为例，build是构建，tar是打包。 Build File: 是Ant构建的配置文件，如果不指定，则是在项目路径下的workspace目录中的build.xml。build.xml文件具体怎么配置，后面再细讲。 properties: 设定一些变量，这些变量可以在build.xml 中被引用。 Send files or execute commands over SSH：发送文件到远程主机或执行命令(脚本) Name: SSH Server的名称。SSH Server可以在jenkins-系统设置中配置。 source files: 需要发送给远程主机的源文件。 Remove prefix: 移除前面的路径。如果不设置这个参数，则远程主机会自动创建构建源 source files 包含的那个路径。 Remote directory: 远程主机目录。 Exec command：在远程主机上执行的命令，或者执行的脚本。 6.构建后操作​ 构建后操作，就是对project构建完成后的一些后续操作，比如生成相应的代码测试报告。 **** 邮件通知 Publish Clover PHP Coverage Report：发布代码覆盖率xml格式的文件报告。路径会在”build.xml”文件中定义 Publish HTML reports：发布代码覆盖率的HTML报告。 Report Crap: 发布crap报告。 E-mail Notification: 邮件通知，构建完成后发邮件到指定的邮箱。 以上配置完成后，点击保存。 7.其他相关配置 SSH Server配置 登录jenkins – 系统管理 – 系统设置 配置请看下图 SSH SERVER SSH Servers: 由于jenkins服务器公钥文件我已经配置好，所以之后新增SSH Servers 只需要配置这一项即可。 Name： 自定义，需要全局唯一。 HostName: 主机名，直接用ip地址即可。 Username: 新增Server的用户名，这里配置的是root。 Remote Directory: 远程目录。jenkins服务器发送文件给新增的server默认是在这个目录。 Ant 配置文件 “build.xml”接下来讲解Ant 构建配置文件”build.xml”。 之所以是build.xml 这是因为官方惯例。就好比任何编程语言的入门都会是打印”Hello world”. 你也可以用其他名称代替”build.xml” . 下面针对配置文件”build.xml” 关键配置进行说明。 project name就是项目名称，和jenkins所创建的对应。 target name&#x3D;”build” 就是构建的名称，和jenkins构建步骤 那里的targets对应。depends指明构建需要进行的一些操作。 property 用来设置变量。 fileset 这一行指明了一个文件夹，用include来指明需要包含的文件，exclude指明不包含的文件，”tar”即是打包这个文件夹中匹配到的文件。 下面的这些target都是一些实际的操作步骤，比如make_runtime这个”target” 就是创建了一些目录。phpcs就是利用PHP_CodeSniffer这个工具 对PHP代码规范与质量检查工具。 最后这个target “tar” 就是打包文件。因为上面的build 并没有包含这个target，所以默认情况下，执行build是不会打包文件的，所以在jenkins project配置界面，Ant构建那一步的targets，我们才会有”build” 和 “tar” 这两个targets。如果build.xml 中 “build”这个target depends中已经包含”tar” , 就不需要在jenkins中增加”tar”了。 其他一些target 都是利用一些工具对php代码的操作，比如phpunit是进行php单元测试。这一些方面我没有深入的研究，只是进行了一些简单的配置，毕竟不是这方面的专业人士。 配置 Gitlab webhooks在gitlab的project页面 打开settings，再打开 web hooks 。点击**”ADD WEB HOOK”** 添加webhook。把之前jenkins配置中的那个url 添加到这里，添加完成后，点击**”TEST HOOK”**进行测试，如果显示SUCCESS 则表示添加成功。 配置phpunit.xml phpunit.xml是phpunit这个工具用来单元测试所需要的配置文件。这个文件的名称同样也是可以自定义的，但是要在”build.xml”中配置好名字就行。默认情况下，用”phpunit.xml”, 则不需要在”build.xml”中配置文件名。 build.xml中phpunit配置 fileset dir 指定单元测试文件所在路径，include指定包含哪些文件，支持通配符匹配。当然也可以用exclude关键字指定不包含的文件。 四、进行jenkins project 构建第一次配置好jenkins project之后，会自动触发一次构建。此后，每当有commit 提交到master分支（前面设置的是master分支，也可以设置为其他分支），就会触发一次构建。当然也可以在project页面手动触发构建。点击左边的”立即构建” 手动触发构建。 手动触发构建 五、构建结果说明构建状态Successful蓝色：构建完成，并且被认为是稳定的。 Unstable黄色：构建完成，但被认为是不稳定的。 Failed红色：构建失败。 Disable灰色：构建已禁用 构建稳定性构建稳定性用天气表示：晴、晴转多云、多云、小雨、雷阵雨。天气越好表示构建越稳定，反之亦然。 构建历史界面 console output： 输出构建的日志信息 六、jenkins权限管理由于jenkins默认的权限管理体系不支持用户组或角色的配置，因此需要安装第三发插件来支持角色的配置，本文将使用Role Strategy Plugin。基于这个插件的权限管理设置请参考这篇文章:https://blog.csdn.net/russ44/article/details/52276222，这里不作详细介绍。 至此，就可以用jenkins周而复始的进行CI了，当然jenkins是一个强大的工具，功能绝不仅仅是以上这些，其他方面要是以后用到，我会更新到这篇文章中。有疑问欢迎在下方留言。 最后，放上一张Jenkins的思维导图","categories":[],"tags":[]},{"title":"","slug":"总结/9、Tomcat优化","date":"2024-12-14T22:02:47.161Z","updated":"2021-01-27T02:46:32.000Z","comments":true,"path":"2024/12/15/总结/9、Tomcat优化/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/9%E3%80%81Tomcat%E4%BC%98%E5%8C%96/","excerpt":"","text":"Tomcat是什么？Tomcat 服务器Apache软件基金会项目中的一个核心项目，是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。 1、Tomcat的缺省端口是多少，怎么修改 找到Tomcat目录下的conf文件夹 进入conf文件夹里面找到server.xml文件 打开server.xml文件 在server.xml文件里面找到下列信息 把Connector标签的8080端口改成你想要的端口 1234&lt;Service name=&quot;Catalina&quot;&gt;&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 2、如何避免用户在访问一个不存在的页面时，Tomcat将其详细版本信息返回给用户。答：解决办法就是将404状态码进行重定向，以便我们自定义返回的页面信息。 3、Tomcat优化答： 1、内存优化； 2、配置优化：Connector 优化、线程池优化 3、禁用DNS查询； 4、开启日志切割功能； 5、关闭404错误返回的版本信息； Tomcat的优化： 1）、内存优化12严重: Exception invoking periodic operation: java.lang.OutOfMemoryError: Java heap space 严重: Error processing request java.lang.OutOfMemoryError: GC overhead limit exceeded ​ 说明Tomcat已经无力支持访问处理，内部GC也已经“无能无力”。所以一般情况下我们需要重新配置Tomcat的相关内存大小。 在bin&#x2F;catalina.bat&#x2F;sh中，在catalina.sh中添加： 1JAVA_OPTS=&quot;-server -Xms1G -Xmx2G -Xss256K -Djava.awt.headless=true -Dfile.encoding=utf-8 -XX:MaxPermSize=256m -XX:PermSize=128M -XX:MaxPermSize=256M&quot; 其中： 1234567891011• -server：启用jdk的server版本。• -Xms：虚拟机初始化时的最小堆内存。• -Xmx：虚拟机可使用的最大堆内存。 #-Xms与-Xmx设成一样的值，避免JVM因为频繁的GC导致性能大起大落• -XX:PermSize：设置非堆内存初始值,默认是物理内存的1/64。• -XX:MaxNewSize：新生代占整个堆内存的最大值。• -XX:MaxPermSize：Perm（俗称方法区）占整个堆内存的最大值，也称内存最大永久保留区域。 ​ 验证 ​ 设置成功后我们可以利用JDK自带的工具进行验证，这些工具都在JAVA_HOME&#x2F;bin目录下： ​ 1）jps：用来显示本地的java进程，以及进程号，进程启动的路径等。 ​ 2）jmap：观察运行中的JVM 物理内存的占用情况，包括Heap size , Perm size 下载地址 等。 ​ 进入命令行模式后，进入JAVA_HOME&#x2F;bin目录下，然后输入jps命令： 1234jps#显示以下结果2340 Bootstrap6696 Jps ​ 其中 Bootstrap进程就是我们启动了的 Tomcat，其进程号为2340. ​ 然后我们利用 jmap工具查看其内存相关配置： 12345678jmap -heap 2340 #显示以下结果 Attaching to process ID 2340, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.65-b04 using thread-local object allocation. Parallel GC with 4 thread(s) 若内存不足，显示500错误，一般调整Xms和Xmx，可以将Xms和Xmx设置成一样 使用：set JAVA_OPTS&#x3D;-Xms512m-Xmx512m 2）Connector优化（运行模式）三种。修改它的运行模式需要在主配置文件中找到connector字段中的protocol进行修改 BIO：同步并阻塞 一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下，在Linux系统中默认使用这种方式。 配制项：protocol&#x3D;”HTTP&#x2F;1.1” NIO：同步非阻塞IO 利用Java的异步IO处理，可以通过少量的线程处理大量的请求，可以复用同一个线程处理多个connection(多路复用)。 Tomcat8在Linux系统中默认使用这种方式。 Tomcat7必须修改Connector配置来启动。 配制项：protocol&#x3D;”org.apache.coyote.http11.Http11NioProtocol” 备注：我们常用的Jetty，Mina，ZooKeeper等都是基于java nio实现. APR：即Apache Portable Runtime，从操作系统层面解决io阻塞问题。AIO方式，异步非阻塞IO (Java NIO2又叫AIO) 主要与NIO的区别主要是操作系统的底层区别.可以做个比喻:比作快递，NIO就是网购后要自己到官网查下快递是否已经到了(可能是多次)，然后自己去取快递；AIO就是快递员送货上门了(不用关注快递进度)。 配制项：protocol&#x3D;”org.apache.coyote.http11.Http11AprProtocol” 备注：需在本地服务器安装APR库。Tomcat7或Tomcat8在Win7或以上的系统中启动默认使用这种方式。Linux如果安装了apr和native，Tomcat直接启动就支持apr。 4、Tomcat工作模式？Tomcat是一个JSP&#x2F;Servlet容器。其作为Servlet容器，有三种工作模式：独立的Servlet容器、进程内的Servlet容器和进程外的Servlet容器。 进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类： Tomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache, IIS, Nginx等； Tomcat作为独立服务器：请求来自于web浏览器； 5、Tomcat有几种部署方式？在Tomcat中部署Web应用的方式主要有如下几种： 利用Tomcat的自动部署。 把web应用拷贝到webapps目录。Tomcat在启动时会加载目录下的应用，并将编译后的结果放入work目录下。 使用Manager App控制台部署。 在tomcat主页点击“Manager App” 进入应用管理控制台，可以指定一个web应用的路径或war文件。 修改conf&#x2F;server.xml文件部署。 修改conf&#x2F;server.xml文件，增加Context节点可以部署应用。 增加自定义的Web部署文件。 在conf&#x2F;Catalina&#x2F;localhost&#x2F; 路径下增加 xyz.xml文件，内容是Context节点，可以部署应用。 6、tomcat容器是如何创建servlet类实例？用到了什么原理？ 当容器启动时，会读取在webapps目录下所有的web应用中的web.xml文件，然后对 xml文件进行解析，并读取servlet注册信息。然后，将每个应用中注册的servlet类都进行加载，并通过 反射的方式实例化。（有时候也是在第一次请求时实例化） 在servlet注册时加上1如果为正数，则在一开始就实例化，如果不写或为负数，则第一次请求实例化。 7、Tomcat工作模式Tomcat作为servlet容器，有三种工作模式： 1、独立的servlet容器，servlet容器是web服务器的一部分； 2、进程内的servlet容器，servlet容器是作为web服务器的插件和java容器的实现，web服务器插件在内部地址空间打开一个jvm使得java容器在内部得以运行。反应速度快但伸缩性不足； 3、进程外的servlet容器，servlet容器运行于web服务器之外的地址空间，并作为web服务器的插件和java容器实现的结合。反应时间不如进程内但伸缩性和稳定性比进程内优； 进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类： Tomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache, IIS, Nginx等； Tomcat作为独立服务器：请求来自于web浏览器； 面试时问到Tomcat相关问题的几率并不高，正式因为如此，很多人忽略了对Tomcat相关技能的掌握，下面这一篇文章整理了Tomcat相关的系统架构，介绍了Server、Service、Connector、Container之间的关系，各个模块的功能，可以说把这几个掌握住了，Tomcat相关的面试题你就不会有任何问题了！另外，在面试的时候你还要有意识无意识的往Tomcat这个地方引，就比如说常见的Spring MVC的执行流程，一个URL的完整调用链路，这些相关的题目你是可以往Tomcat处理请求的这个过程去说的！掌握了Tomcat这些技能，面试官一定会佩服你的！ 学了本章之后你应该明白的是： Server、Service、Connector、Container四大组件之间的关系和联系，以及他们的主要功能点； Tomcat执行的整体架构，请求是如何被一步步处理的； Engine、Host、Context、Wrapper相关的概念关系； Container是如何处理请求的； Tomcat用到的相关设计模式； 8、Tomcat顶层架构俗话说，站在巨人的肩膀上看世界，一般学习的时候也是先总览一下整体，然后逐个部分个个击破，最后形成思路，了解具体细节，Tomcat的结构很复杂，但是 Tomcat 非常的模块化，找到了 Tomcat 最核心的模块，问题才可以游刃而解，了解了 Tomcat 的整体架构对以后深入了解 Tomcat 来说至关重要！ 先上一张Tomcat的顶层结构图（图A），如下： Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，即可以包含多个Service，用于具体提供服务。 Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下： Connector用于处理连接相关的事情，并提供Socket与Request请求和Response响应相关的转化; Container用于封装和管理Servlet，以及具体处理Request请求； 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接，示意图如下（Engine、Host、Context下面会说到）： 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。 另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，下图是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0） 详细的配置文件内容可以到Tomcat官网查看：Tomcat配置文件 上边的配置文件，还可以通过下边的一张结构图更清楚的理解： Server标签设置的端口号为8005，shutdown&#x3D;”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置，一个Service有多个Connector，Service左边的内容都属于Container的，Service下边是Connector。 9、Tomcat顶层架构小结 Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container； Server掌管着整个Tomcat的生死大权； Service 是对外提供服务的； Connector用于接受请求并将请求封装成Request和Response来具体处理； Container用于封装和管理Servlet，以及具体处理request请求； 知道了整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。 10、Connector和Container的微妙关系由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！ Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP&#x2F;IP协议和HTTP协议！ Tomcat既然需要处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！ Connector架构分析 Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。 因此，我们可以把Connector分为四个方面进行理解： Connector如何接受请求的？ 如何将请求封装成Request和Response的？ 封装完之后的Request和Response如何交给Container进行处理的？ Container处理完之后如何交给Connector并返回给客户端的？ 首先看一下Connector的结构图（图B），如下所示： Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。 Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。 Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP&#x2F;IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。 Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，Handler用于处理接收到的Socket，在内部调用Processor进行处理。 至此，我们应该很轻松的回答1，2，3的问题了，但是4还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？ 11、Container架构分析Container用于封装和管理Servlet，以及具体处理Request请求，在Container内部包含了4个子容器，结构图如下（图C）： 4个子容器的作用分别是： Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine； Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点； Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件； Wrapper：每一Wrapper封装着一个Servlet； 下面找一个Tomcat的文件目录对照一下，如下图所示： Context和Host的区别是Context表示一个应用，我们的Tomcat中默认的配置下webapps下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个webapps就是一个Host站点。 我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：www.baidu.com，如果是Host（webapps）下的其他应用，则可以使用www.baidu.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。 看到这里我们知道Container是什么，但是还是不知道Container是如何进行请求处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！ 12、Container如何处理请求的Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意） Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的结果返回，再让下一个处理者继续处理。 但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点： 每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的； 在上层容器的管道的BaseValve中会调用下层容器的管道。 我们知道Container包含四个子容器，而这四个子容器对应的BaseValve分别在：StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve。 Pipeline的处理流程图如下（图D）： Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）； 在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。 当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！ 当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。","categories":[],"tags":[]},{"title":"","slug":"总结/7、Radis和Mongodb","date":"2024-12-14T22:02:47.155Z","updated":"2021-01-27T03:32:38.000Z","comments":true,"path":"2024/12/15/总结/7、Radis和Mongodb/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/7%E3%80%81Radis%E5%92%8CMongodb/","excerpt":"","text":"[TOC] https://thinkwon.blog.csdn.net/article/details/103522351 一、Redis的作用（为什么要用Redis）防止数据库被击穿；可以减轻数据库压力，查询内存比查询数据库效率高。 主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 二、Redis的优势优点： ●Redis基于内存运行并支持持久化（内存运行：处理高热数据优秀，不占用多的进程） ●采用key-value（键值对）的存储形式 ●优点 具有极高的数据读写速度（键值对–提取数据时可以直接提取，无需进行数据类型转换（转换需要消耗资源）） 支持丰富的数据类型 支持数据的持久化（可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用） 原子性 支持数据备份，即master-slave模式的数据备份 缺点 数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 三、持久化为什么需要持久化？ 因为Redis是内存数据库，它将自己的数据存储在内存里面，一旦Redis服务器进程退出或者运行Redis服务器的计算机停机，Redis服务器中的数据就会丢失。 为了避免数据丢失，所以Redis提供了持久化机制，将存储在内存中的数据保存到磁盘中，用于在Redis服务器进程退出或者运行Redis服务器的计算机停机导致数据丢失时，快速的恢复之前Redis存储在内存中的数据。 redis 的持久化机制是什么？各自的优缺点？ Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制 RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储。 1）RDB 优点 RDB 的内容为二进制的数据，占用内存更小，更紧凑，更适合做为备份文件； RDB 对灾难恢复非常有用，它是一个紧凑的文件，可以更快的传输到远程服务器进行 Redis 服务恢复； RDB 可以更大程度的提高 Redis 的运行速度，因为每次持久化时 Redis 主进程都会 fork() 一个子进程，进行数据持久化到磁盘，Redis 主进程并不会执行磁盘 I&#x2F;O 等操作； 与 AOF 格式的文件相比，RDB 文件可以更快的重启。 2）RDB 缺点 因为 RDB 只能保存某个时间间隔的数据，如果中途 Redis 服务被意外终止了，则会丢失一段时间内的 Redis 数据； RDB 需要经常 fork() 才能使用子进程将其持久化在磁盘上。如果数据集很大，fork() 可能很耗时，并且如果数据集很大且 CPU 性能不佳，则可能导致 Redis 停止为客户端服务几毫秒甚至一秒钟。 AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库数据的。 1）AOF优点 数据的完整性和一致性更高 2）AOF缺点 因为AOF记录的内容多，文件会越来越大，数据恢复也会越来越慢。 RDB持久化、AOF持久化的区别 实现方式 RDB持久化是通过将某个时间点Redis服务器存储的数据保存到RDB文件中来实现持久化的。 AOF持久化是通过将Redis服务器执行的所有写命令保存到AOF文件中来实现持久化的。 文件体积 由上述实现方式可知，RDB持久化记录的是结果，AOF持久化记录的是过程，所以AOF持久化生成的AOF文件会有体积越来越大的问题，Redis提供了AOF重写功能来减小AOF文件体积。 安全性 AOF持久化的安全性要比RDB持久化的安全性高，即如果发生机器故障，AOF持久化要比RDB持久化丢失的数据要少。 因为RDB持久化会丢失上次RDB持久化后写入的数据，而AOF持久化最多丢失1s之内写入的数据（使用默认everysec配置的话）。 优先级 由于上述的安全性问题，如果Redis服务器开启了AOF持久化功能，Redis服务器在启动时会使用AOF文件来还原数据，如果Redis服务器没有开启AOF持久化功能，Redis服务器在启动时会使用RDB文件来还原数据，所以AOF文件的优先级比RDB文件的优先级高。 RDB占用的内存大，因为是快照形式，在快照之后的无法备份 四、内存相关MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。 Redis的内存淘汰策略有哪些Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。 全局的键空间选择性移除 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的） allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。 设置过期时间的键空间选择性移除 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。 总结 Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。 Redis主要消耗什么物理资源？内存。 Redis的内存用完了会发生什么？如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。 Redis如何做内存优化？可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面 123456789101112131415[root@localhost ~]# redis-cli127.0.0.1:6379&gt; info memory #查看内存使用总量和内存碎片率# Memoryused_memory:11315280 #内存使用总量used_memory_human:10.79M...省略内容mem_fragmentation_ratio:1.76 #内存碎片率mem_fragmentation_bytes:8566768 ●跟踪内存碎片率对理解redis实例的资源性能是非常重要的 内存碎片率稍大于1是合理的，这个值表示内存碎片率比较低 内存碎片率超过1.5,说明redis消耗了实际需要物理内存的150%，其中50%是内存碎片率 内存碎片率低于1的，说明Redis内存分配超出了物理内存,操作系统正在进行内存交换 （内存碎片率稍大于1是最佳，低于1访问速度会慢，高于1.5说明碎片太多） 内存使用率超过可用最大内存，redis响应速度会变慢，该怎么办？原因：操作系统将开始进行内存与swap空间交换 解决方案：设置内存淘汰策略 五、Redis优化键值对存储 1、避免使用keyskeys *, 这个命令是阻塞的，即操作执行期间，其它任何命令在你的实例中都无法执行。可以去使用SCAN,来代替。 keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长 2、设置 key 有效期我们应该尽可能的利用key有效期。比如一些临时数据（短信校验码），过了有效期Redis就会自动为你清除！ 3、选择回收策略(maxmemory-policy)当 Redis 的实例空间被填满了之后，将会尝试回收一部分key。根据你的使用方式，强烈建议使用 volatile-lru（默认）策略——前提是你对key已经设置了超时。 ​ 4、尽可能地使用slot哈希存储，哈希槽（以集群方式部署）。5、当业务场景不需要数据持久化时，关闭所有的持久化方式可以获得最佳的性能。6、限制redis的内存大小数据量不可预估，并且内存也有限的话，尽量限制下redis使用的内存大小，这样可以避免redis使用swap分区或者出现OOM错误。（使用swap分区，性能较低，如果限制了内存，当到达指定内存之后就不能添加数据了，否则会报OOM错误。可以设置maxmemory-policy，内存不足时删除数据。） 7、Redis常见性能问题和解决方案？ Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。 如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。 尽量避免在压力较大的主库上增加从库 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master&lt;–Slave1&lt;–Slave2&lt;–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。 六、如何回收key？当达到设置的最大阈值时，需选择一种key的回收策略 默认情况下回收策略是noeviction（禁止收回） &#x2F;etc&#x2F;redis&#x2F;6379.conf配置文件中修改maxmemory-policy属性值 volatile-lru:使用LRU算法从已设置过期时间的数据集合中淘汰数据（建议使用） volatile-ttl:从已设置过期时间的数据集合中挑选即将过期的数据淘汰（建议使用） 去掉#注释后，配置文件一定要顶格写 七、主从、哨兵、集群模式之间的区别：主从：Redis主从复制前言 通过持久化功能，Redis保证了即使在服务器重启的情况下也不会丢失（或少量丢失）数据，但是由于数据是存储在一台服务器上的，如果这台服务器出现故障，比如硬盘坏了，也会导致数据丢失。 为了避免单点故障，我们需要将数据复制多份部署在多台不同的服务器上，即使有一台服务器出现故障其他服务器依然可以继续提供服务。 这就要求当一台服务器上的数据更新后，自动将更新的数据同步到其他服务器上，这时候就用到了Redis的主从复制。 Redis提供了复制（replication）功能来自动实现多台redis服务器的数据同步（每天19点 新闻联播，基本从cctv1-8,各大卫视都会播放） 我们可以通过部署多台redis，并在配置文件中指定这几台redis之间的主从关系，主负责写入数据，同时把写入的数据实时同步到从机器，这种模式叫做主从复制，即master&#x2F;slave，并且redis默认master用于写，slave用于读，向slave写数据会导致错误。 哨兵：当主数据库遇到异常中断服务后，开发者可以通过手动的方式选择一个从数据库来升格为主数据库，以使得系统能够继续提供服务。然而整个过程相对麻烦且需要人工介入，难以实现自动化。 为此，Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能。 哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库。 顾名思义，哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个。 （1）监控主数据库和从数据库是否正常运行。 （2）主数据库出现故障时自动将从数据库转换为主数据库。 集群：即使使用哨兵，redis每个实例也是全量存储，每个redis存储的内容都是完整的数据，浪费内存且有木桶效应。为了最大化利用内存，可以采用集群，就是分布式存储。即每台redis存储不同的内容，共有16384个slot。每个redis分得一些slot，hash_slot &#x3D; crc16(key) mod 16384 找到对应slot，键是可用键，如果有{}则取{}内的作为可用键，否则整个键是可用键 集群至少需要3主3从，且每个实例使用不同的配置文件，主从不用配置，集群会自己选。 八、为了达到redis的高可用，有两种部署方式：哨兵模式；集群模式 哨兵机制是redis2.8开始支持。集群模式是redis3.0开始支持。 九、哨兵机制存在的意义： 为了实现redis故障转移的自动化。自动发现，自动转移。不需要人工参与 1、Sentinel的作用： 1)、Master 状态监测 2)、如果Master 异常，则会进行Master-slave 转换，将其中一个Slave作为Master，将之前的Master作为Slave 3)、Master-Slave切换后，master_redis.conf、slave_redis.conf和sentinel.conf的内容都会发生改变，即master_redis.conf中会多一行slaveof的配置，sentinel.conf的监控目标会随之调换 2、Sentinel的工作方式: 1)：每个Sentinel以每秒钟一次的频率向它所知的Master，Slave以及其他 Sentinel 实例发送一个 PING 命令 2)：如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel 标记为主观下线 3)：如果一个Master被标记为主观下线，则正在监视这个Master的所有 Sentinel 要以每秒一次的频率确认Master的确进入了主观下线状态。 4)：当有足够数量的 Sentinel（大于等于配置文件指定的值）在指定的时间范围内确认Master的确进入了主观下线状态， 则Master会被标记为客观下线 5)：若没有足够数量的 Sentinel 同意 Master 已经下线， Master 的客观下线状态就会被移除。 若 Master 重新向 Sentinel 的 PING 命令返回有效回复， Master 的主观下线状态就会被移除。 3、故障转移哨兵leader选举：如果主节点被判定为客观下线之后，就要选取一个哨兵节点来完成后面的故障转移工作，选举出一个leader的流程如下: 主节点客观下线 –》选一个哨兵节点完成故障转移 –》票数大于等于哨兵数&#x2F;2 +1，则是master 4、生产环境中部署技巧1）Sentinel 节点不应该部署在一台物理“机器”上。这里特意强调物理机是因为一台物理机做成了若干虚拟机或者现今比较流行的容器，它们虽然有不同的 IP 地址，但实际上它们都是同一台物理机，同一台物理机意味着如果这台机器有什么硬件故障，所有的虚拟机都会受到影响，为了实现 Sentinel 节点集合真正的高可用，请勿将 Sentinel 节点部署在同一台物理机器上。 2）部署至少三个且奇数个的 Sentinel节点。3个以上是通过增加 Sentinel 节点的个数提高对于故障判定的准确性，因为领导者选举需要至少一半加1个节点，奇数个节点可以在满足该条件的基础上节省一个节点。 5、哨兵重要配置项：#此项只有两个slave节点需要 slaveof 14.0.0.10 6379 #这里配置写上主服务器IP、端口、2个sentinel选举成功后才有效 sentinel monitor mymaster 14.0.0.10 6379 2 #主服务器 redis密码 sentinel auth-pass mymaster 123456 十、Redis5.0.7版本集群模式（redis-cli可以直接使用）1、redis群集 –去中心化模式Redis-Cluster数据分片 ●Redis集群没有使用一致性hash，而是引入了哈希槽概念 ●Redis集群有16384个哈希槽 ●每个key通过CRC16校验后对16384取余来决定放置槽 ●集群的每个节点负责一部分哈希槽 2、redis集群模式怎么搭建？1、6个节点安装redis 2、配置redis集群 &#x2F;etc&#x2F;redis&#x2F;6379.conf cluster-enabled yes #开启群集功能 appendonly yes #开启aof持久化 redis5 和redis3和4的版本区别： 如果您使用的是Redis 5，这很容易完成，这是因为嵌入程序中的Redis Cluster命令行实用程序为我们提供了帮助，该实用程序redis-cli可用于创建新集群，检查或重新分片现有集群等。 对于Redis版本3或4，有一个称为的旧工具redis-trib.rb，它非常相似。您可以src在Redis源代码分发的目录中找到它。您需要安装redis gem才能运行redis-trib。 [root@localhost ~]# gem install redis #Redis版本3或4需要安装 3.创建集群 六个实例分为三组，每组一主一从，–replicas 1表示每组一个从，下面交互的时候需要输入yes才可以创建。 [root@localhost profile.d]# redis-cli –cluster create 14.0.0.30:6379 14.0.0.10:6379 14.0.0.20:6379 14.0.0.40:6379 14.0.0.50:6379 14.0.0.60:6379 –cluster-replicas 1 命令是create，创建一个新集群。选项–cluster-replicas 1意味着我们希望为每个创建的主机都提供一个从机。 4、群集down掉的两种情况 （1）三个master服务器全部宕机 （2）master1宕机，对应的slave1也发生了宕机 十一、Redis和MongoDB比较：项目中用的是MongoDB，但是为什么用其实当时选型的时候也没有太多考虑，只是认为数据量比较大，所以采用MongoDB。 之前也用过redis，当时是用来存储一些热数据，量也不大，但是操作很频繁。现在项目中用的是MongoDB，目前是百万级的数据，将来会有千万级、亿级。 就Redis和MongoDB来说，大家一般称之为Redis缓存、MongoDB数据库。 性能：Redis优于MongoDB 一致性：MongoDB不支持事务,靠客户端保证；Redis支持事务,比较脆,仅能保证事务中的操作按顺序执行 应用场景：MongoDB海量数据的访问效率提升；优于Redis较小数据量的性能和运算 十二、Redis五种数据类型1、stringstring是redis最基本的类型，与Memcached一模一样的类型，一个key对应一个value。 string类型是二进制安全的。redis的string 可以包含任何数据。比如jpg图片或者序列化的对象。string 类型是Redis最基本的数据类型，string 类型的值最大能存储512MB。 2、hashRedis hash是一个键值(key&#x3D;&gt;value)对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。每个hash可以存储2的32次方-1键值对。作用场景:健值对集合,即编程语言中的Map类型适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值 3、listRedis列表是简单的字符串列表，按照插入顺序排序，可以重复。可以添加一个元素到 表的头部（左边）或者尾部（右边） 列表最多可存储2^32 -1 个元素。作用场景:1、增删快,提供了操作某一段元素的API2、最新消息排行等功能(比如朋友圈的时间线)3、消息队列一个key写入多个value时，遵循着先入后出，后入先出的堆栈规则 存入的value可以重 4、setRedis的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是0(1)作用场景:1、比如QQ的共同好友2、利用唯一性,统计访问网站的所有独立ip3、好友推荐时,根据tag求交集,大于某个阈值就可以推荐集合存储，不论有序还是无序都不能存储重复的值 5、zsetRedis zset和set一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。作用场景:将Set中的元素增加一个权重参数score,元素按score有序排列数据插入集合时,已经进行天然排序 1、排行榜 2、带权重的消息队列 十三、缓存异常缓存雪崩缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。 一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。 给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。 缓存穿透缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决方案 接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;&#x3D;0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击 采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力 附加 对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。Bitmap： 典型的就是哈希表缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。 布隆过滤器（推荐） 就是引入了k(k&gt;1)k(k&gt;1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想。Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。 缓存击穿缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。 解决方案 设置热点数据永远不过期。 加互斥锁，互斥锁 缓存预热缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案 直接写个缓存刷新页面，上线时手工操作一下； 数据量不大，可以在项目启动的时候自动进行加载； 定时刷新缓存； 缓存降级当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。 缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。 在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案： 一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级； 警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警； 错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级； 严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。 服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。 热点数据和冷数据热点数据，缓存才有价值 对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存 对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。 数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。 那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。 缓存热点key缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方案 对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询 十四、事务什么是事务？事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 Redis事务的概念Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 Redis事务的三个阶段 事务开始 MULTI 命令入队 事务执行 EXEC 事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队 Redis事务相关命令Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的 Redis会将一个事务中的所有命令序列化，然后按顺序执行。 redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。 如果在一个事务中的命令出现错误，那么所有的命令都不会执行； 如果在一个事务中出现运行错误，那么正确的命令会被执行。 WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。 UNWATCH命令可以取消watch对所有key的监控。 事务管理（ACID）概述原子性（Atomicity）原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性（Consistency）事务前后数据的完整性必须保持一致。 隔离性（Isolation）多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性（Durability）持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响 Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。 Redis事务支持隔离性吗Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 Redis事务保证原子性吗，支持回滚吗Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis事务其他实现 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐 十五、Redis与Memcached的区别两者都是非关系型内存键值数据库，现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！Redis 与 Memcached 主要有以下不同： 对比参数 Redis Memcached 类型 1. 支持内存 2. 非关系型数据库 1. 支持内存 2. 键值对形式 3. 缓存形式 数据存储类型 1. String 2. List 3. Set 4. Hash 5. Sort Set 【俗称ZSet】 1. 文本型 2. 二进制类型 查询【操作】类型 1. 批量操作 2. 事务支持 3. 每个类型不同的CRUD 1.常用的CRUD 2. 少量的其他命令 附加功能 1. 发布&#x2F;订阅模式 2. 主从分区 3. 序列化支持 4. 脚本支持【Lua脚本】 1. 多线程服务支持 网络IO模型 1. 单线程的多路 IO 复用模型 1. 多线程，非阻塞IO模式 事件库 自封转简易事件库AeEvent 贵族血统的LibEvent事件库 持久化支持 1. RDB 2. AOF 不支持 集群模式 原生支持 cluster 模式，可以实现主从复制，读写分离 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据 内存管理机制 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘 Memcached 的数据则会一直在内存中，Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题。但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。 适用场景 复杂数据结构，有持久化，高可用需求，value存储内容较大 纯key-value，数据量非常大，并发量非常大的业务 (1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 (2) redis的速度比memcached快很多 (3) redis可以持久化其数据 十六、Redis的应用场景计数器可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。 缓存将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 会话缓存可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 全页缓存（FPC）除基本的会话token之外，Redis还提供很简便的FPC平台。以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 查找表例如 DNS 记录就很适合使用 Redis 进行存储。查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。 消息队列(发布&#x2F;订阅功能)List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息。不过最好使用 Kafka、RabbitMQ 等消息中间件。 分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。 其它Set 可以实现交集、并集等操作，从而实现共同好友等功能。ZSet 可以实现有序性操作，从而实现排行榜等功能。 MongodbMongodb熟悉吗，一般部署几台？ 部署过，没有深入研究过，一般mongodb部署主从、或者mongodb分片集群；建议3台或5台服务器来部署。MongoDB分片的基本思想就是将集合切分成小块。这些块分散到若干片里面，每个片只负责总数据的一部分。 对于客户端来说，无需知道数据被拆分了，也无需知道服务端哪个分片对应哪些数据。数据在分片之前需要运行一个路由进程，进程名为mongos。这个路由器知道所有数据的存放位置，知道数据和片的对应关系。对客户端来说，它仅知道连接了一个普通的mongod，在请求数据的过程中，通过路由器上的数据和片的对应关系，路由到目标数据所在的片上，如果请求有了回应，路由器将其收集起来回送给客户端。","categories":[],"tags":[]},{"title":"","slug":"总结/6、MySQL数据库","date":"2024-12-14T22:02:47.152Z","updated":"2021-02-22T06:50:18.000Z","comments":true,"path":"2024/12/15/总结/6、MySQL数据库/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/6%E3%80%81MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"[TOC] https://blog.csdn.net/ThinkWon/article/details/104778621 一、数据库管理1）、SQL语言分类：DDL：（Data Definition Language，数据定义语言）：用来建立数据库、数据库对象和定义字段，如CERATE、ALTER、DROP； DML：（Data Manipulation Language，数据操纵语言）：用来插入、删除和修改数据库中的数据，如INSERT、UPDATE、DELETE。 DQL：（Data Query Language，数据查询语言）：用来查询数据库中的数据，如SELECT。 DCL：（Data Control Language，数据控制语言）：用来控制数据库组件的存取许可、存取权限等，如COMMIT、ROLLBACK、GRANT、REVOKE。 2）、创建数据库和表（DDL）：1、create database a1； 2、use a1； 3、create table a2（）； 4、create table 表名（字段01名称 字段01类型 字段01约束，字段02名称 字段02类型 字段02约束,…）存储引擎，字符集 字段01类型： int（4） 整型 代表0000-9999 double 浮点型 decimal（5,2）有效数字是5位，小数点后面保留2位 100.00；099.50 float 单精度浮点 4字节 char 字符 **char （10） ** 固定长度字符串，字符串要用单引号引起来 **varchar（50） ** 可变长度字符串 字段01约束： 非空约束： not null 主键约束： primary key（主键） 默认约束：假如没有填数据，默认预先设定的值填写 default ‘未知’ 自增特性： auto_increment（自动增长） 存储引擎：myisam innodb 字符集：UTF-8 3）、管理数据表中的数据：1、insertinsert into 表名 (字段1，字段2) values (字段1的值，字段2的值) 或insert into 表名 values (所有字段的值) 查看添加的信息：select * from 表名； 2、updateupdate 表名 set 字段名1&#x3D;值1,字段名2&#x3D;值2 where 条件表达式 3、delete（对数据操作用delete，对库和表用drop）格式：delete from 表名 where 条件表达式（不带where代表删除表中所有记录） 4）、数据库高级操作1、清空表delete from info； truncate table info； truncate清空表，表还在；drop是删除表中所有记录。 truncate和delete是两者的新值初始id不同。 2、临时表临时建立的表，用于保存一些临时数据，不会长期存在 create temporary table temp_info(…)engine&#x3D;innodb default charset&#x3D;utf8; innodb 支持事务；写在括号外面的是对整张表的设定。 show tables；看不到临时表 3、克隆表like方法：从info表完整复制结构生成test表 create table test like info; 导入数据：insert into test select * from info; 5）、数据库用户授权1、设置登录密码为abc123的lisi用户，可以从任意终端登录，对所有库和所有表有全部权限： create user ‘lisi‘@’%’ identified by ‘abc123’; grant all on . to ‘lisi‘@’%’ identified by ‘abc123’; 2、设置登录密码为abc123的tom用户，可以从本地终端登录，对mysql库中的user表有select权限 create user ‘tom‘@’localhost’ identified by ‘abc123’; grant select on mysql.user to ‘tom‘@’abc123’ identified by ‘abc123’; 3、查看当前用户的权限：show grants； ​ 查看当前系统中的用户：select user from mysql.user; ​ 查看从本地登录的tom用户的权限： ​ show grants for ‘tom‘@’localhost’; 4、撤销用户的权限： ​ revoke select on mysql.user from ‘tom‘@’localhost’; 二、索引1、索引的概念1、是一个排序的列表，存储着索引值和这个值所对应的物理地址； 2、无需对整个表进行扫描，通过物理地址就可以找到所需数据； 3、是表中一列或者若干列值排序的方法； 4、需要额外的磁盘空间； 补充： 索引需要的额外的磁盘空间伴随着表直接存在 阈值：300行以上的才建立索引，不然浪费磁盘空间 2、索引的作用1、数据库利用各种快速定位技术，能够大大加快查询速率； 2、当表很大或查询涉及到多个表时，可以成千上万倍地提高查询速度； 3、可以降低数据库的IO成本，并且还可以降低数据库的排序成本； 4、通过创建唯一性索引保证数据表数据的唯一性 IO：输入（写入、更改数据），输出（读取数据）； 5、可以加快表与表之间的连接； 6、在使用分组和排序时，可大大减少分组和排序时间 3、索引的分类1）、普通索引最基本的索引类型，没有唯一性之类的限制 1、创建表时创建 create table test(……index id_index(id)); 2、直接创建 create index name_index on test(name); 3、修改表结构方式创建 alter table test add index id_index(id); 2）、唯一性索引与“普通索引”基本相同 与普通索引的区别是索引列的所有值只能出现一次，即必须唯一 1、创建表时创建 create table test(……unique index id_index(id)); 2、直接创建 create unique index name_index on test(name); 3、修改表结构方式创建 alter table test add unique index id_index(id); 3）、主键索引是一种特殊的唯一索引，指定为“primary key” 一个表只能有一个主键，不允许有空值（非空且唯一） 1、创建表时创建 create table test(……primary key(id)); 2、修改表时创建 alter table test add primary key(id); 4）、组合索引（单列索引与多列索引）可以是单列上创建的索引，也可以是在多列上创建的索引 最左原则，从左往右依次执行 创建组合索引：create table test(……index ff(id,name,score)); 5）、全文索引1、创建表时创建全文索引 create table test(……fulltext (name)); 2、在已存在的表上创建全文索引 create fulltext index name_index on test(name); 3、通过SQL语句alter table创建全文索引 alter table test add fulltext index name_index(score); 4、创建索引的原则1、表的主键、外键必须有索引； 2、数据量超过300行的表应该有索引； 3、经常与其他表进行连接的表，在连接字段上应该建立索引； 4、唯一性太差的字段不适合建立索引； 5、更新太频繁地字段不适合创建索引； 6、经常出现在 Where子句中的字段，特别是大表的字段，应该建立索引； 7、索引应该建在选择性高的字段上； 8、索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引 5、查看索引的方法show index from table_name\\G；（\\G：竖着显示） 6、删除索引的方法1、drop index index_name on table_name; 2、alter table table_name drop index index_name; 三、事务1、概念1、事务是一种机制、一个操作序列，包含了一组数据库操作命令，并且把所有的命令作为一个整体一起向系统提交或撤销操作请求，即这一组数据库命令要么都执行，要么都不执行； 2、事务是一个不可分割的工作逻辑单元，在数据库系统上执行并发操作时，事务是最小的控制单元； 3、适用于多用户同时操作的数据库系统的场景，如银行、保险公司及证券交易系统等等； 4、通过事务的整体性以保证数据的一致性； 5、如果事务成功了一部分，一部分未成功，则执行回滚，回到事务的起点，重新开始操作 2、特点1）、原子性（Atomicity）事务是一个完整的操作，事务的各元素是不可分的 事务中的所有元素必须作为一个整体提交或回滚 如果事务中的任何元素失败，则整个事务将失败 2）、一致性（Consistency）当事务完成时，数据必须处于一致状态 在事务开始前，数据库中存储的数据处于一致状态 在正在进行的事务中，数据可能处于不一致的状态 当事务成功完成时，数据必须再回到已知的一致状态 3）、隔离性（Isolation）对数据进行修改的所有并发事务是彼此隔离的，表明事务必须是独立的，它不应以任何方式依赖于或影响其他事务 修改数据的事务可在另一个使用相同数据的事务开始之前访问这些数据，或者在另一个使用相同数据的事务结束之后访问这些数据 4）、持久性指不管系统是否发生故障，事务处理的结果都是永久的 一旦事务被提交，事务的效果会被永久地保留在数据库中 3、控制语句1、MySQL事务默认是自动提交的，当SQL语句提交时事务便自动提交； 2、事务控制语句 begin 事务的开始 commit 提交 rollback 回滚 savepoint 存档点名称 存档点 release savepoint 存档点名称 删除存档点 rollback to 存档点名称 回滚到某个存档点 set transaction 设置事务 set autocommit&#x3D;0 禁止自动提交 set autocommit&#x3D;1 开启自动提交（默认） 4、事务的操作创建的数据表存储引擎必须是innodb，才支持事务（5.7版本默认就是innodb） 具体操作： 1、begin; ​ insert into test values(1,’zhangsan’); ​ commit;（结束事务） 2、begin; ​ insert into test values(1,’zhangsan’); ​ savepoint a; ​ savepoint b; ​ rollback to b; ​ rollback to a ​ 注意：只能向前回滚，无法向后回滚 3、三种情况事务开始： begin； set autocommit&#x3D;0； start transaction 三种情况结束事务： commit； set autocommit&#x3D;1； rollback 四、操作引擎1、概念1、MySQL中的数据用各种不同的技术存储在文件中，每一种技术都使用不同的存储机制、索引技巧、锁定水平并最终提供不同的功能和能力，这些不同的技术以及配套的功能在 MySQL中称为存储引擎 2、存储引擎就是 MySQL将数据存储在文件系统中的存储方式或者存储格式 3、目前 MySQL常用的两种存储引擎 MyISAM InnoDB （innodb支持事务，myisam不支持事务） 4、MySQL存储引擎是 MySQL数据库服务器中的组件，负责为数据库执行实际的数据I&#x2F;O操作 使用特殊存储引擎的主要优点之一在于： 仅需提供特殊应用所需的特性 数据库中的系统开销较小 具有更有效和更高的数据库性能 5、MySQL系统中，存储引擎处于文件系统之上，在数据保存到数据文件之前会传输到存储引擎，之后按照各个存储引擎的存储格式进行存储 2、Myisam1）、特点：1、myisam不支持事务，也不支持外键 2、访问速度快 3、对事物完整性没有要求 4、myisam在磁盘上存储成三个文件 .frm文件存储表定义 数据文件的扩展名为.MYD(MYData) 索引文件的扩展名是.MYI(MYIndex) 5、表级锁定形式，数据在更新时锁定整个表（不允许两个人同时操作） 6、数据库在读写过程中相互阻塞 会在数据写入的过程阻塞用户数据的读取，也会在数据读取的过程中阻塞用户的数据写入 7、数据单独写入或读取，速度过程较快且占用资源相对少 8、myisam 静态表 动态表 （直接写入的，数据会随时变的） 压缩表 2）、适用生产场景1、公司业务不需要事务的支持 2、单方面读取或写入数据比较多的业务 3、myisam存储引擎数据读写都比较频繁场景不适合 4、使用读写并发访问相对较低的业务 5、数据修改相对较少的业务 6、对数据业务一致性要求不是非常高的业务 7、服务器硬件资源相对比较差 3、Innodb1）、特点：1、支持事务：支持4个事务隔离级别 2、行级（读写分离）锁定，但是全表扫描仍然会是表级锁定 3、读写阻塞与事务隔离级别相关 4、具有非常高效的缓存特性：能缓存索引，也能缓存数据 5、表与主键以簇的方式存储 6、支持外键约束，5.5以前不支持全文索引，5.5版本以后支持全文索引 7、对硬件资源要求还是比较高的场合 2）、适用生产场景1、业务需要事务的支持 2、行级锁定对高并发有很好的适应能力，但需确保查询是通过索引来完成 3、业务数据更新较为频繁的场景，如：论坛、微博等 4、业务数据一致性要求较高，如：银行业务 5、硬件设备内存较大（因为事务都先放内存），利用innodb较好的缓存能力来提高内存利用率，减少磁盘IO的压力 4、修改存储引擎1、alter table 修改 alter table table_name engine&#x3D;引擎； 2、修改my.cnf，指定默认存储引擎并重启服务 在[mysqld]下面添加default-storage-engine&#x3D;InnoDB 3、create table创建表时指定存储引擎 create table 表名（字段）engine&#x3D;引擎 4、Mysql_convert_table_format转化存储引擎 Mysql_convert_table_format –user&#x3D;root –password&#x3D;密码 –sock&#x3D;&#x2F;tmp&#x2F;mysql.sock-engine&#x3D;引擎 库名 表名 五、备份与恢复1、备份的分类1）、从物理和逻辑的角度1、物理备份： 对数据库操作系统的物理文件（如数据文件、日志文件等）的备份 冷备份（脱机备份）：是在关闭数据库的时候进行的 热备份（联机备份）：数据库处于运行状态，依赖于数据库的日志文件 温备份：数据库锁定表格（不可写入但可读）的状态下进行备份操作 2、逻辑备份：对数据库逻辑组件（如：表等数据库对象）的备份 2）、从数据库的备份策略角度1、完全备份：每次对数据库进行完整的备份 2、差异备份：备份自从上次完全备份之后被修改过的文件 3、增量备份：只有在上次完全备份或者增量备份后被修改的文件才会被备份 2、常见的备份方法1）、物理冷备备份时数据库处于关闭状态，直接打包数据库文件 备份速度快，恢复时也是最简单的 2）、专业备份工具mysqldump或mysqlhotcopymysqldump常用的逻辑备份工具 mysqlhotcopy仅拥有备份MyISAM和ARCHIVE表 3）、启用二进制日志进行增量备份进行增量备份，需要刷新二进制日志 4）、第三方工具备份免费的MySQL热备份软件Percona XtraBackup 3、完全备份1）、简介1、是对整个数据库、数据库结构和文件结构的备份 2、保存的是备份完成时刻的数据库 3、是差异备份与增量备份的基础 4、每次对数据进行完整的备份，完全备份是增量备份的基础，完全备份保存的是备份完成时刻的数据库 2）、优点备份与恢复操作简单方便 3）、缺点数据存在大量的重复 占用大量的备份空间 备份与恢复时间长 4）、分类1、物理冷备份与恢复关闭MySQL数据库 使用tar命令直接打包数据库文件夹（&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;data） 直接替换现有MySQL目录即可 2)、mysqldump备份与恢复MySQL自带的备份工具，可方便实现对MySQL的备份 可以将指定的库、表导出为SQL脚本（.sql结尾） 使用命令MySQL导入备份的数据 操作步骤： 123456789mysqldump -u root -p --all-databses &gt; all-data-$(date +%F).sql ###备份所有数据库mysqldump -u root -p -databases auth mysql &gt; auth-mysql.sql ###备份auth和mysql库mysqldump -u root -p auth &gt; auth-$(data +%F).sql ###备份auth数据库mysqldump -u root -p mysql user &gt; mysql-user-$(date +%F).sql ###备份mysql的user表mysqldump -u root -p -d mysql user &gt; /tmp/desc-mysql-user.sql ###备份mysql库user表的结构 方法一： 1234567[root@server1 ~]# mysqldump -u root -p test &gt; test-$(date +%F).sqlmysql&gt; drop database test;mysql&gt; create database test2; ###建立空库[root@server1 ~]# mysql -u root -p test2 &lt; test-2020-10-23.sql 方法二： 123456789[root@server1 ~]# mysqldump -u root -p test &gt; test-$(date +%F).sqlmysql&gt; drop database test;mysql&gt; create database test2;mysql&gt; use test2;mysql&gt; source /root/test-2020-10-24.sql; 4、增量备份1、使用mysqldump进行完全备份存在的问题 备份数据中有重复数据 备份时间与恢复时间过长 2、MySQL增量备份是自上一次备份后增加&#x2F;变化的文件或者内容 3、特点 没有重复数据，备份量不大，时间短 恢复需要上次完全备份及完全备份之后所有的增量备份才能恢复，而且要对所有增量备份进行逐个反推恢复 4、MySQL没有提供直接的增量备份方法 可通过MySQL提供的二进制日志间接实现增量备份 5、MySQL二进制日志对备份的意义 二进制日志保存了所有更新或者可能更新数据库的操作 二进制日志在启动MySQL服务器后开始记录，并在文件达到max_binlog_size所设置的大小或者接收到flush logs命令后重新创建新的日志文件 只需定时执行flush logs方法重新创建新的日志，生成二进制文件序列，并及时把这些日志保存到安全的地方就完成了一个时间段的增量备份 5、增量恢复1）、一般恢复将所有备份的二进制日志内容全部恢复 2）、断点恢复1、基于位置恢复数据库在某一时间点可能既有错误的操作也有正确的操作 可以基于精准的位置跳过错误的操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@server1 ~]# mkdir -p /opt/bak_sql[root@server1 ~]# mysqldump -uroot -p test2 &gt; /opt/bak_sql/test2-$(date +%F).sql; ###完整备份[root@server1 ~]# vi /etc/my.cnf[mysqld]log_bin=/usr/local/mysql/data/mysql_bin ###开启增量备份[root@server1 ~]# systemctl restart mysqld[root@server1 ~]# mysqladmin -uroot -p flush-logs ###将二进制日志更新，产生新的日志文件[root@server1 ~]# cd /usr/local/mysql/data/[root@server1 data]# ll ###查询增量备份结果[root@server1 ~]# mysqlbinlog --no-defaults --base64-output=decode-rows -v /usr/local/mysql/data/mysql_bin.000002 ###查询该二进制日志内容是否正确[root@server1 ~]# mysqladmin -u root -p flush-logs; ###将二进制日志更新，产生新的日志文件[root@server1 ~]# cd /usr/local/mysql/data/[root@server1 data]# ll ###查询增量备份结果[root@server1 ~]# mysqlbinlog --no-defaults --base64-output=decode-rows -v /usr/local/mysql/data/mysql_bin.000003 ###查询该二进制日志内容是否正确mysql&gt; use test2;mysql&gt; drop table aa; ###先删掉坏的那张表[root@server1 ~]# mysql -u root -p test2 &lt; /opt/bak_sql/test2-2020-10-24.sql ###还原完全备份的数据库[root@server1 ~]# mysqlbinlog --no-defaults --base64-output=decode-rows -v /usr/local/mysql/data/mysql_bin.000002 ###查询该二进制日志内容[root@server1 ~]# mysqlbinlog --no-defaults --stop-datetime=&#x27;2020-10-24 0:55:25&#x27; /usr/local/mysql/data/mysql_bin.000002 | mysql -u root -p ###停止错误的时间[root@server1 ~]# mysqlbinlog --no-defaults --start-datetime=&#x27;2020-10-24 0:55:48&#x27; /usr/local/mysql/data/mysql_bin.000002 | mysql -u root -p ###开始正确的时间 2、基于时间点恢复跳过某个发生错误的时间点实现数据恢复 前面步骤同基于时间恢复 1234567[root@server1 ~]# mysqlbinlog --no-defaults --base64-output=decode-rows -v /usr/local/mysql/data/mysql_bin.000003 ###查询该二进制日志内容[root@server1 ~]# mysqlbinlog --no-defaults --stop-position=&#x27;2168&#x27; /usr/local/mysql/data/mysql_bin.000003 | mysql -u root -p ###上一次操作正确的位置点停止[root@server1 ~]# mysqlbinlog --no-defaults --start-position=&#x27;2537&#x27; /usr/local/mysql/data/mysql_bin.000003 | mysql -u root -p ###下一次操作正确的位置点开始 六、主从复制 1、原理1、Master将用户对数据库更新的操作以二进制格式保存到Binary Log日志文件中； 2、Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 3、Master接收来自Slave的IO进程的请求后，通过负责复制的IO进程根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置； 4、Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清除的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”； 5、Slave的Sql进程检测到relay-log中新增了内容后，会马上解析relay-log的内容称为在Master端真实执行时候的那些可执行的内容，并在自身执行。 故障及解决方法MySQL复制方式： 半同步复制 –》同步速度慢一点能保证slave同步二进制日志的完整性 异步复制–》同步速度快，但是不能确保二进制日志确实到达slave上 在半同步复制的架构下，当master在将自己binlog发给slave上的时候，要确保slave已经接受到了这个二进制日志以后，才会返回数据给客户端。 对比两种架构：异步复制对于用户来说，可以确保得到快速的响应结构，但是不能确保二进制日志确实到达了slave上；半同步复制对于客户的请求响应稍微慢点，但是他可以保证二进制日志的完整性。 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 故障一： 开发设置的crontab计划任务周期不合理：感觉前端的页面（LAMP）打开有点延迟，平时正常都很快就加载出来了，查找原因： 发现数据库占满了，因为数据要从数据库拿，影响了读取速度； 问题：周期性计划开发设计不合理，本来1个小时的，结果三分钟一次写业务数据到数据库，导致那个数据库很大占满了， 解决方法：通知开发去修改 故障二： MySQL中主从延迟高，怎么解决？主从同步的延迟的原因： 我们知道， 一个服务器开放Ｎ个链接给客户端来连接的，这样有会有大并发的更新操作, 从服务器通过I&#x2F;O的线程去主服务器同步二进制日志，当某个SQL在从服务器上执行的时间稍长 或者由于某个SQL要进行锁表就会导致主服务器的SQL大量积压，未被同步到从服务器里。这就导致了主从不一致，也就是主从延迟。 主从同步延迟的解决办法：软件方面： 因为所有的SQL必须都要在从服务器里面执行一遍，但是主服务器如果不断的有更新操作源源不断的写入，那么一旦有延迟产生，那么延迟加重的可能性就会原来越大。 当然我们可以做一些缓解的措施。 a. 我们知道因为主服务器要负责更新操作， 他对安全性的要求比从服务器高， 所有有些设置可以修改，比如sync_binlog&#x3D;1，innodb_flush_log_at_trx_commit &#x3D; 1 之类的设置，而slave则不需要这么高的数据安全，完全可以将sync_binlog设置为0或者关闭binlog，innodb_flushlog，innodb_flush_log_at_trx_commit 也可以设置为0来提高sql的执行效率 这个能很大程度上提高效率。另外就是使用比主库更好的硬件设备作为slave。 b. 就是把，一台从服务器当作为备份使用，而不提供查询，那边他的负载下来了，执行relay log 里面的SQL效率自然就高了。 硬件方面： 1.主或者从服务器负载过高，给服务器扩容； 2.增加从服务器，这个目的还是分散读的压力， 从而降低服务器负载。 3.判断主从延迟的方法可以通过 show slave status 进行查看，比如可以看看Seconds_Behind_Master参数的值来判断，是否有发生主从延时。 其值有这么几种： NULL - 表示io_thread或是sql_thread有任何一个发生故障，也就是该线程的Running状态是No,而非Yes. 0 - 该值为零，是我们极为渴望看到的情况，表示主从复制状态正常 2、主服务器（Master）1）、日志文件修改配置文件 12345server-id=1 ###mysql服务器id，每个服务器不能相同log_bin=master-bin ###主服务器日志文件log_slave_updates=true ###允许中继日志读取主服务器的二进制日志 2）、授权123mysql&gt; grant replication slave on *.* to &#x27;myslave&#x27;@&#x27;192.168.73.%&#x27; identified by &#x27;abc123&#x27;; ###为所有从服务器授权所有数据库mysql&gt; flush privileges; 3）、记录日志文件及位置点1mysql&gt; show master status; ###记下二进制日志文件及position的值 3、从服务器1）、日志文件12345server-id = 12 ###MySQL服务器的id，需要配置不同数字relay_log=relay-log-bin ###从主服务器上同步日志文件记录到本地中继日志relay_log_index=slave-relay-bin.index ###定义中继日志的索引 2）、与Master同步1234567891011mysql&gt; change master to master_host=&#x27;192.168.73.10&#x27;,master_user=&#x27;myslave&#x27;,master_password=&#x27;abc123&#x27;,master_log_file=&#x27;master-bin.000001&#x27;,master_log_pos=863;mysql&gt; start slave; ###启动从服务器mysql&gt; show slave status\\G ###查看从服务器状态...省略 Slave_IO_Running: Yes Slave_SQL_Running: Yes ###这两项需要为YES 3）、主从复制失败原因1、I&#x2F;O线程显示为NO: 主库与从库网络不通、主库未授权给从库、 2、SQL线程显示为NO：从库日志和位置点与主不同步 3、若从库查看连接主库I&#x2F;0线程状态为conneting，一直是这个状态，考虑双方的防火墙是否开启。 七、读写分离1、原理1、只在主服务器上写，只在从服务器上读 2、主数据库处理事务性查询，从数据库处理select查询 3、数据库复制用于将事务性查询的变更同步到集群中的从数据库 读写分离原理示意图 2、部署amoeba基于读写分离 1）、安装jdk环境2）、部署amoeba代理1234567[root@amoeba opt]# unzip amoeba-mysql-3.0.5-RC-distribution.zip -d /usr/local [root@amoeba ~]# mv /usr/local/amoeba-mysql-3.0.5-RC/ /usr/local/amoeba [root@amoeba ~]# chmod -R 755 /usr/local/amoeba/ [root@amoeba ~]# vim /usr/local/amoeba/jvm.properties 32 #JVM_OPTIONS=&quot;-server -Xms256m -Xmx1024m -Xss196k -XX:PermSize=16m -XX:MaxPe rmSize=96m&quot; 33 JVM_OPTIONS=&quot;-server -Xms1024m -Xmx1024m -Xss256k&quot; 3）、在MySQL上给amoeba授权1mysql&gt; grant all on *.* to &#x27;test&#x27;@&#x27;192.168.73.%&#x27; identified by &#x27;abc123&#x27;; 4）、修改配置文件1、amoeba.xml1、修改连接amoeba使用的用户名和密码 2、给master开放默认池和写池，给slave开放读池 2、dbServer.xml1、指定为amoeba创建的允许读取数据库的用户名和密码（5.7版本没有默认的test数据库） 2、配置三个服务器的主机名和地址 3、指定名为slaves的poolName中pools的主机名 八、MHA高可用及故障切换1、概述 1、传统的MySQL主从架构存在的问题 MySQL主服务器出故障后就无法写入数据了 2、MHA简介一套优秀的MySQL高可用环境下故障切换和主从复制的软件MySQL故障过程中，MHA能做到0-30秒内自动完成故障切换 3、MHA组成MHA Manager（管理节点）和 MHA Node（数据节点）MHA Manager 可以单独部署在一台独立的机器上，管理多个 master-slave 集群（Manger是单独一台监控master服务器健康状态的服务器。）；也可以部署在一台 slave 节点上。MHA Node 运行在每台 MySQL 服务器上，MHA Manager 会定时探测集群中的 master 节点。当 master 出现故障时，它可以自动将最新数据的 slave 提升为新的 master，然后将所有其他的 slave 重新指向新的 master。整个故障转移过程对应用程序完 全透明。 4、MHA特点（优势）在 MHA 自动故障切换过程中，MHA 试图从宕机的主服务器上保存二进制日志，最大程度的保证数据的不丢失，但这并不总是可行的。例如，如果主服务器硬件故障或无法通过 SSH 访问，MHA 没法保存二进制日志，就会出现只进行故障转移但丢失了最新的数据的情况。使用 MySQL 5.5 的半同步复制，可以大大降低数据丢失的风险。MHA 可以与半同步复 制结合起来。如果只有一个 slave 已经收到了最新的二进制日志，MHA 可以将最新的二进 制日志应用于其他所有的 slave 服务器上，因此可以保证所有节点的数据一致性。 5、MHA架构目前MHA支持一主多从架构，最少三台服务，即一主两从 2、具体配置1）、配置一主两从MySQL主配置文件修改： 12345[root@Master ~]# vi /etc/my.cnf[mysqld]server-id = 1log_bin = master-binlog-slave-updates = true MySQL从服务器配置： 1、主备服务器： 123456[root@Slave1 ~]# vi /etc/my.cnf[mysqld]server-id = 2log_bin = master-binrelay_log = relay-log-binrelay_log_index = slave-relay-bin.index 2、从服务器 12345[root@Slave2 ~]# vi /etc/my.cnf[mysqld]server-id = 3relay_log = relay-log-binrelay_log_index = slave-relay-bin.index 2）、MHA软件安装Centos7.6必须安装0.57版本，所有服务器上必须先安装node组件，最后在MHA-Manager节点上安装manager组件，因为manager依赖node组件 3）、配置ssh无密码认证4）、配置虚拟IP12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788[root@Manager ~]# vi /usr/local/bin/master_ip_failover#!/usr/bin/env perluse strict;use warnings FATAL =&gt; &#x27;all&#x27;;use Getopt::Long;my ($command, $ssh_user, $orig_master_host, $orig_master_ip,$orig_master_port, $new_master_host, $new_master_ip, $new_master_port);#############################添加内容部分#########################################my $vip = &#x27;192.168.73.200&#x27;;my $brdc = &#x27;192.168.73.255&#x27;;my $ifdev = &#x27;ens33&#x27;;my $key = &#x27;1&#x27;;my $ssh_start_vip = &quot;/sbin/ifconfig ens33:$key $vip&quot;;my $ssh_stop_vip = &quot;/sbin/ifconfig ens33:$key down&quot;;my $exit_code = 0;#my $ssh_start_vip = &quot;/usr/sbin/ip addr add $vip/24 brd $brdc dev $ifdev label $ifdev:$key;/usr/sbin/arping -q -A -c 1 -I $ifdev $vip;iptables -F;&quot;;#my $ssh_stop_vip = &quot;/usr/sbin/ip addr del $vip/24 dev $ifdev label $ifdev:$key&quot;;##################################################################################GetOptions(&#x27;command=s&#x27; =&gt; \\$command,&#x27;ssh_user=s&#x27; =&gt; \\$ssh_user,&#x27;orig_master_host=s&#x27; =&gt; \\$orig_master_host,&#x27;orig_master_ip=s&#x27; =&gt; \\$orig_master_ip,&#x27;orig_master_port=i&#x27; =&gt; \\$orig_master_port,&#x27;new_master_host=s&#x27; =&gt; \\$new_master_host,&#x27;new_master_ip=s&#x27; =&gt; \\$new_master_ip,&#x27;new_master_port=i&#x27; =&gt; \\$new_master_port,);exit &amp;main();sub main &#123;print &quot;\\n\\nIN SCRIPT TEST====$ssh_stop_vip==$ssh_start_vip===\\n\\n&quot;;if ( $command eq &quot;stop&quot; || $command eq &quot;stopssh&quot; ) &#123;my $exit_code = 1;eval &#123;print &quot;Disabling the VIP on old master: $orig_master_host \\n&quot;;&amp;stop_vip();$exit_code = 0;&#125;;if ($@) &#123;warn &quot;Got Error: $@\\n&quot;;exit $exit_code;&#125;exit $exit_code;&#125;elsif ( $command eq &quot;start&quot; ) &#123;my $exit_code = 10;eval &#123;print &quot;Enabling the VIP - $vip on the new master - $new_master_host \\n&quot;;&amp;start_vip();$exit_code = 0;&#125;;if ($@) &#123;warn $@;exit $exit_code;&#125;exit $exit_code;&#125;elsif ( $command eq &quot;status&quot; ) &#123;print &quot;Checking the Status of the script.. OK \\n&quot;;exit 0;&#125;else &#123;&amp;usage();exit 1;&#125;&#125;sub start_vip() &#123;`ssh $ssh_user\\@$new_master_host \\&quot; $ssh_start_vip \\&quot;`;&#125;# A simple system call that disable the VIP on the old_mastersub stop_vip() &#123;`ssh $ssh_user\\@$orig_master_host \\&quot; $ssh_stop_vip \\&quot;`;&#125;sub usage &#123;print&quot;Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n&quot;;&#125; 5）、配置文件解析12345678910111213141516[server default]manager_log=/var/log/masterha/app1/manager.log #manager日志manager_workdir=/var/log/masterha/app1 #manager工作目录master_binlog_dir=/usr/local/mysql/data #master保存binlog的位置master_ip_failover_script=/usr/local/bin/master_ip_failover #设置自动failover时候的切换脚本master_ip_online_change_script=/usr/local/bin/master_ip_online_change #设置手动切换时的切换脚本password=manager #设置mysql中root用户的密码，这个密码是前面创建监控用户的密码ping_interval=1 #设置监控主库，发送ping包的时间间隔，默认是3秒，尝试三次没有回应的时候自动进行failoverremote_workdir=/tmp #设置远端mysql在发生切换时binlog的保存位置repl_password=abc123 #设置复制用户的密码repl_user=myslave #设置复制用户的账户report_script=/usr/local/send_report #设置发生切换后发送的报警的脚本secondary_check_script=/usr/local/bin/masterha_secondary_check -s 14.0.0.30 -s 14.0.0.40shutdown_script=&quot;&quot; #设置故障发生后关闭故障主机脚本（该脚本的主要作用是关闭主机防止发生脑裂）ssh_user=root #设置ssh的登录用户名user=mha #设置监控用户 6）、ssh无密码认证测试7）、MHA健康检查8）、在MHA的master节点开启虚拟IP九、SQL高级语句1、按关键字排序（order by） 1、使用order by语句来实现排序2、排序可针对一个或多个字段3、ASC：升序，默认排序方式4、DESC：降序5、order by的语法结构 1select 字段1，字段2 from 表名 order by 字段1 desc|asc，字段2 desc|asc； 2、对结果进行分组 1、使用group by语句来实现分组2、通常结合聚合函数一起使用3、可以按一个或多个字段对结果进行分组4、group by的语法结构 1select count(name),score from a where score &gt; 80 group by score; 3、限制结果条目 1、只返回select查询结果的第一行或前几行2、使用limit语句限制条目3、limit语法结构 offset：位置偏移量，从0开始number：返回记录行的最大数目 1select 字段1，字段2 from 表名 limit [offset,] number； offset：位置偏移量，从0开始number：返回记录行的最大数目 例子： 1select * from a limit 2,3; ##从索引2开始，显示3行 4、设置别名 1、使用as语句设置别名，关键字as可省略2、设置别名时，保证不能与库中其他表或字段名称冲突3、别名的语法结构字段别名： 1select 字段 as 别名 from 表名； 表的别名： 1select 字段 from 表名 as 别名； 5、通配符 1、用于替换字符串中的部分字符2、通常配合like一起使用，并协同where完成查询3、常用通配符 12% 表示零个、一个或多个即任意字符_ 表示单个字符 6、子查询 1、也称作内查询或者嵌套查询2、先于主查询被执行，其结果将作为外层主查询的条件3、在增删改查中都可以使用子查询4、支持多层嵌套5、in语句用来判断某个值是否在给定的结果集中 1234例：select id,name from a where id in(1,2);select * from a where id in(select id from a where score&gt;80); 7、null值 null：真空（什么都没有）‘’：空气（还有空气）1、表示缺失的值2、与数字0或者空白（spaces）是不同的3、使用is null或is not null进行判断4、null值和空值（’’）的区别空值长度为0，不占空间；null值的长度为null，占用空间is null无法判断空值空值使用“&#x3D;”或者“&lt;&gt;”来处理count()计算时，null会忽略，空值会加入计算 8、正则表达式 ^ 匹配文本的开始字符 $ 匹配文本的结束字符 . 匹配任何单个字符 * 匹配前面的字符零次或多次 + 匹配前面的字符一次或多次 字符串 匹配包含指定的字符串 p1lp2 匹配p1或p2 […] 匹配字符集合中任一字符 [^…] 匹配不在括号中的任一字符 {n} 匹配前面的字符串n次 {n，m} 匹配前面的字符串至少n次，之多m次 9、运算符 比较运算符 逻辑运算符 （1）逻辑非逻辑非将跟在它后面的逻辑测试取反，把真变为假，把假变为真。如果 NOT 后面的操作数为 0 时，所得值为 1；如果操作数为非 0 时，所得值为 0；如果操作数为 NULL 时，所得值为 NULL。注意：非0值都是1 （2）逻辑与如果所有值都是真返回 1，否则返回 0。 （3）逻辑或（最好用or）逻辑或表示包含的操作数，任意一个为非零值并且不是 NULL 值时，返回 1，否则返回0。 （4）逻辑异或两个非 NULL 值的操作数，如果两者都是 0 或者都是非 0，则返回 0；如果一个为 0， 另一个为非 0，则返回结果为 1；当任意一个值为 NULL 时，返回值为 NULL。运算总结：and运算，只要碰到0就是0，（非0和null是null）or运算，只要碰到非0值就是1，（0和null是null）异或运算，只要碰到null都是null 位运算符位运算符实际上是对二进制数进行计算的运算符。位运算方法：按位与运算10–》101015–》11111010 –》10按位与运算（&amp;），是对应的二进制位都是 1 的，它们的运算结果为 1，否则为 0 按位或运算10–》101015–》11111111–》15按位或运算（|），是对应的二进制位只要是 1 的，它们的运算结果就为 1，否则为 0 按位异或运算10–》101015–》11110101–》5按位异或运算（^），是对应的二进制位不相同时，运算结果 1，否则为 0 按位取反运算1–》00011–》11105–》01010100–》4按位取反（），是对应的二进制数逐位反转，即 1 取反后变为 0, 0 取反后变为 1按位左移运算1&lt;&lt;21–》0001按位左移2位，空缺处补00100–》4 10&lt;&lt;310–》1010按位左移3位，空缺处补01010000–》80按位右移运算10&gt;&gt;210–》1010按位右移2位，多余的位数直接删除0010–》2 15&gt;&gt;215–》1111按位右移2位，多余的位数直接删除0011–》3常用的运算符优先级 10、连接查询通常都是将来自两个或多个表的行结合起来，基于这些表之间的共同字段，进行数据的拼接。要先确定一个主表作为结果集，然后将其他表的行有选择性的连接到选定的主表结果集上。 使用较多的连接查询包括：内连接、左连接和右连接 1、内连接在from子句中使用关键字 inner join 来连接多张表，并使用 on子句设置连接条件。mysql&gt; select info.name,hob.hobbyname from info inner join hob on info.hobby&#x3D;hob.id;内连接是系统默认的表连接，所以在 FROM 子句后可以省略 INNER 关键字，只使用关键字 JOIN。同时有多个表时，也可以连续使用 INNER JOIN 来实现多表的内连接，不过为了更好的性能，建议最好不要超过三个表。 2、外连接左连接，主表在左边，主表内容会全部显示出来，在从表中没匹配到的以NULL显示出来 右连接，主表在右边，主表内容会全部显示出来，在从表中没匹配到的以NULL显示出来 十、MySQL函数数据库函数常用的函数分类1、数学函数2、聚合函数3、字符串函数4、日期时间函数 1、常用的数学函数123456789101112abs(x)：返回x的绝对值rand()：返回0到1的随机数（0-0.9999…，1是取不到的）mod(x,y)：返回x除以y以后的余数power(x,y)：返回x的y次方round(x)：返回离x最近的整数（四舍五入，只看小数点后第一位）round(x,y)：保留x的y位小数四舍五入后的值sqrt(x)：返回x的平方根truncate(x,y)：返回数字x截断为y位小数的值ceil(x)：返回大于或等于x的最小整数（向上取整数）floor(x)：返回小于或等于x的最大整数（向下取整数）greatest(x1,x2…)：返回集合中最大的值least(x1,x2…)：返回集合中最小的值 abs(x)：返回x的绝对值 rand()：返回0到1的随机数（0-0.9999…，1是取不到的） mod(x,y)：返回x除以y以后的余数 power(x,y)：返回x的y次方 round(x)：返回离x最近的整数（四舍五入，只看小数点后第一位） round(x,y)：保留x的y位小数四舍五入后的值 sqrt(x)：返回x的平方根 truncate(x,y)：返回数字x截断为y位小数的值 ceil(x)：返回大于或等于x的最小整数（向上取整数） floor(x)：返回小于或等于x的最大整数（向下取整数） greatest(x1,x2…)：返回集合中最大的值 least(x1,x2…)：返回集合中最小的值 2、聚合函数●对表中数据记录进行集中概括而设计的一类函数●常用的聚合函数（只会产生一个值） 12345avg(字段名) 返回指定字段的平均值count(字段名) 返回指定字段中非NULL值的个数 min(字段名) 返回指定字段的最小值max(字段名) 返回指定字段的最大值sum(字段名) 返回指定字段的所有值之和 avg(字段名) 返回指定字段的平均值 count(字段名) 返回指定字段中非NULL值的个数 min(字段名) 返回指定字段的最小值 max(字段名) 返回指定字段的最大值 sum(字段名) 返回指定字段的所有值之和 3、字符串函数●常用的字符串函数 12345678910111213length(x)：返回字符串x的长度（空格也算）trim()：返回去除指定格式的值（只能去除前后的空格）concat(x,y)：将提供的参数x和y拼接成一个字符串upper(x)：将字符串x的所有字母变成大写字母lower(x)：将字符串x的所有字母变成小写字母left(x,y)：返回字符串x的前y个字符right(x,y)：返回字符串x的后y个字符repeat(x,y)：将字符串x重复y次space(x)：返回x个空格（结合concat使用）replace(x,y,z)：将字符串z替代字符串x中的字符串ystrcmp(x,y)：比较x和y，返回的值可以为-1 &lt;，0 =，1 &gt;substring(x,y,z)：获取从字符串x中的第y个位置开始长度为z的字符串reverse(x)：将字符串x反转 length(x)：返回字符串x的长度（空格也算） trim()：返回去除指定格式的值（只能去除前后的空格） concat(x,y)：将提供的参数x和y拼接成一个字符串 upper(x)：将字符串x的所有字母变成大写字母 lower(x)：将字符串x的所有字母变成小写字母 left(x,y)：返回字符串x的前y个字符 right(x,y)：返回字符串x的后y个字符 repeat(x,y)：将字符串x重复y次 space(x)：返回x个空格（结合concat使用） replace(x,y,z)：将字符串z替代字符串x中的字符串y strcmp(x,y)：比较x和y，返回的值可以为-1 &lt;，0 &#x3D;，1 &gt; substring(x,y,z)：获取从字符串x中的第y个位置开始长度为z的字符串 格式：substring(完整字符串,起始位置,长度)； ##起始位置从1开始 reverse(x)：将字符串x反转 4、日期时间函数●常用的日期时间函数 12345678910curdate()：返回当前时间的年月日curtime()：返回当前时间的时分秒now()：返回当前时间的日期和时间month(x)：返回日期x中的月份值hour(x)：返回x中的小时值minute(x)：返回x中的分钟值second(x)：返回x中的秒钟值dayofweek(x)：返回x是星期几，1星期日，2星期一，3星期二…dayofmonth(x)：计算日期x是本月的第几天dayofyear(x)：计算日期x是本年的第几天 curdate()：返回当前时间的年月日 curtime()：返回当前时间的时分秒 now()：返回当前时间的日期和时间 month(x)：返回日期x中的月份值 hour(x)：返回x中的小时值 minute(x)：返回x中的分钟值 second(x)：返回x中的秒钟值 dayofweek(x)：返回x是星期几，1星期日，2星期一，3星期二… dayofmonth(x)：计算日期x是本月的第几天 dayofyear(x)：计算日期x是本年的第几天 十一、MySQL存储过程1、存储过程简介1、从 5.0 版本才开始支持2、是一组为了完成特定功能的SQL语句集合（封装）3、比传统SQL速度更快、执行效率更高4、存储过程的优点执行一次后，会将生成的二进制代码驻留缓冲区（便于下次执行），提高执行效率SQL语句加上控制语句的集合，灵活性高在服务器端存储，客户端调用时，降低网络负载可多次重复被调用，可随时修改，不影响客户端调用可完成所有的数据库操作，也可控制数据库的信息访问权限5、为什么要用存储过程？1.减轻网络负载；2.增加安全性 传统SQL访问MySQL服务端过程： 使用存储过程访问MySQL服务端： 2、创建存储过程1、使用create procedure语句创建存储过程2、参数分为输入参数：in输出参数：out输入&#x2F;输出参数：inout3、存储过程的主体部分，被称为过程体；以begin开始，以end$$结束4、具体格式 12345678910111213delimiter $$create procedure 存储过程名（in 参数名 参数类型）begin#定义变量declare 变量名 变量类型#变量赋值set 变量名 = 值 sql 语句1； sql 语句2； ...end$$delimiter ；（有空格）123456789101112 示例： 3、调用存储过程12call 存储过程名(实际参数);1 4、查询存储过程12show procedure status where db=&#x27;数据库&#x27;；1 5、修改存储过程存储过程的修改分为特征的修改和业务内容的修改。特征的修改语法结构如下： 12alter procedure 存储过程名 [ &lt;特征&gt; … ]1 存储过程内容的修改方法是通过删除原有存储过程，之后再以相同的名称创建新的存储过程。 6、删除存储过程删除存储过程的语法： 12drop &#123;procedure|function|if exits&#125; &lt;过程名&gt;1 7、传递参数过程示例：1、2、总结：in和inout参数会将全局变量的值传入存储过程中，而out参数不会将全局变量的值传入存储过程中。在存储过程使用中，参数值in，out，inout都会发生改变。 3、总结：调用完存储过程后，发现in参数不会对全局变量的值引起变化，而out和inout参数调用完存储过程后，会对全局变量的值产生变化，会将存储过程引用后的值赋值给全局变量。 in参数赋值类型可以是变量还有定值，而out和inout参数赋值类型必须为变量。 十二、死锁＋慢查询MySQL引擎默认的锁级别： MyISAM和MEMORY采用表级锁(table-level locking)。 BDB采用页面锁(page-level locking)或表级锁，默认为页面锁。 InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁。 Innodb引擎中的行锁与表锁 在Innodb引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表，什么时候或只锁住一行呢？ InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，Oracle者是通过在数据块中对相应数据行加锁来实现的。 InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。 行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。 行级锁的缺点是：如果并发请求大量的锁资源，所以速度慢，内存消耗大。 1、行级锁与死锁 MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。 当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 如何怎么避免死锁？ 1）以固定的顺序访问表和行。 分为两种情景： 对于不同事务访问不同的表，尽量做到访问表的顺序一致； 对于不同事务访问相同的表，尽量对记录的id做好排序，执行顺序一致； 2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。 4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。 5）为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。 怎么解决死锁？ 第一步，查出已锁的进程 查看正在锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 查看等待锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; INNODB_TRX表主要是包含了正在InnoDB引擎中执行的所有事务的信息，包括waiting for a lock和running的事务 1select * from information_schema.innodb_trx 第二步，kill进程 2、慢查询：https://blog.csdn.net/weixin_42030357/article/details/104105932 进行SQL优化的手段也主要是修改SQL写法，或者新增索引。 （1）数据库中设置SQL慢查询 一、第一步.开启mysql慢查询 方式一： ​ 修改配置文件在 my.cnf增加几行: 主要是慢查询的定义时间（超过2秒就是慢查询），以及慢查询log日志记录（ slow_query_log） 方法二：通过MySQL数据库开启慢查询: （2）分析慢查询日志 ​ 直接分析mysql慢查询日志 ,利用explain关键字可以模拟优化器执行SQL查询语句，来分析sql慢查询语句 例如：执行EXPLAIN SELECT * FROM res_user ORDER BYmodifiedtime LIMIT 0,1000得到如下结果： 显示结果分析： 123456789table | type | possible_keys | key |key_len | ref | rows | Extra EXPLAIN列的解释： table 显示这一行的数据是关于哪张表的 type 这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL rows 显示需要扫描行数key 使用的索引 （3）常见的慢查询优化 ①、索引没起作用的情况 12345a、使用LIKE关键字的查询语句在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。只有“%”不在第一个位置索引才会起作用。2、使用多列索引的查询语句MySQL可以为多个字段创建索引。一个索引最多可以包括16个字段。对于多列索引，只有查询条件使用了这些字段中的第一个字段时，索引才会被使用。 ②、优化数据库结构 1234567 合理的数据库结构不仅可以使数据库占用更小的磁盘空间，而且能够使查询速度更快。数据库结构的设计，需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。a、将字段很多的表分解成多个表对于字段比较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。b、增加中间表对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，把需要经常联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询，以此来提高查询效率。 ③、分解关联查询 12345678910111213 将一个大的查询分解为多个小查询是很有必要的。 很多高性能的应用都会对关联查询进行分解，就是可以对每一个表进行一次单表查询，然后将查询结果在应用程序中进行关联，很多场景下这样会更高效，例如： SELECT * FROM tag JOIN tag_post ON tag_id = tag.id JOIN post ON tag_post.post_id = post.id WHERE tag.tag = &#x27;mysql&#x27;; 分解为： SELECT * FROM tag WHERE tag = &#x27;mysql&#x27;;SELECT * FROM tag_post WHERE tag_id = 1234;SELECT * FROM post WHERE post.id in (123,456,567); ④、优化LIMIT分页 在系统中需要分页的操作通常会使用limit加上偏移量的方法实现，同时加上合适的order by 子句。如果有对应的索引，通常效率会不错，否则MySQL需要做大量的文件排序操作。 一个非常令人头疼问题就是当偏移量非常大的时候，例如可能是limit 10000,20这样的查询，这是mysql需要查询10020条然后只返回最后20条，前面的10000条记录都将被舍弃，这样的代价很高。 ​ 优化此类查询的一个最简单的方法是尽可能的使用索引覆盖扫描，而不是查询所有的列。然后根据需要做一次关联操作再返回所需的列。对于偏移量很大的时候这样做的效率会得到很大提升。 对于下面的查询： ​ select id,title from collect limit 90000,10; 该语句存在的最大问题在于limit M,N中偏移量M太大（我们暂不考虑筛选字段上要不要添加索引的影响），导致每次查询都要先从整个表中找到满足条件 的前M条记录，之后舍弃这M条记录并从第M+1条记录开始再依次找到N条满足条件的记录。如果表非常大，且筛选字段没有合适的索引，且M特别大那么这样的代价是非常高的。 试想，如我们下一次的查询能从前一次查询结束后标记的位置开始查找，找到满足条件的100条记录，并记下下一次查询应该开始的位置，以便于下一次查询能直接从该位置 开始，这样就不必每次查询都先从整个表中先找到满足条件的前M条记录，舍弃，在从M+1开始再找到100条满足条件的记录了。 方法一：虑筛选字段（title）上加索引 ​ title字段加索引 （此效率如何未加验证） 方法二：先查询出主键id值 select id,title from collect where id&gt;&#x3D;(select id from collect order by id limit 90000,1) limit 10; 原理：先查询出90000条数据对应的主键id的值，然后直接通过该id的值直接查询该id后面的数据。 方法三：关延迟联” 如果这个表非常大，那么这个查询可以改写成如下的方式： Select news.id, news.description from news inner join (select id from news order by title limit 50000,5) as myNew using(id); ​ 这里的“关延迟联”将大大提升查询的效率，它让MySQL扫描尽可能少的页面，获取需要的记录后再根据关联列回原表查询需要的所有列。这个技术也可以用在优化关联查询中的limit。 十三、优化范围MySQL 服务器硬件和操作系统调节1234567891011121314151617181920212223241. 拥有足够的物理内存来把整个InnoDB文件加载到内存中——在内存中访问文件时的速度要比在硬盘中访问时快的多.2. 不惜一切代价避免使用Swap交换分区 – 交换时是从硬盘读取的，它的速度很慢.3. 使用电池供电的RAM（注：RAM即随机存储器）.4. 使用高级的RAID（注：Redundant Arrays of Inexpensive Disks，即磁盘阵列） – 最好是RAID10或更高.5. 避免RAID5（注：一种存储性能、数据安全和存储成本兼顾的存储解决方案） – 确保数据库完整性的校验是要付出代价的.6. 将操作系统和数据分区分开，不仅仅是逻辑上，还包括物理上 – 操作系统的读写操作会影响数据库的性能.7. 把MySQL临时空间和复制日志与数据放到不同的分区 – 当数据库后台从磁盘进行读写操作时会影响数据库的性能.8. 更多的磁盘空间等于更快的速度.9. 更好更快的磁盘.10. 使用SAS（注： Serial Attached SCSI，即串行连接SCSI）代替SATA（注：SATA，即串口硬盘）.11. 较小的硬盘 比 较大的硬盘快，尤其是在RAID配置的情况下.12. 使用电池支持的高速缓存RAID控制器.13. 避免使用软件磁盘阵列.14. 考虑为数据分区使用固态IO卡 (不是磁盘驱动器) – 这些卡能够为几乎任何数量的数据支持2GB/s的写入速度.15. 在[Linux](http://www.ttlsa.com/linux/)中设置swappiness的值为0 – 在数据库服务器中没有理由缓存文件，这是一个服务器或台式机的优势.16. 如果可以的话，使用 noatime 和 nodirtime 挂载文件系统 – 没有理由更新访问数据库文件的修改时间.17. 使用 XFS 文件系统 – 一种比ext3更快、更小的文件系统，并且有许多日志选项， 而且ext3 已被证实与MySQL有双缓冲问题.18. 调整 XFS 文件系统日志和缓冲变量 – 为了最高性能标准.19. 在 Linux 系统中, 使用 NOOP 或者 DEADLINE IO 定时调度程序 – 同 NOOP 和 DEADLINE定时调度程序相比，这个 CFQ 和 ANTICIPATORY 定时调度程序 显得非常慢.20. 使用64位的操作系统 – 对于MySQL，会有更大的内存支持和使用.21. 删除服务器上未使用的安装包和守护进程 – 更少的资源占用.22. 把使用MySQL的host和你的MySQL host放到一个hosts文件中 – 没有DNS查找.23. 切勿强制杀死一个MySQL进程 – 你会损坏数据库和正在运行备份的程序.24. 把服务器贡献给MySQL – 后台进程和其他服务能够缩短数据库占用CPU的时间. MySQL 配置12345678910111213141525. 当写入时，使用 innodb_flush_method=O_DIRECT 来避免双缓冲.26. 避免使用 O_DIRECT 和 EXT3 文件系统 – 你将序列化所有要写入的.27. 分配足够的 innodb_buffer_pool_size 来加载整个 InnoDB 文件到内存中– 少从磁盘中读取.28. 不要将 innodb_log_file_size 参数设置太大， 这样可以更快同时有更多的磁盘空间 – 丢掉多的日志通常是好的，在数据库崩溃后可以降低恢复数据库的时间.29. 不要混用 innodb_thread_concurrency 和 thread_concurrency 参数– 这2个值是不兼容的.30. 分配一个极小的数量给 max_connections 参数 – 太多的连接会用尽RAM并锁定MySQL服务.31. 保持 thread_cache 在一个相对较高的数字，大约 16 – 防止打开连接时缓慢.32. 使用skip-name-resolve参数 – 去掉 DNS 查找.33. 如果你的查询都是重复的，并且数据不常常发生变化，那么可以使用查询缓存. 但是如果你的数据经常发生变化，那么使用查询缓存会让你感到失望.34. 增大temp_table_size值，以防止写入磁盘35. 增大max_heap_table_size值，以防止写入磁盘36. 不要把sort_buffer_size值设置的太高，否则的话你的内存将会很快耗尽37. 根据key_read_requests和key_reads值来决定key_buffer的大小，一般情况下key_read_requests应该比key_reads值高，否则你不能高效的使用key_buffer38. 将innodb_flush_log_at_trx_commit设置为0将会提高性能，但是如果你要保持默认值（1）的话，那么你就要确保数据的完整性，同时你也要确保复制不会滞后.39. 你要有一个测试环境，来测试你的配置，并且在不影响正常生产的情况下，可以常常进行重启. MySQL模式优化123456789101112131415161718192021222340. 保持你的数据库整理性.41. 旧数据归档 - 删除多余的行返回或搜索查询.42. 将您的数据加上索引.43. 不要过度使用索引，比较与查询.44. 压缩文字和BLOB数据类型 - 以节省空间和减少磁盘读取次数.45. UTF 8和UTF16都低于latin1执行效率.46. 有节制地使用触发器.47. 冗余数据保持到最低限度 - 不重复不必要的数据.48. 使用链接表，而不是扩展行.49. 注意数据类型，在您的真实数据中，尽可能使用最小的一个.50. 如果其他数据经常被用于查询时，而BLOB / TEXT数据不是，就把BLOB / TEXT数据从其他数据分离出来.51. 检查和经常优化表.52. 经常重写InnoDB表优化.53. 有时，当添加列时删除索引，然后在添加回来索引，这样就会更快.54. 针对不同的需求，使用不同的存储引擎.55. 使用归档存储引擎日志表或审计表-这是更有效地写道.56. 会话数据存储在缓存（[memcache](http://www.ttlsa.com/nosql/memcache/)）的而不是MySQL中 - 缓存允许自动自动填值的，并阻止您创建难以读取和写入到MySQL的时空数据.57. 存储可变长度的字符串时使用VARCHAR而不是CHAR - 节省空间，因为固定长度的CHAR，而VARCHAR长度不固定（UTF8不受此影响）.58. 逐步进行模式的变化 - 一个小的变化，可以有巨大的影响.59. 在开发环境中测试所有模式，反映生产变化.60. 不要随意更改你的配置文件中的值，它可以产生灾难性的影响.61. 有时候，在MySQL的configs少即是多.62. 有疑问时使用一个通用的MySQL配置文件. 查询优化12345678910111213141516171819202122232463. 使用慢查询日志去发现慢查询.64. 使用执行计划去判断查询是否正常运行.65. 总是去测试你的查询看看是否他们运行在最佳状态下 –久而久之性能总会变化.66. 避免在整个表上使用count(*),它可能锁住整张表.67. 使查询保持一致以便后续相似的查询可以使用查询缓存.68. 在适当的情形下使用GROUP BY而不是DISTINCT.69. 在WHERE, GROUP BY和ORDER BY子句中使用有索引的列.70. 保持索引简单,不在多个索引中包含同一个列.71. 有时候MySQL会使用错误的索引,对于这种情况使用USE INDEX.72. 检查使用SQL_MODE=STRICT的问题.73. 对于记录数小于5的索引字段，在UNION的时候使用LIMIT不是是用OR.74. 为了 避免在更新前SELECT，使用INSERT ON DUPLICATE KEY或者INSERT IGNORE ,不要用UPDATE去实现.75. 不要使用 MAX,使用索引字段和ORDER BY子句.76. 避免使用ORDER BY RAND().77. LIMIT M，N实际上可以减缓查询在某些情况下，有节制地使用.78. 在WHERE子句中使用UNION代替子查询.79. 对于UPDATES（更新），使用 SHARE MODE（共享模式），以防止独占锁.80. 在重新启动的MySQL，记得来温暖你的数据库，以确保您的数据在内存和查询速度快.81. 使用DROP TABLE，CREATE TABLE DELETE FROM从表中删除所有数据.82. 最小化的数据在查询你需要的数据，使用*消耗大量的时间.83. 考虑持久连接，而不是多个连接，以减少开销.84. 基准查询，包括使用服务器上的负载，有时一个简单的查询可以影响其他查询.85. 当负载增加您的服务器上，使用SHOW PROCESSLIST查看慢的和有问题的查询.86. 在开发环境中产生的镜像数据中 测试的所有可疑的查询. MySQL 备份过程12345678910111213141587. 从二级复制服务器上进行备份.88. 在进行备份期间停止复制，以避免在数据依赖和外键约束上出现不一致.89. 彻底停止MySQL，从数据库文件进行备份.90. 如果使用 MySQL dump进行备份，请同时备份二进制日志文件 – 确保复制没有中断.91. 不要信任LVM 快照 – 这很可能产生数据不一致，将来会给你带来麻烦.92. 为了更容易进行单表恢复，以表为单位导出数据 – 如果数据是与其他表隔离的.93. 当使用mysqldump时请使用 –opt.94. 在备份之前检查和优化表.95. 为了更快的进行导入，在导入时临时禁用外键约束.96. 为了更快的进行导入，在导入时临时禁用唯一性检测.97. 在每一次备份后计算数据库，表以及索引的尺寸，以便更够监控数据尺寸的增长.98. 通过自动调度脚本监控复制实例的错误和延迟.99. 定期执行备份.100. 定期测试你的备份.101: 执行MySQL 监控: Monitis Unveils The World’s First Free On-demand MySQL Monitoring.","categories":[],"tags":[]},{"title":"","slug":"总结/5、集群（LVS，Keeplived，Haproxy）","date":"2024-12-14T22:02:47.148Z","updated":"2021-01-23T06:04:34.000Z","comments":true,"path":"2024/12/15/总结/5、集群（LVS，Keeplived，Haproxy）/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/5%E3%80%81%E9%9B%86%E7%BE%A4%EF%BC%88LVS%EF%BC%8CKeeplived%EF%BC%8CHaproxy%EF%BC%89/","excerpt":"","text":"[TOC] 一、集群的含义由多台主机构成，但对外表现为一个整体 二、集群的分类1、负载均衡集群1、提高应用系统的响应能力，尽可能处理更多的访问请求，减少延迟为目标，获得高并发，高负载的整体性能 2、LB的负载分配依赖于主节点的分流算法 2、高可用集群1、提高应用系统的可靠性，尽可能减少中断时间为目标，确保服务的连续性，达到高可用（HA）的容错效果 2、HA的工作方式包括双工和主从两种模式 3、高性能运算集群1、提高应用系统的CPU运算速度，扩展硬件资源和分析能力为目标，获得相当于大型、超级计算机的高性能运算（HPC）能力 2、高性能依赖于“分布式运算”、“并行计算”，通过专用硬件和软件将多个服务器的CPU、内存等资源整合在一起，实现只有大型、超级计算机才具备的计算能力 三、负载均衡集群1、架构第一层：负载均衡池（LVS） 第二层：服务器池 第三层：共享存储 2、LVS-NAT 1）、介绍1、负载调度器为所有服务节点的网关，即作客户机的访问入口，也是各节点回应客户机的访问出口； 2、服务器节点使用私有IP地址，与调度器处于同一个物理网络，安全性优于其他两种方式。 director（调度器）与RS（服务节点）必须在同一个局域网内。 RS的网关必须指向director的私网DIP。 请求和响应报文都需要director转发，会成为整个系统的瓶颈。 支持端口映射，client访问80端口，director可以映射到RS的8080端口上。 director必须是Linux系统。 director需要两个网卡，一个与互联网通信的公网VIP，一个与RS通信的私网DIP。 2）、环境调度服务器一台： 123IP地址：192.168.100.10（内网）​ 192.168.73.10（外网） Web服务器两台： 123IP地址：192.168.100.11（Server1）IP地址：192.168.100.12（Server2） NFS共享服务器一台：（可省略） 1IP地址：192.168.100.13 （内网） 3）、调度器配置1、两张网卡，一张用于内网通信，一张用于外网通信 123192.168.100.10（VM1）192.168.73.10（VM8） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@LVS-Server ~]# modprobe ip_vs ###加载LVS内核模块（LVS现已成为Linux内核的一部分，默认编译为ip_vs模块，必要时能够自动调用。以下操作能够手动加载ip_vs模块，并查看当前系统中的ip_vs模块的版本信息）[root@LVS-Server ~]# cat /proc/net/ip_vs IP Virtual Server version 1.2.1 (size=4096) ###版本信息Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn[root@LVS-Server ~]# rpm -ivh /mnt/Packages/ipvsadm-1.27-7.el7.x86_64.rpm ###安装管理软件ipvsadm，不需要启动[root@LVS-Server ~]# vi /etc/sysctl.conf net.ipv4.ip_forward = 1 ###开启路由转发[root@LVS-Server ~]# sysctl -pnet.ipv4.ip_forward = 1[root@LVS-Server ~]# vi nat.sh ###编辑调度服务器脚本\\#!/bin/bashecho &quot;1&quot; &gt; /proc/sys/net/ipv4/ip_forward ###开启路由地址转换ipvsadm -C ###清除内核虚拟服务器表中的所有记录ipvsadm -A -t 192.168.73.10:80 -s rr ###创建虚拟服务器ipvsadm -a -t 192.168.73.10:80 -r 192.168.100.11:80 -m ###添加服务器节点ipvsadm -a -t 192.168.73.10:80 -r 192.168.100.12:80 -m ###添加服务器节点ipvsadm -Ln ###查看节点状态，“-n”以数字形式显示显示地址，端口信息[root@LVS-Server ~]# sh nat.sh IP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.73.10:80 rr -&gt; 192.168.100.11:80 Masq 1 0 0 -&gt; 192.168.100.12:80 Masq 1 0 0 ipvsadm命令解析 123456789ipvsadm命令选项解析：-C：清除内核虚拟服务器表中的所有记录-A：增加一台新的虚拟服务器-t：说明虚拟服务器提供的是tcp的服务-s rr：启用轮询算法-a：在一个虚拟服务器中增加一台新的真实服务器-r：指定真实服务器-m：指定LVS的工作模式为NAT模式ipvsadm：启用LVS功能 4）、Web服务器配置1、网关为调度器内网IP； 2、route -n查看是否有默认路由生成； 3、挂载NFS共享 5）、验证方式访问调度器外网IP，刷新看是否会轮询 3、LVS-DR 1）、特点1、保证前端路由将目标地址为VIP的报文全部发送给DS，而不是RS 2、RS的RIP可以使用私有地址，但也可以使用公网地址 3、RS和director必须在同一物理网络中 4、请求报文有director调度，但响应报文不一定经由director 不支持端口映射 5、RS的网关不能指向DIP（LVS服务器IP） 2）、报文转发过程1）client发送请求到vip，real-server配置限制对vip应答arp，而lvs会对vip响应arp，因此client将请求发到LVS。 2）LVS机器收到发往vip的报文后，根据目的IP和目的port匹配ipvs规则，将报文目的mac改为real server的mac，同时将源mac改为LVS的mac后，发送到real server。 3）real server收到之后，mac&#x2F;IP都是本机的，就将报文交由系统处理。 4）回程报文因real server收到的报文源IP就是client IP，real server直接将请求回给client。如果client和real server是同一个网段，响应报文直接通过二层透传发送给client，报文目的mac即为client mac；如果client和real server不是同一个网段，响应报文先发送到gateway，再走三层转发返回client。 3）、环境调度服务器一台； 1IP地址：192.168.100.10 Web服务器两台： 123IP地址：192.168.100.11（Server1）IP地址：192.168.100.12（Server2） NFS共享服务器一台：（可省略） 1IP地址：192.168.100.13 4）、调度器配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@LVS-Server ~]# modprobe ip_vs ###加载LVS内核模块（LVS现已成为Linux内核的一部分，默认编译为ip_vs模块，必要时能够自动调用。以下操作能够手动加载ip_vs模块，并查看当前系统中的ip_vs模块的版本信息）[root@LVS-Server ~]# cat /proc/net/ip_vs IP Virtual Server version 1.2.1 (size=4096) ###版本信息Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn[root@LVS-Server ~]# rpm -ivh /mnt/Packages/ipvsadm-1.27-7.el7.x86_64.rpm ###安装管理软件ipvsadm，不需要启动[root@LVS-Server ~]# vi /etc/sysctl.conf net.ipv4.ip_forward=1[root@LVS-Server ~]# vi dr.sh ###编辑调度服务器脚本#!/bin/bashifconfig ens33:0 192.168.100.100 broadcast 192.168.100.100 netmask 255.255.255.255 up ###添加虚拟地址的虚接口route add -host 192.168.100.100 dev ens33:0 ###给ens33：0添加路由ipvsadm -C ###清除内核虚拟服务器表中的所有记录ipvsadm -A -t 192.168.100.100:80 -s rr ###创建虚拟服务器ipvsadm -a -t 192.168.100.100:80 -r 192.168.100.11:80 -g ###添加服务器节点ipvsadm -a -t 192.168.100.100:80 -r 192.168.100.12:80 -g ###添加服务器节点ipvsadm -Ln ###查看节点状态，加个“-n”将以数字形式显示地址、端口信息[root@LVS-Server ~]# sh dr.shIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.100.100:80 rr -&gt; 192.168.100.12:80 Route 1 0 0 -&gt; 192.168.100.11:80 Route 1 0 0[root@LVS-Server ~]# ip addr ###看是否有虚拟路由生成1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:42:ea:42 brd ff:ff:ff:ff:ff:ff inet 192.168.100.10/24 brd 192.168.100.255 scope global ens33 valid_lft forever preferred_lft forever inet 192.168.100.100/32 brd 192.168.100.100 scope global ens33:0 valid_lft forever preferred_lft forever inet6 fe80::a2b4:ea7e:78e6:4f86/64 scope link valid_lft forever preferred_lft forever3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN qlen 1000 link/ether 52:54:00:ea:07:54 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0 valid_lft forever preferred_lft forever4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 1000 link/ether 52:54:00:ea:07:54 brd ff:ff:ff:ff:ff:ff 5）、Web服务器配置1234567891011121314151617181920212223242526272829303132333435363738394041[root@Web1 ~]# vi web.sh#!/bin/bashifconfig lo:0 192.168.100.100 broadcast 192.168.100.100 netmask 255.255.255.255 uproute add -host 192.168.100.100 dev lo:0echo &quot;1&quot; &gt; /proc/sys/net/ipv4/conf/lo/arp_ignoreecho &quot;2&quot; &gt; /proc/sys/net/ipv4/conf/lo/arp_announceecho &quot;1&quot; &gt; /proc/sys/net/ipv4/conf/all/arp_ignoreecho &quot;2&quot; &gt; /proc/sys/net/ipv4/conf/all/arp_announcesysctl -p &amp;&gt; /dev/null[root@Web1 ~]# sh web.sh[root@Web1 ~]# ifconfig ###查看是否有虚拟路由生成ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.100.11 netmask 255.255.255.0 broadcast 192.168.100.255 inet6 fe80::e654:4e48:ec26:2279 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:9f:bd:e6 txqueuelen 1000 (Ethernet) RX packets 713398 bytes 1035992165 (987.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 59733 bytes 3967447 (3.7 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1 (Local Loopback) RX packets 164 bytes 18108 (17.6 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 164 bytes 18108 (17.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo:0: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 192.168.100.100 netmask 255.255.255.255 loop txqueuelen 1 (Local Loopback)virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:dc:14:16 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 6）、验证方式访问虚拟IP，看是否会进行轮询 四、负载均衡+高可用集群1、LVS+Keepalived1）、实现原理1、由多台路由器组成一个热备组，通过共用的虚拟IP地址对外提供服务； 2、每个热备组内同时只有一台主路由器提供服务，其他路由器处于冗余状态； 3、若当前在线的路由器失效，则其他路由器会根据设置的优先级自动接替虚拟IP地址，继续提供服务 2）、故障切换是由虚拟IP地址的漂移来实现的，适用于各种应用服务器 3）、主要优势1、对LVS负载调度器实现热备切换，提高可用性 2、对服务器池中的节点进行健康检查，自动移除失效节点，恢复后再重新加入 4）、Master节点（1）全局配置、热备参数配置 要为master和backup调度器实现热备功能，漂移地址使用LVS群集的VIP地址 （2）Web服务器池配置 在keepalived的热备配置基础上，添加“virtual_server VIP 端口 {…}”区段来配置虚拟服务器 包括负载调度算法、群集工作模式、健康检查间隔、真实服务器地址等参数 5）、Backup节点和master服务器有三个配置要不相同，其余配置均相同： 12345（1）router_id：设和其他web服务器不一样的名称（2）state：设为BACKUP（3）priority：值要低于主服务器 6）、具体配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@Server1 ~]# modprobe ip_vs ###加载ip_vs模块[root@Server1 ~]# rpm -ivh /mnt/Packages/ipvsadm-1.27-7.el7.x86_64.rpm ###安装管理软件ipvsadm[root@Server1 ~]# yum -y install gcc gcc-c++ make popt-devel kernel-devel openssl-devel[root@Server1 ~]# tar zxf keepalived-2.0.13.tar.gz[root@Server1 ~]# cd keepalived-2.0.13/[root@Server1 keepalived-2.0.13]# ./configure --prefix=/[root@Server1 keepalived-2.0.13]# make &amp;&amp; make install[root@Server1 keepalived-2.0.13]# cp keepalived/etc/init.d/keepalived /etc/init.d ###加入系统管理服务[root@Server1 keepalived-2.0.13]# systemctl enable keepalived.service ###设置开机自启动[root@Server1 ~]# vi /etc/keepalived/keepalived.conf ###编辑配置文件global_defs &#123; #全局参数 router_id LVS_01 #指定名称，各个服务器名称要不一样&#125; vrrp_instance VI_1 &#123; #指定vrrp热备参数 state MASTER #服务器角色是master，备份服务器设置为BACKUP interface ens33 #修改物理网卡名称，默认是centos6的eth0 virtual_router_id 51 #组号相同 priority 100 #优先级，主服务器设置要大于备服务器 authentication &#123; auth_type PASS #验证类型和密码，不建议修改 auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.100.100 #漂移地址（VIP）地址，可以有多个 &#125;&#125;virtual_server 192.168.100.100 80 &#123; #配置虚拟服务器 delay_loop 6 protocol TCP #健康检查用的是TCP还是UDP lb_algo rr #调度算法为轮询 lb_kind DR #LVS的工作模式为DR（直连路由） real_server 192.168.100.12 80 &#123; weight 1 TCP_CHECK &#123; #健康检查参数 connect_port 80 #检查80端口连接是否正常 connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;real_server 192.168.100.13 80 &#123; weight 1 TCP_CHECK &#123; connect_port 80 connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125; 2、Haproxy1）、与LVS的区别1、LVS在企业应用中抗负载能力很强，但存在不足 LVS不支持正则处理，不能实现动静分离（nginx都支持） 对于大型网站，LVS的实施配置复杂，维护成本相对较高 2、Haproxy是一款可提供高可用性、负载均衡、及基于TCP和HTTP应用的代理的软件 适用于负载大的Web站点 运行在硬件上可支持数以万计的并发连接的连接请求 2）、Haproxy调度算法原理第一种（RR）： RR算法是最简单最常用的一种算法，即轮询调度 理解举例 有三个节点A、B、C，第一个用户访问会被指派到节点A，第二个用户访问会被指派到节点B，第三个用户访问会被指派到C节点 第四个用户访问继续指派到节点A，轮询分配访问请求实现负载均衡效果 第二种（LC）： LC算法即最小连接数算法，根据后端的节点连接数大小动态分配前端请求 理解举例 有三个节点A、B、C，各节点的连接数分别为A:4、B:5、C:6，此时如果有第一个用户连接请求，会被指派到A上，连接数变为A:5、B:5、C:6 第二个用户请求会继续分配到A上，连接数变为A6、B：5、C：6；再有新的请求会分配给B，每次将新的请求指派给连接数最小的客户端 由于实际情况下A、B、C的连接数会动态释放，很难会出现一样连接数的情况，因此此算法相比较rr算法有很大改进，是目前用到比较多的一种算法 第三种（SH）： SH即基于来源访问调度算法，此算法用于一些有 Session会话记录在服务器端的场景，可以基于来源的IP、Cookie等做集群调度（上一次请求分给哪个web服务器的下次还是分给该服务器） 理解举例 有三个节点A、B、C，第一个用户第一次访问被指派到了A，第二个用户第次访问被指派到了B 当第一个用户第二次访问时会被继续指派到A，第二个用户第二次访问时依旧会被指派到B，只要负载均衡调度器不重启，第一个用户访问都会被指派到A，第二个用户访问都会被指派到B，实现集群的调度 此调度算法好处是实现会话保持，但某些IP访问量非常大时会引起负载不均衡，部分节点访问量超大，影响业务使用 3）、配置文件详解1、global：全局配置1234567891、log127.0.0.1 lcal0：配置日志记录，local0为日志设备，默认存放到系统日志2、log127.0.0.1 loca1 notice:notice为日志级别，通常有24个级别3、maxconn4096：最大连接数4、uid 99：用户uid5、gid 99：用户gid 2、defaults：默认配置一般会被应用组件继承，如果在应用组件中没有特别声明，将安装默认配置参数设置 1234567891011121314151、log global：定义日志为global配置中的日志定义2、mode http：模式为http3、option httplog：采用http日志格式记录日志4、retries 3：检查节点服务器失败连续达到三次则认为节点不可用5、maxconn2000：最大连接数6、contimeout5000：连接超时时间7、clitimeout50000：客户端超时时间8、srvtimeout50000：服务器超时时间 3、listen：应用组件配置12345678910111、listen appli4- backup 0.0.0.0:10004：定义一个appli4- backup的应用2、option httpchk /index.html检查服务器的index.html文件3、option persist：强制将请求发送到已经down掉的服务器（注释掉，否则起服务会报错）4、balance roundrobin：负载均衡调度算法使用轮询算法5、server inst1 192.168.114.56:80 check inter 2000 fall 3：定义在线节点6、server inst2 192.168 114.56:81 check inter 2000 fall 3 backup：定义备份节点 4）、具体配置12345678910111213141516171819[root@Haproxy haproxy]# vim haproxy.cfg下面两项前面加上注释\\#chroot /usr/share/haproxy #固有目录，可注释掉\\#redispatch #当使用了cookie时，haproxy将会将其请求的后端服务器的serverID插入到cookie中，以保证会话的SESSION持久性；而此时，如果后端的服务器宕掉了，但是客户端的cookie是不会刷新的，如果设置此参数，将会将客户的请求强制定向到另外一个后端server上，以保证服务的正常。需要注释掉将原来的listen配置项删除，添加如下参数：listen webcluster 0.0.0.0:80​ option httpchk GET /index.html​ balance roundrobin​ server web1 192.168.73.20:80 check inter 2000 fall 3​ server web2 192.168.73.30:80 check inter 2000 fall 3 5）、日志管理1、默认是输出到系统的syslog（系统日志）中，生成环境中一般单独定义 2、具体配置 123456789101112131415161718192021[root@Haproxy ~]# vi /etc/haproxy/haproxy.cfgglobal log /dev/log local0 info log /dev/log local0 notice[root@Haproxy ~]# systemctl restart haproxy.service[root@Haproxy ~]# vi /etc/rsyslog.d/haproxy.confif ($programname == &#x27;haproxy&#x27; and $syslogseverity-text == &#x27;info&#x27;)then -/var/log/haproxy/haproxy-info.log&amp;~if ($programname == &#x27;haproxy&#x27; and $syslogseverity-text == &#x27;notice&#x27;)then -/var/log/haproxy/haproxy-notice.log&amp;~[root@Haproxy ~]# systemctl restart rsyslog.service[root@Haproxy ~]# systemctl restart haproxy.service[root@Haproxy ~]# cd /var/log/haproxy/ ###可看日志信息[root@Haproxy haproxy]# ll总用量 8-rw-------. 1 root root 329 11月 5 18:02 haproxy-info.log-rw-------. 1 root root 132 11月 5 18:02 haproxy-notice.log 6）、参数优化1234567891011121314151、maxconn：最大连接数，根据应用实际情况进行调整，推荐使用102402、daemon：守护进程模式， Haproxy可以使用非守护进程模式启动，建议使用守护进程模式启动3、nbproc：负载均衡的并发进程数，建议与当前服务器CPU核数相等或为其2倍4、retries：重试次数，主要用于对集群节点的检查，如果节点多，且并发量大，设置为2次或3次5、option http-server-close：主动关闭http请求选项，建议在生产环境中使用此选项6、timeout http-keep-alive：长连接超时时间，设置长连接超时时间，可以设置为10s7、timeout http-request：http请求超时时间，建议将此时间设置为5~10s，增加http连接释放速度8、timeout client：客户端超时时间，如果访问量过大，节点响应慢，可以将此时间设置短一些，建议设置为1min左右就可以了 五、负载均衡+keepalived故障：1.keepalived服务启动了，但是VIP没出来主要是启动keepalived服务的时候不会报错； vrrp_scripts{}中定义了一个函数，里面放了判断nginx存活的脚本，下面track_script{}注意位置，要包含在vrrp_instance{}实例中； （因为一个括号问题排障了接近一个多小时，VIP就是起不来） 2.负载均衡常见典型故障后台服务器down了，nginx就不会分配请求给它了，但是如果没有down，只是返回异常状态码了，比如504、502、404等，这时需要多一个负载均衡的配置，如下： proxy_next_upstream http_500 | http_502 | http_503 | http_504 |http_404;意思是，当其中一台返回错误码404,500…等错误时，可以分配到下一台服务器程序继续处理，提高平台访问成功率 3.Nginx负载均衡优化（1）健康检查下载nginx_upstream_check模块第三方模块 master.zip upstream web { ​ server 172.16.1.7:80 max_fails&#x3D;2 fail_timeout&#x3D;10s; ​ server 172.16.1.8:80 max_fails&#x3D;2 fail_timeout&#x3D;10s; ​ check interval&#x3D;3000 rise&#x3D;2 fall&#x3D;3 timeout&#x3D;1000 type&#x3D;tcp; ​ #interval 检测间隔时间，单位为毫秒 ​ #rise 表示请求2次正常，标记此后端的状态为up ​ #fall 表示请求3次失败，标记此后端的状态为down ​ #type 类型为tcp ​ #timeout 超时时间，单位为毫秒 location &#x2F;upstream_check { ​ check_status; （2）nginx配置文件中调优1、worker_processes 8; nginx 进程数，建议按照cpu 数目来指定，一般为它的倍数 (如,2个四核的cpu计为8)。 2、worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; 为每个进程分配cpu，上例中将8个进程分配到8个cpu，当然可以写多个，或者将一个进程分配到多个cpu。 3、worker_rlimit_nofile 65535; 这个指令是指当一个nginx 进程打开的最多文件描述符数目，理论值应该是最多打开文 件数（ulimit -n）与nginx 进程数相除，但是nginx 分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 4、use epoll; 使用epoll 的I&#x2F;O 模型 六、如何进一步优化负载均衡群集架构？使用外部两台LVS作为接入入口；进入之后再用多台nginx来分担请求 1.在keepalived+nginx的主备容灾高可用的架构中，nginx是作为外部访问系统的唯一入口，理论上一台nginx的最大并发量可以高达50000，但是当并发量更大的时候，keepalived+nginx的高可用机制是没办法满足需求的，因为keepalived+nginx的架构中只有一台nginx在工作（VIP在一台上），只有当master宕机或异常时候，备份机才会上位。那么如何解决更大的高并发问题呢，也许会问能不能搭建nginx集群，直接对外提供访问？ 很显然这是欠妥当的，因为当nginx作为外部的唯一访问入口，没办法直接以集群的形式对外提供服务。但是在内网环境下，是可以用nginx集群（nginx横向扩展服务集合）的，当然总得有一个对外入口，所以需要在nginx集群之上，在加一层负载均衡器（两个LVS)，作为系统的唯一入口。 2.LVS都有哪些模式，你们公司用的哪一种？ DR模式：客户端请求是到负载均衡器，负载均衡器将请求分发给后面的真实服务器之后，服务器直接回应到客户端 优点： DR模式相对于NAT模式，RealServer直接在不经DirectServer的情况下直接将客户端的访问结果返回至给客户端,效率更加高效； 缺点： 所有的RealServer和DirectServer必须在同一局域网中； 除了这个DR模式之外，我还了解过IP隧道模式和NAT模式，不过没有在生产中用过。 IPVS的运行，使用的服务器资源主要是 CPU、内存I&#x2F;O、网络I&#x2F;O；IPVS完全运行在内存中，并且运行在内核态。 当IPVS的应用在DR模式时，既不耗CPU，也不耗I&#x2F;O，运行非常快，所以系统负载非常的低，跟据我的经验，一般负载总是0。所以 LVS 应用对服务器的配置要求非常低。以为 LVS 很重要，所以配置一个相当高端的服务器，实在是一种浪费。 其实我们可以做一下计算： 就是说，当系统当前有100 万连接的时候，才用去内存 208 M，所以 IPVS 的主机，即使是1G的内存，也足以承载负载。 七、Haproxy和LVS&#x2F;NGINX对比（1）LVS在企业应用中康复在能力很强，但存在不足，LVS不支持正则处理，不能实现动静分离，对于大型网站，LVS的实施配置复杂，维护成本相对较高。 （2）haproxy特别适用于那些高负载、访问量很大，但提供高可用性、负载均衡、及TCP（四层）和HTTP（七层）应用的代理的软件的业务应用，不能实现动静分离。 （3）nginx配置简单，可以在实现负载均衡的同时，处理静态页面，分担Tomcat的负担。 LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？ LVS：Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统。 LVS： 是基于四层的转发 Nginx：Nginx是一款轻量级的web 服务器&#x2F;反向代理服务器及电子邮件（IMAP&#x2F;POP3）代理服务器，并在一个BSD-like协议下发行。 Nginx： 是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发 HAproxy：HAproxy是一个使用C语言编写的自由及开放源代码软件[1]，其提供高可用性，负载均衡，以及基于TCP和HTTP的应用程序代理。 HAproxy： 是基于四层和七层的转发，是专业的代理服务器 区别： LVS由于是基于四层的转发所以只能做端口的转发，而基于URL的、基于目录的这种转发LVS就做不了 总结：通过概念方面，我们可以看得出来，都可以提供高可用和负载均衡等的服务，所以从概念方面，没有什么区别 工作选择： HAproxy和Nginx由于可以做七层的转发，所以URL和目录的转发都可以做 在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大 选择HAproxy或者Nginx足已，由于HAproxy由是专业的代理服务器 配置简单，所以中小型企业推荐使用HAproxy","categories":[],"tags":[]},{"title":"","slug":"总结/4、Ansible","date":"2024-12-14T22:02:47.145Z","updated":"2021-01-18T06:48:38.000Z","comments":true,"path":"2024/12/15/总结/4、Ansible/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/4%E3%80%81Ansible/","excerpt":"","text":"[TOC] Ansible一、Ansible配置文件 二、Ansible基本使用步骤： 1、在配置文件中增加需要管理的主机并进行分组； 2、发送ssh密钥至所有主机 命令示例（支持正则表达式）： 12ansible all -m ping ##给所有主机发送ping命令ansible &quot;192.168.73.10:192.168.72.100&quot; -m ping Ansible命令基本格式 Ansible命令执行过程 ansible-vault加密解密yml文件 三、Ansible的模块1、command 和 shellcommand模块和shell模块功能类似，但shell模块的功能更强大 2、script模块——运行shell脚本 3、Copy模块——复制文件到远程主机 4、Fetch模块——从主机提取文件到主控端 5、File模块——设置文件属性 6、unarchive模块——解压缩 7、archive模块——压缩 8、Corn模块——定时任务 9、yum模块——软件包管理 10、service模块——管理服务 11、User模块——管理用户 12、Lineinfile模块——替换 13、Replace——替换 14、Setup模块——收集主机信息 四、Playbook1、命令格式 2、Handler和notify指令notify类似于监控，添加监控项，当监控项发生变化时，通知handler进行操作 3、Tags指令 4、变量 五、Template模板 template功能及用法 六、迭代器——with_item 七、Role——角色","categories":[],"tags":[]},{"title":"","slug":"总结/2、Docker","date":"2024-12-14T22:02:47.138Z","updated":"2021-02-25T07:40:44.000Z","comments":true,"path":"2024/12/15/总结/2、Docker/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/2%E3%80%81Docker/","excerpt":"","text":"[TOC] 一、Docker是1、简介Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的镜像中，然后发布到任何流行的Linux机器或Windows机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 物理机必须要内核3.8以上才能支持docker 2、特点 服务彼此之间相互独立（服务之间的解耦） 服务可以灵活迁移（Docker引擎 docker-ce） 耦合是指两个或两个以上的体系或两种运动形式间通过相互作用而彼此影响以至联合起来的现象。 解耦就是用数学方法将两种运动分离开来处理问题，常用解耦方法就是忽略或简化对所研究问题影响较小的一种运动，只分析主要的运动。（便于管理，防止数据过多的积累在一个文件中） 3、与虚拟机的区别虚拟机容量占用大，容器不需要装系统，占用容量小 虚拟机安全，容器不安全（共享内核资源），攻击一个内核，其他全部瘫痪，可隔开，做资源控制 特性 虚拟机 容器 隔离级别 操作系统级 进程级 系统策略 Hypervisor CGroups 系统资源 5~15% 0~5% 启动时间 分钟级 秒级 镜像存储 GB-TB KB-MB 群集规模 上百 上万 高可用策略 备份、容灾、迁移 弹性、负载、动态 二、Docker三要素●镜像：一个面向Docker容器引擎的只读模板 ●容器：从镜像创建的运行实例 ●仓库：集中保存镜像的地方；分公有和私有仓库 三、Docker基础命令1、docker容器和本地互传文件：本地向docker容器传送文件 docker cp 本机保存文件的全路径 container_id:docker容器内的文件全路径 1docker cp index.jsp 容器id:/usr/local/tomcat/webapps/ROOT docker容器向本机传送文件 docker cp container_id:docker容器内的文件全路径 本机保存文件的全路径 1docker cp 4a2f08d2c1f8:/data1/configure.txt E:\\PHP\\configure.txt 2、镜像操作●查看docker版本：docker version ●搜索nginx镜像（公有仓库)：docker search nginx ●下载nginx镜像：docker pull nginx；下载后存放在&#x2F;var&#x2F;lib&#x2F;docker ●查看镜像列表 docker images #查看下载镜像信息列表 docker inspect nginx:latest #获取镜像详细信息 ●为镜像添加新标签 docker tag nginx:latest nginx:web ●两种方式删除镜像 注意：删除某一个镜像时，只要有容器在使用某一个镜像，必须先删除容器，才能删除镜像。 （1）删除镜像+标签名 （2）删除镜像id 注意：只有当镜像id对应标签仅剩一个时，才能使用镜像id的方式进行删除；否则出现如下报错 或者也可以在最后加上-f选项，一次性删除 ●存出镜像并命名为nginx，存到&#x2F;opt目录下 1docker save -o /opt/nginx.tar nginx:latest ●载入镜像 docker load &lt; &#x2F;opt&#x2F;nginx 3、容器操作●创建容器 12345docker create -it nginx:latest /bin/bash-i：让容器的标准输入保持打开-t：让Docker分配一个伪终端 ●查看容器运行状态 123docker ps -a -a：列出所有的容器，包括未运行的容器Created：已创建； Up：运行中 ●启动执行命令查看系统根目录 一般启动容器流程： 12345（1）docker pull centos --下载镜像（2）docker create -it centos:latest /bin/bash（3）docker start d4a99affa677 通过run命令启动：（先去查找现有的镜像中有没有，没有先下载，再启动） 1docker run centos:latest /usr/bin/bash -c ls / 执行完成会关闭，状态是Exited（容器可以做一次性的处理，处理完就释放资源，做到了最小成本控制） 容器持续在后台执行（通过执行死循环） 1docker run -d centos:latest /bin/bash -c &quot;while true;do echo hello;done&quot; 使用 docker logs 容器id 命令，可以查看容器内的标准输出 ●终止容器运行 1docker stop 0401f589d5ed（CONTAINER ID） ●进入容器（该容器一定要在Up状态） 1docker exec -it 0f0ba9207b21 /bin/bash ●导出容器 1docker export cc4a8b1d428c &gt; /opt/nginx_bak ●导入容器（会生成镜像，而不会创建容器） 1cat /opt/nginx_bak | docker import - nginx:bak ●删除容器（容器必须为停止状态） 123docker stop e885c37fb2ebdocker rm e885c37fb2eb ●批量删除容器 1docker ps -a | awk &#x27;&#123;print &quot;docker rm &quot;$1&#125;&#x27; | bash 1docker ps -a | sed -n &#x27;2,$p&#x27; | awk &#x27;&#123;print &quot;docker rm &quot;$1&#125;&#x27; | bash 四、Docker镜像的构建1、Docker镜像的分层自下而上制作镜像 1.from 后面跟基础镜像 2.add脚本 3.挂载共享空间 数据卷 4.CMD命令执行脚本 2、基于已有镜像容器创建123456789101112131、docker create -it 原镜像名 /bin/bash2、docker commit -m &quot;new&quot; -a &quot;chen&quot; 已有容器id 新镜像名:标签-m：说明信息-a：作者信息-p：生成过程中停止容器的运行docker images | grep 标签 3、基于本地模板创建1.导入本地镜像debian-7.0-x86-minimal.tar.gz 2.cat debian-7.0-x86-minimal.tar.gz | docker import - 镜像名:标签 3.docker images | grep 标签 4、基于Dockerfile创建●Dockerfile是由一组指令组成的文件 ●Dockerfile结构四部分 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令 ●Dockerfile每行支持一条指令，每条指令可携带多个参数，支持使用以“#”号开头的注释 ●Dockerfile操作指令 指令 含义 FROM 镜像 指定新镜像所基于的镜像，第一条指令必须为FROM指令，每创建一个镜像就需要一条FROM指令。 MAINTAINER 名字 说明新镜像的维护人信息 RUN命令 在所基于的镜像上执行命令，并提交到新的镜像中 CMD [“要运行的程序”,”参数1”,”参数2”] 指令启动容器时要运行的命令或者脚本，Dockerfile只能有一条CMD命令，如果指定多条则只能最后一条被执行 EXPOSE 端口号 指定新镜像加载到Docker时要开启的端口（EXPOSE暴露的是容器内部端口，需要再映射到一个外部端口上） ENV 环境变量 变量值 设置一个环境变量的值，会被后面的RUN使用 ADD 源文件&#x2F;目录 目标文件&#x2F;目录 将源文件复制到目标文件（与COPY的区别是将本地tar文件解压到镜像中） COPY 源文件&#x2F;目录 目标文件&#x2F;目录 将本地主机上的文件&#x2F;目录复制到目标地点，源文件&#x2F;目录要与Dockerfile在相同的目录中 VOLUME [“目录”] 在容器中创建一个挂载点（VOLUME是宿主机中的某一个目录挂载到容器中） USER 用户名&#x2F;UID 指定运行容器时的用户 WORKDIR 路径 为后续的RUN、CMD、ENTRYPOINT指定工作目录（WORKDIR类似于cd，但是只切换目录一次，后续的RUN命令就可以写相对路径了） ONBUILD 命令 指定所生成的镜像作为一个基础镜像时所要运行的命令 HEALTHCHECK 健康检查 CMD指令可以指定容器启动时默认执行的命令，但它可以被docker run命令的参数覆盖掉。 ENTRYPOINT 指令和CMD类似，它也是用户指定容器启动时要执行的命令，但如果dockerfile中也有CMD指令，CMD中的参数会被附加到ENTRYPOINT指令的后面。 如果这时docker run命令带了参数，这个参数会覆盖掉CMD指令的参数，并也会附加到ENTRYPOINT 指令的后面。 这样当容器启动后，会执行ENTRYPOINT 指令的参数部分。 可以看出，相对来说ENTRYPOINT指令优先级更高。 对于目录而言，COPY 和 ADD 命令具有相同的特点：只复制目录中的内容而不包含目录自身 CMD和ENTRYPOINT的区别CMD指令可以指定容器启动时默认执行的命令，但它可以被docker run命令的参数覆盖掉。 ENTRYPOINT 指令和CMD类似，它也是用户指定容器启动时要执行的命令，但如果dockerfile中也有CMD指令，CMD中的参数会被附加到ENTRYPOINT指令的后面。 如果这时docker run命令带了参数，这个参数会覆盖掉CMD指令的参数，并也会附加到ENTRYPOINT 指令的后面。 这样当容器启动后，会执行ENTRYPOINT 指令的参数部分。 可以看出，相对来说ENTRYPOINT指令优先级更高。 优先级：ENTRYPOINT&gt;CMD&gt;docker run Dockerfile文件生成apache镜像实例：12345678910111213141516171.vim Dockerfile （Dockerfile名字不可更改）#新镜像基于的基础镜像（基础镜像未下载会先下载）FROM centos:7#维护镜像的用户信息MAINTAINER This is chen#镜像操作指令安装apache软件RUN yum -y updateRUN yum -y install httpd#开启80端口EXPOSE 80#复制网址首页文件ADD index.html /var/www/html/index.html#将执行脚本复制到镜像中ADD run.sh /run.shRUN chmod 755 /run.sh#启动容器时执行脚本CMD [&quot;/run.sh&quot;] 此处注意一个细节：每加载一步会生成一个临时的容器，加载完后会删除 2.vim run.sh #和Dockerfile文件位于相同目录下 1234567#!/bin/bashrm -rf /run/httpd/* #删除进程文件exec /usr/sbin/apachectl -D FOREGROUND #启动apache 3.vim index.html #编辑首页文件 this is web 4.生成镜像 1docker build -t httpd:test . （注意别忘了末尾有&quot;.&quot;） 5.新镜像运行容器 1docker run -d -p 1216:80 httpd:test -p：映射到宿主机指定端口 -P：映射到宿主机随机端口 6.测试容器是否成功运行 http://14.0.0.10:1126/ 五、Docker的四种网络模式https://blog.csdn.net/lilygg/article/details/88616218 1、实现原理Docker使用Linux桥接，在宿主机虚拟一个Docker容器网桥(docker0)，Docker启动一个容器时会根据Docker网桥的网段分配给容器一个IP地址，称为Container-IP，同时Docker网桥是每个容器的默认网关。因为在同一宿主机内的容器都接入同一个网桥，这样容器之间就能够通过容器的Container-IP直接通信。 Docker网桥是宿主机虚拟出来的，并不是真实存在的网络设备，外部网络是无法寻址到的，这也意味着外部网络无法通过直接Container-IP访问到容器。如果容器希望外部访问能够访问到，可以通过映射容器端口到宿主主机（端口映射），即docker run创建容器时候通过 -p 或 -P 参数来启用，访问容器的时候就通过[宿主机IP]:[容器端口]访问容器。 2、网络详解 123456789[root@localhost ~]# docker network lsNETWORK ID NAME DRIVER SCOPEf9ad4320a5f2 bridge bridge local894917639bf3 host host local39da54945dad none null local #安装docker时，它会自动创建三个网络，bridge（创建容器默认连接到该网络）、none和host Docker网络模式 配置 说明 host模式 –net&#x3D;host 容器和宿主机共享Network namespace。 container模式 –net&#x3D;container:NAME_or_ID 容器和另外一个容器共享Network namespace。 kubernetes中的pod就是多个容器共享一个Network namespace。 none模式 –net&#x3D;none 容器有独立的Network namespace，但并没有对其进行任何网络设置，如分配veth pair 和网桥连接，配置IP等。 bridge模式 –net&#x3D;bridge （默认为该模式） 1）、host模式容器将不会获得一个独立的Network Namespace（网络命令空间），而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口，（也就是说如果容器是个web，那直接访问宿主机:端口，不需要做NAT转换，跟在宿主机跑web一样。容器中除了网络，其他都还是隔离的。） 使用host模式的容器可以直接使用宿主机的IP地址与外界通信，容器内部的服务端口也可以使用宿主机的端口，不需要进行NAT，host最大的优势就是网络性能比较好，但是docker host上已经使用的端口就不能再用了，网络的隔离性不好。 2）、Container模式这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信 3）、none模式使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。该模式关闭了容器的网络功能 这种网络模式下容器只有lo回环网络，没有其他网卡。none模式可以在容器创建时通过–network&#x3D;none来指定。这种类型的网络没有办法联网，封闭的网络能很好的保证容器的安全性。 4）、bridge模式此模式会为每一个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，通过docker0网桥以及iptables nat表配置与宿主机通信 ##当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。 从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。 bridge模式是docker的默认网络模式，不写–net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。 3、自定义网络如何配置#创建容器时默认使用的是桥接模式，但是使用bridge不支持为容器指定IP 12345[root@localhost ~]# docker run -itd --name test1 --network bridge --ip 172.17.0.10 centos:7 /bin/bash20dc45293929f81013a60391bef2626f581a8d3d4f29b8a87ac8b1f9b585ab2adocker: Error response from daemon: user specified IP address is supported on user defined networks only. #提示想要为容器指定IP只能在用户自定义的网络中才行 #配置自定义固定IP 12345[root@localhost ~]# docker network create --subnet=172.31.0.0/24 test #创建自定义网络test[root@localhost ~]# docker run -itd --name web1 --net test --ip 172.31.0.10 centos:7 /bin/bash ​ #创建一个容器，指定网络为test，指定IP地址172.31.0.10 六、Docker数据管理1、为什么要进行数据管理操作●方便查看容器内产生的数据 ●多容器间实现数据共享 例如：需要给多个容器中的网站站点上传网页内容时，可以高效的部署网页。 2、两种管理方式●数据卷 数据卷是容器和宿主之间的数据共享 ●数据卷容器 数据卷容器是容器和容器之间的数据共享 3、数据卷操作实例1.将宿主机目录中的&#x2F;var&#x2F;www挂载到容器中的&#x2F;data1中（如果目录不存在都会自动创建） 1docker run -v /var/www:/data1 --name web1 -it centos:7 /bin/bash 2.在容器中&#x2F;data1目录下创建文件进行测试 123[root@bb58030283e7 /]# cd /data1/[root@bb58030283e7 data1]# touch 123.txt 3.回到宿主机&#x2F;var&#x2F;www查看 12345[root@node1 ~]# cd /var/www/[root@node1 www]# ls123.txt 4、数据卷容器操作实例1.创建数据卷容器web10 1docker run --name web10 -v /data1 -v /data2 -it centos:7 /bin/bash 2.新容器web100挂载数据卷容器web10 123docker run -it --volumes-from web10 --name web100 centos:7 /bin/bash（web100容器会自动关联web10容器中的数据卷） 3.在新容器web100的&#x2F;data1中创建文件进行测试 12345[root@2ad42960c2aa data1]# cd /data1/[root@2ad42960c2aa data1]# ls[root@2ad42960c2aa data1]# touch 1.txt 4.回到数据卷容器web10的&#x2F;data1中查看 12345[root@b10f5d5ae9d5 /]# cd data1/[root@b10f5d5ae9d5 data1]# ls1.txt 5、私有仓库建立步骤1、下载registry镜像 2、客户端设置daemon.json文件，指定私有仓库位置； 1&quot;insecure-registries&quot;: [&quot;14.0.0.10:5000&quot;], 3、生成registry容器，开放5000端口 1docker run -d -p 5000:5000 -v /registry:/data/registry registry 4、给要上传的镜像打上标签 5、上传镜像 6、获取私有仓库列表查看是否上传成功 7、测试私有仓库下载镜像 #–privileged 让容器内的root拥有真正的root权限。否则，container内的root只是外部的一个普通用户权限。 1[root@localhost systemctl]# docker run --privileged -it -v /sys/fs/cgroup:/sys/fs/cgroup:ro 七、Docker Compose容器编排1、Docker Compose简介●一个定义及运行多个Docker容器的工具 ●Docker Compose非常适合组合使用多个容器进行开发的场景 3.2Docker Compose文件格式及编写注意事项 ●YAML是一种标记语言很直观的数据序列化格式 ●文件格式及编写注意事项 不支持制表符tab键缩进，需要使用空格缩进 通常开头缩进2个空格 字符后缩进1个空格，如冒号、逗号、横杆 用#号注释 如果包含特殊字符用单引号引起来 布尔值必须用引号括起来 2、Compose命令说明●基本的使用格式 1docker-compose [options] [COMMAND] [ARGS] ●docker-compose选项 1234567--verbose 输出更多调试信息--version 打印版本并退出-f，--file FILE 使用特定的compose模板文件，默认为docker-compose.yml-p，--project-name NAME 指定项目名称，默认使用目录名称 3、compose部署#以docker-ce为基础 上传docker-compose命令包到&#x2F;usr&#x2F;local&#x2F;bin目录下 1chmod +x /usr/local/bin/docker-compose #在&#x2F;root目录下创建compose_tomcat目录 [root@localhost ~]# mkdir compose_tomcat&#x2F; 目录结构如下： docker-compose.yml配置文件详解：https://www.jianshu.com/p/2217cfed29d7 一份标准配置文件应该包含 version、services、networks 三大部分，其中最关键的就是 services 和 networks 两个部分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@localhost compose_tomcat]# vim docker-compose.ymlversion: &#x27;3&#x27;services: tomcat:​ hostname: tomcat​ build: #基于一份 Dockerfile创建容器​ context: ./tomcat​ dockerfile: Dockerfile​ ports:​ - 8080:8080​ networks:​ - tomcat​ volumes:​ - ./wwwroot:/usr/local/tomcat/webapps/ROOT #注意tomcat站点位置 nginx:​ hostname: nginx​ build:​ context: ./nginx​ dockerfile: Dockerfile​ ports:​ - 1216:80​ - 1226:443​ networks:​ - nginx​ volumes:​ - ./nginxroot:/usr/local/nginx/htmlnetworks: tomcat: nginx:[root@localhost compose_tomcat]# docker-compose -f docker-compose.yml up -d 八、Harbor私有仓库Harbor私有仓库部署与管理 Harbor的每个组件都是以Docker容器的形式构建的，使用docker-compose来对它进行部署 Docker harbor有可视化的web管理界面，可以方便管理Docker镜像，又提供了多个项目的镜像权限管理及控制功能 使用 Docker 命令在本地通过 127.0.0.1 来登录和推送镜像 默认情况下，Register 服务器在端口 80 上侦听。 &#x2F;&#x2F;登录 1docker login -u admin -p Harbor12345 http://127.0.0.1 &#x2F;&#x2F;下载镜像进行测试 下载镜像进行测试 1docker pull nginx &#x2F;&#x2F;镜像打标签 （网页上也会有提示、模板） 1docker tag nginx 127.0.0.1/myimages/nginx:v1 &#x2F;&#x2F;上传镜像到 上传镜像到 Harbor 1docker push 127.0.0.1/myimages/nginx:v1 使用Harbor仓库时遇到的故障： 以上操作都是在 Harbor 服务器本地操作。如果其他客户端上传镜像到 Harbor，就会报 如下错误。 出现这问题的原因 Docker Registry 交互默认使用的是 HTTPS，但是搭建私有镜像默认使用的是 HTTP 服务，所以与私有镜像交互时出现以下错误。 12345[root@localhost ~]# docker login -u admin -p Harbor12345 http://14.0.0.20WARNING! Using --password via the CLI is insecure. Use --password-stdin.Error response from daemon: Get https://14.0.0.20/v2/: dial tcp 14.0.0.20:443: connect: connection refused 如何解决： 1234567[root@client ~]# vim /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --insecure-registry 14.0.0.20 -- containerd=/run/containerd/containerd.sock[root@client ~]# systemctl daemon-reload [root@client ~]# systemctl restart docker 要更改 Harbor 的配置文件时，请先停止现有的 Harbor 实例并更新 Harbor.cfg；然后运行 prepare 脚本来填充配置；最后重新创建并启动 Harbor 的实例。 1.停止现有的 Harbor 实例 12345678910111213docker-compose down -v[root@localhost harbor]# pwd/usr/local/harbor[root@localhost harbor]# lscommon docker-compose.yml harbor.v1.2.2.tar.gz NOTICEdocker-compose.clair.yml harbor_1_1_0_template install.sh preparedocker-compose.notary.yml harbor.cfg LICENSE upgrade 2.更新 Harbor.cfg 1[root@localhost harbor]# vim Harbor.cfg 3.运行 prepare 脚本来填充配置 1[root@localhost harbor]# ./prepare 4.重新创建并启动 Harbor 的实例 如果出现如下报错： docker-compose up -d 九、Docker consul容器服务更新与发现1、consul的介绍由HashiCorp公司使用go语言开发的一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件（开源）。 主要特点： 服务发现及配置； 支持健康检查，并且运行HTTP、GTPC和DNS协议调用API存储键值对； 采用Raft算法，保证服务的高可用 支持安全服务通信； 支持多数据中心； 2、consul agentconsul通过agent来运行的，agent分为server 和client两种类型，这两种类型基本上没有什么区别，server agent是将服务的消息存储，一般来说为了防止单点故障推荐使用3到5个来构建集群架构。 而client agent主要用于注销服务、健康检查及转发server agent的查询等，相当于一个代理，因此它必须要在集群的每台主机上运行。 一种服务或软件工具的产生必然有其使用场景和其优势，否则哪有其立足之地？ 3、consul使用的场景 Docker 容器的注册与配置共享 Coreos 实例的注册与配置共享 SaaS 应用的配置共享、服务发现和健康检查。 vitess 集群 与 confd 服务集成，动态生成 nginx 和 haproxy 配置文件 4、Docker Consul容器服务更新与发现原理 12341. Consul Cluster由部署和运行了Consul Agent的节点组成。 在Cluster中有两种角色:Server和 Client。2. Server和Client的角色和Consul Cluster上运行的应用服务无关, 是基于Consul层面的一种角色划分.3. Consul Server: 用于维护Consul Cluster的状态信息， 实现数据一致性， 响应RPC请求。官方建议是: 至少要运行3个或者3个以上的Consul Server。 多个server之中需要选举一个leader, 这个选举过程Consul基于Raft协议实现. 多个Server节点上的Consul数据信息保持强一致性。 在局域网内与本地客户端通讯，通过广域网与其他数据中心通讯。Consul Client: 只维护自身的状态, 并将HTTP和DNS接口请求转发给服务端。4. Consul 支持多数据中心， 多个数据中心要求每个数据中心都要安装一组Consul cluster，多个数据中心间基于gossip protocol协议来通讯， 使用Raft算法实现一致性 5、工作流程：当后面容器增加时，registrator注册容器中的服务—》通知consul server更新—》consul template模板进行更新，自动修改nginx.conf中的upstream参数 Registrator监控新建的Docker容器，并且检查判定这些容器提供的服务。从我们的目的出发，任何监听在某个端口的程序都是服务。Registrator在容器内发现的任务服务，都将被添加到一个服务注册端，比如Consul或etcd 准备template nginx模板文件，参数以变量形式写入 在consul服务器节点上操作 1234567891011[root@localhost consul]# vim /root/consul/nginx.ctmplupstream http-server &#123; &#123;&#123;range service &quot;nginx&quot;&#125;&#125; server &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125;; &#123;&#123;end&#125;&#125;&#125; consul-template是一个守护进程，用于实时查询consul集群信息，并更新文件系统上任意数量的指定模板，生成配置文件。更新完成后，可以选择运行shell命令执行更新操作，重新加载nginx。这种强大的抽象功能和查询语言模板可以使consul-template特别适合动态的创建配置文件。例如：创建nginx反向代理。 十、资源分配1、为什么要做资源分配？容器和虚拟机的区别： 虚拟机不需要做，因为虚拟机在创建的时候已经做了资源分配（配额），（虚拟CPU,虚拟内存,虚拟磁盘等） 而容器共享内核资源，所以需要做Cgroup，可以按照往年监控的数据，查看cpu等资源的耗用情况来进行分配 2、Cgroup资源配置方法Docker是通过Cgroup来控制容器使用的资源配额，包括CPU、内存、磁盘i&#x2F;o三大方面，基本覆盖了常见的资源配额和使用量控制。 Cgroup是Control Groups的缩写，是Linux内核提供的一种可以限制、记录、隔离进程组所使用的物理资源（如CPU、内存、磁盘IO等）的机制，被docker等很多项目用于实现进程资源控制。Cgroup本身是提供将进程进行分组化管理的功能和接口的基础结构，I&#x2F;O或内存的分配控制等具体的资源管理功能。这些具体的资源管理功能称为Cgroup子系统，有以下几大子系统实现： blkio：设置限制每个块设备的输入输出控制。例如：磁盘，usb等 CPU：使用调度程序为cgroup任务提供CPU的访问。 cpuacct：产生cgroup任务的CPU资源报告。 cpuset：如果是多核心的cpu，这个子系统会为cgroup任务分配单独的CPU和内存。 devices：允许或拒绝cgroup任务对设备的访问。 freezer：暂停和恢复cgroup任务。 memory：设置每个cgroup的内存限制以及产生内存资源报告。 net_cls：标记每个网络包以供cgroup方便使用。 ns：命名空间子系统。 perf_event：增加了对每个group的监测跟踪的能力，可以监测属于某个特定的group的所有线程以及运行在特定CPU上的线程。 3、使用stress工具测试CPU和内存#使用Dockerfile来创建一个基于Centos的stress工具镜像 12345678910111213141516171819[root@localhost ~]# mkdir stress[root@localhost ~]# vim stress/Dockerfile[root@localhost ~]# cd stress/[root@localhost stress]# vim DockerfileFROM centos:7MAINTAINER chenRUN yum -y install wgetRUN wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoRUN yum -y install stress[root@localhost stress]# docker build -t centos:stress . #使用如下命令创建容器，命令中的–cpu-shares参数值不能保证可以获得1个vcpu或者多少GHz的CPU资源，它仅是一个弹性的加权值。 1[root@localhost ~]# docker run -itd --cpu-shares 100 centos:stress 说明：默认情况下，每个Docker容器的CPU份额都是1024。单独一个容器的份额是没有意义的。只有在同时运行容器时，容器的cpu加权的效果才能显现。 例如：两个容器A、B的cpu份额分别为1000和500，在cpu进行实际片分配的时候，容器A比容器B多一倍的机会获得cpu的时间片。但分配的结果取决于当时主机和其他容器的运行状态，实际上也无法保证容器A一定能获得cpu时间片。比如容器A的进程一直是空闲的，那么容器B是可以获取比容器A更多的cpu时间片的。极端情况下，例如主机上只运行了一个容器，即使它的cpu份额只有50，它也可以独占整个主机的cpu。 例如：cpu时间片：1秒 ​ 容器A：50% 0.5秒 ​ 容器B：25% 0.25秒 ​ 容器C：25% 0.25秒 平均值层面：CPU给A容器充0.5秒，给B容器充0.25秒 #可以通过cpu share可以设置容器使用cpu的优先级，比如启动了两个容器及运行查看cpu使用百分比。 1[root@localhost stress]# docker run -itd --name cpu1024 --cpu-shares 1024 centos:stress stress -c 10 ​ #stress -c 10：容器产生10个子函数进程 123[root@localhost stress]# docker exec -it dd0e42e372ff /bin/bash[root@dd0e42e372ff /]# top 12345[root@localhost ~]# docker run -itd --name cpu512 --cpu-shares 512 centos:stress stress -c 10[root@localhost ~]# docker exec -it ddfccf1cac40 /bin/bash[root@ddfccf1cac40 /]# top #可以发现两个容器cpu使用率是2:1 4、CPU周期限制Docker提供了–cpu-period、–cpu-quota两个参数控制容器可以分配到的CPU时钟周期。 –cpu-period是用来指定容器对CPU的使用要在多长时间内做一次重新分配。 –cpu-quota是用来指定在这个周期内，最多可以有多少时间来跑这个容器。 与–cpu-shares不同的是，这种配置是指定一个绝对值，容器对CPU资源的使用绝对不会超过配置的值。 cpu-period和cpu-quota的单位为微秒（μs）。cpu-period的最小值为1000微秒，最大值为1秒，默认值为0.1秒（100000μs） cpu-quota的值默认为-1，表示不做控制。cpu-period和cpu-quota参数一般联合使用。 例如：容器进程需要每1秒使用单个cpu的0.2秒时间，可以将cpu-period设置为1000000即1秒，cpu-quota设置为200000（0.2秒）。 在多核情况下，如果允许容器进程完全占用两个cpu，则可以将cpu-period设置为100000即0.1秒，cpu-quota设置为200000即0.2秒 1234567891011121314151617181920212223242526272829303132333435[root@localhost ~]# docker run -itd --cpu-period 100000 --cpu-quota 200000 centos:stress[root@localhost ~]# docker exec -it 16b6689aabc6 /bin/bash[root@16b6689aabc6 /]# cd /sys/fs/cgroup/[root@16b6689aabc6 cgroup]# lsblkio cpuacct freezer net_cls perf_eventcpu cpuset hugetlb net_cls,net_prio pidscpu,cpuacct devices memory net_prio systemd[root@16b6689aabc6 cgroup]# cd cpu[root@16b6689aabc6 cpu]# lscgroup.clone_children cpu.rt_period_us cpuacct.usagecgroup.event_control cpu.rt_runtime_us cpuacct.usage_percpucgroup.procs cpu.shares notify_on_releasecpu.cfs_period_us cpu.stat taskscpu.cfs_quota_us cpuacct.stat[root@16b6689aabc6 cpu]# cat cpu.cfs_period_us100000[root@16b6689aabc6 cpu]# cat cpu.cfs_quota_us200000 5、CPU Core控制对多核CPU的服务器，Docker还可以控制容器运行使用哪些CPU内核，即使用–cpuset-cpus参数。这对具有多CPU的服务器尤其有用，可以对需要高性能计算的容器进行性能最优的配置。 1[root@localhost ~]# docker run -itd --name cpu1 --cpuset-cpus 1-2 centos:stress #执行以上命令表示创建的容器只能用1、2两个cpu。最终生成的cgroup的cpu内核配置如下： 1234567891011[root@localhost ~]# docker exec -it 75be98d74dcc /bin/bashtop - 07:34:23 up 45 min, 0 users, load average: 0.00, 0.01, 0.04[root@75be98d74dcc /]# cat /sys/fs/cgroup/cpuset/cpuset.cpus #cpuset：cpu集合1-2[root@75be98d74dcc /]# stress -c 5 &amp; #让容器产生5个子函数进程，并在后台运行[root@75be98d74dcc /]# top #使用top命令查看cpu工作情况（top进去后按1，显示每个cpu的工作情况） #通过下面指令可以看到容器中进程与cpu内核的绑定关系 123[root@localhost ~]# docker exec 75be98d74dcc taskset -c -p 1 #-p 1 表示容器中第一个进程pid为1被绑定到cpu1和2上pid 1&#x27;s current affinity list: 1,2 6、CPU配额控制参数的混合使用通过cpuset-cpus参数指定容器A使用cpu内核0，容器B只是用CPU内核1. 在主机上只有这两个容器使用对应cpu内核的情况，它们各自占用全部的内核资源，cpu-shares没有明显效果。 cpuset-cpus、cpuset-mems参数只在多核、多内存节点上的服务器上有效，并且必须与实际的物理配置匹配，否则也无法达到资源控制的目的。 在系统具有多个cpu内核的情况下，需要通过cpuset-cpus参数为容器设置cpu内核才能方便的进行测试。 #创建容器cpu3，仅使用cpu0核心，加权值为512 123[root@localhost ~]# docker run -itd --name cpu3 --cpuset-cpus 0 --cpu-shares 512 centos:stress stress -c 1[root@localhost ~]# docker exec -it 4eb80db7a397 bash #创建容器cpu4，仅使用cpu0核心，加权值为1024 123[root@localhost ~]# docker run -itd --name cpu4 --cpuset-cpus 0 --cpu-shares 1024 centos:stress stress -c 1[root@localhost ~]# docker exec -it 327038e98aa4 bash 7、内存限额与操作系统类似，容器可使用的内存包括两部分：物理内存和Swap Docker通过下面两组参数来控制容器内存的使用量。 -m或–memory：设置内存的使用限额，例如100M、1024M –memory-swap：设置内存+swap的使用限额 执行如下命令允许该容器最多使用200M的内存和300M的swap [root@localhost ~]# docker run -it -m 200M –memory-swap&#x3D;300M progrium&#x2F;stress –vm 1 –vm-bytes 280M –vm 1：启动1个内存工作线程 –vm-bytes 280M：每个线程分配280M内存 默认情况下，容器可以使用主机上的所有空闲内存。 与cpu的cgroups配置类似，Docker会自动为容器在目录&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;docker&#x2F;&lt;容器的完整长id&gt;中创建相应cgroup配置文件 注意：如果让工作线程分配的内存超过300M，分配的内存超过限额，stress线程报错，容器退出。 [root@localhost ~]# docker run -it -m 200M –memory-swap&#x3D;300M progrium&#x2F;stress –vm 1 –vm-bytes 310M 8、bps和iops的限制bps是byte per second，每秒读写的数据量。 iops是io per second，每秒io的次数。 可通过以下参数控制容器的bps和iops： –device-read-bps，限制读某个设备的bps。 –device-write-bps，限制写某个设备的bps。 –device-read-iops，限制读某个设备的iops。 –device-write-iops，限制写某个设备的iops。 示例：限制容器写&#x2F;dev&#x2F;sda的速率为5MB&#x2F;s。 123456789[root@localhost ~]# docker run -it --device-write-bps /dev/sda:5MB centos:stress[root@7675c030fd53 /]# dd if=/dev/zero of=test bs=1M count=1024 oflag=direct^C22+0 records in22+0 records out23068672 bytes (23 MB) copied, 4.40131 s, 5.2 MB/s #通过dd命令测试在容器中写磁盘的速度是否为5MB&#x2F;s。因为容器的文件系统是在主机&#x2F;dev&#x2F;sda上的，在容器中写文件相当于对主机&#x2F;dev&#x2F;sda进行写操作。另外，oflag&#x3D;direct指定用direct IO方式写文件，这样 –device-write-bps才能生效。 结果表明限速5MB&#x2F;s左右。作为对比测试，如果不限速，结果如下： 123456789[root@server ~]# docker run -it centos:stress[root@07b87cdda205 /]# dd if=/dev/zero of=test bs=1M count=1024 oflag=direct1024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 1.01479 s, 1.1 GB/s 十一、Docker-TLS加密通讯1、使用TLS加密通讯原因为了防止链路劫持、会话劫持等问题导致Docker通信时被中间人攻击，c&#x2F;s两端应该通过加密方式通讯。 2、基础知识1.对称密钥，例如DES、3DES、AES，长度不同，长度越长安全越高，解密速度越慢。 2.非对称密钥，分为公钥和私钥，例如RSA 公钥：所有人可知（锁），私钥（钥匙）个人身份信息，不可抵赖。 3.封装在证书中：个人信息，密钥，有效期 4.ca：证书颁发机构 ca证书 密钥key–》身份签名（csr）–》服务器&#x2F;客户端（结合）制作证书pem 证书pem发送给客户端，客户端通过证书验证才能访问容器 3、TLS加密通讯部署过程：1.修改服务器主机名为server，并添加到本地解析文件 1234567[root@localhost ~]# hostnamectl set-hostname server[root@localhost ~]# su[root@server ~]# vim /etc/hosts127.0.0.1 server 2.创建ca密钥（ca-key.pem） 12345678910111213[root@server ~]# openssl genrsa -aes256 -out ca-key.pem 4096 #256为密钥长度；4096为字节数Generating RSA private key, 4096 bit long modulus.......++...............++e is 65537 (0x10001)Enter pass phrase for ca-key.pem: #输入密码123123（自定义）Verifying - Enter pass phrase for ca-key.pem: #确认密码123123 3.创建ca根证书文件（ca.pem） 123[root@server ~]# openssl req -new -x509 -days 1000 -key ca-key.pem -sha256 -subj &quot;/CN=*&quot; -out ca.pem #req：签名；x509：国际标准；sha256：指定哈希256位加密算法；subj：项目名称Enter pass phrase for ca-key.pem: #输入123123 -——————————————————————————————————————————- 4.创建服务器私钥 12345678910111213141516[root@server ~]# openssl genrsa -out server-key.pem 4096 #genrsa：非对称密钥Generating RSA private key, 4096 bit long modulus.....................................................................................++..............................................................++e is 65537 (0x10001)[root@server ~]# lsanaconda-ks.cfg ca.pem server-key.pem 公共 视频 文档 音乐ca-key.pem initial-setup-ks.cfg stress 模板 图片 下载 桌面 5.签名私钥 1[root@server ~]# openssl req -subj &quot;/CN=*&quot; -sha256 -new -key server-key.pem -out server.csr 6.使用ca证书与私钥证书签名 1234567891011121314151617[root@server ~]# openssl x509 -req -days 1000 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pemSignature oksubject=/CN=*Getting CA Private KeyEnter pass phrase for ca-key.pem: #输入123123[root@server ~]# lsanaconda-ks.cfg ca.srl server.csr 公共 图片 音乐ca-key.pem initial-setup-ks.cfg server-key.pem 模板 文档 桌面ca.pem server-cert.pem stress 视频 下载 -————————————————————————————————————————— 7.生成客户端密钥 123456789[root@server ~]# openssl genrsa -out key.pem 4096Generating RSA private key, 4096 bit long modulus....................................++.......................................................................++e is 65537 (0x10001) 8.签名客户端 123[root@server ~]# openssl req -subj &quot;/CN=client&quot; -new -key key.pem -out client.csr 9.创建配置文件 123[root@server ~]# echo extendedKeyUsage=clientAuth &gt; extfile.cnf 10.签名证书，输入123123，需要（签名客户端，ca证书，ca密钥） 12345678910111213[root@server ~]# openssl x509 -req -days 1000 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile.cnfSignature oksubject=/CN=clientGetting CA Private KeyEnter pass phrase for ca-key.pem:\\--------------------------------------------------------------------------------------------------------------------------------- 11.删除多余文件 1[root@server ~]# rm -rf ca.srl client.csr extfile.cnf server.csr 12.修改docker服务文件文件 1[root@server ~]# vim /lib/systemd/system/docker.service 123456789101112131415[root@server ~]# mkdir /tls[root@server ~]# mv ca.pem /tls[root@server ~]# mv server-cert.pem /tls[root@server ~]# mv server-key.pem /tls/[root@server ~]# mv cert.pem /tls/[root@server ~]# mv key.pem /tls/[root@server ~]# ls /tls/ca.pem cert.pem key.pem server-cert.pem server-key.pem 13.重载进程，重启docker服务 123[root@server ~]# systemctl daemon-reload[root@server ~]# systemctl restart docker 14.将&#x2F;tls目录下的ca.pem、cert.pem、key.pem三个文件复制给客户端 12345[root@server tls]# scp ca.pem root@14.0.0.30:/etc/docker/[root@server tls]# scp cert.pem root@14.0.0.30:/etc/docker/[root@server tls]# scp key.pem root@14.0.0.30:/etc/docker/ -———————————————————————————————————————————- 15.到客户端14.0.0.30进行测试 12345678910111213[root@localhost docker]# vim /etc/hosts加入：14.0.0.20 server[root@localhost ~]# cd /etc/docker/ #注意要切换到服务端传证书文件的目录下[root@localhost docker]# docker --tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem -H tcp://server:2376 ps -a #测试成功，成功访问服务端容器CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES327038e98aa4 centos:stress &quot;stress -c 1&quot; About an hour ago Exited (137) About an hour ago cpu44eb80db7a397 centos:stress &quot;stress -c 1&quot; About an hour ago Exited (137) About an hour ago cpu3 十二、故障集1、centos下执行命令service network restart时，抛出主进程被占据的错原因：从镜像创建容器时，通过以下命令创建 docker run -itd –name centos-test centos &#x2F;bin&#x2F;bash 这时，进程（PID：1）会被&#x2F;bin&#x2F;bash占据。 对策：通过以下命令创建容器 docker run -itd –name centos-test –privileged&#x3D;true centos &#x2F;usr&#x2F;sbin&#x2F;init 2、Goland使用Docker插件，连接Docker失败原因：没有开启Docker的远程访问 对策：如下图，Expose daemon on tcp:&#x2F;&#x2F;localhost:2375 without TLS处打勾 3、将要部署的项目创建容器时无法访问数据库原因：docker是一个虚拟容器，其localhost与主机的localhost是不同的，不能互相访问。 对策：工程总配置文件的数据源要填写真实的IP地址。如果是开发环境的话，推荐使用【host.docker.internal】。这个DNS名称将被解析到主机的IP地址，省去了不断更换IP的麻烦。 十三、面试题1、什么Docker？答：Docker是一个容器化平台，它以容器的形式将您的应用程序及其所有依赖项打包在一起，以确保您的应用程序在任何环境中无缝运行。 2、Docker与虚拟机的不同点在哪里？答：Docker不是虚拟化方法。它依赖于实际实现基于容器的虚拟化或操作系统级虚拟化的其他工具。为此，Docker最初使用LXC驱动程序，然后移动到libcontainer现在重命名为runc。Docker主要专注于在应用程序容器内自动部署应用程序。应用程序容器旨在打包和运行单个服务，而系统容器则设计为运行多个进程，如虚拟机。因此，Docker被视为容器化系统上的容器管理或应用程序部署工具。 3、Docker容器有几种状态？答：有四种状态：运行、已暂停、重新启动、已退出。 4、Dockerfile中最常见的指令是什么？答：FROM：指定基础镜像；LABEL：功能是为镜像指定标签；RUN：运行指定的命令；CMD：容器启动时要运行的命令。 5、Dockerfile中的命令COPY和ADD命令有什么区别？答：一般而言，虽然ADD并且COPY在功能上类似，但是首选COPY。 那是因为它比ADD更易懂。COPY仅支持将本地文件复制到容器中，而ADD具有一些功能（如仅限本地的tar提取和远程URL支持），这些功能并不是很明显。因此，ADD的最佳用途是将本地tar文件自动提取到镜像中，如ADD rootfs.tar.xz &#x2F;。 6、什么是Docker镜像？答：Docker镜像是Docker容器的源代码。换句话说，Docker镜像用于创建容器。使用build命令创建镜像，并且在使用run启动时它们将生成容器。镜像存储在Docker注册表中，registry.hub.docker.com因为它们可能变得非常大，镜像被设计为由其他镜像层组成，允许在通过网络传输镜像时发送最少量的数据。 7、解释基本的Docker使用工作流程是怎样的？答：（1）从Dockerfile开始，Dockerfile是镜像的源代码；（2）创建Dockerfile后，可以构建它以创建容器的镜像。图像只是“源代码”的“编译版本”，即Dockerfile；（3）获得容器的镜像后，应使用注册表重新分发容器。注册表就像一个git存储库，可以推送和拉取镜像；接下来，可以使用该图像来运行容器。在许多方面，正在运行的容器与虚拟机（但没有虚拟机管理程序）非常相似。 8、如何在生产中监控Docker？答：Docker提供docker stats和docker事件等工具来监控生产中的Docker。我们可以使用这些命令获取重要统计数据的报告。 Docker统计数据：当我们使用容器ID调用docker stats时，我们获得容器的CPU，内存使用情况等。它类似于Linux中的top命令。 Docker事件：Docker事件是一个命令，用于查看Docker守护程序中正在进行的活动流。一些常见的Docker事件是：attach，commit，die，detach，rename，destroy等。 9、Docker如何在非Linux系统中运行容器？答：通过添加到Linux内核版本2.6.24的名称空间功能，可以实现容器的概念。容器将其ID添加到每个进程，并向每个系统调用添加新的访问控制检查。它由clone（）系统调用访问，该调用允许创建先前全局命名空间的单独实例。 10、什么类型的应用程序无状态或有状态更适合Docker容器？答：最好为Docker Container创建无状态应用程序。我们可以从应用程序中创建一个容器，并从应用程序中取出可配置的状态参数。现在我们可以在生产环境和具有不同参数的QA环境中运行相同的容器。这有助于在不同场景中重用相同的镜像。另外，无状态应用程序比有状态应用程序更容易使用Docker容器进行扩展。 11、你常用的docker命令有哪些？拉取镜像：docker pull IMAGE（镜像名） 例如：docker pull nginx 部署启动容器：docker run –name NAME(容器名) -p PORTS(要使用的端口) IMAGE(镜像名) 查看容器的运行状态：docker ps -a (该命令可以查看容器是否运行，及已创建的容器名称） 进入运行中的容器： docker exec -it CONTAINER_ID&#x2F;NAME(容器id或者是容器名) bash eg:docker exec -it sc_nginx2 &#x2F;bin&#x2F;bash -i ：交互式进入容器 -t : 开启一个终端 sc_nginx : 是容器名 &#x2F;bin&#x2F;bash : 进入容器的程序 查看docker里有哪些镜像：docker images 删除&#x2F;启动&#x2F;停止 容器 ：docker rm&#x2F;start&#x2F;stop CONTAINER_ID&#x2F;NAME 启动docker服务：systemctl start docker 设置开机启动：systemctl enable docker 12、Dockerfile是什么？Dockerfile里有哪些内容？Dockerfile 是用来构建自定义镜像的文本文档 Dockerfile 里包含的信息主要有： 基础镜像 镜像元信息 镜像操作指令 容器启动时执行的命令 -———————————————————- 具体指令： 1 FROM IMAGE(镜像名):version （版本） 基础镜像 eg: FROM mysql:5.6 eg: FORM python （不指定版本，默认最新版本） 2 ENV 用来设置环境变量 eg：ENV NAME World 3 WORKDIR:用来指定工作目录 eg: WORKDIR &#x2F;app 4 ADD 用于将本地文件添加到镜像中 eg: ADD .&#x2F;app 5 CMD 构建容器后执行的命令，即在容器启动时执行的命令 eg: CMD [“python”,”app.py”] 6 EXPOSE 指定与外界进行交互的端口 eg: EXPOSE 80 7 VOLUME 定义匿名卷 用于创建挂载点 我们可以利用VOLUME 将源代码、数据或其他内容添加到镜像中，而又不需要提交到镜像中，并且可以使多个容器共享这些内容 eg: VOLUME [“&#x2F;data_flask”] 8 RUN 构建镜像时执行的命令 eg:RUN pip install -i https://pypi.douban.com/simple/ -r requirements.txt 13、退出容器后，通过docker ps 命令查看不到，数据会丢失吗？​ 可以用 docker ps -a 来查看 ​ 如果仅仅是退出容器的话，数据是不会丢失的，重启容器后，数据依然在 ​ 如果删除容器的话，但数据做了持久化存储，也不会丢。 ​ 容器删除，那数据也会一起被删除 14、docker 的优点资源隔离：比如限制应用最大内存使用量，或者资源加载隔离等。 低消耗：虚拟化本身带来的损耗需要尽量的低。 Docker 很好的权衡了两者，即拥有不错的资源隔离能力，又有很低的虚拟化开销。 15、docker的应用场景1、可以简化配置 不同的软件可能需要不同的运行环境，docker可以将运行环境和配置放在代码中，然后部署，同一个docker可以在不同的环境中使用 2、简化代码流水线（Code Pipeline） 代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 3、提高开发效率 这就带来了一些额外的好处：Docker能提升开发者的开发效率，不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，使用虚拟机的时候，我们经常需要为开发环境的机器加内存，而Docker可以轻易的让几十个服务在Docker中跑起来。 4、隔离应用 有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。 5、整合服务器 正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 6、调试能力 Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。 7、 快速部署 在虚拟机之前，引入新的硬件资源需要消耗几天的时间。Docker的虚拟化技术将这个时间降到了几分钟，Docker只是创建一个容器进程而无需启动操作系统，这个过程只需要秒级的时间。这正是Google和Facebook都看重的特性。你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 16、容器的网络类型有哪些？bridge ：默认 host：容器和宿主机共享ip地址，端口号要区别开 none：只有lo接口，没有其他的接口 container：很多容器共享一个ip地址 17、compose 是什么？容器编排工具 例如：我们现在需要启动10个容器，其中3个nginx，2个redis，3个mysql，1个zabbix，1个ansible,有些容器需求先启动，有容器需要后启动，在启动的时候是有先后顺序的。 这时候需要批量启动容器，而且启动的时候容器之间是有依赖关系，需要考虑启动顺序的 我们可以将编排的内容全部写到一个yaml文件里，docker 的compose根据这个yaml文件里的安排去启动容器。 18、解释基本的Docker使用工作流程一切都从Dockerfile开始。Dockerfile是镜像的源代码。 创建Dockerfile后，您可以构建它以创建容器的镜像。图像只是“源代码”的“编译版本”，即Dockerfile。 获得容器的镜像后，应使用注册表重新分发容器。注册表就像一个git存储库 - 你可以推送和拉取图像。 接下来，您可以使用该图像来运行容器。在许多方面，正在运行的容器与虚拟机（但没有虚拟机管理程序）非常相似。 19、Docker Image和Layer有什么区别？Image：Docker镜像是由一系列只读层构建的 Layer：每个层代表Dockerfile中的指令。 下面的Dockerfile包含四个命令，每个命令都创建一个层。 FROM ubuntu:15.04 COPY . &#x2F;app RUN make &#x2F;app CMD python &#x2F;app&#x2F;app.py 每个层只是与之前层的一组差异。 20、您将如何监控生产中的Docker？Docker提供docker stats和docker events等工具来监控生产中的Docker。我们可以使用这些命令获取重要统计数据的报告。 Docker stats：当我们使用容器ID调用docker stats时，我们获得容器的CPU，内存使用情况等。它类似于Linux中的top命令。 Docker events：Docker events是一个命令，用于查看Docker守护程序中正在进行的任务。 一些常见的Docker事件是：attach，commit，die，detach，rename，destroy等。我们还可以使用各种选项来限制或过滤我们感兴趣的事件。 21、什么是Docker Swarm？Docker Swarm是Docker的群集管理工具。它将Docker主机池转变为一个虚拟Docker主机。Docker Swarm提供标准的Docker API，任何已经与Docker守护进程通信的工具都可以使用Swarm扩展到多个主机。 22、docker的核心是什么镜像（Image） 容器（Container） 仓库（Repository） 镜像：类似虚拟机镜像 容器：类似linux系统环境，运行和隔离应用。容器从镜像启动的时候，docker会在镜像的最上一层创建一个可写层，镜像本身是只读的，保持不变。 仓库：每个仓库存放某一类镜像。 | 23、docker的持久化，网络，发布docker的持久化：在生成容器的同时，加上-v选项，指定把当前服务器的目录映射到容器中 docker的网络： 创建网络：docker network create –subnet&#x3D;10.10.10.0&#x2F;24 docker1，其实就是创建一个虚拟交换机、一个虚拟局域网，只有属于同一个虚拟局域网的容器之间才能够相互通讯 发布docker：我们可以使用-p参数把容器端口和宿主机端口绑定 ​ -p 宿主机端口:容器端口 #宿主端口可以是任意的端口，只要访问的时候加上端口号，即可访问到相应的容易的端口的服务 24、怎么做一个拥有指定功能的镜像方法一：生产一个新的容器&#x3D;&#x3D;&gt;进入模板镜像容器&#x3D;&gt;操作(制作成自己想要的环境)&#x3D;&gt;退出容器&#x3D;&gt;利用容器产生新的镜像 方法二：写Dockerfile文件，把需要对容器的操作写到文件中，通过执行脚本文件来产生新的镜像 25、说下对docker run和docker start的理解？docker run 只在第一次运行时使用，将镜像放到容器中，以后再次启动这个容器时，只需要使用命令docker start 即可。 docker run相当于执行了两步操作：将镜像放入容器中（docker create）,然后将容器启动，使之变成运行时容器（docker start）。 docker start的作用是，重新启动已存在的镜像。也就是说，如果使用这个命令，我们必须事先知道这个容器的ID，或者这个容器的名字，我们可以使用docker ps找到这个容器的信息。 26、docker stop和docker kill的的理解？kill是不管容器同不同意，我直接执行kill -9，强行终止；stop的话，首先给容器发送一个TERM信号，让容器做一些退出前必须的保护性、安全性操作，然后让容器自动停止运行，如果在一段时间内，容器还是没有停止，再进行kill -9，强行终止。","categories":[],"tags":[]},{"title":"","slug":"总结/23、OpenStack","date":"2024-12-14T22:02:47.135Z","updated":"2021-02-07T15:45:02.000Z","comments":true,"path":"2024/12/15/总结/23、OpenStack/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/23%E3%80%81OpenStack/","excerpt":"","text":"OpenStack基础理论[TOC] 前言：openstack是一个开源的云计算管理平台架构，是一系列开源的软件项目的组合。由NASA（美国国家航空航天局）和Rackspace合作研发并发起，以Apache许可证（Apache软件基金会发布的一个自由软件许可证）授权的开源代码项目。 它为私有云和公有云的建设与管理提供了开源软件项目。在传统的运维管理服务器资源的基础上，有了openstack架构之后，才可在其之上部署云平台的服务，进行运维和管理。 一、云计算概述1.1 云计算概念云计算管理的是网络资源、存储资源、服务器资源等物理硬件资源。可以使客户在任何时间任何地点，通过网络，获取所需要的资源或服务，并且按需分配，按用量进行收费。 1.2 云计算服务类型1.2.1 IAAS（基础架构即服务）提供底层IT基础设施服务，包括处理能力，存储空间、网络资源，一般面对对象是IT管理人员。 用户可获取的是硬件或虚拟硬件，包括裸机或虚拟机，可以自行安装操作系统或其他应用程序。openstack就是一种IAAS云服务。 1.2.2 PAAS（平台即服务）把安装好开发环境的系统平台作为一种服务通过互联网提供给用户，一般面对对象是开发人员。 用户可获取的是安装了操作系统以及支撑应用程序运行所需要的资源库等软件的物理机或虚拟机，可以自行安装其他应用程序，但不能修改已经预装好的操作系统和运行环境。 1.2.3 SAAS（软件即服务）直接通过互联网为用户提供部署好的软件和应用程序的服务，可直接使用，一般面向对象是普通用户。 用户可获取的是以租赁的方式来直接使用一些软件，而不是购买。 二、OpenStack 介绍2.1 OpenStack的特性OpenStack优势 控制性完全开源的平台，提供API接口，方便与第三方技术集成 兼容性OpenStack兼容其他公有云，方便用户进行数据迁移 可扩展性模块化设计，可以通过横向扩展，增加节点、添加资源 灵活性根据自己的需要建立相应基础设施、增加集群规模激活 行业标准众多IT领军企业已经加入到OpenStack项目 2.2 OpenStack的核心组件整个OpenStack架构由多个子服务组成：以下是几个核心项目 服务 项目 描述 Compute 计算服务 Nova 负责实例生命周期的管理，计算资源的单位。对Hypervisor进行屏蔽，支持多种虚拟化技术(红帽默认为KVM),支持横向扩展 Network 网络服务 Neutron 负责虚拟网络的管理，为实例创建网络的拓扑结构。是面向租户的网络管理，可以自己定义自己的网络，各个租户之间互不影响 Identify 身份认证服务 Keystone 类似于LDAP服务，对用户、租户和角色、服务进行认证与授权，且支持多认证机制 Dashboard 控制面板服务 Horizon 提供一个Web管理界面，与OpenStack底层服务进行交互 Image Service 镜像服务 Glance 提供虚拟机镜像模板的注册与管理，将做好的操作系统拷贝为镜像模板，在创建虚拟机时直接使用，可支持多格式的镜像 Block Storage 块存储服务 Cinder 负责为运行实例提供持久的块存储设备，可进行方便的扩展，按需付费，支持多种后端存储 Object Storage 对象存储服务 Swift 为OpenStack提供基于云的弹性存储，支持集群无单点故障 Telemetry 计量服务 Ceilometer 用于度量、监控和控制数据资源的集中来源，为OpenStack用户提供记账途径 三、OpenStack 架构学习Openstack的部署和运维之前,应当熟悉其架构和运行机制，OpenStack作为开源、可扩展、富有弹性的云操作系统，其设计基本原则如下:■按照不同的功能和通用性划分不同项目，拆分子系统■按照逻辑计划、规范字系统之间的通信■通过分层设计整个系统架构■不同的功能子系统间提供统一的API接口 3.1 OpenStack 概念架构如下图所示： 上图的核心为虚拟机，所有组件围绕虚拟机，为它提供服务。可将上图的架构分为三个部分： ①蓝色方框为全局组件：keystone：为所有服务模块提供认证与授权ceilometer：度量、监控所有数据资源horizon ：UI平台管理，提供一个web管理页面，与底层交互 ②红色方框为外部辅助组件：ironic 提供裸金属环境trove 提供管理数据库服务（控制关系型和非关系型数据库）heat，sahara 提供对数据管理和编排 ③黄色方框为内部核心组件：glance：提供镜像服务neutron：提供网络服务swift：提供对象存储资源cinder：提供快存储资源nova：管理实例的生命周期，并负责调取以上四个资源给虚拟机使用。 具体流程：云平台用户在经过Keystone服务认证授权后，通过Horizon或者Reset API模式创建虚拟机服务，创建过程中包括利用Nova服务创建虚拟机实例，虚拟机实例采用Glance提供镜像服务，然后使用Neutron为新建的虚拟机分配IP地址,并将其纳入虚拟网络中，之后在通过Cinder创建的卷为虚拟机挂载存储块,整个过程都在Ceilometer模块资源的监控下，Cinder产生的卷(Volume)和Glance提供的镜像(Image) 可以通过Swift的对象存储机制进行保存。 3.2 OpenStack 逻辑架构如下图： 全局架构来看：OpenStack包括相互独立的服务组件。所有服务均可通过一个公共身份服务进行身份验证。除了那些需要管理权限的命令，每个服务之间均可通过公共API进行交互。所以，API即是每个服务内部和外部的交界处，隔离了内外。 服务之间交互过程：每个服务又由若干组件组成，包含多个进程。每个服务至少有一个API进程，用于侦听API请求，对这些请求进行预处理，（ 预处理就是将请求暴露出来的API接口，给keystone进行认证，如果认证通过，则放入队列等待被处理。） 然后将它们传送到自己服务后端的其他组件，对请求进行处理，而不是API进程去处理。也就是说除了认证服务，实际工作都是由具体的进程完成的。 服务内各个进程之间的通信：使用AMQP消息代理，将不同的消息格式进行转换，能够统一处理。服务的状态存储在数据库中。 消息队列：常用的三种类型，包括rabbitmq、 rocketmq、kafka，是两个独立的服务之间，消息传递的载体，解决消息在传输是请求的高并发问题，会以容器的方式，存储消息列表（包括请求、交互、报文），划分重要等级放入队列中，逐个处理，处理完的会自动删除。 OpenStack组件通信关系： ■基于AMQP协议的通信用于每个项目内部各个组件之间的通信。■基于SQL的通信用于各个项目内部的数据库通信。■基于HTTP协议进行通信通过各项目的API建立的通信关系，API都是RESTful Web API。■通过Native API实现通信OpenStack各组件和第三方软硬件之间的通信。 四、OpenStack 的节点物理架构可以从四个节点来看，包括控制节点，网络节点，计算节点，存储节点。 4.1 控制节点定位：运维人员通过控制节点从而控制整个openstack架构。如上图所示：控制节点包括支持服务，基础服务，扩展服务和网络接口服务，外部的裸金属服务提供物理资源支撑。 支持服务：包括数据库支持和通信支持，为整个节点提供数据存储服务和服务之间消息队列的通信服务。 基础服务：为虚拟机提供基础的镜像、网络、计算资源；keystone负责整个架构的认证和授权，运维人员通过horizon可视化的界面进行管理。 扩展服务：主要针对虚拟机的数据管理，heat进行数据的编排和管理。计量服务在此获取虚拟机的数据源，进行资源监控和计量，并记录。 网络接口：专门管理节点的网络服务，用于联系控制其他节点。 4.2 网络节点网络节点：只有一个基础服务，Neutron网络服务。负责整个openstack架构的网络通信。整个网络接口又可分为管理网络、数据网络、外部网络。管理网络负责关联其他节点的网络，让控制节点可管控其他节点的网络。数据网络负责整个架构的数据通信。外部网络负责架构与外部物理网络的连接通信。 4.3 计算节点计算节点包括基础服务、扩展服务、网络接口。基础服务有Nova Hypervisor 和网络插件代理。扩展服务为ceilometer agent 计量代理服务。网络接口为管理网络和数据网络。 4.4 存储节点存储节点包括cinder和swift两个基础的存储服务和网络接口。网络接口为管理网络和数据网络。 五、核心服务精讲5.1 Keystone身份认证服务5.1.1 Keystone概念Keystone (OpenStack Identity Service)是OpenStack中的一个独立的提供安全认证的模块，主要负责openstack用户的身份认证、令牌管理、提供访问资源的服务目录、以及基于用户角色的访问控制。 Keystone类似一个服务总线，或者说是整 个Openstack框架的注册表,其他服务通过keystone来注册其服务的Endpoint (服务访问的URL)，任何服务之间相互的调用，需要经过Keystone的身 份验证，来获得目标服务的Endpoint来找到目标服务。 5.1.2 主要功能 身份认证：负责令牌的发放和校验 用户授权：授权用户有指定的可执行动作的范围 用户管理：管理用户的账户 服务目录：提供可用服务的API端点位置 5.1.3 Keystone的管理对象Keystone服务贯穿整个架构，在进行身份认证服务的整个流程中，有几个重要的概念。 用户（user）：指的是使用openstack架构的用户。 证书（credentials）：用于确认用户身份的凭证，证明自己是自己，包括用户的用户名和密码，或是用户名和API密钥，或者身份管理服务提供的认证令牌。 认证（authentication）：确定用户身份的过程。 项目（project）：可以理解为一个人，或服务所拥有的资源的集合。 角色（role）：用于划分权限，通过给user指定role，使user获得role对应操作权限 服务（service）：openstack架构的组件服务，如nova、neutron、cinder、swift、glance等。 令牌（token）：由字符串表示，作为访问资源的凭证，是用户的身份&#x2F;权限证明文件；token决定了用户的权限范围，在指定的权限内进行操作；也包括令牌的有效期，在指定的时间范围内用户才有这些权限。 端点（endpoint）：一个可以通过网络来访问和定位某个openstack service的地址，即用户创建一个项目过程中需要的各个服务资源的位置 5.1.4 Keystone工作流程以用户想要通过openstack平台创建一个虚拟机的过程为例： 用户通过命令行或者horizon控制面板的方式登录openstack，凭借自己的证书（credentials）给keystone验证。 Keystone对用户的证书验证，验证通过则会发布一个令牌（token）和用户所需服务的位置点（endpoint）给用户。 用户得到了位置点（endpoint）之后，携带自己的令牌，向nova发起请求，请求创建虚拟机。 nova会拿着用户的token向keystone进行认证，看是否允许用户执行这样的操作。 keystone认证通过之后，返回给nova，nova即开始执行创建虚拟机的请求。首先需要镜像资源，nova带着令牌（token）和所需要的镜像名向glance提出镜像资源的请求。 glance会拿着token去向keystone进行认证，看是否允许提供镜像服务。keystone认证成功后，返回给glance。glance向nova提供镜像服务。 创建虚拟机还需要网络服务，nova携带token向neutron发送网络服务的请求 neutron拿着nova给的token向keystone进行认证，看是否允许向其提供网络服务。keystone认证成功后，返回给nuetron。nuetron则给nova提供网络规划服务。 nova获取了镜像和网络之后，开始创建虚拟机，通过hypervisior可调用底层硬件资源进行创建。创建完成返回给用户，成功执行了用户的请求。 5.2 Glance 镜像服务在早期的openstack版本中，glance只有管理镜像的功能，并不具备镜像存储功能，现在，glance已发至成为集镜像上传、检索、管理和存储等多功能的openstack核心服务。 5.2.1 镜像镜像通常指的是一系列文件或一个磁盘驱动的精确副本，将特定的一系列文件按照一定的格式制作成独立的文件，以方便用户的下载和使用。简单来说就是一系列资源&#x2F;服务的集合，也可以作为模板创建多个同样的独立的副本。 5.2.2 镜像服务的功能镜像服务主要是用来灌流镜像，让用户能够发现、获取和保存镜像，主要功能如下： 查询和获取镜像的元数据和镜像本身（元数据：镜像的概要信息和描述信息） 注册和上传虚拟机镜像，包括镜像的创建、.上传、 下载和管理 维护镜像信息，包括元数据和镜像本身。 支持多种方式存储镜像，包括普通的文件系统、Swift、 Amazon S3等 对虚拟机实例执行创建快照命令来创建新的镜像，或者备份虚拟机的状态。 5.2.3 镜像的 API 版本Glance提供的RESTful API有两个版本：V1,V2： v1只提供基本的镜像和成员操作功能，包括镜像创建、删除、下载、列表、详细信息查询、 更新，以及镜像租户成员的创建、删除和列表。 v2除了支持v1的所有功能外，主要增加了镜像位置的添加、删除、修改，元数据和名称空间操作，以及镜像标记操作。 5.2.4 镜像格式5.2.4.1 镜像文件有多种磁盘格式： raw：无结构的磁盘格式，以二进制形式存储镜像，访问速度非常快，但是不支持动态扩容，前期的耗时多。 qcow2：由QEMU仿真支持，可动态扩展，支持写时复制（copy on write）的磁盘格式。 以下不常用：vhd:该格式通用于VMware、Xen、 VirtualBox以及其他虚拟机管理程序vhdx:vhd格式的增强版本，支持更大的磁盘尺寸vmdk:- 种比较通用的虚拟机磁盘格式vdi:由VirtualBox虚拟机监控程序和QEMU仿真器支持的磁盘格式iso:用于光盘(CD-ROM)数据内容的档案格式ploop:由Virtuozzo支持， 用于运行OS容器的磁盘格式aki:在Glance中存储的Amazon内核格式ari:在Glance中存储的Amazon虚拟内存盘(Ramdisk)格式ami:在Glance中存储的Amazon机器格式 5.2.4.2 镜像文件容器格式： bare：没有容器或元数据 “信封” 的镜像，原始的资源集合，所以不存在兼容性问题，不确定选择哪种容器模式时，就在指定为bare最安全。 Docker：在glance中存储的容器文件系统的dockerd的tar档案。能够隔离磁盘存储的数据、元数据。 以下不常用：ovf:开放虚拟化格式ova:在Glance中存储的开放虚拟化设备格式aki:在Glance中存储的Amazon内核格式ari:在Glance中存储的Amazon虚拟内存盘(Ramdisk) 格式 5.2.5 镜像状态 镜像从上传到可识别的几个状态：queued：这是一种初始化状态， 镜像文件刚被创建，在Glance数据库只有其元数据，镜像数据还没有上传至数据库中saving:是镜像的原始数据在上传到数据库中的一种过渡状态,表示正在上传镜像uploading：指已进行导入数据提交调用，可以给服务识别和调用的状态importing：指已经完成导入调用，服务已经识别，可调用，但是镜像还未准备好给虚拟机提供服务 镜像在上载完成后的状态：active:表示当镜像数据成功上传，可使用deactivated:只对管理员开放权限，任何非管理员用户都无权访问镜像数据，禁止下载镜像，也禁止镜像导出和镜像克隆之类的操作klled:表示镜像上传过程中发生错误，镜像不可读deleted:镜像将在不久后被自动删除，该镜像不可再用，但是目前Glance仍然保留该镜像的相关信息和原始数据，删除后可恢复pending_ delete:与deleted相似， Glance还没有清除镜像数据，但处于该状态的镜像不可恢复 5.2.6 镜像访问权限public公共的：可以被所有的项目使用private私有的：只有被镜像所有者所在的项目使用shared共享的：一个非共有的镜像，可以共享给其他项目，通过项目成员（member-*）操作来实现的projected（受保护的）：这种镜像不能被删除 5.2.7 glance架构详解glance服务的使用者即客户端，是 openstack 命令行工具，Horizon控制面板或者Nova服务。 glance-api 是 glance服务后台运行的服务进程，是进入glance的入口，对外提供REST API ，负责接收外部客户端的服务请求，如响应镜像查询、获取和存储的调用。 glance-registry 是服务后台远程的镜像注册服务，负责处理与镜像的元数据相关的外部请求，如镜像大小、类型等信息。glance-api 接收外部请求，若与元数据相关，则将请求转发给glance-registry，glance-registry会解析请求的内容，并与数据库交互，进行存储、处理、检索镜像的元数据，元数据所有信息即存储在后端的数据库中。 glance的DB模块存储的是镜像的元数据，可以选用MySQL、mariaDB、SQLite等数据库。镜像的元数据是通过glance-registry存放在数据库中。镜像本身（chunk 块数据）是通过glance的存储驱动存储在后端的各种存储系统中。 存储后端（store backend）将镜像本身的数据存放在后端存储系统，存储系统有多种存储方式，支持本地文件系统存储、对象存储、RBD块存储、Cinder块存储、分布式文件存储等。具体使用哪种backend，可以在glance的配置文件 &#x2F;etc&#x2F;glance&#x2F;glance-api.conf 中配置 [glance-store] 模块。 5.2.8 glance的工作流程glance是一个C&#x2F;S架构，给外部提供服务，用户通过REST API获取glance所有服务。 首先是对客户端的安全认证流程：openstack的操作都需要经过keystone进行身份认证，并授权，glance也不例外，授权成功再去请求glance服务，glance服务接收到外部请求后，会去keystone进行认证，此请求是否已授权，认证通过后，才会将请求传到后端处理。 glance domain controller 是API和后端功能模块的中间件，相当于调度器，作用是将外部服务分发到下面的各个功能层去处理。在调度时，遵循调度算法，首先有一个预选，排除不符合要求的节点，再进行优选，通过打分机制，对都能够处理此功能的节点进行打分，考虑它们当前的负荷，处理能力和速度，选出最优的一个。对于一些有污点的节点，调度器是直接跳过他们的，如果其余可用节点负担都太大，无法处理外部请求，会有一个容忍机制，由运维人员控制，让调度器接受污点，对污点再进行优选。 调度器的子功能模块：auth授权：控制镜像的访问权限；notifier消息通知：将镜像变化信息和错误添加到 消息队列policy规则定义：定义镜像操作的访问权限,在policy.json中定义quota限额：限制上传镜像的大小location定位：通过glance store 与后台存储进行交互，指明镜像存储位置，还可以检查位置的URL是否正确DB数据库：将镜像转换为相应的格式以存储在数据库中，并将从数据库读取的信息转换为可以操作的镜像对象。 后端有两种服务类型：一种是处理关于元数据的请求，另一种是关于镜像数据的请求。由调度器将请求分配到对应的服务模块。当请求元数据时，glanceDB会与调度器进行交互提供服务，中间还可以通过 registry layer 注册层进行一个安全交互。glanceDB存储着元数据信息，并且对glance内部所有的组件都是共享的。当请求的是关于镜像本身服务时，glance store可以提供一个统一的接口访问后端的存储，并且有一个驱动模块可以调用整个库与外部服务进行交互。后端的存储有多种存储系统，对象存储、文件存储等。 5.3 Nova 计算服务5.3.1 Nova 简介 计算服务是openstack最核心的服务之一 ， 负责维护和管理云环境的计算资源，它在openstack项目中代号是nova。 Nova自身并没有提供任何虚拟化能力，它提供计算服务，使用不同的虚拟化驱动来与底层支持的Hypervisor (虚拟机管理器)进行交互。所有的计算实例(虚拟服务器)由Nova进行生命周期的调度管理(启动、挂起、停止、删除等)，全局来看，nova为整个架构提供虚拟化资源、技术，服务层面来看，nova本身并不具备虚拟化能力，而是通过compute组件与虚拟化管理工具交互实现虚拟资源调度。 Nova需要keystone、glance、 neutron、 cinder和swift等其他服务的支持， 能与这些服务集成，实现如加密磁盘、裸金属计算实例等。会和其他外部组件集成，共同完成请求。 5.3.2 Nova 系统架构整个架构可从外部服务和内部服务来看。外部服务包括keystone，neutron，glance，cinder这些nova服务需要与之交互的服务。内部组件：DB：用于数据存储的sql数据库，存放各组件的工作过程信息，后盾裸金属实际的使用资源信息API：用于接收HTTP请求、转换命令、通过消息队列或HTTP与其他组件通信Scheduler：用于决定哪台计算节点承载计算实例，相当于nova的调度器Network：管理IP转发，网桥或虚拟局域网，是一个网络组件Compute：管理虚拟机管理器（VMM）与虚拟机之间通信，与hypervisor交互实现虚拟化调用底层硬件资源，计算组件Conductor：处理需要协调的的请求（构建虚拟机或调整虚拟机大小的请求），或者处理对象转换 5.3.3 组件介绍5.3.3.1 API： 它是客户端访问nova的http接口，它由nova-api服务实现，nova-api服务接收和响应来自最终用户的计算api请求（包括跟虚拟机声明周期相关的操作），作为openstack对外服务的最主要接口，nova提供了一个集中的可以查询所有api的端点，以便自己平台内部和其他云平台的调用。 所有对nova的请求都首先由nova-api处理，API提供REST标准调用服务，便于与第三方系统集成（其他云平台）。它是外部访问并使用nova提供的各种服务的唯一途径，也是客户端和nova之间的中间件。 最终用户不会直接改送RESTful API 请求，而是通过openstack命令行，horison控制面板和其他需要跟nova交换 的组件来使用这些API。 Nova-api对接收到的HTTP API请求做以下处理：1、检查客户端传入的参数是否合法有效（而不止是进行认证）2、调用nova其他服务来处理客户端HTTP请求3、格式化nova其他子服务返回的结果，并返回给客户端 5.3.3.2 Scheduler：简介 调度器，由nova-scheduler服务实现，主要解决的是选择在哪个计算节点上启动实例。它可以应用多种规则，考虑内存使用率、cpu负载率、CPU架构等多种因素，根据一定的算法，确定虚拟机实例能够运行在哪一台计算机服务器上。 nova-scheduler服务会从消息队列中接收一个虚拟机实例的请求，通过读取数据库的内容，从可用资源池中选择最合适的计算节点，创建新的虚拟机实例。 创建虚拟机实例时，用户会提出实例的资源需求，如CPU、内存、磁盘。openstack将这些需求定义在实例类型中，有多个实例模板存储在nova的数据库中，用户可以指定使用哪个实例类型，再创建实例。 调度器的类型： 随机调度器：从所有正常运行nova-compute服务的节点中随机选择 缓存调度器：是随机调度器的一种特殊类型，在随机调度器的基础上，将主机资源信息缓存在本地内存中，然后通过后台的定时任务，定时从数据库中获取最新的主机资源信息，周期性同步而不是实时获取主机资源信息。 过滤器调度器：根据指定的过滤条件以及权重选择最佳的计算节点，又称为筛选器。 过滤器调度器调度过程：主要分为两个阶段： 通过指定的过滤器选择满足条件的计算节点，比如内存使用率，可以使用多个过滤器依次进行过滤。（预选） 对过滤之后的主机列表进行权重计算并排序，选择最优的计算节点来创建虚拟机实例。（优选） 调度器与DB的交互过程： scheduler组件决定的是虚拟机实例部署在哪台计算节点上并调度，在调度之前，会先向数据库获取宿主机资源信息作为依据； 之后可通过过滤器和权重选择最合适的节点调度，或者指定节点直接调度； 计算节点的 libvirt 工具负责收集宿主机的虚拟化资源，根据已创建的实例再次统计资源，将资源信息更新到数据库中，整个更新资源信息的过程是周期性执行的，而不是实时的； 所以存在一个问题，当刚创建完一个实例，随即又需要创建时，数据库还未来得及更新宿主机的最新状态，那么调度器依据的信息就不正确，有可能所选的节点资源并不够用，而导致调度失败。 这同时也是缓存调度器的缺陷，无法实时获取租主机资源信息。我们可在调度完成时，直接将资源信息返回给数据库，更新数据库状态，解决这个问题。 过滤器 当过滤调度器需要执行调度操作时，会让过滤器对计算节点进行判断，返回ture或false，按照主机列表中的顺序依次过滤。 scheduler_available_filters 选项用于配置可用过滤器，默认是所有nova自带的过滤器都可以使用。 scheduler_default_filters 选项用于指定nova-schedule 服务真正使用的过滤器。 过滤器类型： RetryFilter（再审过滤器）主要作用是过滤掉之前已经调度过的节点（类比污点）。如A、B、C都通过了过滤，A权重最大被选中执行操作，由于某种原因，操作在A上失败了。Nova-filter 将重新执行过滤操作，再审过滤器直接过滤掉A，以免再次失败。 AvailabilityZoneFilter（可用区域过滤器）主要作用是提供容灾性，并提供隔离服务，可以将计算节点划分到不同的可用区域中。Openstack默认有一个命名为nova的可用区域，所有计算节点一开始都在其中。用户可以根据需要创建自己的一个可用区域。创建实例时，需要指定将实例部署在那个可用区域中。通过可用区过滤器，将不属于指定可用区的计算节点过滤掉。 RamFilter（内存过滤器）根据可用内存来调度虚拟机创建，将不能满足实例类型内存需求的计算节点过滤掉，但为了提高系统资源利用率, Openstack在计算节点的可用内存允许超过实际内存大小，可临时突破上限，超过的程度是通过nova.conf配置文件中ram_ allocation_ ratio参数来控制的， 默认值是1.5。（但这只是临时的）Vi &#x2F;etc&#x2F;nova&#x2F;nova . confRam_ allocation_ ratio&#x3D;1 .5 DiskFilter（硬盘过滤器）根据磁盘空间来调度虚拟机创建，将不能满足类型磁盘需求的计算节点过滤掉。磁盘同样允许超量，超量值可修改nova.conf中disk_ allocation_ ratio参数控制，默认值是1.0，（也是临时的）Vi &#x2F;etc&#x2F;nova&#x2F;nova.confdisk_ allocation_ ratio&#x3D;1.0 CoreFilter（核心过滤器）根据可用CPU核心来调度虚拟机创建，将不能满足实例类型vCPU需求的计算节点过滤掉。vCPU也允许超量，超量值是通过修改nova.conf中cpu_ allocation_ratio参数控制，默认值是16。Vi &#x2F;etc&#x2F;nova&#x2F;nova. confcpu_allocation_ ratio&#x3D;16.0 ComputeFilter（计算过滤器）保证只有nova-compute服务正常工作的计算节点才能被nova-scheduler调度，它是必选的过滤器。 ComputeCapabilitiesFilter（计算能力过滤器）根据计算节点的特性来过了，如不同的架构。 ImagePropertiesFilter（镜像属性过滤器）根据所选镜像的属性来筛选匹配的计算节点，通过元数据来指定其属性。如希望镜像只运行在KVM的Hypervisor上，可以通过Hypervisor Type属性来指定。 服务器组反亲和性过滤器要求尽量将实例分散部署到不同的节点上，设置一个服务器组，组内的实例会通过此过滤器部署到不同的计算节点。适用于需要分开部署的实例。 服务器组亲和性过滤器此服务器组内的实例，会通过此过滤器，被部署在同一计算节点上，适用于需要位于相同节点的实例服务。 权重： 在对计算节点进行过滤时，多个过滤器依次进行过滤，而不是并行，相当于一个个预选，避免重复对同一个节点进行所有过滤项检验。过滤之后的节点再通过计算权重进行排序。 所有权重位于nova&#x2F;scheduler&#x2F;weights 目录下，目前默认是RAMweighter，根据计算节点空闲的内存计算权重，内存空闲越多权重越大，计算节点按照内存空闲进行排序。 schedule组件小结：定位：决定实例具体创建在哪个计算节点 调度流程：有多个调度器、过滤器共同合作，一次过滤，首先筛选出符合要求的节点，接着以权重（一般是基于内存空闲）的方式，对节点打分排序，最终决定出一个节点。 子功能：粗过滤（基础资源），例如CPU、内存、磁盘精细化过滤，例如镜像属性，服务性能契合度高级过滤，亲和性&#x2F;反亲和性过滤 过滤之后，可以进行随机调度，会进行再筛选，即污点机制，对剩下的节点过滤，排除掉之前有故障的节点。 5.3.3.3 compute计算组件Nova-compute在计算节点上运行，负责管理节点上的实例。通常一个主机运行一 个Nova-compute服务, 一个实例部署在哪个可用的主机上取决于调度算法。OpenStack对实例的操作最后都是提交给Nova-compute来完成。 Nova-compute可分为两类，一类是定向openstack报告计算节点的状态，另一类是实现实例生命周期的管理。 定期向OpenStack报告计算节点的状态每隔一段时间，nova-compute就会报告当前计算节点的资源使用情况和nova-compute服务状态。nova-compute是通过Hypervisor的驱动获取这些信息的。 实现虚拟机实例生命周期的管理OpenStack对虚拟机实例最主要的操作都是通过nova-compute实现的。包括创建、关闭、重启、挂起、恢复、中止、调整大小迁移、快照。以实例创建为例来说明nova-compute的实现过程。(1)为实例准备资源。(2)创建实例的镜像文件。(3)创建实例的XML定义文件。(4)创建虚拟网络并启动虚拟机。 通过Driver (驱动)架构支持多种Hypervisor虚拟机管理器：面对多种Hypervisor, nova-compute为这些Hypervisor定义统一 的接口，Hypervisor只需要对接这个接口， 就可以Drive驱动的形式即插即用到OpenStack系统中，使得compute组件可以调用虚拟化的资源，创建实例。 5.3.3.4 conductor协调组件 为数据库的访问提供一层安全保障。Nova-conductor作为nova-compute服务与数据库之间交互的中介，避免了compute直接访问，由nova-compute服务对接数据库，compute组件位于第三方上，与架构外部进行交互。这样就可避免一些越权操作，并且为数据库分压。 Nova-compute访问数据库的全部操作都改到nova-conductor中，nova-conductor作为对数据库操作的一个代理，而且nova-conductor是部署在控制节点上的。 Nova-conductor有助于提高数据库的访问性能，nova-compute可以创建多个线程使用远程过程调用(RPC)访问nova-conductor。 在一个大规模的openstack部署环境里，管理员可以通过增加nova-conductor的数量来应对日益增长的计算节点对数据库的访问量。 5.3.3.5 Placement API 以前对资源的管理全部由计算节点承担，在统计资源使用情况时，只是简单的将所有计算节点的资源情况累加起来，但是系统中还存在外部资源，这些资源由外部系统提供。如ceph、nfs等提供的存储资源等。面对多种多样的资源提供者，管理员需要统一的、简单的管理接口来统计系统中资源使用情况，这个接口就是Placement API。 PlacementAPI由nova-placement-api服务来实现， 旨在追踪记录资源提供者的目录和资源使用情况。被消费的资源类型是按类进行跟踪的。 如计算节点类、共享存储池类、IP地址类等。 5.3.4 虚拟机实例化流程 用户访问控制台可以通过多种方式访问虚拟机的控制台：■ Nova-novncproxy守护进程: 通过vnc连接访问正在运行的实例提供一个代理，支持浏览器novnc客户端。（最常用的）■ Nova-spicehtml5proxy守护进程: 通过spice连接访问正在运行的实例提供一个代理，支持基于html5浏览器的客户端。■ Nova-xvpvncproxy守护进程: 通过vnc连接访问正在运行的实例提供一个代理，支持openstack专用的java客户端。■ Nova-consoleauth守护进程: 负责对访问虚拟机控制台提供用户令牌认证。这个服务必须与控制台代理程序共同使用，即上面三种代理方式。 如果无法通过控制台&#x2F;URL连接到内部实例：1、实验场景中，虚拟机资源不足2、实验场景中，镜像问题（公共镜像，作为测试用的镜像）3、生产环境中，路由配置问题，路由网关配置到了其他实例地址，内部有多个实例项目。 首先用户（openstack最终用户或其他程序）执行nova client提供的用于创建虚拟机的命令。 nova-api服务监听到来自客户端的http请求，会将请求转换为AMQP消息之后上传到消息队列。 通过消息队列调用conductor服务 conductor服务从消息队列中接收到虚拟机的实例化请求后，进行准备工作 conductor通过消息队列告诉scheduler服务去选择一个合适的计算节点创建虚拟机，此时scheduler会读取数据库的内容 conductor服务从nova-schedule服务得到了计算节点的信息后，通过消息队列通知compute服务实现虚拟机的创建。 5.3.5 Nova部署架构经典部署架构： 负载均衡部署架构： 5.3.6 Nova的Cell架构 5.3.6.1 使用此架构原因为了解决openstack nova集群的规模变大时,数据库和消息队列服务就会出现瓶颈问题。Nova为提高水平扩展及分布式、大规模的部署能力，同时又不想增加数据库和消息中间件的复杂度，从而引入了Cell概念。 5.3.6.2 cell概念Cell可译为单元。为支持更大规模的部署,openstack将大的nova集群分成小的单元，每个单元都有自己的消息队列和数据库，可以解决规模增大时引起的瓶颈问题。在Cell中，Keystone、Neutron、 Cinder、Glance等资源是共享的。 5.3.6.3 cell架构的数据库 nova-api数据库：存放全局信息，这些全局数据表是从nova库迁移过来的，包括实例模型信息，实例组，配额信息。 nova-cell0数据ku：当实例调度失败时，实例的信息不属于任何一个cell，所以存放到cell0数据库中。 单cell部署：多cell部署：多cell部署的架构，可以从分层的角度来看：为了应付集群规模的扩大，对数据库、消息队列进行扩容，实现nova规模的扩展。通过划分多个小的cell单元，多个单元同时处理业务，每个cell单元都有各自的数据库和消息队列。为了管理这些小的单元，需要一个集中化管理组件 super conductor，主conductor来管理cell单元，同时完成自身的工作。 蓝色方框：是对消息队列的划分。划分为API消息队列，和cell单元中的消息队列。API消息队列中是外部对nova的请求，以及nova内部子服务之间的通信请求，作为它们之间通信的载体。cell消息队列中是cell单元需要处理的业务请求。红色方框：是对conductor的划分，super-conductor用于集中管理cell单元，同时负责通知 nova-scheduler 调度计算节点，调度失败直接写入cell0，成功则控制任务具体下发给哪个cell单元处理，将所有数据写入数据库。cell中的conductor，通过消息队列收到本cell需要创建实例的请求后，会通知nova-compute创建实例，进行具体的处理和协调。粉色方框：是对数据库的划分。API数据库存放全局的交互数据，总的资源统计。cell0数据库存放实例创建失败的信息，cell1&#x2F;2存放各自单元成功创建实例的信息，对应处理的请求数据。nova-api：每个创建实例的请求都有编号，创建成功与否的结果会存储在cell数据库中，nova-api会根据编号，从cell数据库中调取到请求的最终结果，格式化后返回给用户。 5.4 Neutron 网络服务简介：网络是openstack最重要的资源之一， 没有网络，虚拟机将被隔离。Openstack的网络服务最主要的功能就是为虚拟机实例提供网络连接，最初由nova的一-个单独模块nova-compute实现，但是nova-compute支持的网络服务有限，无法适应大规模、高密度和多项目的云计算，现已被专门的网络服务项目Neutron所取代。Neutron为整个openstack环境提供软件定义网络支持，主要功能包括二层交换、三层路由、防火墙、VPN, 以及负载均衡等。Neutron在由其他openstack服务 (如nova)管理的网络接口设备 (如虚拟网卡)之间提供网络连接即服务。 5.4.1 linux网络虚拟化linux网络虚拟化：传统的物理网络：需要对二层物理网络进行抽象和管理： 实现虚拟化后，多个物理服务器可以被虚拟机取代，部署在同一台物理服务器，上。虚拟机由虚拟机管理器(Hypervisor)实现，在Linux系统中Hypervisor通常采用kvm。在对服务器进行虚拟化的同时，也对网络进行虚拟化。 Hypervisor为虚拟机创建一个或多个虚拟网卡(vNIC)，虚拟网卡等同于虚拟机的物理网卡。物理交换机在虚拟网络中被虚拟为虚拟交换机(vSwitch)，虚拟机的虚拟网卡连接到虚拟交换机上，虚拟机交换机再通过物理主机的物理网卡连接到外部网络。 对于物理网络来说，虚拟化的主要工作是对网卡和交换设备的虚拟化。 linux虚拟网桥： 与物理机不同，虚拟机并没有硬件设备，但是也要与物理机和其他虚拟机进行通信。LinuxKVM的解决方案是提供虚拟网桥设备，像物理交换机具有若干网络接口(网卡) 一样，在网桥上创建多个虚拟的网络接口，每个网络接口再与KVM虚拟机的网卡相连。 在Linux的KVM虚拟系统中，为支持虚拟机的网络通信，网桥接口的名称通常以vnet开头，加上从0开始顺序编号，如vnet0、vnet1, 在创建虚拟机时会自动创建这些接口。虚拟网桥br1和br2分别连接到物理主机的物理网卡1和物理网卡2。 虚拟局域网： 一个网桥可以桥接若干虚拟机，当多个虚拟机连接在同一网桥时，每个虚拟机发出的广播包会引发广播风暴，影响虚拟机的网络性能。通常使用虚拟局域网(VLAN)将部分虚拟机的广播包限制在特定范围内，不影响其他虚拟机的网络通信。 通常使用VLAN将部分虚拟机的广播包限制在特定范围内，不影响其他虚拟机的网络通信。 将多个虚拟机划分到不同的VLAN中，同一VL AN的虚拟机相当于连接同一网桥上。 在Linux虚拟化环境中，通常会将网桥与VLAN对应起来，也就是将网桥划分到不同的VLAN中 VLAN协议为802.1Q，VLAN是具有802.1Q标签的网络。 开放虚拟交换机 开放虚拟交换机 (Open vSwitch) 是与硬件交换机具备相同特性，可在不同虚拟平台之间移植（保持自己的功能特性，同时具有兼容性），具有产品级质量的虚拟交换机，适合在生产环境中部署。 交换设备的虚拟化对虚拟网络来说至关重要。在传统的数据中心，管理员可以对物理交换机进行配置，控制服务器的网络接入，实现网络隔离、流量监控、QoS配置、流量优化等目标。在云环境中，采用Open vSwitch技术的虚拟交换机可使虚拟网络的管理、网络状态和流量的监控得以轻松实现。 Open Switch在云环境中的虚拟化平台上实现分布式虚拟交换机，可以将不同主机上的OpenvSwitch交换机连接起来，形成一个大规模的虚拟网络。 OpenStack网络服务提供一个API让用户在云中建立和定义网络连接。该网络服务的项目名称是Neutron。OpenStack网络负责创建和管理 虚拟网络基础架构，包括网络、交换机、子网和路由器，这些设备由OpenStack计算服务Nova管理。同时，网络服务还提供防火墙和VPN这样的高级服务。可以将网络服务部署到特定主机.上。OpenStack网络组件与身份服务、计算服务和仪表板等多个OpenStack组件进行整合 5.4.2 Neutron网络结构 一个简化的典型的Neutron网络结构如图所示，包括一个外部网络、一个内部网络和一个路由器。 外部网络负责连接OpenStack项目之外的网络环境，又称公共网络。与其他网络不同，它不仅仅是-一个虚拟网络，更重要的是，它表示OpenStack网络能被外部物理网络接入并访问。外部网络可能是企业的局域网(Intranet) ，也可能是互联网(Internet) ，这类网络并不是由Neutron直接管理。 内部网络完全由软件定义，又称私有网络。它是虚拟机实例所在的网络，能够直接连接到虚拟机。项目用户可以创建自己的内部网络。默认情况下，项目之间的内部网络是相互隔离的，不能共享。该网络由Neutron直接配置与管理。 路由器用于将内部网络与外部网络连接起来，因此，要使虚拟机访问外部网络，必须创建一个路由器。 Neutron需要实现的主要是内部网络和路由器。内部网络是对二层(L 2)网络的抽象，模拟物理网络的二层局域网，对于项目来说，它是私有的。路由器则是对三层(L3) 网络的抽象，模拟物理路由器，为用户提供路由、NAT等服务。 网络子网与端口 网络：一个隔离的二二层广 播域，类似交换机中的VLAN。Neutron支持多种类型的网络， 如FLAT、VLAN、VXLAN等。 子网：一个IPV4或者IPV6的地址段及其相关配置状态。虚拟机实例的IP地址从子网中分配。每个子网需要定义IP地址的范围和掩码(这个有点像DHCP中定义的作用域的概念)。 端口：连接设备的连接点，类似虚拟交换机上的一个网络端口。端口定义了MAC地址和IP地址，当虚拟机的虚拟网卡绑定到端口时，端口会将MAC和IP分配给该虚拟网卡。 通常可以创建和配置网络、子网和端口来为项目搭建虚拟网络。网络必须属于某个项目，一个项目中可以创建多个网络。一个子网只能属于某个网络，一个网络可以有多个子网。一个端口必须属于某个子网，一个子网可以有多个端口。 网络拓扑类型 LocalLocal网络与其他网络和节点隔离。该网络中的虚拟机实例只能与位于同-节点上同- -网络的虚拟机实例通信，实际意义不大，主要用于测试环境。位于同一Local网络的实例之间可以通信，位于不同Local网络的示例之间无法通信。一个Local网络只能位于同一个物理节点上，无法跨节点部署。 FlatFlat是一种简单的扁平网络拓扑，所有的虚拟机实例都连接在同一网络中，能与位于同一网络的实例进行通信，并且可以跨多个节点。这种网络不使用VLAN,没有对数据包打VLAN标签，无法进行网络隔离。Flat是基于不使用VLAN的物理网络实施的虚拟网络。每个物理网络最多只能实现- -个虚拟网络。 VLANVLAN是支持802.1q协议的虚拟局域网，使用VLAN标签标记数据包，实现网络隔离。同一VLAN网络中的实例可以通信，不同VLAN网络中的实例只能通过路由器来通信。VLAN网络可以跨节点。 VXLANVXLAN (虚拟扩展局域网)可以看作是VLAN的一种扩展，相比于VLAN,它有更大的扩展性和灵活性，是目前支持大规模多租房网络环境的解决方案。由于VLAN包头部限长是12位， 导致VLAN的数量限制是4096 (2^12) 个，不能满足网络空间日益增长的需求。目前VXLAN的封包头部有24位用作VXLAN标识符(VNID)来区分VXLAN网段，最多可以支持16777216 (2^24) 个网段。VXLAN使用STP防止环路，导致- -半的网络路径被阻断。VXLAN的数据包是封装到UDP通过三层传输和转发的，可以完整地利用三层路由，能克服VLAN和物理网络基础设施的限制，更好地利用已有的网络路径。 GREGRE (通用路由封装)是用一种网络层协议去封装另一种网络层协议的隧道技术。GRE的隧道由两端的源IP地址和目的IP地址定义，它允许用户使用IP封装IP等协议，并支持全部的路由协议。在OpenStack环境中使用GRE意味着”IP over IP”,GRE与VXLAN的主要区别在于，它是使用IP包而非UDP进行封装的。 GENEVEGENEVE(通用网络虚拟封装)的目标宣称是仅定义封装数据格式，尽可能实现数据格式的弹性和扩展性。GENEVE封装的包通过标准的网络设备传送，即通过单播或多播寻址，包从一个隧道端点传送到另一个或多个隧道端点。GENEVE帧格式由- -个封装在IPV4或IPV6的UDP里的简化的隧道头部组成。GENEVE推出的主要目的是为了解决封装时添加的元数据信息问题(到底多少位, 怎么用GENEVE自动识别与调整) ，以适应各种虚拟化场景。 总结随着云计算、大数据、移动互联网等新技术的普及，网络虚拟化技术的趋势在传统单层网络基础上叠加一层逻辑网络。这将网络分为两个层次，传统单层网络称为Underlay (承载网络)，叠加其上的逻辑网络称为Overlay (叠加网络或覆盖网络)。Overlay网络的节点通过虚拟的或逻辑的连接进行通信，每一个虚拟的或逻辑的连接对应于Underlay网络的一条路径，由多个前后衔接的连接组成。Overlay网络无须对基础网络进行大规模修改，不用关心这些底层实现，是实现云网融合的关键。 VXLAN、GRE、GENEVE都是基于隧道技术的overlay网络。 VLAN和VXLAN：vlan是虚拟局域网，通过不同的vlan标签进行网络隔离；vxlan是vlan的扩展，能充分利用三层路由，解决传统vlan和物理设备的环境限制问题，并且比vlan支持更多的网段，基于udp协议。 简单理解：随着目前互联网技术的发展，对于网络部分，使用虚拟化的方案，实现对传统网络的扩展，利用叠加的方式，常用的叠加网络VXLAN、GRE。 5.4.3 openstack中的网络基本架构 Neutron仅有一个主要服务进程neutron-server。它是运行在控制节点上的，对外提供Openstack网络API作为访问Neutron的入口，收到请求后调用插件进行处理，最终由计算节点和网络节点上的各种代理完成请求。 网络提供者是指提供OpenStack网络服务的虚拟或物理网络设备，如Linux Bridge、Open vSwitch,或者其他支持Neutron的物理交换机。 与其他服务一样，Neutron的各组件服务之间需要相互协调和通信，neutron-server. 插件和代理之间通过消息队列进行通信和相互调用。 数据库用于存放OpenStack的网络状态信息，包括网络、子网、端口、路由器等。 客户端是指使用Neutron服务的应用程序，可以是命令行工具、Horizon和Nova计算服务等。 实例：以一个创建VLAN 100虚拟网络的流程为例说明这些组件如何协同工作。 neutron-server收到创建网络的请求，通过消息队列通知已注册的Linux Bridge插件。(插件可以有很多，这里举例创建虚拟网络的插件是Linux Bridge插件) 该插件将要创建的网络信息(如名称、VLAN ID等)保存到数据库中，并通过消息队列通知运行在各节点上的代理 代理收到消息后会在节点上的物理网卡上创建VLAN设备(比如eth1.100)，并创建一个网桥(比如brqxxx)来桥接VLAN设备。 neutron-server详解：结构： RESTful API:直接对客户端提供API服务，属于最前端的API，包括Core API和Extension API两种类型。Core API提供管理网络、子网和端口核心资源的RESTful API; Extension API则提供管理路由器、防火墙、负载均衡、安全组等扩展资源的RESTful API。 Common Service:通用服务，负责对API请求进行检验、认证，并授权。 Neutron Core:核心处理程序，调用相应的插件API来处理API请求。 Plugin API:定义插件的抽象功能集合，提供调用插件的API接口，包括Core Plugin API 和 ExtensionPlugin API两种类型。Neutron Core通过Core Plugin API调用相应的Core Plugin，","categories":[],"tags":[]},{"title":"","slug":"总结/22、KVM","date":"2024-12-14T22:02:47.133Z","updated":"2020-12-08T14:20:40.000Z","comments":true,"path":"2024/12/15/总结/22、KVM/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/22%E3%80%81KVM/","excerpt":"","text":"一、虚拟化1、背景美国环境保护EPA报告中曾经统计过一组统计数据：EPA研究服务器和数据中心得能源效率时发现，实际上服务器只有5%得时间时在工作的，其他时间一直处于休眠状态，这样服务器的利用率就会很低，而且这些服务器一直处于开机状态，消耗的能源自然就很多 2、虚拟化技术介绍通过虚拟化技术将一台计算机虚拟为多台逻辑计算机，在一台计算机上同时运行多个逻辑计算机，同时每个逻辑计算机可运行不同的操作系统，应用程序都可以在相互独立的空间内运行而互相不影响，从而提高计算机的工作效率 3、虚拟化技术发展雏形：1961年，IBM709机器实现了分时系统，将CPU占用切分为多个极短的时间片(1&#x2F;100sec)每一个时间片执行不同的工作，通过对这些时间片进行轮询从而将一个CPU伪装成多个CPU1972年， IBM正式将system370机的分时系统命名为虚拟机1990年， IBM推出的system390机支持逻辑分区（将一个CPU分为多份，相互独立，也就是逻辑分割）Xen 2003年问世，是一个外部的hypervisor程序（虚拟机管理程序），能够控制虚拟机和给多个客户机分配资源KVM：2007年问世，现已内置在kernel内核中的Xen 支持的虚拟化技术：全虚拟化，半虚拟化KVM：支持的虚拟化技术：全虚拟化 4、虚拟化类型1）全虚拟化：将物理硬件资源全部通过软件的方式抽象化，最后进行调用使用的方法:使用hypervisor（VMM）软件，其原理是在底层硬件和服务器之间建立一个抽象层，而基于核心的虚拟机是面向Linux系统的开源产品hypervisor(VMM)可以捕捉CPU的指令，为指令访问硬件控制器和外设充当中介。 2）半虚拟化：需要修改操作系统 3）直通：直接使用物理硬件资源（需要支持，还不完善) 5、虚拟化的特性特性：优势： 集中化管理（远程管理、维护） 提高硬件利用率（物理资源利用 率低-例如峰值，虚拟化解决了“空闲”容量） 动态调整机器&#x2F;资源配置（虚拟化把系统的应用程序和服务硬件分离、提高了灵活性） 高可靠（可部署额外的功能和方案，可提高透明负载均衡、迁移、恢复复制等应用环境） 劣势： 前期高额费用（ 初期的硬件支持） 降低硬件利用率（特定场景-例如极度吃资源的应用不一定适合虚拟化） 更大的错误影响面（本地物理机down机会导致虚拟机均不可用，同时可能虚拟机中文件全部损坏） 实施配置复杂、管理复杂（管理人员运维、排障困难） 一定的限制性（虚拟化技术涉及各种限制，必须与支持&#x2F;兼容虚拟化的服务器、应用程序及供应商结合使用） 安全性（虚拟化技术自身的安全隐患） 二、KVM概述1、KVM简介Kernel-based Virtual Machine的简称，是一个开源的系统虚拟化模块，是RHEL 5.4推出的最新虚拟化技术，目前红帽只支持在64位的RHEL l5.4以上运行KVM，同时硬件需要支持VT技术，必须在64位bit环境中使用KVM。自Linux 2.6.20之后集成在Linux的各个主要发行版本中。它使用Linux自身的调度器进行管理，所以相对于Xen，其核心源码很少。KVM已成为学术界的主流VMM之一。 KVM的虚拟化需要硬件支持（如Intel VT技术或者AMD V技术)。是基于硬件的完全虚拟化。而Xen早期则是基于软件模拟的Para-Virtualization，新版本则是基于硬件支持的完全虚拟化。但Xen本身有自己的进程调度器，存储管理模块等，所以代码较为庞大。广为流传的商业系统虚拟化软件VMware ESX系列是基于软件模拟的Full-Virtualization。 通过一下命令可以查看系统是否支持VT cat &#x2F;proc&#x2F;cpuinfo | grep ‘vmx’ KVM的前身是QEMU，08年被红帽收购并获得一项技术hypervisor，不过RedHat的KVM被认为是将成为未来Linux hypervisor的主流。 2、KVM的作用 提高物理服务器的资源利用率(较少的在硬件上的投入) 可以批量部署 实现实时快照技术 支持克隆技术 可以实现虚拟机的离线迁移和动态迁移(提高IT部署的灵活性) 可以将资源动态调整(传统的IT架构资源是固定的，无法动态分配) 3、KVM 虚拟化架构&#x2F;三种模式 客户模式(guestOS):VM中的OS为GuestOS 客户机在操作系统中运行的模式，客户机分为用户模式和Linux内核模式，作用如下： 用户模式：为用户提供虚拟机管理的用户空间工具以及代表用户执行I&#x2F;O，Qemu工作在此模式下（Qemu的主要功能） Linux内核模式：模拟CPU、内存，实现客户模式切换，处理从客户模式的推出，KVM即运行在此模式下 4、KVM核心组件及作用 Guest：客户机系统，包括CPU（vCPU）、内存、驱动（Console、网卡、I&#x2F;O 设备驱动等），被 KVM 置于一种受限制的 CPU 模式下运行。 KVM：运行在内核空间，提供CPU 和内存的虚级化，以及客户机的 I&#x2F;O 拦截。Guest 的 I&#x2F;O 被 KVM 拦截后，交给 QEMU 处理。 QEMU：修改过的为 KVM 虚机使用的 QEMU 代码，运行在用户空间，提供硬件 I&#x2F;O 虚拟化，通过 IOCTL &#x2F;dev&#x2F;kvm 设备和 KVM 交互。 5、KVM工作流程用户模式的 Qemu 利用接口 libkvm 通过 ioctl 系统调用进入内核模式。 KVM驱动为虚拟机创建虚拟 CPU 和虚拟内存，然后执行 VMLAU·NCH 指令进入客户模式，装载 Guest OS 并运行。Guest OS 运行过程中如果发生异常，则暂停Guest OS的运行并保存当前状态同时退出到内核模式来处理这些异常。 内核模式处理这些异常时如果不需要 I&#x2F;O则处理完成后重新进入客户模式。如果需要 I&#x2F;O 则进入到用户模式， 则由 Qemu 来处理 I&#x2F;O，处理完成后进入内核模式，再进入客户模式 6、KVM的优势1）KVM与VMware的优势 ESX的底层是VMkernel + linux，VMkernel启动后开始接管对硬件管理，然后启动第一个linux虚拟机，协助VMkernel一起来管理和调度硬件资源。 KVM是直接将linux kernel变成hypervisor，只需要从标准linux内核启动即可，linux kernel拥有的特性可以全部利用上。KVM架构上的优势使得它非常简洁，在开发出来仅三个多月就被合并到了标准内核。 2）KVM与Xen的优势 Kvm和xen的最大区别就是架构，KVM是直接构建在linux kernel之上，把linux kernel变成hypervisor，是利用kernel已有的的功能基础上开发KVM所不具备的功能。 Xen的hypervisor是自己从头开始构建的，对硬件资源的调度管理，虚拟机的管理，还有很多接口与linux kernel不兼容，然后对于xen，hypervisor需要通过Domain0对虚拟机提供硬件访问驱动支持。 虽然说现在3.0的kernel中结合了xen，但是大部分linux OS的内核还是2.6，即便是一些新的发行版OS，也还是2.6的内核，新内核要单独去升级。 三、KVM虚拟化平台部署1、虚拟机资源CPU：双核双线程-CPU虚拟化开启 内存：8G 硬盘：300G 双网卡：单网卡 操作系统：Centos 7.4（1708） 2、实验环境1）修改主机名12[root@kvm ~]# hostnamectl set-hostname kvm [root@kvm ~]# bash 2）将镜像光盘设为自动&#x2F;永久挂载12345[root@kvm ~]# vim /etc/fstab/dev/cdrom /mnt iso9660 defaults 0 0[root@kvm ~]# mount -a [root@kvm ~]# df -hT/dev/sr0 iso9660 4.3G 4.3G 0 100% /mnt 3）环境优化#是否反解DNS，设置为NO可以让客户端连接服务器更快 12[root@kvm ~]# vim /etc/ssh/sshd_config UseDNS no #取消DNS注释，改为NO 4）制作本地YUM仓库1234567891011121314151617[root@kvm ~]# mkdir /abc[root@kvm ~]# cd /etc/yum.repos.d/[root@kvm yum.repos.d]# lsCentOS-Base.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repoCentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo[root@kvm yum.repos.d]# mkdir bak[root@kvm yum.repos.d]# mv CentOS-* bak[root@kvm yum.repos.d]# lsbak[root@kvm yum.repos.d]# vim local.repo[local]name=kvmbaseurl=file:///abcgpgcheck=0enabled=1[root@kvm yum.repos.d]# yum clean all [root@kvm yum.repos.d]# yum repolist 5）关闭防火墙、核心防护12345[root@kvm yum.repos.d]# systemctl stop firewalld[root@kvm yum.repos.d]# systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.[root@kvm yum.repos.d]# setenforce 0 3、安装KVM1） 安装KVM基本组件12345678910111213141516# 安装 GNOME 桌面环境 如果装了图形界面可以不需要装yum groupinstall -y &quot;GNOME Desktop&quot;# KVM 模块yum -y install qemu-kvm# 安装KVM 调试工具,可不安装yum -y install qemu-kvm-tools# 构建虚拟机的命令行工具yum -y install virt-install# qemu 组件,创建磁盘、启动虚拟机等yum -y install qemu-img# 网络支持工具yum -y install bridge-utils# 虚拟机管理工具yum -y install libvirt# 图形界面管理虚拟机 yum -y install virt-manager 2）检测CPU是否支持虚拟化12[root@kvm ~]# cat /proc/cpuinfo | grep vmx flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid mpx rdseed adx smap clflushopt xsaveopt xsavec arat spec_ctrl intel_stibp flush_l1d arch_capabilities 3）查看KVM模块是否已安装Lsmod：显示已载入的系统模块 1234[root@kvm ~]# lsmod | grep kvmkvm_intel 183621 0 kvm 586948 1 kvm_intelirqbypass 13503 1 kvm 4） 设置开启启动界面的显示模式1[root@kvm ~]# ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target 4、设置KVM网络1）KVM网络的两种模式：① NAT: 默认设置，数据包由 NAT 方式通过主机的接口进行 传送，可以访问外网，但是无法从外部访问虚拟机网络 ② 网桥：这种模式允许虚拟机像一台独立的主机一样拥有网络，外部的机器可以直接访问到虚拟机内部，但需要网卡支持(一般有线网卡都支持) 使用Bridge网桥模式进行部署 1234567891011121314151617181920[root@bdqn ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=ens33UUID=aac61931-47a6-40e5-9511-68adf20e97c5DEVICE=ens33ONBOOT=yes#IPADDR=192.168.100.46#PREFIX=24#GATEWAY=192.168.100.1BRIDGE=br0 #删除原先地址，设置为网桥模式，关联br0网卡 2）创建、编辑桥接网卡1234567891011121314151617181920[root@bdqn ~]# vim /etc/sysconfig/network-scripts/ifcfg-br0TYPE=BridgeBOOTPROTO=staticDEFROUTE=yesPEERDNS=yesPEERROUTES=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_PEERDNS=yesIPV6_PEERROUTES=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=br0DEVICE=br0ONBOOT=yesIPADDR=192.168.226.200NETMASK=255.255.255.0GATEWAY=192.168.226.2 5、KVM部署与管理1）创建KVM存储和镜像数据的目录、上传centos7镜像1234567891011[root@kvm network-scripts]# mkdir -p /data_kvm/iso[root@kvm network-scripts]# mkdir -p /data/_kvm/store[root@kvm network-scripts]# mkdir /abc[root@kvm network-scripts]# mount.cifs //192.168.226.1/LAMP-C7 /mntPassword for root@//192.168.226.1/LAMP-C7: [root@kvm network-scripts]# cd /abc[root@kvm abc]# cp -p CentOS-7-x86_64-DVD-1806.iso /data_kvm/iso/#查看镜像[root@kvm abc]# ll /data_kvm/iso/总用量 4481024-rwxr-xr-x. 1 root root 4588568576 2月 20 2019 CentOS-7-x86_64-DVD-1810.iso 2）使用虚拟系统管理器管理虚拟机1[root@kvm kvmdata]# virt-manager 3）操作步骤① 第一步：创建存储池 一样的操作创建iso存储池 ② 在store存储池中创建存储卷 ③ 创建虚拟机 启用最小化安装，节省空间","categories":[],"tags":[]},{"title":"","slug":"总结/21、MySQL迁移","date":"2024-12-14T22:02:47.130Z","updated":"2021-01-27T02:49:58.000Z","comments":true,"path":"2024/12/15/总结/21、MySQL迁移/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/21%E3%80%81MySQL%E8%BF%81%E7%A7%BB/","excerpt":"","text":"不同场景下MySQL的迁移方案一 、为什么要迁移MySQL 迁移是 DBA 日常维护中的一个工作。迁移，究其本义，无非是把实际存在的物体挪走，保证该物体的完整性以及延续性。就像柔软的沙滩上，两个天真无邪的小孩，把一堆沙子挪向其他地方，铸就内心神往的城堡。 生产环境中，有以下情况需要做迁移工作，如下：1、磁盘空间不够 比如一些老项目，选用的机型并不一定适用于数据库。随着时间的推移，硬盘很有可能出现短缺； 2、业务出现瓶颈。 比如项目中采用单机承担所有的读写业务，业务压力增大，不堪重负。如果 IO 压力在可接受的范围，会采用读写分离方案； 3、机器出现瓶颈 机器出现瓶颈主要在磁盘 IO 能力、内存、CPU，此时除了针对瓶颈做一些优化以外，选择迁移是不错的方案； 4、项目改造 某些项目的数据库存在跨机房的情况，可能会在不同机房中增加节点，或者把机器从一个机房迁移到另一个机房。再比如，不同业务共用同一台服务器，为了缓解服务器压力以及方便维护，也会做迁移。 一句话，迁移工作是不得已而为之。实施迁移工作，目的是让业务平稳持续地运行。 二、MySQL 迁移方案概览MySQL 迁移无非是围绕着数据做工作，再继续延伸，无非就是在保证业务平稳持续地运行的前提下做备份恢复。那问题就在怎么快速安全地进行备份恢复。 一方面，备份。针对每个主节点的从节点或者备节点，都有备份。这个备份可能是全备，可能是增量备份。在线备份的方法，可能是使用 mysqldump，可能是 xtrabackup，还可能是 mydumper。针对小容量（10GB 以下）数据库的备份，我们可以使用 mysqldump。但针对大容量数据库（数百GB 或者 TB 级别），我们不能使用 mysqldump 备份，一方面，会产生锁；另一方面，耗时太长。这种情况，可以选择 xtrabackup 或者直接拷贝数据目录。直接拷贝数据目录方法，不同机器传输可以使用 rsync，耗时跟网络相关。使用 xtrabackup，耗时主要在备份和网络传输。如果有全备或者指定库的备份文件，这是获取备份的最好方法。如果备库可以容许停止服务，直接拷贝数据目录是最快的方法。如果备库不允许停止服务，我们可以使用 xtrabackup（不会锁定 InnoDB 表），这是完成备份的最佳折中办法。 另一方面，恢复。针对小容量（10GB 以下）数据库的备份文件，我们可以直接导入。针对大容量数据库（数百GB 或者 TB 级别）的恢复，拿到备份文件到本机以后，恢复不算困难。具体的恢复方法可以参考第三节。 三、MySQL 迁移实战我们搞明白为什么要做迁移，以及迁移怎么做以后，接下来看看生产环境是怎样操作的。不同的应用场景，有不同的解决方案。 阅读具体的实战之前，假设和读者有如下约定： 1.为了保护隐私，本文中的服务器 IP 等信息经过处理； 2.如果服务器在同一机房，用服务器 IP 的 D 段代替服务器，具体的 IP 请参考架构图； 3.如果服务器在不同机房，用服务器 IP 的 C 段 和 D 段代替服务器，具体的 IP 请参考架构图； 4.每个场景给出方法，但不会详细地给出每一步执行什么命令，因为一方面，这会导致文章过长；另一方面，我认为只要知道方法，具体的做法就会迎面扑来的，只取决于掌握知识的程度和获取信息的能力； 5.实战过程中的注意事项请参考第四节。 3.1、 场景一：一主一从结构迁移从库遵循从易到难的思路，我们从简单的结构入手。A 项目，原本是一主一从结构。101 是主节点，102 是从节点。因业务需要，把 102 从节点迁移至 103，架构图如图一。102 从节点的数据容量过大，不能使用 mysqldump 的形式备份。和研发沟通后，形成一致的方案。 图一 一主一从结构迁移从库架构图 步骤：1.研发将 102 的读业务切到主库； 2.确认 102 MySQL 状态（主要看 PROCESS LIST），观察机器流量，确认无误后，停止 102 从节点的服务； 3.103 新建 MySQL 实例，建成以后，停止 MySQL 服务，并且将整个数据目录 mv 到其他地方做备份； 4.将 102 的整个 mysql 数据目录使用 rsync 拷贝到 103； 5.拷贝的同时，在 101 授权，使 103 有拉取 binlog 的权限（REPLICATION SLAVE, REPLICATION CLIENT）； 6.待拷贝完成，修改 103 配置文件中的 server_id，注意不要和 102 上的一致； 7.在 103 启动 MySQL 实例，注意配置文件中的数据文件路径以及数据目录的权限； 8.进入 103 MySQL 实例，使用 SHOW SLAVE STATUS 检查从库状态，可以看到 Seconds_Behind_Master 在递减； 9.Seconds_Behind_Master 变为 0 后，表示同步完成，此时可以用 pt-table-checksum 检查 101 和 103 的数据一致，但比较耗时，而且对主节点有影响，可以和开发一起进行数据一致性的验证； 10.和研发沟通，除了做数据一致性验证外，还需要验证账号权限，以防业务迁回后访问出错； 11.做完上述步骤，可以和研发协调，把 101 的部分读业务切到 103，观察业务状态； 12.如果业务没有问题，证明迁移成功。 3.2、场景二：一主一从结构迁移指定库我们知道一主一从只迁移从库怎么做之后，接下来看看怎样同时迁移主从节点。因不同业务同时访问同一服务器，导致单个库压力过大，还不便管理。于是，打算将主节点 101 和从节点 102 同时迁移至新的机器 103 和 104，103 充当主节点，104 充当从节点，架构图如图二。此次迁移只需要迁移指定库，这些库容量不是太大，并且可以保证数据不是实时的。 图二 一主一从结构迁移指定库架构图 步骤：1、103 和 104 新建实例，搭建主从关系，此时的主节点和从节点处于空载； 2、102 导出数据，正确的做法是配置定时任务，在业务低峰做导出操作，此处选择的是 mysqldump； 3、102 收集指定库需要的账号以及权限； 4、102 导出数据完毕，使用 rsync 传输到 103，必要时做压缩操作； 5、103 导入数据，此时数据会自动同步到 104，监控服务器状态以及 MySQL 状态； 6、103 导入完成，104 同步完成，103 根据 102 收集的账号授权，完成后，通知研发检查数据以及账户权限； 7、上述完成后，可研发协作，将 101 和 102 的业务迁移到 103 和 104，观察业务状态； 8、如果业务没有问题，证明迁移成功。 3.3、场景三 ：一主一从结构双边迁移指定库接下来看看一主一从结构双边迁移指定库怎么做。同样是因为业务共用，导致服务器压力大，管理混乱。于是，打算将主节点 101 和从节点 102 同时迁移至新的机器 103、104、105、106，103 充当 104 的主节点，104 充当 103 的从节点，105 充当 106 的主节点，106 充当 105 的从节点，架构图如图三。此次迁移只需要迁移指定库，这些库容量不是太大，并且可以保证数据不是实时的。我们可以看到，此次迁移和场景二很类似，无非做了两次迁移。 图三 一主一从结构双边迁移指定库架构图 步骤：1、103 和 104 新建实例，搭建主从关系，此时的主节点和从节点处于空载； 2、102 导出 103 需要的指定库数据，正确的做法是配置定时任务，在业务低峰做导出操作，此处选择的是 mysqldump； 3、102 收集 103 需要的指定库需要的账号以及权限； 4、102 导出103 需要的指定库数据完毕，使用 rsync 传输到 103，必要时做压缩操作； 5、103 导入数据，此时数据会自动同步到 104，监控服务器状态以及 MySQL 状态； 6、103 导入完成，104 同步完成，103 根据 102 收集的账号授权，完成后，通知研发检查数据以及账户权限； 7、上述完成后，和研发协作，将 101 和 102 的业务迁移到 103 和 104，观察业务状态； 8、105 和 106 新建实例，搭建主从关系，此时的主节点和从节点处于空载；9、102 导出 105 需要的指定库数据，正确的做法是配置定时任务，在业务低峰做导出操作，此处选择的是 mysqldump； 10、102 收集 105 需要的指定库需要的账号以及权限； 11、102 导出 105 需要的指定库数据完毕，使用 rsync 传输到 105，必要时做压缩操作； 12、105 导入数据，此时数据会自动同步到 106，监控服务器状态以及 MySQL 状态； 13、105 导入完成，106 同步完成，105 根据 102 收集的账号授权，完成后，通知研发检查数据以及账户权限； 14、上述完成后，和研发协作，将 101 和 102 的业务迁移到 105 和 106，观察业务状态； 15、如果所有业务没有问题，证明迁移成功。 3.4、场景四：一主一从结构完整迁移主从接下来看看一主一从结构完整迁移主从怎么做。和场景二类似，不过此处是迁移所有库。因 101 主节点 IO 出现瓶颈，打算将主节点 101 和从节点 102 同时迁移至新的机器 103 和 104，103 充当主节点，104 充当从节点。迁移完成后，以前的主节点和从节点废弃，架构图如图四。此次迁移是全库迁移，容量大，并且需要保证实时。这次的迁移比较特殊，因为采取的策略是先替换新的从库，再替换新的主库。所以做法稍微复杂些。 图四 一主一从结构完整迁移主从架构图 步骤：1、研发将 102 的读业务切到主库； 2、确认 102 MySQL 状态（主要看 PROCESS LIST，MASTER STATUS），观察机器流量，确认无误后，停止 102 从节点的服务；3、104 新建 MySQL 实例，建成以后，停止 MySQL 服务，并且将整个数据目录 mv 到其他地方做备份，注意，此处操作的是 104，也就是未来的从库；4、将 102 的整个 mysql 数据目录使用 rsync 拷贝到 104；5、拷贝的同时，在 101 授权，使 104 有拉取 binlog 的权限（REPLICATION SLAVE, REPLICATION CLIENT）；6、待拷贝完成，修改 104 配置文件中的 server_id，注意不要和 102 上的一致；7、在 104 启动 MySQL 实例，注意配置文件中的数据文件路径以及数据目录的权限；8、进入 104 MySQL 实例，使用 SHOW SLAVE STATUS 检查从库状态，可以看到 Seconds_Behind_Master 在递减；9、Seconds_Behind_Master 变为 0 后，表示同步完成，此时可以用 pt-table-checksum 检查 101 和 104 的数据一致，但比较耗时，而且对主节点有影响，可以和开发一起进行数据一致性的验证；10、除了做数据一致性验证外，还需要验证账号权限，以防业务迁走后访问出错；11、和研发协作，将之前 102 从节点的读业务切到 104；12、利用 102 的数据，将 103 变为 101 的从节点，方法同上；13、接下来到了关键的地方了，我们需要把 104 变成 103 的从库；- 104 STOP SLAVE；- 103 STOP SLAVE IO_THREAD;- 103 STOP SLAVE SQL_THREAD，记住 MASTER_LOG_FILE 和 MASTER_LOG_POS；- 104START SLAVE UNTIL到上述 MASTER_LOG_FILE 和 MASTER_LOG_POS；- 104 再次 STOP SLAVE；- 104 RESET SLAVE ALL 清除从库配置信息；- 103 SHOW MASTER STATUS，记住 MASTER_LOG_FILE 和 MASTER_LOG_POS；- 103 授权给 104 访问 binlog 的权限；- 104 CHANGE MASTER TO 103；- 104 重启 MySQL，因为 RESET SLAVE ALL 后，查看 SLAVE STATUS，Master_Server_Id 仍然为 101，而不是 103；- 104 MySQL 重启后，SLAVE 回自动重启，此时查看 IO_THREAD 和 SQL_THREAD 是否为 YES；- 103 START SLAVE；- 此时查看 103 和 104 的状态，可以发现，以前 104 是 101 的从节点，如今变成 103 的从节点了。14、业务迁移之前，断掉 103 和 101 的同步关系；15、做完上述步骤，可以和研发协调，把 101 的读写业务切回 102，读业务切到 104。需要注意的是，此时 101 和 103 均可以写，需要保证 101 在没有写入的情况下切到 103，可以使用 FLUSH TABLES WITH READ LOCK 锁住 101，然后业务切到 103。注意，一定要业务低峰执行，切记；16、切换完成后，观察业务状态；17、如果业务没有问题，证明迁移成功。 3.5、场景五：双主结构跨机房迁移接下来看看双主结构跨机房迁移怎么做。某项目出于容灾考虑，使用了跨机房，采用了双主结构，双边均可以写。因为磁盘空间问题，需要对 A 地的机器进行替换。打算将主节点 1.101 和从节点 1.102 同时迁移至新的机器 1.103 和 1.104，1.103 充当主节点，1.104 充当从节点。B 地的 2.101 和 2.102 保持不变，但迁移完成后，1.103 和 2.101 互为双主。架构图如图五。因为是双主结构，两边同时写，如果要替换主节点，单方必须有节点停止服务。 图五 双主结构跨机房迁移架构图 步骤：1、1.103 和 1.104 新建实例，搭建主从关系，此时的主节点和从节点处于空载；2、确认 1.102 MySQL 状态（主要看 PROCESS LIST），注意观察 MASTER STATUS 不再变化。观察机器流量，确认无误后，停止 1.102 从节点的服务；3、1.103 新建 MySQL 实例，建成以后，停止 MySQL 服务，并且将整个数据目录 mv 到其他地方做备份；4、将 1.102 的整个 mysql 数据目录使用 rsync 拷贝到 1.103；5、拷贝的同时，在 1.101 授权，使 1.103 有拉取 binlog 的权限（REPLICATION SLAVE, REPLICATION CLIENT）；6、待拷贝完成，修改 1.103 配置文件中的 server_id，注意不要和 1.102 上的一致；7、在 1.103 启动 MySQL 实例，注意配置文件中的数据文件路径以及数据目录的权限；8、进入 1.103 MySQL 实例，使用 SHOW SLAVE STATUS 检查从库状态，可以看到 Seconds_Behind_Master 在递减；9、Seconds_Behind_Master 变为 0 后，表示同步完成，此时可以用 pt-table-checksum 检查 1.101 和 1.103 的数据一致，但比较耗时，而且对主节点有影响，可以和开发一起进行数据一致性的验证；10、我们使用相同的办法，使 1.104 变成 1.103 的从库；11、和研发沟通，除了做数据一致性验证外，还需要验证账号权限，以防业务迁走后访问出错；12、此时，我们要做的就是将 1.103 变成 2.101 的从库，具体的做法可以参考场景四；13、需要注意的是，1.103 的单双号配置需要和 1.101 一致；14、做完上述步骤，可以和研发协调，把 1.101 的读写业务切到 1.103，把 1.102 的读业务切到 1.104。观察业务状态；15、如果业务没有问题，证明迁移成功。 3.6、场景六：多实例跨机房迁移接下来我们看看多实例跨机房迁移证明做。每台机器的实例关系，我们可以参考图六。此次迁移的目的是为了做数据修复。在 2.117 上建立 7938 和 7939 实例，替换之前数据异常的实例。因为业务的原因，某些库只在 A 地写，某些库只在 B 地写，所以存在同步过滤的情况。 图六 多实例跨机房迁移架构图 步骤：1、1.113 针对 7936 实例使用 innobackupex 做数据备份，注意需要指定数据库，并且加上 slave-info 参数； 2、备份完成后，将压缩文件拷贝到 2.117；3、2.117 创建数据目录以及配置文件涉及的相关目录；4、2.117 使用 innobackupex 恢复日志；5、2.117 使用 innobackupex 拷贝数据；6、2.117 修改配置文件，注意如下参数：replicate-ignore-db、innodb_file_per_table &#x3D; 1、read_only &#x3D; 1、 server_id；7、2.117 更改数据目录权限；8、1.112 授权，使 2.117 有拉取 binlog 的权限（REPLICATION SLAVE, REPLICATION CLIENT）；9、2.117 CHANGE MASTE TO 1.112，LOG FILE 和 LOG POS 参考 xtrabackup_slave_info；10、2.117 START SLAVE，查看从库状态； 11、2.117 上建立 7939 的方法类似，不过配置文件需要指定 replicate-wild-do-table；12、和开发一起进行数据一致性的验证和验证账号权限，以防业务迁走后访问出错；13、做完上述步骤，可以和研发协调，把相应业务迁移到 2.117 的 7938 实例和 7939 实例。观察业务状态；14、如果业务没有问题，证明迁移成功。 四、注意事项介绍完不同场景的迁移方案，需要注意如下几点： 1、数据库迁移，如果涉及事件，记住主节点打开 event_scheduler 参数；2、不管什么场景下的迁移，都要随时关注服务器状态，比如磁盘空间，网络抖动；另外，对业务的持续监控也是必不可少的；3、CHANGE MASTER TO 的 LOG FILE 和 LOG POS 切记不要找错，如果指定错了，带来的后果就是数据不一致；4、执行脚本不要在 $HOME 目录，记住在数据目录；5、迁移工作可以使用脚本做到自动化，但不要弄巧成拙，任何脚本都要经过测试；6、每执行一条命令都要三思和后行，每个命令的参数含义都要搞明白；7、多实例环境下，关闭 MySQL 采用 mysqladmin 的形式，不要把正在使用的实例关闭了；8、从库记得把 read_only &#x3D; 1 加上，这会避免很多问题；9、每台机器的 server_id 必须保证不一致，否则会出现同步异常的情况；10、正确配置 replicate-ignore-db 和 replicate-wild-do-table；11、新建的实例记得把 innodb_file_per_table 设置为 1，上述中的部分场景，因为之前的实例此参数为 0，导致 ibdata1 过大，备份和传输都消耗了很多时间；12、使用 gzip 压缩数据时，注意压缩完成后，gzip 会把源文件删除。13、所有的操作务必在从节点或者备节点操作，如果在主节点操作，主节点很可能会宕机； 14、xtrabackup 备份不会锁定 InnoDB 表，但会锁定 MyISAM 表。所以，操作之前记得检查下当前数据库的表是否有使用 MyISAM 存储引擎的，如果有，要么单独处理，要么更改表的 Engine； 五、技巧在 MySQL 迁移实战中，有如下技巧可以使用： 1、任何迁移 LOG FILE 以 relay_master_log_file（正在同步 master 上的 binlog 日志名）为准，LOG POS 以 exec_master_log_pos（正在同步当前 binlog 日志的 POS 点）为准；2、使用 rsync 拷贝数据，可以结合 expect、nohup 使用，绝对是绝妙组合；3、在使用 innobackupex 备份数据的同时可以使用 gzip 进行压缩；4、在使用 innobackupex 备份数据，可以加上 –slave-info 参数，方便做从库；5、在使用 innobackupex 备份数据，可以加上 –throttle 参数，限制 IO，减少对业务的影响。还可以加上 –parallel&#x3D;n 参数，加快备份，但需要注意的是，使用 tar 流压缩，–parallel 参数无效。 6、做数据的备份与恢复，可以把待办事项列个清单，画个流程，然后把需要执行的命令提前准备好；7、本地快速拷贝文件夹，有个不错的方法，使用 rsync，加上如下参数：-avhW –no-compress –progress；8、 不同分区之间快速拷贝数据，可以使用 dd。或者用一个更靠谱的方法，备份到硬盘，然后放到服务器上。异地还有更绝的，直接快递硬盘。 六、总结本文从为什么要迁移讲起，接下来讲了迁移方案，然后讲解了不同场景下的迁移实战，最后给出了注意事项以及实战技巧。归纳起来，也就以下几点： 第一，迁移的目的是让业务平稳持续地运行； 第二，迁移的核心是怎么延续主从同步，我们需要在不同服务器和不同业务之间找到方案； 第三，业务切换需要考虑不同 MySQL 服务器之间的权限问题；需要考虑不同机器读写分离的顺序以及主从关系；需要考虑跨机房调用对业务的影响。","categories":[],"tags":[]},{"title":"","slug":"总结/20、灰度发布","date":"2024-12-14T22:02:47.127Z","updated":"2021-01-27T02:49:34.000Z","comments":true,"path":"2024/12/15/总结/20、灰度发布/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/20%E3%80%81%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/","excerpt":"","text":"什么是灰度发布?# 什么是灰度发布，以及灰度发布A&#x2F;B测试 在一般情况下，升级服务器端应用，需要将应用源码或程序包上传到服务器，然后停止掉老版本服务，再启动新版本。但是这种简单的发布方式存在两个问题，一方面，在新版本升级过程中，服务是暂时中断的，另一方面，如果新版本有BUG，升级失败，回滚起来也非常麻烦，容易造成更长时间的服务不可用。 为了解决这些问题，人们研究出了多种发布策略，下面我们一一介绍。 蓝绿部署 所谓蓝绿部署，是指同时运行两个版本的应用，如上图所示，蓝绿部署的时候，并不停止掉老版本，而是直接部署一套新版本，等新版本运行起来后，再将流量切换到新版本上。但是蓝绿部署要求在升级过程中，同时运行两套程序，对硬件的要求就是日常所需的二倍，比如日常运行时，需要10台服务器支撑业务，那么使用蓝绿部署，你就需要购置二十台服务器。 滚动发布滚动发布能够解决掉蓝绿部署时对硬件要求增倍的问题。 所谓滚动升级，就是在升级过程中，并不一下子启动所有新版本，是先启动一台新版本，再停止一台老版本，然后再启动一台新版本，再停止一台老版本，直到升级完成，这样的话，如果日常需要10台服务器，那么升级过程中也就只需要11台就行了。 但是滚动升级有一个问题，在开始滚动升级后，流量会直接流向已经启动起来的新版本，但是这个时候，新版本是不一定可用的，比如需要进一步的测试才能确认。那么在滚动升级期间，整个系统就处于非常不稳定的状态，如果发现了问题，也比较难以确定是新版本还是老版本造成的问题。 为了解决这个问题，我们需要为滚动升级实现流量控制能力。 灰度发布（金丝雀发布）灰度发布也叫金丝雀发布，起源是，矿井工人发现，金丝雀对瓦斯气体很敏感，矿工会在下井之前，先放一只金丝雀到井中，如果金丝雀不叫了，就代表瓦斯浓度高。 在灰度发布开始后，先启动一个新版本应用，但是并不直接将流量切过来，而是测试人员对新版本进行线上测试，启动的这个新版本应用，就是我们的金丝雀。如果没有问题，那么可以将少量的用户流量导入到新版本上，然后再对新版本做运行状态观察，收集各种运行时数据，如果此时对新旧版本做各种数据对比，就是所谓的A&#x2F;B测试。 当确认新版本运行良好后，再逐步将更多的流量导入到新版本上，在此期间，还可以不断地调整新旧两个版本的运行的服务器副本数量，以使得新版本能够承受越来越大的流量压力。直到将100%的流量都切换到新版本上，最后关闭剩下的老版本服务，完成灰度发布。 如果在灰度发布过程中（灰度期）发现了新版本有问题，就应该立即将流量切回老版本上，这样，就会将负面影响控制在最小范围内。 使用脉冲云轻松地实现灰度发布脉冲云的部署管理可以轻松实现上述的带有流量管理功能的灰度发布。正常编辑应用信息后点击保存，然后脉冲云会提示直接升级或灰度发布。 直接升级就是使用一般的滚动升级，点击灰度发布后可以人工干预升级过程，进行流量控制。 选择灰度发布后，就会呈现灰度发布控制面板。 在这个控制面板上，可以拖拉滑块，快速调整新旧版本的运行副本数量，同时也可以按百分比，将流量导入到新版本上。此外，还可以通过匹配HTTP Header，指定个别用户的流量到新版本上。 除了匹配用户流量的HTTP请求头，还可以直接指定匹配请求头中的Cookie信息，匹配规则支持精确匹配、包含、正则、前缀、后缀等，甚至还允许反向匹配。 当确认新版本运行无误后，就可以点击 完成升级 按钮，就会将流量全部切换到新版本上，并且销毁掉所有老版本应用。如果新版本出了问题，可以点击 取消升级 按钮，立即将流量切回老版本，并销毁掉新版本应用。 总结在新版本应用发布时，为了服务器不停机升级，使用灰度发布策略，在灰度发布开始时，使用HTTP Header 匹配指定测试人员的流量到新版本上，然后当新版本内部测试通过后，可以再按百分比，将用户流量一点一点导入到新版本中，比如先导入10%观察一下运行情况，然后再导入20%，如此累加，直到将流量全部导入到新版本上，最后完成升级，如果期间发现问题，就立即取消升级，将流量切回到老版本。 运用灰度发布，就再也不需要加班到深夜进行停机升级了，在白天就可以放心大胆地、安全地发布新版本。 相关资料： 脉冲云：脉冲云 灰度发布视频：https://v.youku.com/v_show/id_XMzc4OTU3OTA3Mg==.html?spm=a2h3j.8428770.3416059.1","categories":[],"tags":[]},{"title":"","slug":"总结/19、阿里云产品","date":"2024-12-14T22:02:47.120Z","updated":"2021-02-22T06:52:48.000Z","comments":true,"path":"2024/12/15/总结/19、阿里云产品/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/19%E3%80%81%E9%98%BF%E9%87%8C%E4%BA%91%E4%BA%A7%E5%93%81/","excerpt":"","text":"一、阿里云四大件分类说比较好理解，先说阿里云的明星产品四大件吧，即云服务器ECS、云数据库RDS、负载均衡SLB和对象存储OSS。 1、云服务器ECS云服务器ECS：云计算产品的基本款，几乎每个客户都必买的，云服务器从1核1G到32核64G（随着时间推移，配置会越来越高），各种优惠都有，不同时段有不同的优惠活动，可以参考阿里云惠网；关于服务器配置还可以随意升降配置，可以包年包月，也可以按量随用随买。对于很多小公司及个人，只购买一台云服务器ECS就够用了。对于稍微大一点的企业从性能、安全、加载速度等方面诸多考虑，可能需要购买其他的阿里云产品。 阿里云从云服务器ECS衍生出来很多云服务器系列，例如适用于初级用户的轻量应用服务器，还有为了迎合各种高性能场景的云服务器，诸如GPU云服务器、FPGA云服务器、神龙云服务器等，总之都是云服务器，是企业上云的基本款。 2、云数据库RDS云数据库：目前主流是MySQL，阿里云提供MySQL、PostgreSQL，SQL Server，MongoDB，Memcache（Redis）等不同的数据库产品。相对于云服务器，云数据库属于非必需品，因为用户完全可以在云服务器上搭建数据库。由于自身业务发展需要，将数据库独立出来，这时候就需要阿里云的RDS云数据库了。 3、负载均衡SLB负载均衡SLB：对多台云服务器进行流量分发服务。为了应对业务需求，企业往往会有多台云服务器提供服务器，负载均衡就是将用户的请求按照企业自定义的策略转发到最优的服务器。 4、对象存储OSS如果企业静态文件较多（图片、视频等大文件），可以将大量的存储内容转移独立出来，放到对象存储OSS里面。 5、其他的云计算产品以上四款产品，系阿里云云计算产品的四件套。下面再介绍阿里云其他的云计算产品： 1）内容分发网络CDN：内容分发网络，假设企业的云服务器在杭州，那么位于东北地区的用户访问速度就会比较慢，CDN可以解决这个问题，CDN将源站内容分发至最接近用户的节点，使用户可就近取得所需内容，提高用户访问的响应速度和成功率。 2）专有网络 VPC：大家普遍会给阿里云打上公有云的标签，实际上阿里云可以提供的不仅仅是公有云，还有私有云、混合云等。专有网络VPC可以帮助企业在阿里云构建出一个隔离的网络环境，用户可以自定义IP 地址范围、网段、路由表和网关等，VPC可以提供更安全和灵活的网络环境，为我们构建混合云提供服务。 2）弹性伸缩：传统的企业自建的私有机房是不具有弹性伸缩功能的，假设企业遇到业务波峰，只能通过人为的升级硬件来应对，业务回落时就会造成硬件资源的浪费，而弹性伸缩很好的解决了这个痛点。阿里云弹性伸缩可以管理您的集群，在高峰期自动增加ECS实例，在业务回落时自动减少ECS实例，节省基础设施成本。另外，这个弹性伸缩是免费的。 二、阿里云安全系列产品有些乱，来说下阿里云应对网络安全方面的产品吧： 1、DDoS高防IPDDoS是目前比较常见的攻击方式，为了抵御DDoS攻击，用户可以通过配置高防IP，将攻击流量引流到高防IP，确保源站的稳定可靠。讲真，阿里云的DDoS高防IP还挺贵的。 2、安骑士当用户购买了云服务器ECS后，可能会受到阿里云发送的安骑士漏洞风险短信，安骑士一款主机安全软件，为您提供主机漏洞检测、基线检查、病毒查杀、资产统一管理等功能，为您建立安全运维管理平台。安骑士企业版可以免费试用7天，之后想再使用，是需要付费的。 3、证书服务我们在访问网站时，会在浏览器的地址栏中看到绿色的锁，意思是该网站是基于HTTPS协议的。前几年网站基本上都是基于http协议，阿里云百科网目前还是基于http协议，相对于http协议，https提供了一层加密服务，会更加安全一些。网站想要实现HTTPS，可以向阿里云申请签发证书服务器，即我们常说的SSL证书。阿里云目前可以申请到免费的SSL证书（Symantec赛门铁克品牌）。 4、态势感知态势感知说起来还比较高端，有点类似于先知的意思。态势感知会收集企业20种原始日志和网络空间威胁情报，利用机器学习还原已发生的攻击，并预测未发生的攻击，帮客户扩大安全可见性，并集中管理云上资产安全事件。 5、堡垒机日防夜防家贼难防，开个玩笑哈。企业往往更加关注外部的安全威胁而忽略了企业内部，实际上运维人员误操作或者仿冒运维人员将对企业造成很严重的损失，更有甚至是致命的。例如：携程的宕机12小时事件，由于员工错误操作，删除了服务器代码，据不完全统计，携程宕机带来的直接损失就是每小时160万美金。堡垒机基于协议正向代理实现，对SSH、Windows远程桌面、SFTP等常见运维协议的 数据流进行全程记录，再通过协议数据流重组的方式进行录像回放，达到运维审计的目的。 三、阿里云网络中间件相关消息队列MQ说起消息队列，最典型的应用场景就是一年一度的双十一购物节，消息队列是一个真正具备低延迟、高并发、高可用、高可靠，可支撑万亿级数据洪峰的分布式消息中间件。当小仙女们开启大规模的剁手模式时，用户大量并发访问商品数据库，消息队列可以缓解瓶颈，减少页面响应时间，当然还有其他方面的功能优势，咱这里阿里云百科网就不过多赘述，双十一就是MQ的典型应用场景，大概就是这么个意思。 四、阿里云万网产品1、域名这个好像大家都知道，我还说说吧，比如阿里云百科的域名就是aliyunbaike.com 2、虚拟主机新手建站一般都是从虚拟主机开始的，无需自己配置web环境，简单易管理，价格也便宜。 3、企业邮箱企业邮箱就是以公司域名为后缀的邮箱，企业自建的邮件系统。目前各大互联网大佬，例如：阿里云、腾讯云、网易等都有提供免费版的企业邮箱，如果想解除诸多限制，可以选购阿里云的企业邮箱付费版。 4、云解析DNSDNS就是将你的域名解析到服务器的IP上，一般来讲域名解析是免费的，免费版就够用了。 公有云和私有云概念bai公有云，第三方提供商用户能够使使用的云，公有云一般可通过 Internet 使用，可能是免费或成本低廉的。私有云，是指企业自己使用的云，它所有的服务不是供别人使用，而是供自己内部人员或分支机构使用。私有云的部署比较适合于有众多分支机构的大型企业或政府部门。随着这些大型企业数据中心的集中化，私有云将会成为他们部署IT系统的主流模式。简单的说：私有云就是你自己的家，只有自己或者你允许的人能住，一般你不愿意外人知道的隐私都放在家里；公有云就是收费景点，买了门票的都能进。公有云和私有云的区别IT设施的位置：当企业自己构建一个私有云平台的时候，IT基础设施是自己的，一般位于企业内部。而采用公有云平台的时候，IT基础设施是位于一个第三方的数据中心。这里有一个例外，那就是现在有一些服务提供商提出的虚拟私有云（VPC， Virtual Private Cloud）的概念，它指的是在第三方数据中心内部通过技术手段隔离出来的一个专用计算环境，并通过安全通道与企业相连接。基础设施差异性：对于许多大型企业，由于经过了多年的IT建设和技术演变，他们的IT基础设施往往采用了不同的技术和平台，也就是说，这些企业采用的是异构平台环境。但是，对于目前大部分公有云服务提供商来说，他们的平台往往是通过廉价和标准的硬件平台来构建的。这些标准化方式构建的平台能够以比较好的性价比满足大部分用户的需求。另外，在服务的提供方面，公有云服务提供商往往提供最为大众化的、需求量最为广泛和集中的服务。因此，对于公有云服务来说，其服务和环境往往是同构的，这与企业自建的IT环境不一样。商务模式：企业如果选择自己构建IT系统，那么显然需要进行一次性的大量投资来采购软、硬件设备，甚至包括数据中心的基础建设等。在企业的财务报表中，这体现为一个比较大的固定成本。但是，如果企业采用第三方提供的公有云服务，那么根据目前云计算服务的收费方式，企业可以选择按月服务费的方式或者按IT资源使用量的方式来进行付费。这样，对于企业来说不需要一个大量的前期投入就可以使用IT服务，其体现为一个持续的运营成本。控制程度的不同：企业自己构建的IT系统是作为企业资产完全由企业自己拥有，并由企业自己来运维。虽然企业需要自己的IT运维团队，但好处是企业可以独立控制IT系统，并根据实际需要来进行改造和客户化。而对于公有云服务，企业实际上是采用租用服务的方式，好处是不需要自己来管理基础平台服务， 概念 https://www.aliyun.com/product/rds/mysql?spm=a2cls.b92374736.J_8058803260.154.7c22357aBf09Yz存储与内容分发服务 - CDN内容分发网络：可以将源站资源备份到多个CDN节点上，这样如果用户（在北京）要看一个视频，就不用从源站（广州）获取，直接取离自己最近的一个CDN节点上的备份即可，加快内容分发速度，尤其适合直播类应用。 - OSS对象存储服务：非结构化存储，支持大量存储视频、语音等对象。 - OAS开放归档服务：如果有的数据已经过了几年，不再有高重要性，可以采用便宜大碗的云端归档存储。 - KVStore键值存储：使用键值对存储方法，使用于对快速检索有需求的业务。 弹性计算服务 - ECS云服务器：阿里云最最基础的产品，很多小型用户只租用一台云服务器就能满足基本需求。 - SLB负载均衡：当服务器比较多形成集群后，为了科学有效地分摊负载，需要购买负载均衡产品。 - VPC专有网络：多为大型企业选购，用于内网搭建，提升数据安全。 - ESS自动伸缩：自动完成资源自动扩展或缩减，适用于并发量大的业务。 数据存储&amp;云计算 - RDS：云数据库，也是阿里云基础产品之一。 - OTS：大规模快速查询，适用于药品查询、图书查询等业务。 - OCS：内存存储。 - DRDS：当有多个RDS对象时，进行统一访问与管理。 大规模计算服务 - ODPS：最基础的大规模计算产品，用于大量数据的批量离线计算。用户可以写一段批处理sql，提交任务后可能选择在凌晨开始运行，上班前拿到结果（根据数据量而定）。 - ADS：适用于业务紧急、无法忍受ODPS离线计算时长的用户，秒级的大规模查询分析。 应用服务 - ACE云引擎：用户可以通过安装云引擎简化对开发环境的部署和运维过程，直接在上面编写程序即可。 - PTS：实现对程序的海量压力测试，模拟真实环境。 安全与管理服务 - 云盾 - 云安全 - DDos ecs创建操作步骤 前往实例创建页。 完成基础配置。 选择计费方式。本示例中，选择按量付费。 选择地域和可用区，例如华东1（杭州），可用区默认选择随机分配。 说明 实例创建完成后，不可更改地域和可用区，请谨慎选择。 选择实例规格并设置实例数量。本示例中，选择***所有代* &gt; *x86 计算* &gt; *共享型* &gt; **突发性能实例 t5****。 可供选择的实例规格由您所选择的地域决定。详情请参见实例规格。 选择镜像。本示例中，选择公共镜像 CentOS 7.6 64位。 选择存储。本示例中，仅使用系统盘，默认选择高效云盘 40 GiB。 单击下一步：网络和安全组，完成网络和安全组设置。 选择网络类型为专有网络。本示例中，选择默认专有网络和默认交换机。 设置公网带宽。本示例中，选择分配公网IPv4地址为实例分配一个公网IP地址，并选择按使用流量对公网带宽计费。 选择安全组。如果您没有创建安全组，可以使用默认安全组。 添加弹性网卡。如果所选实例规格不支持弹性网卡，跳过这一步。 单击下一步：系统配置。 您可以选填此页面中的选项，建议您设置登录凭证和实例名称。本示例中，选择自定义密码，并将实例名称设为ecs-01。 单击下一步：分组设置。 您可以选填此页面中的选项，有多台实例时，建议添加标签方便管理。 单击下一步：确认订单。 确认所选配置，您也可以单击编辑图标返回修改配置。 阅读和确认云服务器ECS服务条款，然后单击创建实例。 ecs优势选择云服务器ECS，您可以轻松构建具有以下优势的计算资源： 无需自建机房，无需采购以及配置硬件设施。 分钟级交付，快速部署，缩短应用上线周期。 快速接入部署在全球范围内的数据中心和BGP（Border Gateway Protocol，边界网关协议）机房。 成本透明，按需使用，支持根据业务波动随时扩展和释放资源。 提供GPU和FPGA等异构计算服务器、弹性裸金属服务器以及通用的x86架构服务器。 支持通过内网访问其他阿里云服务，形成丰富的行业解决方案，降低公网流量成本。 提供虚拟防火墙、角色权限控制、内网隔离、防病毒攻击及流量监控等多重安全方案。 提供性能监控框架和主动运维体系。 提供行业通用标准API，提高易用性和适用性。 产品架构云服务器ECS主要包含以下功能组件： 实例：等同于一台虚拟服务器，内含CPU、内存、操作系统、网络配置、磁盘等基础的计算组件。实例的计算性能、内存性能和适用业务场景由实例规格决定，其具体性能指标包括实例vCPU核数、内存大小、网络性能等。 镜像：提供实例的操作系统、初始化应用数据及预装的软件。操作系统支持多种Linux发行版和多种Windows Server版本。 块存储：块设备类型产品，具备高性能和低时延的特性。提供基于分布式存储架构的云盘以及基于物理机本地存储的本地盘。 快照：某一时间点一块云盘的数据状态文件。常用于数据备份、数据恢复和制作自定义镜像等。 安全组：由同一地域内具有相同保护需求并相互信任的实例组成，是一种虚拟防火墙，用于设置实例的网络访问控制。 网络 ： 专有网络（Virtual Private Cloud）：逻辑上彻底隔离的云上私有网络。您可以自行分配私网IP地址范围、配置路由表和网关等。 经典网络：所有经典网络类型实例都建立在一个共用的基础网络上。由阿里云统一规划和管理网络配置。 更多功能组件详情，请参见云服务器ECS产品详情页。 以下为云服务器ECS的产品组件架构图，图中涉及的功能组件的详细介绍请参见相应的帮助文档。 产品定价云服务器ECS支持包年包月、按量付费、预留实例券、抢占式实例等多种账单计算模式。更多详情，请参见计费概述和云产品定价页。 管理工具通过注册阿里云账号，您可以在任何地域下，通过阿里云提供的以下途径创建、使用或者释放云服务器ECS： ECS管理控制台：具有交互式操作的Web服务页面。关于管理控制台的操作，请参见常用操作导航。 ECS API：支持GET和POST请求的RPC风格API。关于API说明，请参见 API参考 。以下为调用云服务器ECS API的常用开发者工具： 命令行工具CLI：基于阿里云API建立的灵活且易于扩展的管理工具。您可基于命令行工具封装阿里云的原生API，扩展出您需要的功能。 OpenAPI Explorer：提供快速检索接口、在线调用API和动态生成SDK示例代码等服务。 阿里云SDK：提供Java、Python、PHP等多种编程语言的SDK。 资源编排（Resource Orchestration Service）：通过创建一个描述您所需的所有阿里云资源的模板，然后资源编排将根据模板，自动创建和配置资源。 运维编排服务（Operation Orchestration Service）：自动化管理和执行运维任务。您可以在执行模板中定义执行任务、执行顺序、执行输入和输出等，通过执行模板达到自动化完成运维任务的目的。 Terraform：能够通过配置文件在阿里云以及其他支持Terraform的云商平台调用计算资源，并对其进行版本控制的开源工具。 阿里云App：移动端类型的管理工具。 Alibaba Cloud Toolkit：阿里云针对IDE平台为开发者提供的一款插件，用于帮助您高效开发并部署适合在云端运行的应用。 部署建议您可以从以下维度考虑如何启动并使用云服务器ECS： 地域和可用区 地域指阿里云的数据中心，地域和可用区决定了ECS实例所在的物理位置。一旦成功创建实例后，其元数据（仅专有网络VPC类型ECS实例支持获取元数据）将确定下来，并无法更换地域。您可以从用户地理位置、阿里云产品发布情况、应用可用性、以及是否需要内网通信等因素选择地域和可用区。例如，如果您同时需要通过阿里云内网使用云数据库RDS，RDS实例和ECS实例必须处于同一地域中。更多详情，请参见地域和可用区。 高可用性 为保证业务处理的正确性和服务不中断，建议您通过快照实现数据备份，通过跨可用区、部署集、负载均衡（Server Load Balancer）等实现应用容灾。 网络规划 阿里云推荐您使用专有网络VPC，可自行规划私网IP，全面支持新功能和新型实例规格。此外，专有网络VPC支持多业务系统隔离和多地域部署系统的使用场景。更多详情，请参见专有网络（Virtual Private Cloud）。 安全方案 您可以使用云服务器ECS的安全组，控制ECS实例的出入网访问策略以及端口监听状态。对于部署在云服务器ECS上的应用，阿里云为您提供了免费的DDoS基础防护和基础安全服务 通过DDoS高防IP保障源站的稳定可靠。更多详情，请参见DDoS高防IP文档。 通过云安全中心保障云服务器ECS的安全。更多详情，请参见云安全中心文档。 相关服务使用云服务器ECS的同时，您还可以选择以下阿里云服务： 根据业务需求和策略的变化，使用弹性伸缩（Auto Scaling）自动调整云服务器ECS的数量。更多详情，请参见弹性伸缩。 使用专有宿主机（Dedicated Host）部署ECS实例，可让您独享物理服务器资源、降低上云和业务部署调整的成本、满足严格的合规和监管要求。更多详情，请参见专有宿主机DDH。 使用容器服务Kubernetes版在一组云服务器ECS上通过Docker容器管理应用生命周期。更多详情，请参见容器服务Kubernetes版。 通过负载均衡（Server Load Balancer）对多台云服务器ECS实现流量分发的负载均衡目的。更多详情，请参见负载均衡。 通过云监控（CloudMonitor）制定实例、系统盘和公网带宽等的监控方案。更多详情，请参见云监控。 在同一阿里云地域下，采用关系型云数据库（Relational Database Service）作为云服务器ECS的数据库应用是典型的业务访问架构，可极大降低网络延时和公网访问费用，并实现云数据库RDS的最佳性能。云数据库RDS支持多种数据库引擎，包括MySQL、SQL Server、PostgreSQL、PPAS和MariaDB。更多详情，请参见关系型云数据库。 在云市场获取由第三方服务商提供的基础软件、企业软件、网站建设、代运维、云安全、数据及API、解决方案等相关的各类软件和服务。您也可以成为云市场服务供应商，提供软件应用及服务。更多详情，请参见云市场文档。 什么是RDS阿里云关系型数据库RDS（Relational Database Service）是一种稳定可靠、可弹性伸缩的在线数据库服务。基于阿里云分布式文件系统和SSD盘高性能存储，RDS支持MySQL、SQL Server、PostgreSQL、PPAS（Postgre Plus Advanced Server，高度兼容Oracle数据库）和MariaDB TX引擎，并且提供了容灾、备份、恢复、监控、迁移等方面的全套解决方案，彻底解决数据库运维的烦恼。 为什么选择云数据库RDS选择云数据库RDS，您可以快速搭建稳定可靠的数据库服务，相比自建数据库有如下优势： 便宜易用，具有灵活计费、按需变配、即开即用等优点。 高性能，包括参数优化、SQL优化建议等。 高可用架构和多种容灾方案。 高安全性，提供多种安全措施保障数据安全。 在性价比、可用性、可靠性、易用性、性能等方面，云数据库RDS都有很大优势，价格相比ECS自建数据库，仅需约1&#x2F;3，相比自购服务器搭建数据库，仅需约1&#x2F;10。 mysql迁移上RDSmysql https://help.aliyun.com/document_detail/126875.html?spm=a2c4g.11186623.2.2.16a17070MP5GaB vpc阿里云解释 专有网络是您自己独有的云上私有网络。您可以完全掌控自己的专有网络，例如选择IP地址范围、配置路由表和网关等，您可以在自己定义的专有网络中使用阿里云资源如云服务器、云数据库RDS版和负载均衡等。 您可以将专有网络连接到其他专有网络或本地网络，形成一个按需定制的网络环境，实现应用的平滑迁移上云和对数据中心的扩展。 、 基础架构- 基于目前主流的隧道技术，专有网络（VPC）隔离了虚拟网络。每个VPC都有一个独立的隧道号，一个隧道号对应一个虚拟化网络。 背景信息随着云计算的不断发展，对虚拟化网络的要求越来越高，例如弹性（scalability）、安全性（security）、可靠性（reliability）和私密性（privacy），并且还有极高的互联性能（performance）需求，因此催生了多种多样的网络虚拟化技术。 比较早的解决方案，是将虚拟机的网络和物理网络融合在一起，形成一个扁平的网络架构，例如大二层网络。随着虚拟化网络规模的扩大，这种方案中的ARP欺骗、广播风暴、主机扫描等问题会越来越严重。为了解决这些问题，出现了各种网络隔离技术，把物理网络和虚拟网络彻底隔开。其中一种技术是用户之间用VLAN进行隔离，但是VLAN的数量最大只能支持4096个，无法支撑巨大的用户量。 原理描述基于目前主流的隧道技术，专有网络隔离了虚拟网络。每个VPC都有一个独立的隧道号，一个隧道号对应着一个虚拟化网络。一个VPC内的ECS（Elastic Compute Service）实例之间的传输数据包都会加上隧道封装，带有唯一的隧道ID标识，然后送到物理网络上进行传输。不同VPC内的ECS实例因为所在的隧道ID不同，本身处于两个不同的路由平面，所以不同VPC内的ECS实例无法进行通信，天然地进行了隔离。 基于隧道技术和软件定义网络SDN（Software Defined Network）技术，阿里云的研发在硬件网关和自研交换机设备的基础上实现了VPC产品。 逻辑架构如下图所示，VPC包含交换机、网关和控制器三个重要的组件。交换机和网关组成了数据通路的关键路径，控制器使用自研协议下发转发表到网关和交换机，完成了配置通路的关键路径，配置通路和数据通路互相分离。交换机是分布式的结点，网关和控制器都是集群部署并且是多机房互备的，并且所有链路上都有冗余容灾，提升了VPC的整体可用性。 [](ht 应用场景更新时间：2020-09-21 17:15 我的收藏 本页目录 托管应用程序 托管主动访问公网的应用程序 跨可用区容灾 业务系统隔离 构建混合云 多个应用流量波动大 专有网络（VPC）是完全隔离的网络环境，配置灵活，可满足不同的应用场景。 托管应用程序您可以将对外提供服务的应用程序托管在VPC中，并且可以通过创建安全组规则、访问控制白名单等方式控制Internet访问。您也可以在应用程序服务器和数据库之间进行访问控制隔离，将Web服务器部署在能够进行公网访问的子网中，将应用程序的数据库部署在没有配置公网访问的子网中。 托管主动访问公网的应用程序您可以将需要主动访问公网的应用程序托管在VPC中的一个子网内，通过网络地址转换（NAT）网关路由其流量。通过配置SNAT规则，子网中的实例无需暴露其私网IP地址即可访问Internet，并可随时替换公网IP，避免被外界攻击。 跨可用区容灾您可以通过创建交换机为专有网络划分一个或多个子网。同一专有网络内不同交换机之间内网互通。您可以通过将资源部署在不同可用区的交换机中，实现跨可用区容灾。 业务系统隔离不同的VPC之间逻辑隔离。如果您有多个业务系统例如生产环境和测试环境要严格进行隔离，那么可以使用多个VPC进行业务隔离。当有互相通信的需求时，可以在两个VPC之间建立对等连接。详细信息，请参见什么是对等连接。 构建混合云VPC提供专用网络连接，可以将本地数据中心和VPC连接起来，扩展本地网络架构。通过该方式，您可以将本地应用程序无缝地迁移至云上，并且不必更改应用程序的访问方式。 多个应用流量波动大如果您的应用带宽波动很大，您可以通过NAT网关配置DNAT转发规则，然后将EIP添加到共享带宽中，实现多IP共享带宽，减轻波峰波谷效应，从而减少您的成本。 cdn阿里云解释 https://help.aliyun.com/learn/learningpath/cdn.html?spm=5176.7933777.J_1398156.3.ac85163dpKW5Os 基本概念内容分发网络：可以将源站资源备份到多个CDN节点上，这样如果用户（在北京）要看一个视频，就不用从源站（广州）获取，直接取离自己最近的一个CDN节点上的备份即可，加快内容分发速度，尤其适合直播类应用。 https://help.aliyun.com/document_detail/27102.html?spm=5176.208361.1107621.4.274e57e08VcL4P#title-n8s-dbg-26b) 本文为您介绍使用阿里云CDN时，难以理解的名词，便于您更准确的理解和使用产品。 加速域名加速域名即您需要使用CDN加速的域名。域名是一组服务器的地址，可以是网站、电子邮件、FTP等。在阿里云CDN帮助文档中，加速域名通常指域名。 CNAME记录CNAME（Canonical Name）即别名，用来把一个域名解析到另一个域名，再由另一个域名提供IP地址。例如： 您有一台服务器上存放了很多资料，使用docs.example.com访问该资源，但又希望通过documents.example.com也能访问。 那么您可以在您的DNS解析服务商添加一条CNAME记录，将documents.example.com指向docs.example.com。 添加该条CNAME记录后，所有访问documents.example.com的请求都会被转到docs.example.com，获得相同的内容。 CNAME域名接入CDN，在阿里云控制台添加加速域名后，阿里云CDN将给您分配一个CNAME域名。该CNAME域名的形式为*.*kunlun*.com。 您需要在您的DNS解析服务商添加一条CNAME记录，将自己的加速域名指向*.*kunlun*.com的域名。记录生效后，域名解析的工作就正式转向CDN服务，该域名所有的请求都将转向CDN节点，达到加速效果。 DNSDNS（Domain Name System），即域名解析服务。DNS的作用：把域名转换成为网络可以识别的IP地址。人们习惯记忆域名，但机器间互相只识别IP地址。域名与IP地址之间是一一对应的，它们之间的转换工作称为域名解析，域名解析需要由专门的域名解析服务器来完成，整个过程自动进行。例如：您上网时输入的www.baidu.com会自动转换成为220.181.112.143。您可以使用阿里云云解析，也可以使用其他DNS服务商。 SSL&#x2F;TLSSSL（Secure Sockets Layer，安全通讯协议），是一个架构于TCP之上的安全套接层。它可以有效协助Internet应用软件提升通讯时的资料完整性以及安全性。标准化之后的SSL名称改为TLS（Transport Layer Security，传输层安全协议），因此很多相关的文档将二者并称（SSL&#x2F;TLS）。 边缘节点在阿里云CDN的帮助文档中，边缘节点、CDN节点、Cache节点、缓存节点、加速节点、阿里云节点、节点等都指阿里云边缘节点。边缘节点是相对于网络的复杂结构而提出的一个概念，指与最终接入的用户之间具有较少中间环节的网络节点，对最终接入用户有相对于源站而言更好的响应能力和连接速度。其作用是将访问量较大的内容缓存到边缘节点的服务器上，以此来提高网络终端用户访问网站内容的速度和质量。 源站您实际业务的服务器。源站类型可以选择OSS域名、IP、源站或函数计算域名。 回源CDN节点未缓存请求资源或缓存资源已到期时，回源站获取资源，返回给客户端。 例如：您访问某个URL时，如果解析到的CDN节点未缓存该资源，则您的访问请求会直接到源站获取资源，并根据URL请求返回给您。 回源HOST源站决定了回源时，请求到哪个IP。回源HOST决定回源请求访问到该IP上的哪个站点。 例1：源站是域名。 源站为www.a.com，回源HOST为www.b.com，那么实际回源是请求到www.a.com解析到的IP，即对应的主机上的站点www.b.com。 例2：源站是IP。 源站为1.1.1.1，回源HOST为www.b.com，那么实际回源的是1.1.1.1对应的主机上的站点www.b.com。 协议回源指回源时使用的协议和客户端访问资源时的协议保持一致，即如果客户端使用HTTPS方式请求资源，当CDN节点上未缓存该资源时，节点会使用相同的HTTPS方式回源获取资源。同理，如果客户端使用HTTP协议的请求，CDN节点回源时也使用HTTP协议。 过滤参数借助过滤参数开关，您可以根据实际业务需要，决定在缓存时是否过滤用户请求URL中? 如果开启过滤参数，则CDN节点会截取没有参数的URL向源站请求，且CDN节点仅保留一份副本。 如果关闭过滤参数，则每个不同的URL会缓存不同的副本在CDN的节点上。 若您的资源URL中不同参数代表相同的内容，建议开启过滤参数，有效提升缓存命中率。 SLB负载均衡什么是负载均衡阿里云文档地址 https://help.aliyun.com/product/27537.html?spm=5176.7921785.J_5253785160.5.6c922229uXyluL 负载均衡SLB（Server Load Balancer）是将访问流量根据转发策略分发到后端多台云服务器（ECS实例）的流量分发控制服务。负载均衡扩展了应用的服务能力，增强了应用的可用性。 概述负载均衡通过设置虚拟服务地址，将添加的同一地域的多台ECS实例虚拟成一个高性能和高可用的后端服务池，并根据转发规则，将来自客户端的请求分发给后端服务器池中的ECS实例。 负载均衡默认检查云服务器池中的ECS实例的健康状态，自动隔离异常状态的ECS实例，消除了单台ECS实例的单点故障，提高了应用的整体服务能力。此外，负载均衡还具备抗DDoS攻击的能力，增强了应用服务的防护能力。 组成部分负载均衡由以下三个部分组成： 负载均衡实例 （Instances） 一个负载均衡实例是一个运行的负载均衡服务，用来接收流量并将其分配给后端服务器。要使用负载均衡服务，您必须创建一个负载均衡实例，并至少添加一个监听和两台ECS实例。 监听 （Listeners） 监听用来检查客户端请求并将请求转发给后端服务器。监听也会对后端服务器进行健康检查。 后端服务器（Backend Servers） 后端服务器是一组接收前端请求的ECS实例。您可以单独添加ECS实例到后端服务器池，也可以通过虚拟服务器组或主备服务器组来批量添加和管理。 产品优势 高可用 采用全冗余设计，无单点，支持同城容灾。 根据应用负载进行弹性扩容，在流量波动情况下不中断对外服务。 可扩展 您可以根据业务的需要，随时增加或减少后端服务器的数量，扩展应用的服务能力。 低成本 与传统硬件负载均衡系统高投入相比，成本可下降60%。 安全 结合云盾，可提供5 Gbps的防DDoS攻击能力。 高并发 集群支持亿级并发连接，单实例提供千万级并发能力。 mysql迁移上云RDS https://help.aliyun.com/document_detail/126875.html?spm=a2c4g.11186623.2.10.14554b43IHTNie","categories":[],"tags":[]},{"title":"","slug":"总结/18、Ceph存储","date":"2024-12-14T22:02:47.117Z","updated":"2021-01-27T02:49:00.000Z","comments":true,"path":"2024/12/15/总结/18、Ceph存储/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/18%E3%80%81Ceph%E5%AD%98%E5%82%A8/","excerpt":"","text":"优秀的性能、可靠性和可扩展性而设计的统一的、分布式文件系统 Ceph 的统一体现在可以提供文件系统、块存储和对象存储，分布式体现在可以动态扩展 Ceph 俨然已经发展为一整套存储解决方案，上层能够提供对象存储(RGW)、块存储(RBD)和CephFS，可以说是一套适合各种场景，非常灵活，非常有可发挥空间的存储解决方案 组件基本组件Monitor：一个 Ceph 集群需要多个 Monitor 组成的小集群，它们通过 Paxos 同步数据，用来保存 OSD 的元数据。 OSD：全称 Object Storage Device，也就是负责响应客户端请求返回具体数据的进程，一个 Ceph 集群一般都有很多个 OSD。主要功能用于数据的存储，当直接使用硬盘作为存储目标时，一块硬盘称之为 OSD，当使用一个目录作为存储目标的时候，这个目录也被称为 OSD。 MDS：全称 Ceph Metadata Server，是 CephFS 服务依赖的元数据服务，对象存储和块设备存储不需要该服务。 Object：Ceph 最底层的存储单元是 Object 对象，一条数据、一个配置都是一个对象，每个 Object 包含 ID、元数据和原始数据。 Pool：Pool 是一个存储对象的逻辑分区，它通常规定了数据冗余的类型与副本数，默认为3副本。对于不同类型的存储，需要单独的 Pool，如 RBD。 PG：全称 Placement Grouops，是一个逻辑概念，一个 OSD 包含多个 PG。引入 PG 这一层其实是为了更好的分配数据和定位数据。每个 Pool 内包含很多个 PG，它是一个对象的集合，服务端数据均衡和恢复的最小单位就是 PG。 pool 是 ceph 存储数据时的逻辑分区，它起到 namespace 的作用 每个 pool 包含一定数量(可配置)的 PG PG 里的对象被映射到不同的 Object 上 pool 是分布到整个集群的 FileStore与BlueStore：FileStore 是老版本默认使用的后端存储引擎，如果使用 FileStore，建议使用 xfs 文件系统。BlueStore 是一个新的后端存储引擎，可以直接管理裸硬盘，抛弃了 ext4 与 xfs 等本地文件系统。可以直接对物理硬盘进行操作，同时效率也高出很多。 RADOS：全称 Reliable Autonomic Distributed Object Store，是 Ceph 集群的精华，用于实现数据分配、Failover 等集群操作。 Librados：Librados 是 Rados 提供库，因为 RADOS 是协议很难直接访问，因此上层的 RBD、RGW 和 CephFS 都是通过 librados 访问的，目前提供 PHP、Ruby、Java、Python、C 和 C++ 支持。 CRUSH：CRUSH 是 Ceph 使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。 RBD：全称 RADOS Block Device，是 Ceph 对外提供的块设备服务，如虚拟机硬盘，支持快照功能。 RGW：全称是 RADOS Gateway，是 Ceph 对外提供的对象存储服务，接口与 S3 和 Swift 兼容。 CephFS：全称 Ceph File System，是 Ceph 对外提供的文件系统服务。 块存储典型设备 磁盘阵列，硬盘，主要是将裸磁盘空间映射给主机使用的。 优点 通过 Raid 与 LVM 等手段，对数据提供了保护。 多块廉价的硬盘组合起来，提高容量。 多块磁盘组合出来的逻辑盘，提升读写效率。 缺点 采用 SAN 架构组网时，光纤交换机，造价成本高。 主机之间无法共享数据。 使用场景 Docker 容器、虚拟机磁盘存储分配。 日志存储 文件存储 … 文件存储典型设备 FTP、NFS 服务器，为了克服块存储文件无法共享的问题，所以有了文件存储，在服务器上架设 FTP 与 NFS 服务器，就是文件存储。 优点 造价低，随便一台机器就可以了 方便文件可以共享 缺点 读写速率低 传输速率慢 使用场景 日志存储 有目录结构的文件存储 … 对象存储典型设备 内置大容量硬盘的分布式服务器(swift, s3)；多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。 优点 具备块存储的读写高速。 具备文件存储的共享等特性 使用场景：(适合更新变动较少的数据) 图片存储 视频存储 … 特点：1、高性能： a. 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。 b.考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。 c. 能够支持上千个存储节点的规模，支持TB到PB级的数据。 2、高可用性： a. 副本数可以灵活控制。 b. 支持故障域分隔，数据强一致性。 c. 多种故障场景自动进行修复自愈。 d. 没有单点故障，自动管理。 3、高可扩展性： a. 去中心化。 b. 扩展灵活。 c. 随着节点增加而线性增长。 4、特性丰富： a. 支持三种存储接口：块存储、文件存储、对象存储。 b. 支持自定义接口，支持多种语言驱动。 详细配置分布式文件系统（Distributed File System）是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点相连。 分布式文件系统的设计基于客户机&#x2F;服务器模式 常用分布式文件系统 Lustre ， Hadoop ， FsatDFS , Ceph , GlusterFS Ceph组件 OSDs：存储设备 Monitors：集群监控组件 MDSs：存放文件系统的元数据（对象存储和块存储不需要该组件） Client：ceph客户端 一、准备机器 hostname ip role 描述 admin-node 192.168.0.130 ceph-deploy 管理节点 node1 192.168.0.131 mon.node1 ceph节点，监控节点 node2 192.168.0.132 osd.0 ceph节点，OSD节点 node3 192.168.0.133 osd.1 ceph节点，OSD节点 管理节点：admin-node ceph节点：node1, node2, node3 所有节点：admin-node, node1, node2, node3 1、修改主机名2、修改hosts文件12345# vi /etc/hosts192.168.0.130 admin-node192.168.0.131 node1192.168.0.132 node2192.168.0.133 node3 二、Ceph节点安装1. 安装NPT（所有节点）我们建议在所有 Ceph 节点上安装 NTP 服务（特别是 Ceph Monitor 节点），以免因时钟漂移导致故障，详情见时钟。 1# sudo yum install ntp ntpdate ntp-doc 2、管理节点配置无密码ssh登陆3、关闭核心防护4、配置yum源12345678910111213141516171819202122232425262728vi /etc/yum.repos.d/ceph.repo[Ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/enabled=1gpgcheck=0type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/enabled=1gpgcheck=0type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS/enabled=1gpgcheck=0type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1 三、搭建集群1. 安装准备，创建文件夹在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。 123$ cd ~$ mkdir my-cluster$ cd my-cluster 注：若安装ceph后遇到麻烦可以使用以下命令进行清除包和配置： 123456// 删除安装包$ ceph-deploy purge admin-node node1 node2 node3// 清除配置$ ceph-deploy purgedata admin-node node1 node2 node3$ ceph-deploy forgetkeys 2. 创建集群和监控节点创建集群并初始化监控节点： 1$ ceph-deploy new &#123;initial-monitor-node(s)&#125; 这里node1是monitor节点，所以执行： 1$ ceph-deploy new node1 完成后，my-clster 下多了3个文件：ceph.conf、ceph-deploy-ceph.log 和 ceph.mon.keyring。 问题：如果出现 “[ceph_deploy][ERROR ] RuntimeError: remote connection got closed, ensure requiretty is disabled for node1”，执行 sudo visudo 将 Defaults requiretty 注释掉。 3. 修改配置文件1$ cat ceph.conf 内容如下： 1234567[global]fsid = 89933bbb-257c-4f46-9f77-02f44f4cc95cmon_initial_members = node1mon_host = 192.168.0.131auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把 osd pool default size &#x3D; 2 加入 [global] 段： 1$ sed -i &#x27;$a\\osd pool default size = 2&#x27; ceph.conf 如果有多个网卡，可以把 public network 写入 Ceph 配置文件的 [global] 段： 1public network = &#123;ip-address&#125;/&#123;netmask&#125; 4. 安装Ceph在所有节点上安装ceph： 1$ ceph-deploy install admin-node node1 node2 node3 问题：[ceph_deploy][ERROR ] RuntimeError: Failed to execute command: yum -y install epel-release 解决方法： 1sudo yum -y remove epel-release 5. 配置初始 monitor(s)、并收集所有密钥1$ ceph-deploy mon create-initial 完成上述操作后，当前目录里应该会出现这些密钥环： 1234&#123;cluster-name&#125;.client.admin.keyring&#123;cluster-name&#125;.bootstrap-osd.keyring&#123;cluster-name&#125;.bootstrap-mds.keyring&#123;cluster-name&#125;.bootstrap-rgw.keyring 6. 添加2个OSD 登录到 Ceph 节点、并给 OSD 守护进程创建一个目录，并添加权限。 123456789$ ssh node2$ sudo mkdir /var/local/osd0$ sudo chmod 777 /var/local/osd0/$ exit$ ssh node3$ sudo mkdir /var/local/osd1$ sudo chmod 777 /var/local/osd1/$ exit 然后，从管理节点执行 ceph-deploy 来准备 OSD 。 1$ ceph-deploy osd prepare node2:/var/local/osd0 node3:/var/local/osd1 最后，激活 OSD 。 1$ ceph-deploy osd activate node2:/var/local/osd0 node3:/var/local/osd1 7.把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点1$ ceph-deploy admin admin-node node1 node2 node3 8. 确保你对 ceph.client.admin.keyring 有正确的操作权限1$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring 9. 检查集群的健康状况和OSD节点状况1234567891011121314151617181920[zeng@admin-node my-cluster]$ ceph healthHEALTH_OK[zeng@admin-node my-cluster]$ ceph -s cluster a3dd419e-5c99-4387-b251-58d4eb582995 health HEALTH_OK monmap e1: 1 mons at &#123;node1=192.168.0.131:6789/0&#125; election epoch 3, quorum 0 node1 osdmap e10: 2 osds: 2 up, 2 in flags sortbitwise,require_jewel_osds pgmap v22: 64 pgs, 1 pools, 0 bytes data, 0 objects 12956 MB used, 21831 MB / 34788 MB avail 64 active+clean [zeng@admin-node my-cluster]$ ceph osd dfID WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 0.01659 1.00000 17394M 6478M 10915M 37.24 1.00 64 1 0.01659 1.00000 17394M 6478M 10915M 37.25 1.00 64 TOTAL 34788M 12956M 21831M 37.24 MIN/MAX VAR: 1.00/1.00 STDDEV: 0 四、扩展集群（扩容） 1. 添加OSD在 node1 上添加一个 osd.2。 创建目录 1234$ ssh node1$ sudo mkdir /var/local/osd2$ sudo chmod 777 /var/local/osd2/$ exit 准备OSD 1$ ceph-deploy osd prepare node1:/var/local/osd2 激活OSD 1$ ceph-deploy osd activate node1:/var/local/osd2 检查集群状况和OSD节点： 123456789101112131415161718[zeng@admin-node my-cluster]$ ceph -s cluster a3dd419e-5c99-4387-b251-58d4eb582995 health HEALTH_OK monmap e1: 1 mons at &#123;node1=192.168.0.131:6789/0&#125; election epoch 3, quorum 0 node1 osdmap e15: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v37: 64 pgs, 1 pools, 0 bytes data, 0 objects 19450 MB used, 32731 MB / 52182 MB avail 64 active+clean[zeng@admin-node my-cluster]$ ceph osd dfID WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 0.01659 1.00000 17394M 6478M 10915M 37.24 1.00 41 1 0.01659 1.00000 17394M 6478M 10915M 37.24 1.00 43 2 0.01659 1.00000 17394M 6494M 10899M 37.34 1.00 44 TOTAL 52182M 19450M 32731M 37.28 MIN/MAX VAR: 1.00/1.00 STDDEV: 0.04 2. 添加MONITORS在 ndoe2 和 node3 添加监控节点。 修改 mon_initial_members、mon_host 和 public network 配置： 12345678910[global]fsid = a3dd419e-5c99-4387-b251-58d4eb582995mon_initial_members = node1,node2,node3mon_host = 192.168.0.131,192.168.0.132,192.168.0.133auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd pool default size = 2public network = 192.168.0.120/24 推送至其他节点： 1$ ceph-deploy --overwrite-conf config push node1 node2 node3 添加监控节点: 1$ ceph-deploy mon add node2 node3 查看集群状态和监控节点： 12345678910111213[zeng@admin-node my-cluster]$ ceph -s cluster a3dd419e-5c99-4387-b251-58d4eb582995 health HEALTH_OK monmap e3: 3 mons at &#123;node1=192.168.0.131:6789/0,node2=192.168.0.132:6789/0,node3=192.168.0.133:6789/0&#125; election epoch 8, quorum 0,1,2 node1,node2,node3 osdmap e25: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v3919: 64 pgs, 1 pools, 0 bytes data, 0 objects 19494 MB used, 32687 MB / 52182 MB avail 64 active+clean [zeng@admin-node my-cluster]$ ceph mon state3: 3 mons at &#123;node1=192.168.0.131:6789/0,node2=192.168.0.132:6789/0,node3=192.168.0.133:6789/0&#125;, election epoch 8, quorum 0,1,2 node1,node2,node3 未完待续","categories":[],"tags":[]},{"title":"","slug":"总结/17、网络","date":"2024-12-14T22:02:47.115Z","updated":"2021-02-24T11:08:30.000Z","comments":true,"path":"2024/12/15/总结/17、网络/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/17%E3%80%81%E7%BD%91%E7%BB%9C/","excerpt":"","text":"[TOC] 一、交换机工作原理根据源MAC地址学习，根据目标MAC地址转发除源端口外的端口广播未知数据帧 接收方回应交换机实现单播通信 更新：老化时间300秒 交换机对应端口的MAC 地址发生变化时 二、路由路由:跨越从源主机到目标主机的一个互联网络来转发数据包的过程。 路由表：路由器根据路由表做路径选择 路由器的工作原理:根据路由表选择最佳路径,每个路由器都维护着一张路由表,这是路由器转发数据包的关键,每条路由表记录指明了到达某个子网或主机应从路由器的哪个物理端口发送,通过此端口可到达该路径的下一个路由器的地址。 三、DHCP基本原理: 第一步:客户端通过广播发送DHCP Discover报文寻找服务器端 第二步:服务器端通过单播发送DHCP Offer报文向客户端提供IP地址等网络信息 第三步:客户端通过广播发送DHCP Request报文告知服务器端本地选择使用哪个IP地址第四步:服务器通过单播发送DHCP Ack报文告知客户端lP地址是合法可用的 四、TCP和UDP的区别： UDP TCP 是否连接 无连接 面向连接 是否可靠 不可靠传输，不使用流量控制和拥塞控制 可靠传输，使用流量控制和拥塞控制 连接对象个数 支持一对一，一对多，多对一和多对多交互通信 只能是一对一通信 传输方式 面向报文 面向字节流 首部开销 首部开销小，仅8字节 首部最小20字节，最大60字节 场景 适用于实时应用（IP电话、视频会议、直播等） 适用于要求可靠传输的应用，例如文件传输 TCP:可靠的、面向连接的传输层协议。主要它有三次握手、四次断开、窗口滑动、数据分段、数据重组、数据重传机制保证数据的可靠性。。 UDP:不可靠的、面向无连接的传输层协议。它没有什么机制保证数据可靠性,当数据量非常庞大时可以通过此协议来保证数据的高效低延时。。 以tcp&#x2F;ip协议为核心,分五层。tcp工作在第4层，主要有tcp和udp协议。其中tcp是可靠协议，udp是不可靠协议。 tcp传输之前，需要建立连接，通过三次握手实现。 每一个应用层（TCP&#x2F;IP参考模型的最高层）协议一般都会使用到两个传输层协议之一： 运行在TCP协议上的协议： HTTP（Hypertext Transfer Protocol，超文本传输协议），主要用于普通浏览。 HTTPS（HTTP over SSL，安全超文本传输协议）,HTTP协议的安全版本。 FTP（File Transfer Protocol，文件传输协议），用于文件传输。 POP3（Post Office Protocol, version 3，邮局协议），收邮件用。 SMTP（Simple Mail Transfer Protocol，简单邮件传输协议），用来发送电子邮件。 TELNET（Teletype over the Network，网络电传），通过一个终端（terminal）登陆到网络。 SSH（Secure Shell，用于替代安全性差的TELNET），用于加密安全登陆用。 运行在UDP协议上的协议： BOOTP（Boot Protocol，启动协议），应用于无盘设备。 NTP（Network Time Protocol，网络时间协议），用于网络同步。 DHCP（Dynamic Host Configuration Protocol，动态主机配置协议），动态配置IP地址。 运行在TCP和UDP协议上： DNS（Domain Name Service，域名服务），用于完成地址查找，邮件转发等工作。 五、DNS默认端口为53。 DNS端口分为TCP和UDP。DNS协议运行在UDP协议之上 一、TCP是用来做区域传送 二、UDP是用来做DNS解析的。 六、ARP工作原理每台主机都会在自己的ARP缓冲区中建立一个 ARP列表，以表示IP地址和MAC地址的对应关系。 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己 ARP列表中是否存在该 IP地址对应的MAC地址。 如果有，就直接将数据包发送到这个MAC地址；如果没有，就向本地网段发起一个ARP请求的广播包，查询此目的主机对应的MAC地址。 此ARP请求数据包里包括源主机的IP地址、硬件地址、以及目的主机的IP地址。网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。 如果不相同就忽略此数据包；如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中。 如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个 ARP响应数据包，告诉对方自己是它需要查找的MAC地址。 源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。 如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 七、Ping和Traceroutehttps://blog.csdn.net/tomatolee221/article/details/89531048 七、三次握手和四次挥手三次握手的本质是确认通信双方收发数据的能力 1.发送方向接收方发送SYN请求 ⒉接收方接收到此请求后会主动回复一个ACK，并且同时也发送一个SYN请求 3.发送方接收到接收方发来的SYN请求后，给出一个ACK确认。。 四次挥手的目的是关闭一个连接 1.发送方向接收方发送一个FIN请求 ⒉接收方收到此请求后给出一个ACK确认（半关闭状态） 3.接收方发送一个FIN请求给发送方 4.发送方收到接收方的FIN请求后，回复一个ACK。 八、常见面试题1、为什么TCP连接的时候是3次？2次不可以吗？因为需要考虑连接时丢包的问题，如果只握手2次，第二次握手时如果服务端发给客户端的确认报文段丢失，此时服务端已经准备好了收发数(可以理解服务端已经连接成功)据，而客户端一直没收到服务端的确认报文，所以客户端就不知道服务端是否已经准备好了(可以理解为客户端未连接成功)，这种情况下客户端不会给服务端发数据，也会忽略服务端发过来的数据。 如果是三次握手，即便发生丢包也不会有问题，比如如果第三次握手客户端发的确认ack报文丢失，服务端在一段时间内没有收到确认ack报文的话就会重新进行第二次握手，也就是服务端会重发SYN报文段，客户端收到重发的报文段后会再次给服务端发送确认ack报文。 2、为什么TCP连接的时候是3次，关闭的时候却是4次？因为只有在客户端和服务端都没有数据要发送的时候才能断开TCP。而客户端发出FIN报文时只能保证客户端没有数据发了，服务端还有没有数据发客户端是不知道的。而服务端收到客户端的FIN报文后只能先回复客户端一个确认报文来告诉客户端我服务端已经收到你的FIN报文了，但我服务端还有一些数据没发完，等这些数据发完了服务端才能给客户端发FIN报文(所以不能一次性将确认报文和FIN报文发给客户端，就是这里多出来了一次)。 3、为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL，所以需要等这么长时间来确认服务端确实已经收到了。 4、如果已经建立了连接，但是客户端突然出现故障了怎么办？TCP设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 5、什么是HTTP，HTTP 与 HTTPS 的区别HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范 区别 HTTP HTTPS 协议 运行在 TCP 之上，明文传输，客户端与服务器端都无法验证对方的身份 身披 SSL( Secure Socket Layer )外壳的 HTTP，运行于 SSL 上，SSL 运行于 TCP 之上， 是添加了加密和认证机制的 HTTP。 端口 80 443 资源消耗 较少 由于加解密处理，会消耗更多的 CPU 和内存资源 开销 无需证书 需要证书，而证书一般需要向认证机构购买 加密机制 无 共享密钥加密和公开密钥加密并用的混合加密机制 安全性 弱 由于加密机制，安全性强 6、常用HTTP状态码HTTP状态码表示客户端HTTP请求的返回结果、标识服务器处理是否正常、表明请求出现的错误等。 状态码的类别： 类别 原因短语 1XX Informational（信息性状态码） 接受的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 常用HTTP状态码： 2XX 成功（这系列表明请求被正常处理了） 200 OK，表示从客户端发来的请求在服务器端被正确处理 204 No content，表示请求成功，但响应报文不含实体的主体部分 206 Partial Content，进行范围请求成功 3XX 重定向（表明浏览器要执行特殊处理） 301 moved permanently，永久性重定向，表示资源已被分配了新的 URL 302 found，临时性重定向，表示资源临时被分配了新的 URL 303 see other，表示资源存在着另一个 URL，应使用 GET 方法获取资源（对于301&#x2F;302&#x2F;303响应，几乎所有浏览器都会删除报文主体并自动用GET重新请求） 304 not modified，表示服务器允许访问资源，但请求未满足条件的情况（与重定向无关） 307 temporary redirect，临时重定向，和302含义类似，但是期望客户端保持请求方法不变向新的地址发出请求 4XX 客户端错误 400 bad request，请求报文存在语法错误 401 unauthorized，表示发送的请求需要有通过 HTTP 认证的认证信息 403 forbidden，表示对请求资源的访问被服务器拒绝，可在实体主体部分返回原因描述 404 not found，表示在服务器上没有找到请求的资源 5XX 服务器错误 500 internal sever error，表示服务器端在执行请求时发生了错误 501 Not Implemented，表示服务器不支持当前请求所需要的某个功能 503 service unavailable，表明服务器暂时处于超负载或正在停机维护，无法处理请求 6、GET和POST区别说道GET和POST，就不得不提HTTP协议，因为浏览器和服务器的交互是通过HTTP协议执行的，而GET和POST也是HTTP协议中的两种方法。 HTTP全称为Hyper Text Transfer Protocol，中文翻译为超文本传输协议，目的是保证浏览器与服务器之间的通信。HTTP的工作方式是客户端与服务器之间的请求-应答协议。 HTTP协议中定义了浏览器和服务器进行交互的不同方法，基本方法有4种，分别是GET，POST，PUT，DELETE。这四种方法可以理解为，对服务器资源的查，改，增，删。 GET：从服务器上获取数据，也就是所谓的查，仅仅是获取服务器资源，不进行修改。 POST：向服务器提交数据，这就涉及到了数据的更新，也就是更改服务器的数据。 PUT：英文含义是放置，也就是向服务器新添加数据，就是所谓的增。 DELETE：从字面意思也能看出，这种方式就是删除服务器数据的过程。 GET和POST区别 Get是不安全的，因为在传输过程，数据被放在请求的URL中；Post的所有操作对用户来说都是不可见的。 但是这种做法也不时绝对的，大部分人的做法也是按照上面的说法来的，但是也可以在get请求加上 request body，给 post请求带上 URL 参数。 Get请求提交的url中的数据最多只能是2048字节，这个限制是浏览器或者服务器给添加的，http协议并没有对url长度进行限制，目的是为了保证服务器和浏览器能够正常运行，防止有人恶意发送请求。Post请求则没有大小限制。 Get限制Form表单的数据集的值必须为ASCII字符；而Post支持整个ISO10646字符集。 Get执行效率却比Post方法好。Get是form提交的默认方法。 GET产生一个TCP数据包；POST产生两个TCP数据包。 对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）； 而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。 7、什么是对称加密与非对称加密对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方； 而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，非常的慢 8、什么是HTTP2HTTP2 可以提高了网页的性能。 在 HTTP1 中浏览器限制了同一个域名下的请求数量（Chrome 下一般是六个），当在请求很多资源的时候，由于队头阻塞当浏览器达到最大请求数量时，剩余的资源需等待当前的六个请求完成后才能发起请求。 HTTP2 中引入了多路复用的技术，这个技术可以只通过一个 TCP 连接就可以传输所有的请求数据。多路复用可以绕过浏览器限制同一个域名下的请求数量的问题，进而提高了网页的性能。 9、Session、Cookie和Token的主要区别HTTP协议本身是无状态的。什么是无状态呢，即服务器无法判断用户身份。 什么是cookie cookie是由Web服务器保存在用户浏览器上的小文件（key-value格式），包含用户相关的信息。客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户身份。 什么是session session是依赖Cookie实现的。session是服务器端对象 session 是浏览器和服务器会话过程中，服务器分配的一块储存空间。服务器默认为浏览器在cookie中设置 sessionid，浏览器在向服务器请求过程中传输 cookie 包含 sessionid ，服务器根据 sessionid 获取出会话中存储的信息，然后确定会话的身份信息。 cookie与session区别 存储位置与安全性：cookie数据存放在客户端上，安全性较差，session数据放在服务器上，安全性相对更高； 存储空间：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie，session无此限制 占用服务器资源：session一定时间内保存在服务器上，当访问增多，占用服务器性能，考虑到服务器性能方面，应当使用cookie。 什么是Token Token的引入：Token是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。 Token的定义：Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。 使用Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。 Token 是在服务端产生的。如果前端使用用户名&#x2F;密码向服务端请求认证，服务端认证成功，那么在服务端会返回 Token 给前端。前端可以在每次请求的时候带上 Token 证明自己的合法地位 session与token区别 session机制存在服务器压力增大，CSRF跨站伪造请求攻击，扩展性不强等问题； session存储在服务器端，token存储在客户端 token提供认证和授权功能，作为身份认证，token安全性比session好； session这种会话存储方式方式只适用于客户端代码和服务端代码运行在同一台服务器上，token适用于项目级的前后端分离（前后端代码运行在不同的服务器下） 10、Servlet是线程安全的吗Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。 解决的办法是尽量不要定义name属性，而是要把name变量分别定义在doGet()和doPost()方法内。虽然使用synchronized(name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。 注意：多线程的并发的读写Servlet类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此Servlet里的只读属性最好定义为final类型的。 11、如果客户端禁止 cookie 能实现 session 还能用吗？Cookie 与 Session，一般认为是两个独立的东西，Session采用的是在服务器端保持状态的方案，而Cookie采用的是在客户端保持状态的方案。 但为什么禁用Cookie就不能得到Session呢？因为Session是用Session ID来确定当前对话所对应的服务器Session，而Session ID是通过Cookie来传递的，禁用Cookie相当于失去了Session ID，也就得不到Session了。 假定用户关闭Cookie的情况下使用Session，其实现途径有以下几种： 手动通过URL传值、隐藏表单传递Session ID。 用文件、数据库等形式保存Session ID，在跨页过程中手动调用。 九、TCP11种状态1、CLOSED状态 初始状态，表示TCP连接是“关闭的”或者“未打开的”。 2、LISTEN状态 表示服务端的某个端口正处于监听状态，正在等待客户端连接的到来。 3、SYN_SENT状态 当客户端发送SYN请求建立连接之后，客户端处于SYN_SENT状态，等待服务器发送SYN+ACK。 4、SYN_RCVD状态 当服务器收到来自客户端的连接请求SYN之后，服务器处于SYN_RCVD状态，在接收到SYN请求之后会向客户端回复一个SYN+ACK的确认报文。 5、ESTABLISED状态 当客户端回复服务器一个ACK和服务器收到该ACK（TCP最后一次握手）之后，服务器和客户端都处于该状态，表示TCP连接已经成功建立。 6、FIN_WAIT_1状态 当数据传输期间当客户端想断开连接，向服务器发送了一个FIN之后，客户端处于该状态。 7、FIN_WAIT_2状态 当客户端收到服务器发送的连接断开确认ACK之后，客户端处于该状态。 8、CLOSE_WAIT状态 当服务器发送连接断开确认ACK之后但是还没有发送自己的FIN之前的这段时间，服务器处于该状态。 9、TIME_WAIT状态 当客户端收到了服务器发送的FIN并且发送了自己的ACK之后，客户端处于该状态。 10、LAST_ACK状态 表示被动关闭的一方（比如服务器）在发送FIN之后，等待对方的ACK报文时，就处于该状态。 11、CLOSING状态 连接断开期间，一般是客户端发送一个FIN，然后服务器回复一个ACK，然后服务器发送完数据后再回复一个FIN，当客户端和服务器同时接受到FIN时，客户端和服务器处于CLOSING状态，也就是此时双方都正在关闭同一个连接。","categories":[],"tags":[]},{"title":"","slug":"总结/16、Linux中top命令参数详解","date":"2024-12-14T22:02:47.112Z","updated":"2021-02-26T06:06:10.000Z","comments":true,"path":"2024/12/15/总结/16、Linux中top命令参数详解/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/16%E3%80%81Linux%E4%B8%ADtop%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Linux中top命令参数详解因为面试经常会问top命令用法，以及各个参数的含义。因此转载补充了了一下，以便自己学习。 top命令经常用来监控linux的系统状况，是常用的性能分析工具，能够实时显示系统中各个进程的资源占用情况。 一、命令格式1top [-d number] | top [-bnp] 二、参数解释-d：number代表秒数，表示top命令显示的页面更新一次的间隔。默认是5秒。 -b：以批次的方式执行top。 -n：与-b配合使用，表示需要进行几次top命令的输出结果。 -p：指定特定的pid进程号进行观察。 在top命令显示的页面还可以输入以下按键执行相应的功能（注意大小写区分的）： ?：显示在top当中可以输入的命令 P：以CPU的使用资源排序显示 M：以内存的使用资源排序显示 N：以pid排序显示 T：由进程使用的时间累计排序显示 k：给某一个pid一个信号。可以用来杀死进程 r：给某个pid重新定制一个nice值（即优先级） q：退出top（用ctrl+c也可以退出top）。 三、top各输出参数含义下面是使用top命令来进行性能检测的截图： (centos)： 1、第1行任务队列信息1top - 17:29:09 up 53days, 1 users, load average: 0.00, 0.01, 0.05 第1行是任务队列信息，其参数如下： 内容 含义 17:29:09 表示当前时间 up 53days 系统运行时间 格式为时：分 1 users 当前登录用户数 load average: 0.00, 0.01, 0.05 系统负载，即任务队列的平均长度。 三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。 load average: 如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 2、第2行（进程信息）1234567Tasks: 69 total, 2 running, 67 sleeping, 0 stopped, 0 zombie 69 total 进程总数2 running 正在运行的进程数158 sleeping 睡眠的进程数0 stopped 停止的进程数0 zombie 僵尸进程数 3、第3行（CPU信息）有多个CPU时会超过两行 12345678910%Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st0.0 us 用户空间占用CPU百分比0.0 sy 内核空间占用CPU百分比0.0 ni 用户进程空间内改变过优先级的进程占用CPU百分比100.0 id 空闲CPU百分比0.0 wa 等待输入输出的CPU时间百分比0.0 hi 硬中断（Hardware IRQ）占用CPU的百分比0.0 si 软中断（Software Interrupts）占用CPU的百分比0.0 st 用于有虚拟cpu的情况，用来指示被虚拟机偷掉的cpu时间。 4、第4、5行12KiB Mem: 1016168 total, 68820used, 567720 free, 379628 buffers KiB Swap: 0 total, 0 free ,0 used,.293196 avail Mem 第4、5行为内存信息 内容 含义 KiB Mem: 1016168 total 物理内存总量 567720 used 使用的物理内存总量 68820 free 空闲内存总量 379628 buffers（buff&#x2F;cache） 用作内核缓存的内存量 KiB Swap: 0 total 交换区总量 0 used 使用的交换区总量 0 free 空闲交换区总量 0 cached Mem 缓冲的交换区总量。 293196 avail Mem 代表可用于进程下一次分配的物理内存数量 上述最后提到的缓冲的交换区总量，这里解释一下，所谓缓冲的交换区总量，即内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小。相应的内存再次被换出时可不必再对交换区写入。 计算可用内存数有一个近似的公式：第四行的free + 第四行的buffers + 第五行的cached 四、进程信息 列名 含义 PID 进程id PPID 父进程id RUSER Real user name UID 进程所有者的用户id USER 进程所有者的用户名 GROUP 进程所有者的组名 TTY 启动进程的终端名。不是从终端启动的进程则显示为 PR 优先级 NI nice值。负值表示高优先级，正值表示低优先级 P 最后使用的CPU，仅在多CPU环境下有意义 %CPU 上次更新到现在的CPU时间占用百分比 TIME 进程使用的CPU时间总计，单位秒 TIME+ 进程使用的CPU时间总计，单位1&#x2F;100秒 %MEM 进程使用的物理内存百分比 VIRT 进程使用的虚拟内存总量，单位kb。VIRT&#x3D;SWAP+RES SWAP 进程使用的虚拟内存中，被换出的大小，单位kb RES 进程使用的、未被换出的物理内存大小，单位kb。RES&#x3D;CODE+DATA CODE 可执行代码占用的物理内存大小，单位kb DATA 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb SHR 共享内存大小，单位kb nFLT 页面错误次数 nDRT 最后一次写入到现在，被修改过的页面数。 S 进程状态。D&#x3D;不可中断的睡眠状态 R&#x3D;运行 S&#x3D;睡眠 T&#x3D;跟踪&#x2F;停止 Z&#x3D;僵尸进程 COMMAND 命令名&#x2F;命令行 WCHAN 若该进程在睡眠，则显示睡眠中的系统函数名 Flags 任务标志 其他默认进入top时，各进程是按照CPU的占用量来排序的。 五、键盘数字“1”监控每个逻辑CPU的状况 六、敲击键盘‘b’（打开关闭加亮效果）top视图变换如下： PID为9为当前top视图中唯一的运行态进程。也可以敲击键盘‘y’来打开或者关闭运行态进程的加亮效果。 七、敲击键盘‘x’（打开&#x2F;关闭排序列的加亮效果）top视图变换如下： 可以看到现在是按”%CPU”进行排序的，可以按”shift+&gt;”或者”shift+&lt;”左右改变排序序列。 八、敲击”f”，改变进程显示字段在top基本视图中，敲击”f”进入另一个视图，在这里可以编辑基本视图中的显示字段： 用上下键选择选项，按下空格键可以决定是否在基本视图中显示这个选项。 top命令是一个非常强大的功能，但是它监控的最小单位是进程，如果想监控更小单位时，就需要用到ps或者netstate命令来满足我们的要求。","categories":[],"tags":[]},{"title":"","slug":"总结/15、Linux 查询 OS、CPU、内存、硬盘信息","date":"2024-12-14T22:02:47.109Z","updated":"2021-02-07T15:15:50.000Z","comments":true,"path":"2024/12/15/总结/15、Linux 查询 OS、CPU、内存、硬盘信息/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/15%E3%80%81Linux%20%E6%9F%A5%E8%AF%A2%20OS%E3%80%81CPU%E3%80%81%E5%86%85%E5%AD%98%E3%80%81%E7%A1%AC%E7%9B%98%E4%BF%A1%E6%81%AF/","excerpt":"","text":"Linux 查询 OS、CPU、内存、硬盘信息一.前言当我们接手了一台或者几台服务器的时候，首先我们有必要对服务器的基本配置有所认识，这样才可以对症下药，对以后的软件部署，系统运维会有事半功倍的效果。 二.关于服务器基本配置查询服务器的基本配置一般查询操作系统，CPU，内存，硬盘，下面进行逐一讲解。 2.1 操作系统基本配置查询查看操作系统版本 12345#cat /etc/redhat-release这个命令主要是查看红帽发行的操作系统的版本号[root@node5 ~]# cat /etc/redhat-releaseCentOS Linux release 7.4.1708 (Core) #cat /etc/issue这个命令适用于大多数linux发行版[root@node5 ~]# cat /etc/issue\\SKernel \\r on an \\m 查看操作系统内核版本 12[root@node5 ~]# uname -r3.10.0-693.el7.x86_64 查看操作系统详细信息 12345678[root@node5 ~]# uname -aLinux node5 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux#从上面这段输出可以看出，该服务器主机名是node5，linux内核版本是3.10.0-693.el7.x86_64，CPU是x86架构#该命令可以查看更多信息[root@node5 ~]# more /etc/*release::::::::::::::/etc/centos-release::::::::::::::CentOS Linux release 7.4.1708 (Core) ::::::::::::::/etc/os-release::::::::::::::NAME=&quot;CentOS Linux&quot;VERSION=&quot;7 (Core)&quot;ID=&quot;centos&quot;ID_LIKE=&quot;rhel fedora&quot;VERSION_ID=&quot;7&quot;PRETTY_NAME=&quot;CentOS Linux 7 (Core)&quot;ANSI_COLOR=&quot;0;31&quot;CPE_NAME=&quot;cpe:/o:centos:centos:7&quot;HOME_URL=&quot;https://www.centos.org/&quot;BUG_REPORT_URL=&quot;https://bugs.centos.org/&quot;CENTOS_MANTISBT_PROJECT=&quot;CentOS-7&quot;CENTOS_MANTISBT_PROJECT_VERSION=&quot;7&quot;REDHAT_SUPPORT_PRODUCT=&quot;centos&quot;REDHAT_SUPPORT_PRODUCT_VERSION=&quot;7&quot;::::::::::::::/etc/redhat-release::::::::::::::CentOS Linux release 7.4.1708 (Core) ::::::::::::::/etc/system-release::::::::::::::CentOS Linux release 7.4.1708 (Core) 2.2 CPU基本配置查询名词解释 名词 含义 CPU物理个数 主板上实际插入的cpu数量 CPU核心数 单块CPU上面能处理数据的芯片组的数量，如双核、四核等 （cpu cores） 逻辑CPU数&#x2F;线程数 一般情况下，逻辑cpu&#x3D;物理CPU个数×每颗核数，如果不相等的话，则表示服务器的CPU支持超线程技术 查看 CPU 物理个数 12[root@node5 ~]# grep &#x27;physical id&#x27; /proc/cpuinfo | sort -u | wc -l1 查看 CPU 核心数量 12[root@node5 ~]# grep &#x27;core id&#x27; /proc/cpuinfo | sort -u | wc -l4 查看 CPU 线程数 1234#逻辑cpu数：一般情况下，逻辑cpu=物理CPU个数×每颗核数，如果不相等的话，则表示服务器的CPU支持超线程技术（HT：简单来说，它可使处理#器中的1 颗内核如2 颗内核那样在操作系统中发挥作用。这样一来，操作系统可使用的执行资源扩大了一倍，大幅提高了系统的整体性能，此时逻#辑cpu=物理CPU个数×每颗核数x2）[root@node5 ~]# cat /proc/cpuinfo| grep &quot;processor&quot;|wc -l4[root@node5 ~]# grep &#x27;processor&#x27; /proc/cpuinfo | sort -u | wc -l4 查看 CPU 型号 1234[root@node5 ~]# cat /proc/cpuinfo | grep name | sort | uniqmodel name : Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz[root@node5 ~]# dmidecode -s processor-version | uniq #使用uniq进行去重Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz 查看 CPU 的详细信息 123456789101112131415161718192021222324252627#CPU有几个核，就会输出几个重复的信息[root@node5 ~]# cat /proc/cpuinfoprocessor : 0vendor_id : GenuineIntelcpu family : 6model : 142model name : Intel(R) Core(TM) i7-8550U CPU @ 1.80GHzstepping : 10microcode : 0x96cpu MHz : 2000.921cache size : 8192 KBphysical id : 0siblings : 4core id : 0cpu cores : 4apicid : 0initial apicid : 0fpu : yesfpu_exception : yescpuid level : 22wp : yesflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch tpr_shadow vnmi ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid mpx rdseed adx smap clflushopt xsaveopt xsavec aratbogomips : 4002.00clflush size : 64cache_alignment : 64address sizes : 43 bits physical, 48 bits virtualpower management: 查看CPU的详细信息 1234567891011121314151617181920212223242526[root@node5 ~]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 4On-line CPU(s) list: 0-3Thread(s) per core: 1Core(s) per socket: 4Socket(s): 1NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 142Model name: Intel(R) Core(TM) i7-8550U CPU @ 1.80GHzStepping: 10CPU MHz: 2000.921BogoMIPS: 4002.00Virtualization: VT-xHypervisor vendor: VMwareVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 8192KNUMA node0 CPU(s): 0-3Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch tpr_shadow vnmi ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 invpcid mpx rdseed adx smap clflushopt xsaveopt xsavec arat CPU配置总结 通过以上的查询，我们可以知道该服务器是1路4核的CPU ，CPU型号是Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz，该CPU没有超线程。 2.3 内存基本配置查询名词解释 名词 含义 Mem 内存的使用情况总览表 Swap 虚拟内存。即可以把数据存放在硬盘上的数据，当物理内存不足时，拿出部分硬盘空间当SWAP分区（虚拟成内存）使用，从而解决内存容量不足的情况。SWAP意思是交换，顾名思义，当某进程向OS请求内存发现不足时，OS会把内存中暂时不用的数据交换出去，放在SWAP分区中，这个过程称为SWAP OUT。当某进程又需要这些数据且OS发现还有空闲物理内存时，又会把SWAP分区中的数据交换回物理内存中，这个过程称为SWAP IN。当然，swap大小是有上限的，一旦swap使用完，操作系统会触发OOM-Killer机制，把消耗内存最多的进程kill掉以释放内存。 shared 共享内存，即和普通用户共享的物理内存值， 主要用于进程间通信 buffers 用于存放要输出到disk（块设备）的数据的 cached 存放从disk上读出的数据 total 总的物理内存，total&#x3D;used+free used 使用掉的内存 free 空闲的内存 查询服务器内存 12345678910111213[root@node5 ~]# free -m total used free shared buff/cache availableMem: 3941 286 3446 19 208 3407Swap: 2047 0 2047 #注释#linux的内存管理机制的思想包括（不敢说就是）内存利用率最大化。内核会把剩余的内存申请为cached，而cached不属于free范畴。当系统运#行时间较久，会发现cached很大，对于有频繁文件读写操作的系统，这种现象会更加明显。直观的看，此时free的内存会非常小，但并不代表可##用的内存小，当一个程序需要申请较大的内存时，如果free的内存不够，内核会把部分cached的内存回收，回收的内存再分配给应用程序。所以#对于linux系统，可用于分配的内存不只是free的内存，还包括cached的内存（其实还包括buffers）。#对于操作系统：#MemFree=total-used#MemUsed = MemTotal - MemFree#对于应用程序：#MemFree=buffers+cached+free 每隔3秒查询一下内存 12345678910111213141516[root@node5 ~]# free -s 3 total used free shared buff/cache availableMem: 4036316 361144 3458272 19536 216900 3419776Swap: 2097148 0 2097148 total used free shared buff/cache availableMem: 4036316 361144 3458272 19536 216900 3419776Swap: 2097148 0 2097148 total used free shared buff/cache availableMem: 4036316 361144 3458272 19536 216900 3419776Swap: 2097148 0 2097148 2.4 硬盘基本配置查询查询磁盘整体使用情况 1234567891011121314[root@node5 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 17G 4.1G 13G 24% /devtmpfs 2.0G 0 2.0G 0% /devtmpfs 2.0G 8.0K 2.0G 1% /dev/shmtmpfs 2.0G 8.7M 2.0G 1% /runtmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup/dev/sda1 1014M 125M 890M 13% /boottmpfs 395M 0 395M 0% /run/user/0#命令拓展#df -a 显示全部的文件系统的使用情况#df -i显示inode信息#df -k 已字节数显示区块占用情况#df -T 显示文件系统的类型 查询某个目录磁盘占用情况 12345678910111213141516171819202122232425262728293031323334353637#命令拓展#du -s 指定目录大小汇总#du -h带计量单位#du -a 含文件#du --max-depth=1 子目录深度#du -c 列出明细的同时，增加汇总值[root@node5 ~]# du -sh /home/1.7G /home/ [root@node5 ~]# du -ach --max-depth=2 /home/4.0K /home/www/.bash_logout4.0K /home/www/.bash_profile4.0K /home/www/.bashrc4.0K /home/www/web16K /home/www4.0K /home/nginx/.bash_logout4.0K /home/nginx/.bash_profile4.0K /home/nginx/.bashrc12K /home/nginx4.0K /home/esnode/.bash_logout4.0K /home/esnode/.bash_profile4.0K /home/esnode/.bashrc4.0K /home/esnode/.oracle_jre_usage4.3M /home/esnode/elasticsearch-analysis-ik-6.2.2.zip80M /home/esnode/kibana-6.2.2-linux-x86_64.tar.gz300M /home/esnode/x-pack-6.2.2.zip28M /home/esnode/elasticsearch-6.2.2.tar.gz4.0K /home/esnode/.bash_history294M /home/esnode/elasticsearch-6.2.24.0K /home/esnode/.ssh4.0K /home/esnode/x-pack生成的秘钥.txt1014M /home/esnode/kibana-6.2.2-linux-x86_648.0K /home/esnode/.viminfo1.7G /home/esnode1.7G /home/1.7G total 查看目录结构 12345678910111213141516171819#tree命令默认没有安装，需要手动安装一下[root@node5 ~]# yum -y install tree#-L指定目录深度[root@node5 ~]# tree -L 2 /home//home/├── esnode│ ├── elasticsearch-6.2.2│ ├── elasticsearch-6.2.2.tar.gz│ ├── elasticsearch-analysis-ik-6.2.2.zip│ ├── kibana-6.2.2-linux-x86_64│ ├── kibana-6.2.2-linux-x86_64.tar.gz│ ├── x-pack-6.2.2.zip│ └── x-pack\\347\\224\\237\\346\\210\\220\\347\\232\\204\\347\\247\\230\\351\\222\\245.txt├── nginx└── www └── web 6 directories, 5 files 以树状的格式显示所有可用的块设备信息 12345678910111213141516171819[root@node5 ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP]sdb 8:16 0 1G 0 disk └─sdb1 8:17 0 200M 0 part sr0 11:0 1 1024M 0 rom #注释#NAME —— 设备的名称#MAJ:MIN —— Linux 操作系统中的每个设备都以一个文件表示，对块（磁盘）设备来说，这里用主次设备编号来描述设备。#RM —— 可移动设备。如果这是一个可移动设备将显示 1，否则显示 0。#TYPE —— 设备的类型#MOUNTPOINT —— 设备挂载的位置#RO —— 对于只读文件系统，这里会显示 1，否则显示 0。#SIZE —— 设备的容量 列出所有可用的设备、通用唯一识别码（UUID）、文件系统类型以及卷标 123456[root@node5 ~]# blkid/dev/sda1: UUID=&quot;6503b4ad-2975-4152-a824-feb7bea1b622&quot; TYPE=&quot;xfs&quot; /dev/sda2: UUID=&quot;nqZ4uJ-ksnN-KzYS-N42b-00m3-Ohc2-BJXunP&quot; TYPE=&quot;LVM2_member&quot; /dev/sdb1: UUID=&quot;94396e17-4821-4957-aa76-d41f33958ff5&quot; TYPE=&quot;xfs&quot; /dev/mapper/centos-root: UUID=&quot;c1d38b37-821d-48e7-8727-3937ccc657a4&quot; TYPE=&quot;xfs&quot; /dev/mapper/centos-swap: UUID=&quot;c2fcaf11-42d8-4e4c-bf9e-6464f0777198&quot; TYPE=&quot;swap&quot;","categories":[],"tags":[]},{"title":"","slug":"总结/14、Linux系统调优","date":"2024-12-14T22:02:47.106Z","updated":"2021-02-25T09:05:32.000Z","comments":true,"path":"2024/12/15/总结/14、Linux系统调优/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/14%E3%80%81Linux%E7%B3%BB%E7%BB%9F%E8%B0%83%E4%BC%98/","excerpt":"","text":"系统硬件资源1．CPU CPU是操作系统稳定运行的根本，CPU的速度与性能在很大程度上决定了系统整体的性能，因此，CPU数量越多、主频越高，服务器性能也就相对越好。但事实并非完全如此。 目前大部分CPU在同一时间内只能运行一个线程，超线程的处理器可以在同一时间运行多个线程，因此，可以利用处理器的超线程特性提高系统性能。在Linux系统下，只有运行SMP内核才能支持超线程，但是，安装的CPU数量越多，从超线程获得的性能方面的提高就越少。另外，Linux内核会把多核的处理器当作多个单独的CPU来识别，例如两个4核的CPU，在Lnux系统下会被当作8个单核CPU。但是从性能角度来讲，两个4核的CPU和8个单核的CPU并不完全等价，根据权威部门得出的测试结论，前者的整体性能要比后者低25％~30％。 可能出现CPU瓶颈的应用有邮件服务器、动态Web服务器等，对于这类应用，要把CPU的配置和性能放在主要位置。 2．内存 内存的大小也是影响Linux性能的一个重要的因素，内存太小，系统进程将被阻塞，应用也将变得缓慢，甚至失去响应；内存太大，导致资源浪费。Linux系统采用了物理内存和虚拟内存两种方式，虚拟内存虽然可以缓解物理内存的不足，但是占用过多的虚拟内存，应用程序的性能将明显下降，要保证应用程序的高性能运行，物理内存一定要足够大；但是过大的物理内存，会造成内存资源浪费，例如，在一个32位处理器的Linux操作系统上，超过8GB的物理内存都将被浪费。因此，要使用更大的内存，建议安装64位的操作系统，同时开启Linux的大内存内核支持。 由于处理器寻址范围的限制，在32位Linux操作系统上，应用程序单个进程最大只能使用2GB的内存，这样以来，即使系统有更大的内存，应用程序也无法“享”用，解决的办法就是使用64位处理器，安装64位操作系统。在64位操作系统下，可以满足所有应用程序对内存的使用需求，几乎没有限制。 可能出现内存性能瓶颈的应用有打印服务器、数据库服务器、静态Web服务器等，对于这类应用要把内存大小放在主要位置。 3．磁盘I&#x2F;O性能磁盘的I&#x2F;O性能直接影响应用程序的性能，在一个有频繁读写的应用中，如果磁盘I&#x2F;O性能得不到满足，就会导致应用停滞。好在现今的磁盘都采用了很多方法来提高I&#x2F;O性能，比如常见的磁盘RAID技术。 RAID的英文全称为：RedundantArrayofIndependentDisk，即独立磁盘冗余阵列，简称磁盘阵列。RAID通过将多块独立的磁盘（物理硬盘）按不同方式组合起来形成一个磁盘组（逻辑硬盘），从而提供比单个硬盘更高的I&#x2F;O性能和数据冗余。 通过RAID技术组成的磁盘组，就相当于一个大硬盘，用户可以对它进行分区格式化、建立文件系统等操作，跟单个物理硬盘一模一样，唯一不同的是RAID磁盘组的I&#x2F;O性能比单个硬盘要高很多，同时在数据的安全性也有很大提升。 根据磁盘组合方式的不同，RAID可以分为RAID0，RAID1、RAID2、RAID3、RAID4、RAID5、RAID6、RAID7、RAID0+1、RAID10等级别，常用的RAID级别有RAID0、RAID1、RAID5、RAID0+1，这里进行简单介绍。 RAID0：通过把多块硬盘粘合成一个容量更大的硬盘组，提高了磁盘的性能和吞吐量。这种方式成本低，要求至少两个磁盘，但是没有容错和数据修复功能，因而只能用在对数据安全性要求不高的环境中。 RAID1：也就是磁盘镜像，通过把一个磁盘的数据镜像到另一个磁盘上，最大限度地保证磁盘数据的可靠性和可修复性，具有很高的数据冗余能力，但磁盘利用率只有50%，因而，成本最高，多用在保存重要数据的场合。 RAID5：采用了磁盘分段加奇偶校验技术，从而提高了系统可靠性，RAID5读出效率很高，写入效率一般，至少需要3块盘。允许一块磁盘故障，而不影响数据的可用性。 RAID0+1：把RAID0和RAID1技术结合起来就成了RAID0+1，至少需要4个硬盘。此种方式的数据除分布在多个盘上外，每个盘都有其镜像盘，提供全冗余能力，同时允许一个磁盘故障，而不影响数据可用性，并具有快速读&#x2F;写能力。 通过了解各个RAID级别的性能，可以根据应用的不同特性，选择适合自身的RAID级别，从而保证应用程序在磁盘方面达到最优性能。 4．网络宽带 Linux下的各种应用，一般都是基于网络的，因此网络带宽也是影响性能的一个重要因素，低速的、不稳定的网络将导致网络应用程序的访问阻塞，而稳定、高速的网络带宽，可以保证应用程序在网络上畅通无阻地运行。幸运的是，现在的网络一般都是千兆带宽或光纤网络，带宽问题对应用程序性能造成的影响也在逐步降低。 操作系统相关资源 基于操作系统的性能优化也是多方面的，可以从系统安装、系统内核参数、网络参数、文件系统等几个方面进行衡量，下面依次进行简单介绍。 1．系统安装优化 系统优化可以从安装操作系统开始，当安装Linux系统时，磁盘的划分，SWAP内存的分配都直接影响以后系统的运行性能，例如，磁盘分配可以遵循应用的需求：对于对写操作频繁而对数据安全性要求不高的应用，可以把磁盘做成RAID0；而对于对数据安全性较高，对读写没有特别要求的应用，可以把磁盘做成RAID1；对于对读操作要求较高，而对写操作无特殊要求，并要保证数据安全性的应用，可以选择RAID5；对于对读写要求都很高，并且对数据安全性要求也很高的应用，可以选择RAID01。这样通过不同的应用需求设置不同的RAID级别，在磁盘底层对系统进行优化操作。 随着内存价格的降低和内存容量的日益增大，对虚拟内存SWAP的设定，现在已经没有了所谓虚拟内存是物理内存两倍的要求，但是SWAP的设定还是不能忽略，根据经验，如果内存较小（物理内存小于4GB），一般设置SWAP交换分区大小为内存的2倍；如果物理内存大于4GB小于16GB，可以设置SWAP大小等于或略小于物理内存即可；如果内存大小在16GB以上，原则上可以设置SWAP为0，但并不建议这么做，因为设置一定大小的SWAP还是有一定作用的。 2．内核参数优化 系统安装完成后，优化工作并没有结束，接下来还可以对系统内核参数进行优化，不过内核参数的优化要和系统中部署的应用结合起来整体考虑。例如，如果系统部署的是Oracle数据库应用，那么就需要对系统共享内存段（kernel.shmmax、kernel.shmmni、kernel.shmall）、系统信号量（kernel.sem）、文件句柄（fs.file-max）等参数进行优化设置；如果部署的是Web应用，那么就需要根据Web应用特性进行网络参数的优化，例如修改net.ipv4.ip_local_port_range、net.ipv4.tcp_tw_reuse、net.core.somaxconn等网络内核参数。 3．文件系统优化 文件系统的优化也是系统资源优化的一个重点，在Linux下可选的文件系统有ext2、ext3、xfs、ReiserFS，根据不同的应用，选择不同的文件系统。 Linux标准文件系统是从VFS开始的，然后是ext，接着就是ext2，应该说，ext2是Linux上标准的文件系统，ext3是在ext2基础上增加日志形成的，从VFS到ext3，其设计思想没有太大变化，都是早期UNIX家族基于超级块和inode的设计理念。 XFS文件系统是SGI开发的一个高级日志文件系统，后来移植到了Linux系统下，XFS通过分布处理磁盘请求、定位数据、保持Cache的一致性来提供对文件系统数据的低延迟、高带宽的访问，因此，XFS极具伸缩性，非常健壮，具有优秀的日志记录功能、可扩展性强、快速写入性能等优点。 ReiserFS是在HansReiser领导下开发出来的一款高性能的日志文件系统，它通过完全平衡树结构来管理数据，包括文件数据，文件名及日志支持等，与ext2&#x2F;ext3相比，最大的优点是访问性能和安全性大幅提升。ReiserFS具有高效、合理利用磁盘空间，先进的日志管理机制，特有的搜寻方式，海量磁盘存储等优点。 应用程序软件资源 应用程序的优化其实是整个优化工程的核心，如果一个应用程序存在BUG，那么即使所有其他方面都达到了最优状态，整个应用系统还是性能低下，所以，对应用程序的优化是性能优化过程的重中之重，这就对程序架构设计人员和程序开发人员提出了更高的要求 一 linux服务器性能查看1.1 cpu性能查看1、查看物理cpu个数：1cat /proc/cpuinfo |grep &quot;physical id&quot;|sort|uniq|wc -l 2、查看每个物理cpu中的core个数：1cat /proc/cpuinfo |grep &quot;cpu cores&quot;|wc -l 3、逻辑cpu的个数：1cat /proc/cpuinfo |grep &quot;processor&quot;|wc -l 物理cpu个数*核数&#x3D;逻辑cpu个数（不支持超线程技术的情况下） 1.2 内存查看1、查看内存使用情况：12345678910111213141516171819#free -m total used free shared buffers cachedMem: 3949 2519 1430 0 189 1619-/+ buffers/cache: 710 3239Swap: 3576 0 3576total：内存总数used：已经使用的内存数free：空闲内存数shared：多个进程共享的内存总额- buffers/cache：(已用)的内存数，即used-buffers-cached+ buffers/cache：(可用)的内存数，即free+buffers+cachedBuffer Cache用于针对磁盘块的读写；Page Cache用于针对文件inode的读写，这些Cache能有效地缩短I/O系统调用的时间。对操作系统来说free/used是系统可用/占用的内存；对应用程序来说-/+ buffers/cache是可用/占用内存,因为buffers/cache很快就会被使用。 我们工作时候应该从应用角度来看。 1.3 硬盘查看1、查看硬盘及分区信息：1fdisk -l 2、查看文件系统的磁盘空间占用情况：41df -h 3、查看硬盘的I&#x2F;O性能（每隔一秒显示一次，显示5次）：1iostat -x 1 5 iostat是含在套装systat中的,可以用yum -y install systat来安装。 常关注的参数： 12如%util接近100%,说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。如idle小于70%，I/O的压力就比较大了，说明读取进程中有较多的wait。 4、查看linux系统中某目录的大小：1du -sh /root 如发现某个分区空间接近用完，可以进入该分区的挂载点，用以下命令找出占用空间最多的文件或目录，然后按照从大到小的顺序，找出系统中占用最多空间的前10个文件或目录： 1du -cksh *|sort -rn|head -n 10 1.4 查看平均负载有时候系统响应很慢，但又找不到原因，这时就要查看平均负载了，看它是否有大量的进程在排队等待。 最简单的命令： 1uptime--查看过去的1分钟、5分钟和15分钟内进程队列中的平均进程数量。 还有动态命令top我们只关心以下部分： 12345top - 21:33:09 up 1:00, 1 user, load average: 0.00, 0.01, 0.05如果每个逻辑cpu当前的活动进程不大于3，则系统性能良好；如果每个逻辑cpu当前的活动进程不大于4，表示可以接受；如果每个逻辑cpu当前的活动进程大于5，则系统性能问题严重。 一般计算方法：负载值&#x2F;逻辑cpu个数 还可以结合vmstat命令来判断系统是否繁忙，其中： 123456789101112131415161718192021222324252627procsr：等待运行的进程数。b：处在非中断睡眠状态的进程数。w：被交换出去的可运行的进程数。memeoryswpd：虚拟内存使用情况，单位为KB。free：空闲的内存，单位为KB。buff：被用来作为缓存的内存数，单位为KB。swapsi：从磁盘交换到内存的交换页数量，单位为KB。so：从内存交换到磁盘的交换页数量，单位为KB。iobi：发送到块设备的块数，单位为KB。bo：从块设备接受的块数，单位为KB。systemin：每秒的中断数，包括时钟中断。cs：每秒的环境切换次数。cpu按cpu的总使用百分比来显示。us：cpu使用时间。sy：cpu系统使用时间。id：闲置时间。 1.5 其他参数1234567891011121314151617查看内核版本号：uname -a简化命令：uname -r查看系统是32位还是64位的：file /sbin/init查看发行版：cat /etc/issue或lsb_release -a查看系统已载入的相关模块：lsmod查看pci设置：lspci 二 Linux服务器性能评估2.1.1 影响Linux服务器性能的因素1. 操作系统级1234CPU内存磁盘I/O带宽网络I/O带宽 2. 程序应用级2.1.2 系统性能评估标准 影响性能因素 好 坏 糟糕 CPU user% + sys%&lt; 70% user% + sys%&#x3D; 85% user% + sys% &gt;&#x3D;90% 内存 Swap In（si）＝0 Swap Out（so）＝0 Per CPU with 10 page&#x2F;s More Swap In &amp; Swap Out 磁盘 iowait % &lt; 20% iowait % &#x3D;35% iowait % &gt;&#x3D; 50% 其中： 12345%user：表示CPU处在用户模式下的时间百分比。%sys：表示CPU处在系统模式下的时间百分比。%iowait：表示CPU等待输入输出完成时间的百分比。swap in：即si，表示虚拟内存的页导入，即从SWAP DISK交换到RAMswap out：即so，表示虚拟内存的页导出，即从RAM交换到SWAP DISK 2.1.3 系统性能分析工具1.常用系统命令Vmstat、sar、iostat、netstat、free、ps、top等 2.常用组合方式1234vmstat、sar、iostat检测是否是CPU瓶颈free、vmstat检测是否是内存瓶颈iostat检测是否是磁盘I/O瓶颈netstat检测是否是网络带宽瓶颈 2.1.4 Linux性能评估与优化系统整体性能评估（uptime命令）uptime 16:38:00 up 118 days, 3:01, 5 users,load average: 1.22, 1.02, 0.91 注意： load average三值大小一般不能大于系统CPU的个数。 系统有8个CPU,如load average三值长期大于8，说明CPU很繁忙，负载很高，可能会影响系统性能。 但偶尔大于8，一般不会影响系统性能。 如load average输出值小于CPU个数，则表示CPU有空闲时间片，比如本例中的输出，CPU是非常空闲的 2.2.1 CPU性能评估1.利用vmstat命令监控系统CPU显示系统各种资源之间相关性能简要信息，主要看CPU负载情况。 下面是vmstat命令在某个系统的输出结果： 12345678910111213[root@node1 ~]#vmstat 2 3procs ———–memory———- —swap– —–io—- –system– —–cpu——r b swpd freebuff cache si so bi bo incs us sy idwa st0 0 0 162240 8304 67032 0 0 13 21 1007 23 0 1 98 0 00 0 0 162240 8304 67032 0 0 1 0 1010 20 0 1 100 0 00 0 0 162240 8304 67032 0 0 1 1 1009 18 0 1 99 0 0Procs r–运行和等待cpu时间片的进程数，这个值如果长期大于系统CPU的个数，说明CPU不足，需要增加CPU b–在等待资源的进程数，比如正在等待I&#x2F;O、或者内存交换等。 CPUus用户进程消耗的CPU 时间百分比。us的值比较高时，说明用户进程消耗的cpu时间多，但是如果长期大于50%，就需要考虑优化程序或算法。 sy内核进程消耗的CPU时间百分比。Sy的值较高时，说明内核消耗的CPU资源很多。 根据经验，us+sy的参考值为80%，如果us+sy大于 80%说明可能存在CPU资源不足。 2.利用sar命令监控系统CPUsar对系统每方面进行单独统计，但会增加系统开销，不过开销可以评估，对系统的统计结果不会有很大影响。 下面是sar命令对某个系统的CPU统计输出： 123456789101112131415161718192021222324252627[root@webserver ~]# sar -u 3 5Linux 2.6.9-42.ELsmp (webserver) 11/28/2008_i686_ (8 CPU)11:41:24 AM CPU %user %nice%system %iowait %steal %idle11:41:27 AM all 0.88 0.00 0.29 0.00 0.00 98.8311:41:30 AM all 0.13 0.00 0.17 0.21 0.00 99.5011:41:33 AM all 0.04 0.00 0.04 0.00 0.00 99.9211:41:36 AM all 90.08 0.00 0.13 0.16 0.00 9.6311:41:39 AM all 0.38 0.00 0.17 0.04 0.00 99.41Average: all 0.34 0.00 0.16 0.05 0.00 99.45 输出解释如下： 1234567%user列显示了用户进程消耗的CPU 时间百分比。%nice列显示了运行正常进程所消耗的CPU 时间百分比。%system列显示了系统进程消耗的CPU时间百分比。%iowait列显示了IO等待所占用的CPU时间百分比%steal列显示了在内存相对紧张的环境下pagein强制对不同的页面进行的steal操作 。%idle列显示了CPU处在空闲状态的时间百分比。问题 你是否遇到过系统CPU整体利用率不高，而应用缓慢的现象？ 在一个多CPU的系统中，如果程序使用了单线程，会出现这么一个现象，CPU的整体使用率不高，但是系统应用却响应缓慢，这可能是由于程序使用单线程的原因，单线程只使用一个CPU，导致这个CPU占用率为100%，无法处理其它请求，而其它的CPU却闲置，这就导致了整体CPU使用率不高，而应用缓慢现象的发生。 2.3.1 内存性能评估1.利用free指令监控内存free是监控Linux内存使用状况最常用的指令，看下面的一个输出： 123456789101112131415[root@webserver ~]# free -mtotal used freeshared buffers cachedMem: 8111 7185 926 0 243 6299 -/+ buffers/cache: 643 7468Swap: 8189 0 8189 经验公式： 123应用程序可用内存/系统物理内存&gt;70%，表示系统内存资源非常充足，不影响系统性能;应用程序可用内存/系统物理内存&lt;20%，表示系统内存资源紧缺，需要增加系统内存;20%&lt;应用程序可用内存/系统物理内存&lt;70%，表示系统内存资源基本能满足应用需求，暂时不影响系统性能 2.利用vmstat命令监控内存1234567891011121314[root@node1 ~]# vmstat 2 3procs ———–memory———- —swap– —–io—- –system– —–cpu——r b swpd freebuff cache si so bi bo incs us sy idwa st0 0 0 162240 8304 67032 0 0 13 21 1007 23 0 1 98 0 00 0 0 162240 8304 67032 0 0 1 0 1010 20 0 1 100 0 00 0 0 162240 8304 67032 0 0 1 1 1009 18 0 1 99 0 0 memory 1234swpd--切换到内存交换区的内存数量（k为单位)。如swpd值偶尔非0，不影响系统性能free--当前空闲的物理内存数量（k为单位）buff--buffers cache的内存数量，一般对块设备的读写才需要缓冲cache--page cached的内存数量 一般作为文件系统cached，频繁访问的文件都会被cached，如cache值较大，说明cached的文件数较多，如果此时IO中bi比较小，说明文件系统效率比较好。 swap 12si--由磁盘调入内存，也就是内存进入内存交换区的数量。so--由内存调入磁盘，也就是内存交换区进入内存的数量。 si、so的值长期不为0，表示系统内存不足。需增加系统内存。 2.4.1磁盘I&#x2F;O性能评估1.磁盘存储基础频繁访问的文件或数据尽可能用内存读写代替直接磁盘I&#x2F;O，效率高千倍。 将经常进行读写的文件与长期不变的文件独立出来，分别放置到不同的磁盘设备上。 对于写操作频繁的数据，可以考虑使用裸设备代替文件系统。 裸设备优点： 123数据可直接读写，不需经过操作系统级缓存，节省内存资源，避免内存资源争用;避免文件系统级维护开销，如文件系统需维护超级块、I-node等;避免了操作系统cache预读功能，减少了I/O请求 使用裸设备的缺点是： 数据管理、空间管理不灵活，需要很专业的人来操作。 2.利用iostat评估磁盘性能123456789101112131415161718192021222324252627282930[root@webserver ~]# iostat -d 2 3Linux 2.6.9-42.ELsmp (webserver) 12/01/2008_i686_ (8 CPU) Device: tps Blk_read/sBlk_wrtn/sBlk_read Blk_wrtnsda 1.87 2.58 114.12 6479462 286537372 Device: tps Blk_read/sBlk_wrtn/sBlk_read Blk_wrtnsda 0.00 0.00 0.00 0 0 Device: tps Blk_read/sBlk_wrtn/sBlk_read Blk_wrtnsda 1.00 0.00 12.00 0 24 解释如下： 1234Blk_read/s--每秒读取数据块数Blk_wrtn/s--每秒写入数据块数Blk_read--读取的所有块数Blk_wrtn--写入的所有块数 可通过Blk_read&#x2F;s和Blk_wrtn&#x2F;s值对磁盘的读写性能有一个基本的了解.如Blk_wrtn&#x2F;s值很大，表示磁盘写操作频繁，考虑优化磁盘或程序，如Blk_read&#x2F;s值很大，表示磁盘直接读操作很多，可将读取的数据放入内存 规则遵循： 长期的、超大的数据读写，肯定是不正常的，这种情况一定会影响系统性能。 3.利用sar评估磁盘性能通过“sar –d”组合，可以对系统的磁盘IO做一个基本的统计，请看下面的一个输出： 123456789101112131415161718192021222324252627282930313233[root@webserver ~]# sar -d 2 3Linux 2.6.9-42.ELsmp (webserver) 11/30/2008_i686_ (8 CPU)11:09:33 PM DEV tps rd_sec/swr_sec/savgrq-sz avgqu-sz await svctm %util11:09:35 PM dev8-0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0011:09:35 PM DEV tps rd_sec/swr_sec/savgrq-sz avgqu-sz await svctm %util11:09:37 PM dev8-0 1.00 0.00 12.00 12.00 0.00 0.00 0.00 0.0011:09:37 PM DEV tps rd_sec/swr_sec/savgrq-sz avgqu-sz await svctm %util11:09:39 PM dev8-0 1.99 0.00 47.76 24.00 0.00 0.50 0.25 0.05Average: DEV tps rd_sec/swr_sec/savgrq-sz avgqu-sz await svctm %utilAverage: dev8-0 1.00 0.00 19.97 20.00 0.00 0.33 0.17 0.02 参数含义： 123await--平均每次设备I/O操作等待时间（毫秒）svctm--平均每次设备I/O操作的服务时间（毫秒）%util--一秒中有百分之几的时间用于I/O操作 对磁盘IO性能评判标准： 正常svctm应小于await值，而svctm和磁盘性能有关，CPU、内存负荷也会对svctm值造成影响，过多的请求也会间接的导致svctm值的增加。 1234await值取决svctm和I/O队列长度以及I/O请求模式，如果svctm的值与await很接近，表示几乎没有I/O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I/O队列等待太长，系统上运行的应用程序将变慢，此时可以通过更换更快的硬盘来解决问题。 %util–衡量磁盘I&#x2F;O重要指标， 如%util接近100%，表示磁盘产生的I&#x2F;O请求太多，I&#x2F;O系统已经满负荷工作，该磁盘可能存在瓶颈。 可优化程序或者 通过更换 更高、更快的磁盘。 2.5.1. 网络性能评估1234（1）通过ping命令检测网络的连通性（2）通过netstat –i组合检测网络接口状况（3）通过netstat –r组合检测系统的路由表信息（4）通过sar –n组合显示系统的网络运行状态 三 Linux服务器性能调优1.为磁盘I&#x2F;O调整Linux内核电梯算法选择文件系统后，该算法可以平衡低延迟需求，收集足够数据，有效组织对磁盘读写请求。 2.禁用不必要的守护进程，节省内存和CPU资源1234许多守护进程或服务通常非必需，消耗宝贵内存和CPU时间。将服务器置于险地。禁用可加快启动时间，释放内存。减少CPU要处理的进程数 一些应被禁用的Linux守护进程，默认自动运行： 序号 守护进程 描述1 Apmd 高级电源管理守护进程2 Nfslock 用于NFS文件锁定3 Isdn ISDN Moderm支持4 Autofs 在后台自动挂载文件系统(如自动挂载CD-ROM)5 Sendmail 邮件传输代理6 Xfs X Window的字体服务器 3.关掉GUI4、清理不需要的模块或功能服务器软件包中太多被启动的功能或模块实际上是不需要的(如Apache中的许多功能模块)，禁用掉有助于提高系统内存可用量，腾出资源给那些真正需要的软件，让它们运行得更快。 5、禁用控制面板在Linux中，有许多流行的控制面板，如Cpanel，Plesk，Webmin和phpMyAdmin等，禁用释放出大约120MB内存，内存使用量大约下降30-40%。 6、改善Linux Exim服务器性能使用DNS缓存守护进程，可降低解析DNS记录需要的带宽和CPU时间，DNS缓存通过消除每次都从根节点开始查找DNS记录的需求，从而改善网络性能。 Djbdns是一个非常强大的DNS服务器，它具有DNS缓存功能，Djbdns比BIND DNS服务器更安全，性能更好，可以直接通过http://cr.yp.to/下载，或通过Red Hat提供的软件包获得。 7、使用AES256增强gpg文件加密安全为提高备份文件或敏感信息安全，许多Linux系统管理员都使用gpg进行加密，在使用gpg时，最好指定gpg使用AES256加密算法，AES256使用256位密钥，它是一个开放的加密算法，美国国家安全局(NSA)使用它保护绝密信息。 8、远程备份服务安全安全是选择远程备份服务最重要的因素，大多数系统管理员都害怕两件事：(黑客)可以删除备份文件，不能从备份恢复系统。 为了保证备份文件100%的安全，备份服务公司提供远程备份服务器，使用scp脚本或RSYNC通过SSH传输数据，这样，没有人可以直接进入和访问远程系统，因此，也没有人可以从备份服务删除数据。在选择远程备份服务提供商时，最好从多个方面了解其服务强壮性，如果可以，可以亲自测试一下。 9、更新默认内核参数设置为了顺利和成功运行企业应用程序，如数据库服务器，可能需要更新一些默认的内核参数设置，例如，2.4.x系列内核消息队列参数msgmni有一个默认值(例如，共享内存，或shmmax在Red Hat系统上默认只有33554432字节)，它只允许有限的数据库并发连接，下面为数据库服务器更好地运行提供了一些建议值(来自IBM DB2支持网站)： kernel.shmmax&#x3D;268435456 (32位)kernel.shmmax&#x3D;1073741824 (64位)kernel.msgmni&#x3D;1024fs.file-max&#x3D;8192kernel.sem&#x3D;”250 32000 32 1024″ 10、优化TCP优化TCP协议有助于提高网络吞吐量，跨广域网的通信使用的带宽越大，延迟时间越长时，建议使用越大的TCP Linux大小，以提高数据传输速率，TCP Linux大小决定了发送主机在没有收到数据传输确认时，可以向接收主机发送多少数据。 11、选择正确的文件系统 使用ext4文件系统取代ext3 ● Ext4是ext3文件系统的增强版，扩展了存储限制 ●具有日志功能，保证高水平的数据完整性(在非正常关闭事件中) ●非正常关闭和重启时，它不需要检查磁盘(这是一个非常耗时的动作) ●更快的写入速度，ext4日志优化了硬盘磁头动作 12、使用noatime文件系统挂载选项在文件系统启动配置文件fstab中使用noatime选项，如果使用了外部存储，这个挂载选项可以有效改善性能。 13、调整Linux文件描述符限制Linux限制了任何进程可以打开的文件描述符数量，默认限制是每进程1024，这些限制可能会阻碍基准测试客户端(如httperf和apachebench)和Web服务器本身获得最佳性能，Apache每个连接使用一个进程，因此不会受到影响，但单进程Web服务器，如Zeus是每连接使用一个文件描述符，因此很容易受默认限制的影响。 打开文件限制是一个可以用ulimit命令调整的限制，ulimit -aS命令显示当前的限制，ulimit -aH命令显示硬限制(在未调整&#x2F;proc中的内核参数前，你不能增加限制)。 Linux第三方应用程序性能技巧 对于运行在Linux上的第三方应用程序，一样有许多性能优化技巧，这些技巧可以帮助你提高Linux服务器的性能，降低运行成本。 14、正确配置MySQL为了给MySQL分配更多的内存，可设置MySQL缓存大小，要是MySQL服务器实例使用了更多内存，就减少缓存大小，如果MySQL在请求增多时停滞不动，就增加MySQL缓存。 15、正确配置Apache检查Apache使用了多少内存，再调整StartServers和MinSpareServers参数，以释放更多的内存，将有助于你节省30-40%的内存。 16、分析Linux服务器性能提高系统效率最好的办法是找出导致整体速度下降的瓶颈并解决掉，下面是找出系统关键瓶颈的一些基本技巧： ● 当大型应用程序，如OpenOffice和Firefox同时运行时，计算机可能会开始变慢，内存不足的出现几率更高。 ● 如果启动时真的很慢，可能是应用程序初次启动需要较长的加载时间，一旦启动好后运行就正常了，否则很可能是硬盘太慢了。 ●CPU负载持续很高，内存也够用，但CPU利用率很低，可以使用CPU负载分析工具监控负载时间。 17、学习5个Linux性能命令使用几个命令就可以管理Linux系统的性能了，下面列出了5个最常用的Linux性能命令，包括top、vmstat、iostat、free和sar，它们有助于系统管理员快速解决性能问题。 (1)top当前内核服务的任务，还显示许多主机状态的统计数据，默认情况下，它每隔5秒自动更新一次。如：当前正常运行时间，系统负载，进程数量和内存使用率， 此外，这个命令也显示了那些使用最多CPU时间的进程(包括每个进程的各种信息，如运行用户，执行的命令等)。 (2)vmstatVmstat命令提供当前CPU、IO、进程和内存使用率的快照，它和top命令类似，自动更新数据，如： $ vmstat 10 (3)iostatIostat提供三个报告：CPU利用率、设备利用率和网络文件系统利用率，使用-c，-d和-h参数可以分别独立显示这三个报告。 (4)free显示主内存和交换空间内存统计数据，指定-t参数显示总内存，指定-b参数按字节为单位，使用-m则以兆为单位，默认情况下千字节为单位。 Free命令也可以使用-s参数加一个延迟时间(单位：秒)连续运行，如： $ free -s 5 (5)sar收集，查看和记录性能数据，这个命令比前面几个命令历史更悠久，它可以收集和显示较长周期的数据。 其它 下面是一些归类为其它的性能技巧： 18、将日志文件转移到内存中当一台机器处于运行中时，最好是将系统日志放在内存中，当系统关闭时再将其复制到硬盘，当你运行一台开启了syslog功能的笔记本电脑或移动设备时，ramlog可以帮助你提高系统电池或移动设备闪存驱动器的寿命，使用ramlog的一个好处是，不用再担心某个守护进程每隔30秒向syslog发送一条消息，放在以前，硬盘必须随时保持运转，这样对硬盘和电池都不好。 19、先打包，后写入在内存中划分出固定大小的空间保存日志文件，这意味着笔记本电脑硬盘不用一直保持运转，只有当某个守护进程需要写入日志时才运转，注意ramlog使用的内存空间大小是固定的，否则系统内存会很快被用光，如果笔记本使用固态硬盘，可以分配50-80MB内存给ramlog使用，ramlog可以减少许多写入周期，极大地提高固态硬盘的使用寿命。 20、一般调优技巧尽可能使用静态内容替代动态内容，如果你在生成天气预告，或其它每隔1小时就必须更新的数据，最好是写一个程序，每隔1小时生成一个静态的文件，而不是让用户运行一个CGI动态地生成报告。 为动态应用程序选择最快最合适的API，CGI可能最容易编程，但它会为每个请求产生一个进程，通常，这是一个成本很高，且不必要的过程，FastCGI是更好的选择，和Apache的mod_perl一样，都可以极大地提高应用程序的性能。","categories":[],"tags":[]},{"title":"","slug":"总结/13、Linux（全）","date":"2024-12-14T22:02:47.103Z","updated":"2021-01-28T07:24:42.000Z","comments":true,"path":"2024/12/15/总结/13、Linux（全）/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/13%E3%80%81Linux%EF%BC%88%E5%85%A8%EF%BC%89/","excerpt":"","text":"[TOC] Linux 概述什么是LinuxLinux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 什么是默认登录 Shell ？在 Linux 操作系统，&quot;/bin/bash&quot; 是默认登录 Shell，是在创建用户时分配的。 使用 chsh 命令可以改变默认的 Shell 。示例如下所示： 12## chsh &lt;用户名&gt; -s &lt;新shell&gt;## chsh ThinkWon -s /bin/sh 什么是运维？什么是游戏运维？1）运维是指大型组织已经建立好的网络软硬件的维护，就是要保证业务的上线与运作的正常，在他运转的过程中，对他进行维护，他集合了网络、系统、数据库、开发、安全、监控于一身的技术运维又包括很多种，有DBA运维、网站运维、虚拟化运维、监控运维、游戏运维等等2）游戏运维又有分工，分为开发运维、应用运维（业务运维）和系统运维开发运维：是给应用运维开发运维工具和运维平台的应用运维：是给业务上线、维护和做故障排除的，用开发运维开发出来的工具给业务上线、维护、做故障排查系统运维：是给应用运维提供业务上的基础设施，比如：系统、网络、监控、硬件等等总结：开发运维和系统运维给应用运维提供了“工具”和“基础设施”上的支撑开发运维、应用运维和系统运维他们的工作是环环相扣的 Unix和Linux有什么区别？Linux和Unix都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至有一部分人错误地认为Unix和Linux操作系统是一样的，然而，事实并非如此，以下是两者的区别。 开源性Linux是一款开源操作系统，不需要付费，即可使用；Unix是一款对源码实行知识产权保护的传统商业软件，使用需要付费授权使用。 跨平台性Linux操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix操作系统跨平台性能较弱，大多需与硬件配套使用。 可视化界面Linux除了进行命令行操作，还有窗体管理系统；Unix只是命令行下的系统。 硬件环境Linux操作系统对硬件的要求较低，安装方法更易掌握；Unix对硬件要求比较苛刻，按照难度较大。 用户群体Linux的用户群体很广泛，个人和企业均可使用；Unix的用户群体比较窄，多是安全性要求高的大型企业使用，如银行、电信部门等，或者Unix硬件厂商使用，如Sun等。相比于Unix操作系统，Linux操作系统更受广大计算机爱好者的喜爱，主要原因是Linux操作系统具有Unix操作系统的全部功能，并且能够在普通PC计算机上实现全部的Unix特性，开源免费的特性，更容易普及使用！ 什么是 Linux 内核？Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。 系统内存管理 应用程序管理 硬件设备管理 文件系统管理 Linux的基本组件是什么？就像任何其他典型的操作系统一样，Linux拥有所有这些组件：内核，shell和GUI，系统实用程序和应用程序。Linux比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费下载。 Linux 的体系结构从大的方面讲，Linux 体系结构可以分为两块： 用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。 内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 。 为什么 Linux 体系结构要分为用户空间和内核空间的原因？ 1、现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。 2、Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。 用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：1）系统调用；2）硬件中断。 BASH和DOS之间的基本区别是什么？BASH和DOS控制台之间的主要区别在于3个方面： BASH命令区分大小写，而DOS命令则不区分; 在BASH下，&#x2F; character是目录分隔符，\\作为转义字符。在DOS下，&#x2F;用作命令参数分隔符，\\是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没有遵循这样的惯例。 Linux 开机启动过程？ 了解即可。 1、主机加电自检，加载 BIOS 硬件信息。 2、读取 MBR 的引导文件(GRUB、LILO)。 3、引导 Linux 内核。 4、运行第一个进程 init (进程号永远为 1 )。 5、进入相应的运行级别。 6、运行终端，输入用户名和密码。 Linux系统缺省的运行级别？ 关机。 单机用户模式。 字符界面的多用户模式(不支持网络)。 字符界面的多用户模式。 未分配使用。 图形界面的多用户模式。 重启。 Linux 使用的进程间通信方式？ 1、管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 2、信号(signal) 。 3、消息队列。 4、共享内存。 5、信号量。 6、套接字(socket) 。 Linux 有哪些系统日志文件？比较重要的是 /var/log/messages 日志文件。 该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。 另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。 Linux系统安装多个桌面环境有帮助吗？通常，一个桌面环境，如KDE或Gnome，足以在没有问题的情况下运行。尽管系统允许从一个环境切换到另一个环境，但这对用户来说都是优先考虑的问题。有些程序在一个环境中工作而在另一个环境中无法工作，因此它也可以被视为选择使用哪个环境的一个因素。 什么是交换空间？交换空间是Linux使用的一定空间，用于临时保存一些并发运行的程序。当RAM没有足够的内存来容纳正在执行的所有程序时，就会发生这种情况。 什么是root帐户root帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为每个帐户分配不同的权限。每次安装Linux时都是默认帐户。 什么是LILO？LILO是Linux的引导加载程序。它主要用于将Linux操作系统加载到主内存中，以便它可以开始运行。 什么是BASH？BASH是Bourne Again SHell的缩写。它由Steve Bourne编写，作为原始Bourne Shell（由&#x2F; bin &#x2F; sh表示）的替代品。它结合了原始版本的Bourne Shell的所有功能，以及其他功能，使其更容易使用。从那以后，它已被改编为运行Linux的大多数系统的默认shell。 什么是CLI？命令行界面（英语：command-line interface，缩写]：CLI）是在图形用户界面得到普及之前使用最为广泛的用户界面，它通常不支持鼠标，用户通过键盘输入指令，计算机接收到指令后，予以执行。也有人称之为字符用户界面（CUI）。 通常认为，命令行界面（CLI）没有图形用户界面（GUI）那么方便用户操作。因为，命令行界面的软件通常需要用户记忆操作的命令，但是，由于其本身的特点，命令行界面要较图形用户界面节约计算机系统的资源。在熟记命令的前提下，使用命令行界面往往要较使用图形用户界面的操作速度要快。所以，图形用户界面的操作系统中，都保留着可选的命令行界面。 什么是GUI？图形用户界面（Graphical User Interface，简称 GUI，又称图形用户接口）是指采用图形方式显示的计算机操作用户界面。 图形用户界面是一种人与计算机通信的界面显示格式，允许用户使用鼠标等输入设备操纵屏幕上的图标或菜单选项，以选择命令、调用文件、启动程序或执行其它一些日常任务。与通过键盘输入文本或字符命令来完成例行任务的字符界面相比，图形用户界面有许多优点。 开源的优势是什么？开源允许你将软件（包括源代码）免费分发给任何感兴趣的人。然后，人们可以添加功能，甚至可以调试和更正源代码中的错误。它们甚至可以让它运行得更好，然后再次自由地重新分配这些增强的源代码。这最终使社区中的每个人受益。 GNU项目的重要性是什么？这种所谓的自由软件运动具有多种优势，例如可以自由地运行程序以及根据你的需要自由学习和修改程序。它还允许你将软件副本重新分发给其他人，以及自由改进软件并将其发布给公众。 磁盘、目录、文件简单 Linux 文件系统？在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念：一切都是文件。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 Linux 支持 5 种文件类型，如下图所示： Linux 的目录结构是怎样的？ 这个问题，一般不会问。更多是实际使用时，需要知道。 Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： &#x2F;bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里； &#x2F;etc： 存放系统管理和配置文件； &#x2F;home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是&#x2F;home&#x2F;user，可以用~user表示； **&#x2F;usr **： 用于存放系统应用程序； &#x2F;opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； &#x2F;proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； &#x2F;root： 超级用户（系统管理员）的主目录（特权阶级o）； &#x2F;sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； &#x2F;dev： 用于存放设备文件； &#x2F;mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； &#x2F;boot： 存放用于系统引导时使用的各种文件； **&#x2F;lib **： 存放着和系统运行相关的库文件 ； &#x2F;tmp： 用于存放各种临时文件，是公用的临时文件存储点； &#x2F;var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； &#x2F;lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 什么是 inode ？ 一般来说，面试不会问 inode 。但是 inode 是一个重要概念，是理解 Unix&#x2F;Linux 文件系统和硬盘储存的基础。 理解inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程？ 如果看的一脸懵逼，也没关系。一般来说，面试官不太会问这个题目。 Linux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。 inode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。 Linux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。 什么是硬链接和软链接？1）硬链接 由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。 不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。 2）软链接 软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。 不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。 实际场景下，基本是使用软链接。总结区别如下： 硬链接不可以跨分区，软件链可以跨分区。 硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。 删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。 Raid卷 RAID 全称为独立磁盘冗余阵列(Redundant Array of Independent Disks)，基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统只会把它当做一个硬盘。 RAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应用中，可以依据自己的实际需求选择不同的 RAID 方案。 当然，因为很多公司都使用云服务，大家很难接触到 RAID 这个概念，更多的可能是普通云盘、SSD 云盘酱紫的概念。 创建Raid1234567891011mdadm –C 创建磁盘 -n # 使用#块盘创建raid -l # raid级别 # -a &#123;yes,no&#125; 是否自动创建目标raid设备的设备文件 -c chunk_size 指明块大小、单位为K -x # 指明空磁盘的个数（备份） Raid分类介绍Raid0： 称为条带化存储，以连续或字节为单位进行数据分割，将数据分段存储在各个硬盘中，并行读&#x2F;写数据，具有很高的数据传输率，但没有数据冗余，单个磁盘的损坏将影响所有数据。所以，RAID 0不能应用于数据安全性要求高的场合 ； Raid1： 称为镜像存储，通过磁盘数据镜像实现数据冗余，原理是在成堆的独立的磁盘上产生互为备份的数据，因为数据被同等地写入成对的磁盘中，所以性能比较慢。但当原始数据繁忙时，可以直接镜像拷贝读取数据，因此读取性能比较快。 Raid5（最少3块）： RAID5是一种存储性能、数据安全与存储成本兼顾的存储解决方案。RAID 5具有和RAID 0相近似的数据读取速度，只是多了一个奇偶校验信息，写入数据的速度比对单个磁盘进行写入操作稍慢。同时由于多个数据对应一个奇偶校验信息，RAID 5的磁盘空间利用率要比RAID 1高，存储成本相对较低，是目前运用较多的一种解决方案。 Raid1+0（最少4块）：是RAID1和RAID0的结合，先做镜像(1)，再做条带（0），兼顾了raid1的容错能力与raid0的条带化读写数据的优点，性能好、可靠性高。属于混合型RAID。 RAID1＋0两边的RAID1中可以同时各坏一块硬盘，但不能同时坏掉单边的两个硬盘。 12（以4块磁盘组成raid10为例，首先组成两个raid1，然后两个raid1组成一个raid0，由于raid1实际存储空间利用率为1/2，raid0为全部，所以raid10的利用率为1/2，更多磁盘也是如此。和raid5相比，raid10磁盘空间利用率较低，但读写性能上较好，特别是盘多的情况。） 安全一台 Linux 系统初始化环境后需要做一些什么安全工作？ 1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。 修改 SSH 端口不一定绝对哈。当然，如果要暴露在外网，建议改下。 2、服务器使用密钥登陆，禁止密码登陆。 3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。 4、装 fail2ban 这种防止 SSH 暴力破击的软件。 5、设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要) 也可以安装 VPN 等软件，只允许连接 VPN 到服务器上。 6、修改历史命令记录的条数为 10 条。 7、只允许有需要的服务器可以访问外网，其它全部禁止。 8、做好软件层面的防护。 8.1 设置 nginx_waf 模块防止 SQL 注入。 8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。 什么叫 CC 攻击？什么叫 DDOS 攻击？ CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。 DDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。 攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 怎么预防 CC 攻击和 DDOS 攻击？ 防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。 流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般是买服务，毕竟攻击不会是持续长时间。 什么是网站数据库注入？ 由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。 SQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 如何过滤与预防？ 数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。 实战如何选择 Linux 操作系统版本?一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。 根据具体要求： 安全性要求较高，则选择 Debian 或者 FreeBSD 。 需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。 想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布版本。 【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 6 系列，现在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。 CentOS 7 系列，也慢慢使用的会比较多了。 如何规划一台 Linux 主机，步骤是怎样？ 1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。 不同的用途，机器的配置会有所不同。 2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。 3、需要优化系统的哪些参数，需要创建哪些用户等等的。 请问当用户反馈网站访问慢，你会如何处理？有哪些方面的因素会导致网站网站访问慢？ 1、服务器出口带宽不够用 本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。 跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。 2、服务器负载过大，导致响应不过来 可以从两个方面入手分析： 分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。 如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。 3、数据库瓶颈 如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、网站开发代码没有优化好 例如 SQL 语句没有优化，导致数据库读写相当耗时。 针对网站访问慢，怎么去排查？ 1、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。 不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。 2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。 3、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。 4、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。 怎么去解决？ 1、如果是出口带宽问题，那么久申请加大出口带宽。 2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。 3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。 4、申请购买 CDN 服务，加载用户的访问。 5、如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。 Linux 性能调优都有哪几种方法？ 1、Disabling daemons (关闭 daemons)。 2、Shutting down the GUI (关闭 GUI)。 3、Changing kernel parameters (改变内核参数)。 4、Kernel parameters (内核参数)。 5、Tuning the processor subsystem (处理器子系统调优)。 6、Tuning the memory subsystem (内存子系统调优)。 7、Tuning the file system (文件系统子系统调优)。 8、Tuning the network subsystem（网络子系统调优)。 文件管理命令cat 命令cat 命令用于连接文件并打印到标准输出设备上。 cat 主要有三大功能： 1.一次显示整个文件: 1cat filename 2.从键盘创建一个文件: 1cat &gt; filename 只能创建新文件，不能编辑已有文件。 3.将几个文件合并为一个文件: 1cat file1 file2 &gt; file -b 对非空输出行号 -n 输出所有行号 实例： （1）把 log2012.log 的文件内容加上行号后输入 log2013.log 这个文件里 1cat -n log2012.log log2013.log （2）把 log2012.log 和 log2013.log 的文件内容加上行号（空白行不加）之后将内容附加到 log.log 里 1cat -b log2012.log log2013.log log.log （3）使用 here doc 生成新文件 12345678910cat &gt;log.txt &lt;&lt;EOF&gt;Hello&gt;World&gt;PWD=$(pwd)&gt;EOFls -l log.txtcat log.txtHelloWorldPWD=/opt/soft/test （4）反向列示 1234tac log.txtPWD=/opt/soft/testWorldHello chmod 命令Linux&#x2F;Unix 的文件调用权限分为三级 : 文件拥有者、群组、其他。利用 chmod 可以控制文件如何被他人所调用。 用于改变 linux 系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。 以文件 log2012.log 为例： 12-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log1 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。 常用参数： 12-c 当发生改变时，报告处理信息-R 处理指定目录以及其子目录下所有文件 权限范围： 1234u ：目录或者文件的当前的用户g ：目录或者文件的当前的群组o ：除了目录或者文件的当前用户或群组之外的用户或者群组a ：所有的用户及群组 权限代号： 12345r ：读权限，用数字4表示w ：写权限，用数字2表示x ：执行权限，用数字1表示- ：删除权限，用数字0表示s ：特殊权限 实例： （1）增加文件 t.log 所有用户可执行权限 1chmod a+x t.log （2）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息 1chmod u=r t.log -c （3）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限 1chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) （4）将 test 目录及其子目录所有文件添加可读权限 1chmod u+r,g+r,o+r -R text/ -c chown 命令chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。 12-c 显示更改的部分的信息-R 处理指定目录及子目录 实例： （1）改变拥有者和群组 并显示改变信息 1chown -c mail:mail log2012.log （2）改变文件群组 1chown -c :mail t.log （3）改变文件夹及子文件目录属主及属组为 mail 1chown -cR mail: test/ cp 命令将源文件复制至目标文件，或将多个源文件复制至目标目录。 注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！ 123-i 提示-r 复制目录及目录内所有项目-a 复制的文件与原文件时间一样 实例： （1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。 1cp -ai a.txt test （2）为 a.txt 建议一个链接（快捷方式） 1cp -s a.txt link_a.txt find 命令用于在文件树中查找文件，并作出相应的处理。 命令格式： 1find pathname -options [-print -exec -ok ...] 命令参数： 1234pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。-print： find命令将匹配的文件输出到标准输出。-exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&#x27;command&#x27; &#123; &#125; \\;，注意&#123; &#125;和\\；之间的空格。-ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项： 1234567891011-name 按照文件名查找文件-perm 按文件权限查找文件-user 按文件属主查找文件-group 按照文件所属的组来查找文件。-type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 实例： （1）查找 48 小时内修改过的文件 1find -atime -2 （2）在当前目录查找 以 .log 结尾的文件。 . 代表当前目录 1find ./ -name &#x27;*.log&#x27; （3）查找 &#x2F;opt 目录下 权限为 777 的文件 1find /opt -perm 777 （4）查找大于 1K 的文件 1find -size +1000c 查找等于 1000 字符的文件 1find -size 1000c -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。 head 命令head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 行。 常用参数： 1-n&lt;行数&gt; 显示的行数（行数为复数表示从最后向前数） 实例： （1）显示 1.log 文件中前 20 行 1head 1.log -n 20 （2）显示 1.log 文件前 20 字节 1head -c 20 log2014.log （3）显示 t.log最后 10 行 1head -n -10 t.log less 命令less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 常用命令参数： 12345678910111213141516171819-i 忽略搜索时的大小写-N 显示每行的行号-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来-s 显示连续空行为一行/字符串：向下搜索“字符串”的功能?字符串：向上搜索“字符串”的功能n：重复前一个搜索（与 / 或 ? 有关）N：反向重复前一个搜索（与 / 或 ? 有关）-x &lt;数字&gt; 将“tab”键显示为规定的数字空格b 向后翻一页d 向后翻半页h 显示帮助界面Q 退出less 命令u 向前滚动半页y 向前滚动一行空格键 滚动一行回车键 滚动一页[pagedown]： 向下翻动一页[pageup]： 向上翻动一页 实例： （1）ps 查看进程信息并通过 less 分页显示 1ps -aux | less -N （2）查看多个文件 1less 1.log 2.log 可以使用 n 查看下一个，使用 p 查看前一个。 ln 命令功能是为文件在另外一个位置建立一个同步的链接，当在不同目录需要该问题时，就不需要为每一个目录创建同样的文件，通过 ln 创建的链接（link）减少磁盘占用量。 链接分类：软件链接及硬链接 软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 需要注意： 第一：ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二：ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 第三：ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用参数： 123-b 删除，覆盖以前建立的链接-s 软链接（符号链接）-v 显示详细处理过程 实例： （1）给文件创建软链接，并显示操作信息 1ln -sv source.log link.log （2）给文件创建硬链接，并显示操作信息 1ln -v source.log link1.log （3）给目录创建软链接 1ln -sv /opt/soft/test/test3 /opt/soft/test/test5 locate 命令locate 通过搜寻系统内建文档数据库达到快速找到档案，数据库由 updatedb 程序来更新，updatedb 是由 cron daemon 周期性调用的。默认情况下 locate 命令在搜寻数据库时比由整个由硬盘资料来搜寻资料来得快，但较差劲的是 locate 所找到的档案若是最近才建立或 刚更名的，可能会找不到，在内定值中，updatedb 每天会跑一次，可以由修改 crontab 来更新设定值 (etc&#x2F;crontab)。 locate 与 find 命令相似，可以使用如 *、? 等进行正则匹配查找 常用参数： 123-l num（要显示的行数）-f 将特定的档案系统排除在外，如将proc排除在外-r 使用正则运算式做为寻找条件 实例： （1）查找和 pwd 相关的所有文件(文件名中包含 pwd） 1locate pwd （2）搜索 etc 目录下所有以 sh 开头的文件 1locate /etc/sh （3）查找 &#x2F;var 目录下，以 reason 结尾的文件 1locate -r &#x27;^/var.*reason$&#x27;（其中.表示一个字符，*表示任务多个；.*表示任意多个字符） more 命令功能类似于 cat, more 会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。 命令参数： 123456789+n 从笫 n 行开始显示-n 定义屏幕大小为n行+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能-l 忽略Ctrl+l（换页）字符-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似-s 把连续的多个空行显示为一行-u 把文件内容中的下画线去掉 常用操作命令： 123456789Enter 向下 n 行，需要定义。默认为 1 行Ctrl+F 向下滚动一屏空格键 向下滚动一屏Ctrl+B 返回上一屏= 输出当前行的行号:f 输出文件名和当前行的行号V 调用vi编辑器!命令 调用Shell，并执行命令q 退出more 实例： （1）显示文件中从第3行起的内容 1more +3 text.txt （2）在所列出文件目录详细信息，借助管道使每次显示 5 行 1ls -l | more -5 按空格显示下 5 行。 mv 命令移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。 实例： （1）将文件 test.log 重命名为 test1.txt 1mv test.log test1.txt （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中 1mv llog1.txt log2.txt log3.txt /test3 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖 1mv -i log1.txt log2.txt （4）移动当前文件夹下的所有文件到上一级目录 1mv * ../ rm 命令删除一个目录中的一个或多个文件或目录，如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。 1rm [选项] 文件… 实例： （1）删除任何 .log 文件，删除前逐一询问确认： 1rm -i *.log （2）删除 test 子目录及子目录中所有档案删除，并且不用一一确认： 1rm -rf test （3）删除以 -f 开头的文件 1rm -- -f* tail 命令用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 常用参数： 12-f 循环读取（常用于查看递增的日志文件）-n&lt;行数&gt; 显示行数（从后向前） （1）循环读取逐渐增加的文件内容 1ping 127.0.0.1 &gt; ping.log &amp; 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。 1tail -f ping.log （查看日志） touch 命令Linux touch命令用于修改文件或者目录的时间属性，包括存取时间和更改时间。若文件不存在，系统会建立一个新的文件。 ls -l 可以显示档案的时间记录。 语法 1touch [-acfm][-d&lt;日期时间&gt;][-r&lt;参考文件或目录&gt;] [-t&lt;日期时间&gt;][--help][--version][文件或目录…] 参数说明： a 改变档案的读取时间记录。 m 改变档案的修改时间记录。 c 假如目的档案不存在，不会建立新的档案。与 –no-create 的效果一样。 f 不使用，是为了与其他 unix 系统的相容性而保留。 r 使用参考档的时间记录，与 –file 的效果一样。 d 设定时间与日期，可以使用各种不同的格式。 t 设定档案的时间记录，格式与 date 指令相同。 –no-create 不会建立新档案。 –help 列出指令格式。 –version 列出版本讯息。 实例 使用指令”touch”修改文件”testfile”的时间属性为当前系统时间，输入如下命令： 1$ touch testfile #修改文件的时间属性 首先，使用ls命令查看testfile文件的属性，如下所示： 1234$ ls -l testfile #查看文件的时间属性 #原来文件的修改时间为16:09 -rw-r--r-- 1 hdd hdd 55 2011-08-22 16:09 testfile 123 执行指令”touch”修改文件属性以后，并再次查看该文件的时间属性，如下所示： 1234$ touch testfile #修改文件时间属性为当前系统时间 $ ls -l testfile #查看文件的时间属性 #修改后文件的时间属性为当前系统时间 -rw-r--r-- 1 hdd hdd 55 2011-08-22 19:53 testfile 使用指令”touch”时，如果指定的文件不存在，则将创建一个新的空白文件。例如，在当前目录下，使用该指令创建一个空白文件”file”，输入如下命令： 1$ touch file #创建一个名为“file”的新的空白文件 vim 命令Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 打开文件并跳到第 10 行：vim +10 filename.txt 。 打开文件跳到第一个匹配的行：vim +/search-term filename.txt 。 以只读模式打开文件：vim -R /etc/passwd 。 基本上 vi&#x2F;vim 共分为三种模式，分别是命令模式（Command mode），**输入模式（Insert mode）**和**底线命令模式（Last line mode）**。 简单的说，我们可以将这三个模式想成底下的图标来表示： whereis 命令whereis 命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。whereis 及 locate 都是基于系统内建的数据库进行搜索，因此效率很高，而find则是遍历硬盘查找文件。 常用参数： 1234-b 定位可执行文件。-m 定位帮助文件。-s 定位源代码文件。-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 实例： （1）查找 locate 程序相关文件 1whereis locate （2）查找 locate 的源码文件 1whereis -s locate （3）查找 lcoate 的帮助文件 1whereis -m locate which 命令在 linux 要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索： 12345which 查看可执行文件的位置。whereis 查看文件的位置。locate 配合数据库查看文件位置。find 实际搜寻硬盘查询文件名称。1234 which 是在 PATH 就是指定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果。使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 常用参数： 1-n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 实例： （1）查看 ls 命令是否存在，执行哪个 1which ls （2）查看 which 1which which （3）查看 cd 1which cd（显示不存在，因为 cd 是内建命令，而 which 查找显示是 PATH 中的命令） 查看当前 PATH 配置： 1echo $PATH 或使用 env 查看所有环境变量及对应值 文档编辑命令grep 命令强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。 grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 命令格式： 1grep [option] pattern file|dir 常用参数： 123456789-A n --after-context显示匹配字符后n行-B n --before-context显示匹配字符前n行-C n --context 显示匹配字符前后n行-c --count 计算符合样式的列数-i 忽略大小写-l 只列出文件内容符合指定的样式的文件名称-f 从文件中读取关键词-n 显示匹配内容的所在文件中行数-R 递归查找文件夹 grep 的规则表达式: 12345678910111213141516^ #锚定行的开始 如：&#x27;^grep&#x27;匹配所有以grep开头的行。 $ #锚定行的结束 如：&#x27;grep$&#x27;匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：&#x27;gr.p&#x27;匹配gr后接一个任意字符，然后是p。 * #匹配零个或多个先前字符 如：&#x27;*grep&#x27;匹配所有一个或多个空格后紧跟grep的行。.* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如&#x27;[Gg]rep&#x27;匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：&#x27;[^A-FH-Z]rep&#x27;匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \\(..\\) #标记匹配字符，如&#x27;\\(love\\)&#x27;，love被标记为1。 \\&lt; #锚定单词的开始，如:&#x27;\\&lt;grep&#x27;匹配包含以grep开头的单词的行。\\&gt; #锚定单词的结束，如&#x27;grep\\&gt;&#x27;匹配包含以grep结尾的单词的行。x\\&#123;m\\&#125; #重复字符x，m次，如：&#x27;0\\&#123;5\\&#125;&#x27;匹配包含5个o的行。 x\\&#123;m,\\&#125; #重复字符x,至少m次，如：&#x27;o\\&#123;5,\\&#125;&#x27;匹配至少有5个o的行。 x\\&#123;m,n\\&#125; #重复字符x，至少m次，不多于n次，如：&#x27;o\\&#123;5,10\\&#125;&#x27;匹配5--10个o的行。 \\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：&#x27;G\\w*p&#x27;匹配以G后跟零个或多个文字或数字字符，然后是p。 \\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \\b #单词锁定符，如: &#x27;\\bgrep\\b&#x27;只匹配grep。 实例： （1）查找指定进程 1ps -ef | grep svn （2）查找指定进程个数 1ps -ef | grep svn -c （3）从文件中读取关键词 1cat test1.txt | grep -f key.log （4）从文件夹中递归查找以grep开头的行，并只列出文件 1grep -lR &#x27;^grep&#x27; /tmp （5）查找非x开关的行内容 1grep &#x27;^[^x]&#x27; test.txt （6）显示包含 ed 或者 at 字符的内容行 1grep -E &#x27;ed|at&#x27; test.txt wc 命令wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出 命令格式： 1wc [option] file.. 命令参数： 1234-c 统计字节数-l 统计行数-m 统计字符数-w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串 实例： （1）查找文件的 行数 单词数 字节数 文件名 1wc text.txt 结果： 17 8 70 test.txt （2）统计输出结果的行数 1cat test.txt | wc -l 磁盘管理命令cd 命令cd(changeDirectory) 命令语法： 1cd [目录名] 说明：切换当前目录至 dirName。 实例： （1）进入要目录 1cd / （2）进入 “home” 目录 1cd ~ （3）进入上一次工作路径 1cd - （4）把上个命令的参数作为cd参数使用。 1cd !$ df 命令显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示： 1234567-a 全部文件系统列表-h 以方便阅读的方式显示信息-i 显示inode信息-k 区块为1024字节-l 只显示本地磁盘-T 列出文件系统类型123456 实例： （1）显示磁盘使用情况 1df -l （2）以易读方式列出所有文件系统及其类型 1df -haT du 命令du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看： 命令格式： 1du [选项] [文件] 常用参数： 1234567-a 显示目录中所有文件大小-k 以KB为单位显示文件大小-m 以MB为单位显示文件大小-g 以GB为单位显示文件大小-h 以易读方式显示文件大小-s 仅显示总计-c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 实例： （1）以易读方式显示文件夹内及子文件夹大小 1du -h scf/ （2）以易读方式显示文件夹内所有文件大小 1du -ah scf/ （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 1du -hc test/ scf/ （4）输出当前目录下各个子目录所使用的空间 1du -hc --max-depth=1 scf/ ls命令就是 list 的缩写，通过 ls 命令不仅可以查看 linux 文件夹包含的文件，而且可以查看文件权限(包括目录、文件夹、文件权限)查看目录信息等等。 常用参数搭配： 1234567ls -a 列出目录所有文件，包含以.开始的隐藏文件ls -A 列出除.及..的其它文件ls -r 反序排列ls -t 以文件修改时间排序ls -S 以文件大小排序ls -h 以易读大小显示ls -l 除了文件名之外，还将文件的权限、所有者、文件大小等信息详细列出来 实例： (1) 按易读方式按时间反序排序，并显示文件详细信息 1ls -lhrt (2) 按大小反序显示文件详细信息 1ls -lrS (3)列出当前目录中所有以”t”开头的目录的详细内容 1ls -l t* (4) 列出文件绝对路径（不包含隐藏文件） 1ls | sed &quot;s:^:`pwd`/:&quot; (5) 列出文件绝对路径（包含隐藏文件） 1find $pwd -maxdepth 1 | xargs ls -ld mkdir 命令mkdir 命令用于创建文件夹。 可用选项： -m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 实例： （1）当前工作目录下创建名为 t的文件夹 1mkdir t （2）在 tmp 目录下创建路径为 test&#x2F;t1&#x2F;t 的目录，若不存在，则创建： 1mkdir -p /tmp/test/t1/t pwd 命令pwd 命令用于查看当前工作目录路径。 实例： （1）查看当前路径 1pwd （2）查看软链接的实际路径 1pwd -P rmdir 命令从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。 注意：不能删除非空目录 实例： （1）当 parent 子目录被删除后使它也成为空目录的话，则顺便一并删除： 1rmdir -p parent/child/child11 网络通讯命令ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 iptables 命令iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如： 把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp --dport 80 -j REJECT 。 开启 80 端口，因为web对外都是这个端口 1iptables -A INPUT -p tcp --dport 80 -j ACCEPT 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 netstat 命令Linux netstat命令用于显示网络状态。 利用netstat指令可让你得知整个Linux系统的网络情况。 语法 1netstat [-acCeFghilMnNoprstuvVwx][-A&lt;网络类型&gt;][--ip] 参数说明： -a或–all 显示所有连线中的Socket。 -A&lt;网络类型&gt;或–&lt;网络类型&gt; 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定”-A unix”参数相同。 –ip或–inet 此参数的效果和指定”-A inet”参数相同。 实例 如何查看系统都开启了哪些端口？ 12345678910[root@centos6 ~ 13:20 #55]# netstat -lnpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshdtcp 0 0 :::22 :::* LISTEN 1035/sshdudp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclientActive UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node PID/Program name Pathunix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstartunix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 如何查看网络连接状况？ 1234567[root@centos6 ~ 13:22 #58]# netstat -anActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address Statetcp 0 0 0.0.0.0:22 0.0.0.0:* LISTENtcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHEDtcp 0 0 :::22 :::* LISTENudp 0 0 0.0.0.0:68 0.0.0.0:* 如何统计系统当前进程连接数？ 输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？ 严格来说，这个题目考验的是对 awk 的使用。 首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下： 1234567tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHEDtcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHEDtcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHEDtcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHEDtcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHEDtcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHEDtcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHED ping 命令Linux ping命令用于检测主机。 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 指定接收包的次数 1ping -c 2 www.baidu.com telnet 命令Linux telnet命令用于远端登入。 执行telnet指令开启终端机阶段作业，并登入远端主机。 语法 1telnet [-8acdEfFKLrx][-b&lt;主机别名&gt;][-e&lt;脱离字符&gt;][-k&lt;域名&gt;][-l&lt;用户名称&gt;][-n&lt;记录文件&gt;][-S&lt;服务类型&gt;][-X&lt;认证形态&gt;][主机名称或IP地址&lt;通信端口&gt;] 参数说明： -8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b&lt;主机别名&gt; 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e&lt;脱离字符&gt; 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定”-F”参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k&lt;域名&gt; 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l&lt;用户名称&gt; 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n&lt;记录文件&gt; 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S&lt;服务类型&gt; 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X&lt;认证形态&gt; 关闭指定的认证形态。 实例 登录远程主机 12# 登录IP为 192.168.0.5 的远程主机telnet 192.168.0.5 系统管理命令date 命令显示或设定系统的日期与时间。 命令参数： 123456789101112131415-d&lt;字符串&gt; 显示字符串所指的日期与时间。字符串前后必须加上双引号。-s&lt;字符串&gt; 根据字符串来设置日期与时间。字符串前后必须加上双引号。-u 显示GMT。%H 小时(00-23)%I 小时(00-12)%M 分钟(以00-59来表示)%s 总秒数。起算时间为1970-01-01 00:00:00 UTC。%S 秒(以本地的惯用法来表示)%a 星期的缩写。%A 星期的完整名称。%d 日期(以01-31来表示)。%D 日期(含年月日)。%m 月份(以01-12来表示)。%y 年份(以00-99来表示)。%Y 年份(以四位数来表示)。 实例： （1）显示下一天 1date +%Y%m%d --date=&quot;+1 day&quot; //显示下一天的日期 （2）-d参数使用 1234567date -d &quot;nov 22&quot; 今年的 11 月 22 日是星期三date -d &#x27;2 weeks&#x27; 2周后的日期date -d &#x27;next monday&#x27; (下周一的日期)date -d next-day +%Y%m%d（明天的日期）或者：date -d tomorrow +%Y%m%ddate -d last-day +%Y%m%d(昨天的日期) 或者：date -d yesterday +%Y%m%ddate -d last-month +%Y%m(上个月是几月)date -d next-month +%Y%m(下个月是几月) free 命令显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。 命令参数： 1234567-b 以Byte显示内存使用情况-k 以kb为单位显示内存使用情况-m 以mb为单位显示内存使用情况-g 以gb为单位显示内存使用情况-s&lt;间隔秒数&gt; 持续显示内存-t 显示内存使用总合123456 实例： （1）显示内存使用情况 123freefree -kfree -m （2）以总和的形式显示内存的使用信息 1free -t （3）周期性查询内存使用情况 1free -s 10 kill 命令发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果任无法终止该程序可用”-KILL” 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 常用参数： 12345-l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称-a 当处理当前进程时，不限制命令名和进程号的对应关系-p 指定kill 命令只打印相关进程的进程号，而不发送任何信号-s 指定发送信号-u 指定用户 实例： （1）先使用ps查找进程pro1，然后用kill杀掉 1kill -9 $(ps -ef | grep pro1) ps 命令ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps 工具标识进程的5种状态码: 12345D 不可中断 uninterruptible sleep (usually IO)R 运行 runnable (on run queue)S 中断 sleepingT 停止 traced or stoppedZ 僵死 a defunct (”zombie”) process 命令参数： 12345678-A 显示所有进程a 显示所有进程-a 显示同一终端下所有进程c 显示进程真实名称e 显示环境变量f 显示进程间的关系r 显示当前终端运行的进程-aux 显示所有包含其它使用的进程 实例： （1）显示当前所有进程环境变量及进程间关系 1ps -ef （2）显示当前所有进程 1ps -A （3）与grep联用查找某进程 1ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码 1ps aux | grep &#x27;(cron|syslog)&#x27; rpm 命令Linux rpm 命令用于管理套件。 rpm(redhat package manager) 原本是 Red Hat Linux 发行版专门用来管理 Linux 各项套件的程序，由于它遵循 GPL 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。RPM 套件管理方式的出现，让 Linux 易于安装，升级，间接提升了 Linux 的适用度。 123456# 查看系统自带jdkrpm -qa | grep jdk# 删除系统自带jdkrpm -e --nodeps 查看jdk显示的数据# 安装jdkrpm -ivh jdk-7u80-linux-x64.rpm top 命令显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等 常用参数： 1234-c 显示完整的进程命令-s 保密模式-p &lt;进程号&gt; 指定进程显示-n &lt;次数&gt;循环显示次数 实例： 12345678top - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombieCpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%stMem: 32949016k total, 14411180k used, 18537836k free, 169884k buffersSwap: 32764556k total, 0k used, 32764556k free, 3612636k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 1234567 前五行是当前系统情况整体的统计信息区。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 12345675.9%us — 用户空间占用CPU的百分比。3.4% sy — 内核空间占用CPU的百分比。0.0% ni — 改变过优先级的进程占用CPU的百分比90.4% id — 空闲CPU百分比0.0% wa — IO等待占用CPU的百分比0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比0.2% si — 软中断（Software Interrupts）占用CPU的百分比 备注：在这里CPU的使用比率和windows概念不同，需要理解linux系统用户空间和内核空间的相关知识！ 第四行，内存状态，具体信息如下： 123432949016k total — 物理内存总量（32GB）14411180k used — 使用中的内存总量（14GB）18537836k free — 空闲内存总量（18GB）169884k buffers — 缓存的内存量 （169M） 第五行，swap交换分区信息，具体信息说明如下： 123432764556k total — 交换区总量（32GB）0k used — 使用的交换区总量（0K）32764556k free — 空闲交换区总量（32GB）3612636k cached — 缓冲的交换区总量（3.6GB） 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： 123456789101112PID — 进程idUSER — 进程所有者PR — 进程优先级NI — nice值。负值表示高优先级，正值表示低优先级VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RESRES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATASHR — 共享内存大小，单位kbS — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程%CPU — 上次更新到现在的CPU时间占用百分比%MEM — 进程使用的物理内存百分比TIME+ — 进程使用的CPU时间总计，单位1/100秒COMMAND — 进程名称（命令名/命令行） top 交互命令 1234567h 显示top交互命令帮助信息c 切换显示命令名称和完整命令行m 以内存使用率排序P 根据CPU使用百分比大小进行排序T 根据时间/累计时间进行排序W 将当前设置写入~/.toprc文件中o或者O 改变显示项目的顺序 yum 命令yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。 基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install 4.仅更新指定的软件命令：yum update 5.列出所有可安裝的软件清单命令：yum list 6.删除软件包命令：yum remove 7.查找软件包 命令：yum search 8.清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (&#x3D; yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 实例 安装 pam-devel 1[root@www ~]# yum install pam-devel crontab命令分,时,日,月,周 crontab crontab -e 编辑crontab文件 crontab -l 列出crontab文件 crontab -r 删除crontab文件 linux中table按不能补全代码Linux系统table无法补全时要安装补全插件 1yum install bash-completion 如果安装成功后仍然无效，bash重新载入 备份压缩命令bzip2 命令 创建 *.bz2 压缩文件：bzip2 test.txt 。 解压 *.bz2 文件：bzip2 -d test.txt.bz2 。 gzip 命令 创建一个 *.gz 的压缩文件：gzip test.txt 。 解压 *.gz 文件：gzip -d test.txt.gz 。 显示压缩的比率：gzip -l *.gz 。 tar 命令用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。 弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件 常用参数： 12345678910-c 建立新的压缩文件-f 指定压缩文件-r 添加文件到已经压缩文件包中-u 添加改了和现有的文件到压缩包中-x 从压缩包中抽取文件-t 显示压缩文件中的内容-z 支持gzip压缩-j 支持bzip2压缩-Z 支持compress解压文件-v 显示操作过程 有关 gzip 及 bzip2 压缩: 12345gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gzbz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 实例： （1）将文件全部打包成 tar 包 1tar -cvf log.tar 1.log,2.log 或tar -cvf log.* （2）将 &#x2F;etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩 1tar -zcvf /tmp/etc.tar.gz /etc （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的） 1tar -ztvf /tmp/etc.tar.gz （4）要压缩打包 &#x2F;home, &#x2F;etc ，但不要 &#x2F;home&#x2F;dmtsai 1tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc unzip 命令 解压 *.zip 文件：unzip test.zip 。 查看 *.zip 文件的内容：unzip -l jasper.zip 。","categories":[],"tags":[]},{"title":"","slug":"总结/12、Linux（基础命令和三剑客）","date":"2024-12-14T22:02:47.101Z","updated":"2021-02-07T15:05:56.000Z","comments":true,"path":"2024/12/15/总结/12、Linux（基础命令和三剑客）/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/12%E3%80%81Linux%EF%BC%88%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E5%92%8C%E4%B8%89%E5%89%91%E5%AE%A2%EF%BC%89/","excerpt":"","text":"[TOC] 一、基础正则表达式：元字符总结^ : 匹配输入字符串的开始位置，除非再方括号表达式中使用，表示不包含该字符集合 . : 匹配除“\\r\\n”之处的任何单个字符 \\ : 反斜杠，又叫转义字符，去除其后紧跟的元字符或通配符的特殊意义 ***:** 匹配前面的子表达式零次或多次 [ ] : 字符集合，匹配所包含的任意一个字符 [^] : 赋值字符集合，匹配未包含的一个任意字符 [n1-n2] : 字符范围，匹配指定范围内的任意一个字符 {n} : n是一个非负整数，匹配确定的n次 {n,} : n是一个非负整数，至少匹配n次 {n,m} : m和n均为非负整数，其中n&lt;&#x3D;m,最少匹配n次最多匹配m次 二、grep用法grep的使用规则： 1234567-n 表示显示行号-I 表示不区分大小写-v 表示过滤[ ] 查找集合字符 查找包含the的字符（准备一个查找文件） 1grep -n ‘the’ test.txt 不区分大小写查找包含the的字符 1grep -in ‘the’ test.txt 不查找包含the的字符 1grep -vn ‘the’ test.txt 利用中括号[ ]来查找集合字符 1grep -n ‘sh[io]t’ test.txt 查找包含重复单个字符oo时 1grep -n ‘oo’ test.txt 若查找oo前面不是w的字符串 1grep -n ‘[^w]oo’ test.txt 若不希望oo前面存在小写字母 1grep -n ‘[^a-z]oo’ test.txt 查询以小写字母开头的行 1grep -n ‘^[a-z]’ test.txt 查询不以字母开头的行 1grep -n ‘[^a-zA-Z]’ text.txt 查询以.结尾的行 1grep -n ‘\\.$’ test.txt 若想要查询oo、ooo、oooo等字符串，则需要使用星号（*）元字符等，o*表示拥有零个或任意多个o字符，如果时“oo*”，则第一个o必须存在,后面的o*表示零个或任意多个o字符 1grep -n ‘ooo*’ test.txt 查询以w开头d结尾，中间至少一个 1grep -n ‘woo*d’ test.tx 查询以w开头d结尾，中间字符可有可无 1grep -n ‘w.*d’ test.txt 查询两个o的字符 1grep -n ‘o\\&#123;2\\&#125;’ test.txt 查询以w开头d结尾，中间包含两个及两个以上o 1grep -n ‘wo\\&#123;2,\\&#125;d’ test.txt 三、sed命令常见用法sed [选项] ‘操作’ 参数 sed [选项] -f scripfile 参数 选项的基本命令如下： 123456789-e script ：指定sed编辑命令-f scriptfile ：指定的文件中是sed编辑命令-h ：显示帮助-n ：表示仅显示处理后的结果-i ：直接编辑文本文件 操作的基本命令如下： 12345678910111213a ：增加，在当前行下面增加一行指定内容c ：替换，将选定行替换为指定内容d ：删除，删除选定的行i ：插入，在选定行上面插入一行指定内容p ：打印s ：替换，替换指定字符y ：字符转换 1、输出符合条件的文本输出第三行 1sed -n ‘3p’ test.txt 输出第3-5行 1sed -n ‘3,5p’ test.txt 输出所有奇数行 1sed -n ‘p:n’ test.txt 输出所有偶数行 1sed -n ‘n:p’ test.txt 输出第1-5行之间的奇数行 1sed -n ‘1,5&#123;p:n&#125;’ test.txt 输出第10行之后的偶数行 1sed -n ’10,$&#123;n:p&#125;’ test.txt 输出包含the的行 1sed -n ‘/the/p’ test.txt 输出从第4行的第一个包含the的行 1sed -n ‘4,/the/p’ test.txt 输出以PI开头的行 1sed -n ‘/^PI/p’ test.txt 输出以数字结尾的行 1sed -n ‘/[0-9]$/p’ test.txt 输出包含wood的行 1sed -n ‘/\\&lt;wood&gt;/p’ test.txt 2、删除符合条件的文本删除第3行 1sed ‘3d’ test.txt 删除第3-5行 1sed ‘3,5d’ test.txt 删除以小写字母开头的行 1sed ‘/^[a-z]/d’ test.txt 3、替换符合条件的文本将每行中的第一个the替换为THE 1sed ‘s/the/THE/’ test.txt 将每行中的第2个l替换为L 1sed ‘s/l/L/2’ test.txt 在包含the的每行行首插入#号 1sed ‘/the/s//#/’ test.txt 在每行行尾插入字符串EOF 1sed ‘s/$/EOF/’ test.txt 将第3-5行中的所有the替换为THE 1sed ‘3,5s/the/THE/g’ test.txt 将包含the的所有行中的o替换为O 1sed ‘/the/s/o/O/g’ test.txt 将文件中所有的o删除 1sed ‘s/o//g’ test.txt 4、迁移符合条件的文本123456789H ：复制到剪切板g、G ：将剪贴板中的数据覆盖/追加到指定行w ：保存为文件r ：读取指定文件a ：追加指定内容 将包含the的行迁移至文件末尾 1sed ‘/the/&#123;H;d&#125;;$G’ test.txt 将第1-5行内容转移至第17行后 1sed ‘1,5&#123;H’d&#125;;17G’ test.txt 将包含the的行号另存为文件out.file 1sed ‘/the/w out.file’ test.txt 将文件&#x2F;etc&#x2F;hostname的内容添加到包含the的每行以后 1sed ‘/the/r /etc/hostname’ test.txt 在第3行后插入一个新行，内容为NEW 1sed ‘/the/3aNEW’ test.txt 在包含the的每行后插入一个新行，内容为NEW 1sed ‘/the/aNEW’ test.txt 在第3行后插入多行内容，中间的\\n表示换行 1sed ‘3aNEW1\\nNEW3’ test.txt 四、awk工具awk 选项 ‘模式或条件{编辑命令}’ 文件1 文件2…. &#x2F;&#x2F;过滤并输出文件中符合条件的内容 awk -f 脚本文件 文件1 文件2… &#x2F;&#x2F;从脚本中调用的编辑指令，过滤并输出内容 awk包含几个特殊的内建变量（可直接用）如下所示： 12345678910111213FS：指定每行文本的字段分隔符，默认为空格或制表位；NF：当前处理的行的字段个数；NR：当前处理的行的行号（序数）；$0：当前处理的行的整行内容；$n：当前处理的行的第n个字段（第n列）；FILENAME：被处理的文件名；RS：数据记录分隔，默认为\\n，即每行为一条记录。 1、按行输出文本输出所有内容，等同于cat test.txt 123awk ‘&#123;print&#125;’ test.txtawk ‘&#123;print $0&#125;’ test.txt 输出第1~3行内容 1awk ‘NR==1,NR==3&#123;print&#125;’ test.txt 输出第1~3行内容 1awk ‘（NR&gt;=1）&amp;&amp;(NR&lt;=3)&#123;print&#125;’ test.txt 输出第1行，第3行的内容 1awk ‘NR==1||NR==3&#123;print&#125;’ test.txt 输出所有奇数行的内容 1awk ‘(NR%2)==1&#123;print&#125;’ test.txt 输出所有偶数行的内容 1awk ‘(NR%2)==0&#123;print&#125;’ test.txt 输出以root开头的行 1awk ‘/^root/&#123;print&#125;’ /etc/passwd 输出以nologin结尾的行 1awk ‘/nologin$/&#123;print&#125;’ /etc/passwd 统计以&#x2F;bin&#x2F;bash结尾的行数，等同于grep –c “&#x2F;bin&#x2F;bash$” &#x2F;etc&#x2F;passwd 1awk ‘BEGIN &#123;x=0&#125;;/\\/bin\\/bash$/&#123;x++&#125;;END &#123;print x&#125;’ /etc/passwd 统计以空行分隔的文本段落数 1awk ‘BEGIN &#123;RS=””&#125; ;END&#123;print NR&#125;’ test.txt 2、按字段输出文本输出每行中（以空格或制表符分隔）的第三个字段 1awk ‘&#123;print $3&#125;’ test.txt 输出每行中第1，3个字段 1awk ‘&#123;print $1,$3&#125;’ test.txt 输出密码为空的用户的shadow记录 1awk -F ”:” ‘$2==””&#123;print&#125;’ /etc/shadow 输出密码为空的用户的shadow记录 1awk ‘BEGIN &#123;FS=”:”&#125; ; $2==””&#123;print&#125;’ /etc/shadow 输出以冒号分隔且第7个字段中包含&#x2F;bash的行的第一个字段 1awk -F ”:” ‘$7~”/bash”&#123;print $1&#125;’ /etc/shadow 输出包含8个字段且第一个字段中包含nfs的行的第1，2个字段 1awk ‘($1~”nfs”)&amp;&amp;(NF==8)&#123;print $1,$2&#125;’ /etc/services 输出第七个字段既不为&#x2F;bin&#x2F;bash也不为&#x2F;sbin&#x2F;nologin的所有行 1awk -F “:” ‘($7 != “/bin/bash”)&amp;&amp;($7 != “/sbin/nologin”)&#123;print&#125;’ /etc/passwd 打印文本文件的总行数 1awk &#x27;END&#123;print NR&#125;&#x27; filename 打印文本第二行第一列 1sed -n &quot;2, 1p&quot; filename | awk &#x27;print $1&#x27; 输出文件的倒数第二列以”，”隔开 1awk -F &#x27;,&#x27; &#x27;END &#123;print $2&#125;&#x27; BIRoomTime20160229.txt shell里面的赋值方法有两种，格式为 arg&#x3D;(命令) arg&#x3D;$(命令)因此，如果想要把某一文件的总行数赋值给变量nlines，可以表达为： nlines&#x3D;(awk &#39;END&#123;print NR&#125;&#39; filename)或者 nlines&#x3D;$(awk ‘END{print NR}’ filename) 3、通过管道、双引号调用Shell命令调用wc –l命令统计使用bash的用户的个数，等同于grep –c “bash$” &#x2F;etc&#x2F;passwd 1awk -F: ‘/bash$/&#123;print | “wc -l”&#125;’ /etc/passwd 调用w命令，并用来统计在线用户数 1awk ‘BEGIN &#123;while (“w” | getline) n++ ; &#123;print n-2&#125;&#125;’ 调用hostname，并输出当前主机名 1awk ‘BEGIN &#123;“hostname” | getline ; print $0&#125;’ 五、sort工具sort是一种以行为单位对文件内容进行排序的工具，也可以根据不同的数据类型来排序 用法：sort命令的语法为“sort [选项] 参数”，其中常用的选项包括一下几种 1234567891011121314151617-f：忽略大小写；-b：忽略每行前面的空格；-M：按照月份进行排序；-n：按照数字进行排序；-r：反向排序；-u：等同于uniq，表示相同的数据仅显示一份；-t：指定分隔符，默认使用[TAB]键分隔；-o&lt;输出文件&gt;：将排序后的结果转存至指定文件；-k：指定排序区域 六、uniq工具uniq工具在Linux系统中通常与sort命令结合使用，用于报告或者忽略文件中的重复行 12345-c：进行计数；-d：仅显示重复行；-u：仅显示出现一次的行 七、tr工具tr命令常用来对来自标准输入的字符进行替换、压缩和删除 tr具体的命令语法格式为：tr [选项] [参数] 1234567-c：取代所有不属于第一字符集的字符；-d：删除所有属于第一字符集的字符；-s：把连续重复的字符以单独一个字符表示；-t：先删除第一字符集较第二字符集多出的字符，做替换 八、Linux命令锁定账户第一种： 1、passwd -l tang 锁定tang用户 2、passwd -u tang 解锁tang用户 第二种：（权限优于第一种） 1、usermod -L tang 锁定tang用户 2、usermod -U tang 解锁tang用户 usermod命令锁定和解锁用户的的时候没有交互信息（就是不会有提示） 第三种：（锁定文件） 1、用chattr +i 命令锁定&#x2F;etc&#x2F;passwd 和 &#x2F;etc&#x2F;shadow文件 2、用chattr -i 命令解锁&#x2F;etc&#x2F;passwd 和 &#x2F;etc&#x2F;shadow文件 第四种：修改shell环境让账户无法登陆 1、vi &#x2F;etc&#x2F;passwd进入编译 2、把想锁定的账户后面改成&#x2F;sbin&#x2F;nologin find：1、用 find 查找三天之前以log结尾的文件并删除？ find &#x2F;log -mtime +3 -type f -name “*.log” 添加用户是哪个命令？ useradd 改一个文件的属组和属主？ chown 如何让命令在后台执行？ 命令&amp; 这种会绑定终端，终端关闭进程就结束。 nohub 命令 &amp; 这种关了终端可以运行 怎么判断某个变量是否为空？ ​ ！ $A 你们系统用的哪个发行版？ centos ,ubuntu 如何检查linux上开放的端口netstat -anptu &#x2F;&#x2F;后面的t是tcp、u是udp netstat -an &#x2F;&#x2F;查看服务器开启哪些端口？ 常用的linux命令有哪些ls netstart cd chown chmod grep uptime ifconfig route…… 查看硬盘使用情况的命令fdsik -l #查看分区情况 df -h #查看挂载和使用情况 查看cpu的命令cat &#x2F;proc&#x2F;cpuinfo | grep ‘model name’ 或者lscpu 查看内存的命令free -m 如何查看当前Linux系统的状态，如CPU使用、内存使用、负载情况等？top 查看cpucat &#x2F;proc&#x2F;cpuinfo 查看内核版本uname -r 查看内存信息cat &#x2F;proc&#x2F;meminfo 查看核心防护状态getenforce 说明硬链接与软链接的差别，分别有什么好处？ln，创建软连接 –软连接–&gt; 原始文档–&gt; i节点–&gt; 文档数据 –ln-s 原始文件或目录软连接文件 若原始文件或目录被删除，连接文件将失效 软连接可存放在不同分区&#x2F;文件系统 •ln，创建硬连接 –硬连接–&gt; i节点–&gt; 文档数据 –ln原始文件硬连接文件 若原始文件被删除，连接文件仍可用 硬连接与原始文件必须在同一分区&#x2F;文件系统 Linux现连接一个新的存储（如&#x2F;dev&#x2F;sdf，容量2T）一个应用程序需要在&#x2F;data目录使用此存储的500G的存储空间（做成LVM），需要哪些步骤，请描述。12345678910111213fdisk /dev/sdf 分区n，p，1，1 +500G w 分区步骤pvcreate /dev/sdf1 创建物理卷vgcreate vg01 /dev/sdf1 创建卷组lvcreate -n lv01 -l 100%free vg01 #使用所有空间创建逻辑卷mkfs.ext4 /dev/vg01/lv01 格式化这个卷mount /dev/vg01/lv01 /data 挂载这个卷","categories":[],"tags":[]},{"title":"","slug":"总结/11、Shell","date":"2024-12-14T22:02:47.098Z","updated":"2021-02-25T05:24:14.000Z","comments":true,"path":"2024/12/15/总结/11、Shell/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/11%E3%80%81Shell/","excerpt":"","text":"[TOC] ShellShell 脚本是什么？一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工作任务。 什么是默认登录 Shell ？ 在 Linux 操作系统，&quot;/bin/bash&quot; 是默认登录 Shell，是在创建用户时分配的。 使用 chsh 命令可以改变默认的 Shell 。示例如下所示： 12## chsh &lt;用户名&gt; -s &lt;新shell&gt;## chsh ThinkWon -s /bin/sh 在 Shell 脚本中，如何写入注释？ 注释可以用来描述一个脚本可以做什么和它是如何工作的。每一行注释以 # 开头。例子如下： 123#!/bin/bash## This is a commandecho “I am logged in as $USER” 语法级可以在 Shell 脚本中使用哪些类型的变量？在 Shell 脚本，我们可以使用两种类型的变量： 系统定义变量 系统变量是由系统系统自己创建的。这些变量通常由大写字母组成，可以通过 set 命令查看。 用户定义变量 用户变量由系统用户来生成和定义，变量的值可以通过命令 &quot;echo $&lt;变量名&gt;&quot; 查看。 Shell脚本中 $? 标记的用途是什么？ 在写一个 Shell 脚本时，如果你想要检查前一命令是否执行成功，在 if 条件中使用 $? 可以来检查前一命令的结束状态。 如果结束状态是 0 ，说明前一个命令执行成功。例如： 1234root@localhost:~## ls /usr/bin/shar/usr/bin/sharroot@localhost:~## echo $?0 如果结束状态不是0，说明命令执行失败。例如： 1234root@localhost:~## ls /usr/bin/sharels: cannot access /usr/bin/share: No such file or directoryroot@localhost:~## echo $?2 Bourne Shell(bash) 中有哪些特殊的变量？ 下面的表列出了 Bourne Shell 为命令行设置的特殊变量。 12345678内建变量 解释$0 命令行中的脚本名字$1 第一个命令行参数$2 第二个命令行参数….. …….$9 第九个命令行参数$## 命令行参数的数量$* 所有命令行参数，以空格隔开 如何取消变量或取消变量赋值？ unset 命令用于取消变量或取消变量赋值。语法如下所示： 1## unset &lt;变量名&gt; Shell 脚本中 if 语法如何嵌套?1234567891011121314151617if [ 条件 ]then命令1命令2…..elseif [ 条件 ]then命令1命令2….else命令1命令2…..fifi 在 Shell 脚本中如何比较两个数字？ 在 if-then 中使用测试命令（ -gt 等）来比较两个数字。例如： 123456789#!/bin/bashx=10y=20if [ $x -gt $y ]thenecho “x is greater than y”elseecho “y is greater than x”fi Shell 脚本中 case 语句的语法?基础语法如下： 1234567891011121314case 变量 in值1)命令1命令2…..最后命令!!值2)命令1命令2……最后命令;;esac Shell 脚本中 for 循环语法？基础语法如下： 1234567for 变量 in 循环列表do命令1命令2….最后命令done Shell 脚本中 while 循环语法？如同 for 循环，while 循环只要条件成立就重复它的命令块。不同于 for循环，while 循环会不断迭代，直到它的条件不为真。 基础语法： 1234while [ 条件 ]do命令…done do-while 语句的基本格式？ do-while 语句类似于 while 语句，但检查条件语句之前先执行命令（LCTT 译注：意即至少执行一次。）。下面是用 do-while 语句的语法： 1234do&#123;命令&#125; while (条件) Shell 脚本中 break 命令的作用？ break 命令一个简单的用途是退出执行中的循环。我们可以在 while 和 until 循环中使用 break 命令跳出循环。 Shell 脚本中 continue 命令的作用？ continue 命令不同于 break 命令，它只跳出当前循环的迭代，而不是整个循环。continue 命令很多时候是很有用的，例如错误发生，但我们依然希望继续执行大循环的时候。 如何使脚本可执行?使用 chmod 命令来使脚本可执行。例子如下：chmod a+x myscript.sh 。 #!&#x2F;bin&#x2F;bash 的作用？ #!/bin/bash 是 Shell 脚本的第一行，称为释伴（shebang）行。 这里 # 符号叫做 hash ，而 ! 叫做 bang。 它的意思是命令通过 /bin/bash 来执行。 如何调试 Shell脚本？ 使用 -x&#39; 数（sh -x myscript.sh）可以调试 Shell脚本。 另一个种方法是使用 -nv 参数(sh -nv myscript.sh)。 如何将标准输出和错误输出同时重定向到同一位置? 方法一：2&gt;&amp;1 (如## ls /usr/share/doc &gt; out.txt 2&gt;&amp;1 ) 。 方法二：&amp;&gt; (如## ls /usr/share/doc &amp;&gt; out.txt ) 。 在 Shell 脚本中，如何测试文件？ test 命令可以用来测试文件。基础用法如下表格： 12345678Test 用法-d 文件名 如果文件存在并且是目录，返回true-e 文件名 如果文件存在，返回true-f 文件名 如果文件存在并且是普通文件，返回true-r 文件名 如果文件存在并可读，返回true-s 文件名 如果文件存在并且不为空，返回true-w 文件名 如果文件存在并可写，返回true-x 文件名 如果文件存在并可执行，返回true 在 Shell 脚本如何定义函数呢？函数是拥有名字的代码块。当我们定义代码块，我们就可以在我们的脚本调用函数名字，该块就会被执行。示例如下所示： 1234567$ diskusage () &#123; df -h ; &#125;译注：下面是我给的shell函数语法，原文没有[ function ] 函数名 [()]&#123;命令;[return int;]&#125; 如何让 Shell 就脚本得到来自终端的输入? read 命令可以读取来自终端（使用键盘）的数据。read 命令得到用户的输入并置于你给出的变量中。例子如下： 123456789## vi /tmp/test.sh#!/bin/bashecho ‘Please enter your name’read nameecho “My Name is $name”## ./test.shPlease enter your nameThinkWonMy Name is ThinkWon 如何执行算术运算？ 有两种方法来执行算术运算： 1、使用 expr 命令：## expr 5 + 2 。 2、用一个美元符号和方括号（$[ 表达式 ]）：test=$[16 + 4] ; test=$[16 + 4] 。 编程题判断一文件是不是字符设备文件，如果是将其拷贝到 /dev 目录下？12345#!/bin/bashread -p &quot;Input file name: &quot; FILENAMEif [ -c &quot;$FILENAME&quot; ];then cp $FILENAME /devfi 添加一个新组为 class1 ，然后添加属于这个组的 30 个用户，用户名的形式为 stdxx ，其中 xx 从 01 到 30 ？12345678910#!/bin/bashgroupadd class1for((i=1;i&lt;31;i++))do if [ $i -le 10 ];then useradd -g class1 std0$i else useradd -g class1 std$i fidone 编写 Shell 程序，实现自动删除 50 个账号的功能，账号名为stud1 至 stud50 ？ 12345#!/bin/bashfor((i=1;i&lt;51;i++))do userdel -r stud$idone 写一个 sed 命令，修改 /tmp/input.txt 文件的内容？要求： 删除所有空行。 一行中，如果包含 “11111”，则在 “11111” 前面插入 “AAA”，在 “11111” 后面插入 “BBB” 。比如：将内容为 0000111112222 的一行改为 0000AAA11111BBB2222 。 1234567891011121314151617181920212223242526272829303132[root@~]## cat -n /tmp/input.txt 1 000011111222 2 3 000011111222222 4 11111000000222 5 6 7 111111111111122222222222 8 2211111111 9 112222222 10 1122 11## 删除所有空行命令[root@~]## sed &#x27;/^$/d&#x27; /tmp/input.txt0000111112220000111112222221111100000022211111111111112222222222222111111111122222221122## 插入指定的字符[root@~]## sed &#x27;s#\\(11111\\)#AAA\\1BBB#g&#x27; /tmp/input.txt0000AAA11111BBB2220000AAA11111BBB222222AAA11111BBB000000222AAA11111BBBAAA11111BBB1112222222222222AAA11111BBB1111122222221122 一、工作中你都写过什么脚本?1、监控脚本(监控系统、监控服务、监控硬件信息、监控性能、安全监控等) 2、系统初始化脚本(创建目录,创建账户,安装软件包,设置权限,修改内核参数等) 一键部署(源码安装脚本) 3、备份脚本(自动备份数据库,备份网站数据,备份日志,备份配置文件等) 4、日志分析脚本(分析日志数据,汇总并统计相关信息,如 PV、UV 等) 二、如何获取一个文件每一行的第三个元素?# awk ‘{print $3}’ 文件名 备注:awk 支持按列输出,通过内置变量$1,$2,$3…可以单独显示任意列,默认列是以 空格或 Tab 缩进为分隔符,也可以使用-F 选项指定其他分隔符。 三、 shell 函数能解决什么实际问题?定义函数的格式: function 函数名{ 代码块 } 函数名(){ 代码块 } 使用函数可以避免代码重复 使用函数可以将大的工程分割为若干小的功能模块,代码的可读性更强 四、使用 awk 统计 httpd 访问日志中每个客户端 IP 的出现次数?# awk ‘{ip[$1]++}END{for(i in ip){print ip[i],i}}’ &#x2F;var&#x2F;log&#x2F;httpd&#x2F;access_log 备注:定义数组,数组名称为 ip,数字的下标为日志文件的第 1 列(也就是客户端的 I P 地址) ,++的目的在于对客户端进行统计计数,客户端 IP 出现一次计数器就加 1。END 中的指令在读取完文件后执行,通过循环将所有统计信息输出。 4、哪些方式可以将标准输出和错误输出重定向到文件? 答案: # 命令 &amp;&gt; 文件名 # 命令 &gt; 文件名 1 2&gt; 文件名 2 # 命令 &gt; 文件名 2&gt;&amp;1 # 命令 &amp;&gt;&gt; 文件名 # 命令 &gt;&gt; 文件名 1 2&gt;&gt; 文件名 2 # 命令 &gt;&gt; 文件名 2&gt;&amp;1 五、正则表达式符号: *、+、?、[]、[^]、{n}分别代表什么含义?*表示匹配前面的字符出现了任意次(包括 0 次) +表示匹配前面的字符出现了至少 1 次(1 次或多次) ?表示匹配前面的字符出现了 0 次或 1 次 []表示集合,匹配集合中的任意单个字符 [^]表示对集合取反 {n}表示精确匹配前面的字符出现了 n 次 六、shell 中对变量字串进行截取的方式有哪些?# echo ${变量名:开始位置:长度} #注意,起始位置从 0 开始 # expr substr $变量名 #注意,起始位置从 1 开始 # echo $变量名 | 开始位置 长度 cut -b 开始位置-结束位置 #注意,起始位置从 1 开始 \\7. 使用 sed 命令如何将文件中所有的大写字母 Q 转换为小写字母 q? 答案: # sed -i ‘s&#x2F;Q&#x2F;q&#x2F;g’ 文件名 七、编写脚本,用户输入密码,脚本判断密码是否正确,输入正确则提示正确,连续输入错误密码 3 次,则报警?1234567891011121314151617181920212223vim test.sh#!/bin/bashinit=123456for I in &#123;1..3&#125;doread -p &quot;请输入密码:&quot; passif [ $pass == $init ];thenecho &quot;密码正确&quot;breakfidoneecho &quot;警告:密码错误&quot; 八、编写脚本,自动生成一个 8 位随机密码?答案: 123456789101112131415161718192021vim test.sh#!/bin/bashStr=&quot;abcdefghijklnmopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;pass=&quot;&quot;for i in &#123;1..8&#125;donum=$[RANDOM%$&#123;#Str&#125;]tmp=$&#123;Str:num:1&#125;pass+=$tmpdoneecho $pass 九、递归函数，遍历目录递归遍历目录 通过定义递归函数 files来实现 Shell也可以实现递归函数,就是可以调用自己本身的函数。在Linux系统上编写Shel脚本的时候,经常需要递归遍历系统的目录,列出目录下的文件和目录,逐层递归列出,并对这些层级关系进行展示。具体的实现过程如下所示。 12345678910111213141516171819202122232425function list files()&#123; for f in `ls $1`; do if [-d&quot;$1/Sf&quot;]: then echo&quot;$2$f&quot; list_files &quot;$1/$2&quot; &quot; $2&quot; else echo &quot;$2$&quot; fi done&#125;list_files &quot;/var/log&quot; &quot; 十、shell实现nginx日志自动切割脚本思路【按天分割日志】 a、获取昨天的日期（date -d yesterday +%Y%m%d）,用来作为分割后日志的名称 b、将源日志文件移动到新的nohuplogs文件夹里，并按时间重命名 c、在源日志文件夹（logs）里新建默认日志文件(access.log) d、给nginx一个信号量，重新打开日志 f、设置一个定时任务，定时执行日志切割的脚本 操作步骤 a、新建日志分割的文件夹nohuplogs（mkdir &#x2F;jboss&#x2F;nginx&#x2F;nohuplogs） b、编写脚本，暂且命名为：splitlogs.sh吧，脚本内容如下： 12345678910111213LOGPATH=/jboss/nginx/logs/access.logBASEPATH=/jboss/nginx/nohuplogsLOGBAK=$BASEPATH/$(date -d yesterday +%Y%m%d).log#echo $LOGBAKmv $LOGPATH $LOGBAKtouch $LOGPATHkill -USR1 `cat /jboss/nginx/logs/nginx.pid &#96; c、配置定时任务 crontab -e，如下图，增加日志分割的脚本，每天晚上23点59分切割 59 23 * * * sh &#x2F;jboss&#x2F;nginx&#x2F;splitlogs.sh 十一、冒泡算法请结合使用冒泡排序方法把 123.txt 文件中的数字按照降序排序输出在一行当中，并要求没有重复数字。（20分） cat 123.txt 1 4 7 9 2 5 8 3 3 6 9 7 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bashmyarray() &#123;array=(`echo $@`)\\#echo $&#123;array[*]&#125;a=$&#123;#array[*]&#125;for ((i=1; i&lt;$a; i++))do for ((j=0; j&lt;$a-i; j++)) do if [ $&#123;array[$j]&#125; -lt $&#123;array[$[$j + 1]]&#125; ];then temp=$&#123;array[$j]&#125; array[$j]=$&#123;array[$[$j+1]]&#125; array[$[$j+1]]=$temp fi donedoneecho $&#123;array[*]&#125;&#125; ###主体代码 12345678910111213141516171819list=`for i in $(cat 123.txt); do echo $i; done | sort -n | uniq`a=0for value in $listdo arr[$a]=$value let a++doneresult=`myarray $&#123;arr[*]&#125;` echo “排序后数组为：$result” 监控网站状态是否正常，异常发邮件 题目要求： 写一个shell脚本，通过curl -I 返回状态码来判定所访问的网站是否正常，比如当代码状态200，才算正常 写一个发邮件的脚本 习题分析： 1、关键问题，截取出代码状态 2、在写出该shell脚本时，应该先在命令下面使用curl -I http://www.51xit.top/命令测试，然后通过awk截取到状态码 3、写发邮件的脚本，用的是sendEmail。生产环境有配套的模板 4、判断和发邮件关联 curl -I http://www.51xit.top/ 我们抓包会有交互信息 200 ###创建触发器及邮件报警测试## 【安装邮件组件】 [root@tang ~]# wget http://caspian.dotconf.net/menu/Software/SendEmail/sendEmail-v1.56.tar.gz [root@tang ~]# tar -zxvf sendEmail-v1.56.tar.gz [root@tang ~]# cp sendEmail-v1.56&#x2F;sendEmail &#x2F;usr&#x2F;local&#x2F;bin&#x2F; [root@tang ~]# chmod 755 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;sendEmail [root@tang ~]# vi &#x2F;opt&#x2F;sendEmail.sh #!&#x2F;bin&#x2F;bash # # Filename: SendEmail.sh # Revision: 1.0 # Date: 2019&#x2F;05&#x2F;29 # Author: Qicheng # Email: # Website: http://51xit.top/ # Description: tang邮件告警脚本 # Notes: 使用sendEmail # # 脚本的日志文件 LOGFILE&#x3D;”&#x2F;tmp&#x2F;Email.log” :&gt;”$LOGFILE” exec 1&gt;”$LOGFILE” exec 2&gt;&amp;1 SMTP_server&#x3D;’smtp.qq.com’ # SMTP服务器，变量值需要自行修改 username&#x3D;‘&#x31;&#x35;&#x38;&#x31;&#x32;&#x37;&#x33;&#x31;&#53;&#52;&#x40;&#x71;&#x71;&#46;&#x63;&#111;&#x6d;‘ # 用户名，变量值需要自行修改 password&#x3D;’cgdxhxqddtrafijh’ # 密码(QQ邮箱用的是授权码)，变量值需要自行修改 from_email_address&#x3D;‘&#x31;&#x35;&#56;&#x31;&#50;&#55;&#51;&#49;&#x35;&#52;&#x40;&#113;&#x71;&#x2e;&#99;&#x6f;&#x6d;‘ #### 发件人Email地址，变量值需要自行修改 to_email_address&#x3D;”$1” # 收件人Email地址，tang传入的第一个参数 message_subject_utf8&#x3D;”$2” # 邮件标题，tang传入的第二个参数 message_body_utf8&#x3D;”$3” # 邮件内容，tang传入的第三个参数 # 转换邮件标题为GB2312，解决邮件标题含有中文，收到邮件显示乱码的问题。 message_subject_gb2312&#x3D;&#96;iconv -t GB2312 -f UTF-8 &lt;&lt; EOF $message_subject_utf8 EOF&#96; [ $? -eq 0 ] &amp;&amp; message_subject&#x3D;”$message_subject_gb2312” || message_subject&#x3D;”$message_subject_utf8” # 转换邮件内容为GB2312，解决收到邮件内容乱码 message_body_gb2312&#x3D;&#96;iconv -t GB2312 -f UTF-8 &lt;&lt; EOF $message_body_utf8 EOF&#96; [ $? -eq 0 ] &amp;&amp; message_body&#x3D;”$message_body_gb2312” || message_body&#x3D;”$message_body_utf8” # 发送邮件 sendEmail&#x3D;’&#x2F;usr&#x2F;local&#x2F;bin&#x2F;sendEmail’ set -x $sendEmail -s “$SMTP_server” -xu “$username” -xp “$password” -f “$from_email_address” -t “$to_email_address” -u “$message_subject” -m “$message_body” -o message-content-type&#x3D;text -o message-charset&#x3D;gb2312 [root@tang ~]# chmod +x &#x2F;opt&#x2F;sendEmail.sh [root@tang ~]# &#x2F;opt&#x2F;sendEmail.sh &#x31;&#x35;&#x38;&#49;&#x32;&#55;&#51;&#x31;&#53;&#52;&#x40;&#x71;&#113;&#46;&#99;&#111;&#x6d; 测试 测试 监控磁盘情况，异常发邮件 12345678910111213141516171819#!/bin/bashdisk_sda1=df -h |sed -n &#x27;3p&#x27;|awk &#x27;&#123;print $4&#125;&#x27;|cut -f 1 -d &#x27;%&#x27;if ((disk_sda1 &gt; 80));then echo “this is error” echo date “192.168.56.128 this is over 70%” |mail -s “disk over 70%” 12345300@qq.com,5645645@qq.comelse echo “this is ok”fi 计划任务： 1[root@localhost tmp]# crontab -e 添加 12345673个小时检查一次\\* */3 * * * /var/tmp/check_disk.sh或者十分钟检查一次*/10 * * * * /var/tmp/check_disk.sh 十二、特殊符号$0：当前脚本的文件名 $n：第n个位置参数 $*：传递给脚本或函数的所有参数，$*会将这些参数视为一个整体 $@：传递给脚本或函数的所有参数，$@会将所有参数当作同一字符串中的多个独立的单词 $#：脚本运行时携带的参数个数 $*：表示所有位置参数的内容 $?：最近一个命令的退出状态码 $$：当前shell的进程ID（PID） $!：最近一个后台命令的PID !!：执行上一条命令 IFS：内部字段分隔符，IFS环境变量定义了shell用作字段分隔符的一系列字符。默认情况下，shell会将下列字符当做字段分隔符： 空格 制表符 换行符 &amp;&gt;：将STDERR和STDOUT的输出重定向到同一个输出文件 &amp;-：要关闭文件描述符，可以将它重定向到特殊符号&amp;- .：点操作符，点操作符是source命令的别名，它会在shell上下文中执行点操作符指定的脚本，而不是创建一个新的shell。 [ 操作符 文件或目录 ] 常用的测试操作符 -d：测试是否为目录（Directory） -e：测试目录或文件是否存在（Exist） -f：测试是否为文件（File） -r：测试当前用户是否有权限读取（Read） -w：测试当前用户是否有权限写入（Write） -x：测试当前用户是否有权限执行（eXcute）","categories":[],"tags":[]},{"title":"","slug":"总结/10、Apache","date":"2024-12-14T22:02:47.094Z","updated":"2021-01-23T07:17:04.000Z","comments":true,"path":"2024/12/15/总结/10、Apache/","permalink":"http://bingfly.top/2024/12/15/%E6%80%BB%E7%BB%93/10%E3%80%81Apache/","excerpt":"","text":"httpd（Apache）1、请说一下你对httpd服务的了解？答：Apache是一个模块化服务，支持的模块比较多，采用servlet处理模型，同步阻塞模型，工作模式多变，对于高并发的场景处理速度会比较慢，运行稳定。支持异步读写，可以通过正则表达式做动静分离。 2、httpd服务的三种工作模式你了解多少？答：httpd有三种工作模式。 prefork：预派生子进程prefork模式可以算是很古老但是很稳定的模式。httpd服务在刚启动时，就会fork出一些子进程（默认为5个），一个子进程对应一个线程，然后等待request进来，并且总是试图保持一些空闲的子进程，之所以这样做，是为了减少频繁创建和销毁进程的开销。在同一个时间点内，一个线程只能处理一个进程。 worker工作模式worker模式和prefork模式比起来，是使用了多进程+多线程的模式，它也是预先fork了几个子进程，每个子进程能够生成一些服务线程和一个监听线程，该监听线程及接入请求并传递给服务线程处理和应答。worker工作模式占用的内存较少，在高并发下表现还算优异。不过必须要考虑线程安全的问题，因为多个子进程是共享父进程的内存地址的。如果使用keep-alive的长连接方式，也许中间几乎没有请求，这是就会发生阻塞，线程被挂起，需要一直等待到超时才会被释放。如果过多的线程，就这样被占据，也会导致在高并发场景下的无服务线程可用。（该问题同样会发生在prefork模式）。 event工作模式这是Apache最新的工作模式，它和worker模式很像，最大的区别在于，它解决了keep-alive场景下，线程被长期被占用的的资源浪费问题。 event工作模式中，会有一个专门的线程来管理这些keep-alive类型的线程，当有真实请求过来的时候偶，将请求传递给服务线程，执行完毕后，又允许它释放。这样，一个线程就能处理多个请求了，实现异步非阻塞。 event工作模式在遇到某些不兼容的模块时，它会失效，并退回到worker模式，一个工作线程处理一个请求。官方自带的模块，全部都是支持event工作模式的。 3、可以从哪几个方面着手优化httpd？合理配置其进程及线程数；开启httpd的deflate压缩功能；开启expires缓存功能；禁止httpd进行目录遍历；隐藏httpd的版本信息；开启日志切割功能；配置防盗链； 4、nginx和Apache的区别？两者最核心的区别在于apache是同步多进程模型，一个request对应一个进程，而nginx是异步的，多个连接（万级别）可以对应一个进程。 一般来说，需要性能的web服务，用nginx，如果不需要性能只求稳定，更考虑Apache，后者的各种模块实现的比前者好很多，epoll网络IO模型是nginx处理性能高的根本，但并不是所有情况下epoll大获全胜的，如果本身提供静态服务的只有几个文件，apache的select模型或许比epoll更高性能。当然，这只是一个假设，真正还需要实测了再说。 更通用的方案是，前端nginx抗并发，后端apache集群，配合起来会更好。","categories":[],"tags":[]},{"title":"Python基础题","slug":"Python-100-Days-master/Python面试宝典-基础篇-2020","date":"2024-12-13T16:00:00.000Z","updated":"2024-12-14T22:10:53.363Z","comments":true,"path":"2024/12/14/Python-100-Days-master/Python面试宝典-基础篇-2020/","permalink":"http://bingfly.top/2024/12/14/Python-100-Days-master/Python%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8-%E5%9F%BA%E7%A1%80%E7%AF%87-2020/","excerpt":"","text":"Python面试宝典 - 基础篇 - 2020题目001: 在Python中如何实现单例模式。 点评：单例模式是指让一个类只能创建出唯一的实例，这个题目在面试中出现的频率极高，因为它考察的不仅仅是单例模式，更是对Python语言到底掌握到何种程度，建议大家用装饰器和元类这两种方式来实现单例模式，因为这两种方式的通用性最强，而且也可以顺便展示自己对装饰器和元类中两个关键知识点的理解。 方法一：使用装饰器实现单例模式。 12345678910111213141516171819from functools import wrapsdef singleton(cls): &quot;&quot;&quot;单例类装饰器&quot;&quot;&quot; instances = &#123;&#125; @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper@singletonclass President: pass 扩展：装饰器是Python中非常有特色的语法，用一个函数去装饰另一个函数或类，为其添加额外的能力。通常通过装饰来实现的功能都属横切关注功能，也就是跟正常的业务逻辑没有必然联系，可以动态添加或移除的功能。装饰器可以为代码提供缓存、代理、上下文环境等服务，它是对设计模式中代理模式的践行。在写装饰器的时候，带装饰功能的函数（上面代码中的wrapper函数）通常都会用functools模块中的wraps再加以装饰，这个装饰器最重要的作用是给被装饰的类或函数动态添加一个__wrapped__属性，这个属性会将被装饰之前的类或函数保留下来，这样在我们不需要装饰功能的时候，可以通过它来取消装饰器，例如可以使用President = President.__wrapped__来取消对President类做的单例处理。需要提醒大家的是：上面的单例并不是线程安全的，如果要做到线程安全，需要对创建对象的代码进行加锁的处理。在Python中可以使用threading模块的RLock对象来提供锁，可以使用锁对象的acquire和release方法来实现加锁和解锁的操作。当然，更为简便的做法是使用锁对象的with上下文语法来进行隐式的加锁和解锁操作。 方法二：使用元类实现单例模式。 123456789101112131415class SingletonMeta(type): &quot;&quot;&quot;自定义单例元类&quot;&quot;&quot; def __init__(cls, *args, **kwargs): cls.__instance = None super().__init__(*args, **kwargs) def __call__(cls, *args, **kwargs): if cls.__instance is None: cls.__instance = super().__call__(*args, **kwargs) return cls.__instanceclass President(metaclass=SingletonMeta): pass 扩展：Python是面向对象的编程语言，在面向对象的世界中，一切皆为对象。对象是通过类来创建的，而类本身也是对象，类这样的对象是通过元类来创建的。我们在定义类时，如果没有给一个类指定父类，那么默认的父类是object，如果没有给一个类指定元类，那么默认的元类是type。通过自定义的元类，我们可以改变一个类默认的行为，就如同上面的代码中，我们通过元类的__call__魔术方法，改变了President类的构造器那样。 补充：关于单例模式，在面试中还有可能被问到它的应用场景。通常一个对象的状态是被其他对象共享的，就可以将其设计为单例，例如项目中使用的数据库连接池对象和配置对象通常都是单例，这样才能保证所有地方获取到的数据库连接和配置信息是完全一致的；而且由于对象只有唯一的实例，因此从根本上避免了重复创建对象造成的时间和空间上的开销，也避免了对资源的多重占用。再举个例子，项目中的日志操作通常也会使用单例模式，这是因为共享的日志文件一直处于打开状态，只能有一个实例去操作它，否则在写入日志的时候会产生混乱。 题目002：不使用中间变量，交换两个变量a和b的值。 点评：典型的送人头的题目，通常交换两个变量需要借助一个中间变量，如果不允许使用中间变量，在其他编程语言中可以使用异或运算的方式来实现交换两个变量的值，但是Python中有更为简单明了的做法。 方法一： 123a = a ^ bb = a ^ ba = a ^ b 方法二： 1a, b = b, a 扩展：需要注意，a, b = b, a这种做法其实并不是元组解包，虽然很多人都这样认为。Python字节码指令中有ROT_TWO指令来支持这个操作，类似的还有ROT_THREE，对于3个以上的元素，如a, b, c, d = b, c, d, a，才会用到创建元组和元组解包。想知道你的代码对应的字节码指令，可以使用Python标准库中dis模块的dis函数来反汇编你的Python代码。 题目003：写一个删除列表中重复元素的函数，要求去重后元素相对位置保持不变。 点评：这个题目在初中级Python岗位面试的时候经常出现，题目源于《Python Cookbook》这本书第一章的第10个问题，有很多面试题其实都是这本书上的原题，所以建议大家有时间好好研读一下这本书。 12345678def dedup(items): no_dup_items = [] seen = set() for item in items: if item not in seen: no_dup_items.append(item) seen.add(item) return no_dup_items 如果愿意也可以把上面的函数改造成一个生成器，代码如下所示。 123456def dedup(items): seen = set() for item in items: if item not in seen: yield item seen.add(item) 扩展：由于Python中的集合底层使用哈希存储，所以集合的in和not in成员运算在性能上远远优于列表，所以上面的代码我们使用了集合来保存已经出现过的元素。集合中的元素必须是hashable对象，因此上面的代码在列表元素不是hashable对象时会失效，要解决这个问题可以给函数增加一个参数，该参数可以设计为返回哈希码或hashable对象的函数。 题目004：假设你使用的是官方的CPython，说出下面代码的运行结果。 点评：下面的程序对实际开发并没有什么意义，但却是CPython中的一个大坑，这道题旨在考察面试者对官方的Python解释器到底了解到什么程度。 1234567891011a, b, c, d = 1, 1, 1000, 1000print(a is b, c is d)def foo(): e = 1000 f = 1000 print(e is f, e is d) g = 1 print(g is a)foo() 运行结果： 123True FalseTrue FalseTrue 上面代码中a is b的结果是True但c is d的结果是False，这一点的确让人费解。CPython解释器出于性能优化的考虑，把频繁使用的整数对象用一个叫small_ints的对象池缓存起来造成的。small_ints缓存的整数值被设定为[-5, 256]这个区间，也就是说，在任何引用这些整数的地方，都不需要重新创建int对象，而是直接引用缓存池中的对象。如果整数不在该范围内，那么即便两个整数的值相同，它们也是不同的对象。 CPython底层为了进一步提升性能还做了另一个设定，对于同一个代码块中值不在small_ints缓存范围内的整数，如果同一个代码块中已经存在一个值与其相同的整数对象，那么就直接引用该对象，否则创建新的int对象。需要大家注意的是，这条规则对数值型适用，但对字符串则需要考虑字符串的长度，这一点大家可以自行证明。 扩展：如果你用PyPy（另一种Python解释器实现，支持JIT，对CPython的缺点进行了改良，在性能上优于CPython，但对三方库的支持略差）来运行上面的代码，你会发现所有的输出都是True。 题目005：Lambda函数是什么，举例说明的它的应用场景。 点评：这个题目主要想考察的是Lambda函数的应用场景，潜台词是问你在项目中有没有使用过Lambda函数，具体在什么场景下会用到Lambda函数，借此来判断你写代码的能力。因为Lambda函数通常用在高阶函数中，主要的作用是通过向函数传入函数或让函数返回函数最终实现代码的解耦合。 Lambda函数也叫匿名函数，它是功能简单用一行代码就能实现的小型函数。Python中的Lambda函数只能写一个表达式，这个表达式的执行结果就是函数的返回值，不用写return关键字。Lambda函数因为没有名字，所以也不会跟其他函数发生命名冲突的问题。 扩展：面试的时候有可能还会考你用Lambda函数来实现一些功能，也就是用一行代码来实现题目要求的功能，例如：用一行代码实现求阶乘的函数，用一行代码实现求最大公约数的函数等。 12fac = lambda x: __import__(&#x27;functools&#x27;).reduce(int.__mul__, range(1, x + 1), 1)gcd = lambda x, y: y % x and gcd(y % x, x) or x Lambda函数其实最为主要的用途是把一个函数传入另一个高阶函数（如Python内置的filter、map等）中来为函数做解耦合，增强函数的灵活性和通用性。下面的例子通过使用filter和map函数，实现了从列表中筛选出奇数并求平方构成新列表的操作，因为用到了高阶函数，过滤和映射数据的规则都是函数的调用者通过另外一个函数传入的，因此这filter和map函数没有跟特定的过滤和映射数据的规则耦合在一起。 123items = [12, 5, 7, 10, 8, 19]items = list(map(lambda x: x ** 2, filter(lambda x: x % 2, items)))print(items) # [25, 49, 361] 扩展：用列表的生成式来实现上面的代码会更加简单明了，代码如下所示。 123items = [12, 5, 7, 10, 8, 19]items = [x ** 2 for x in items if x % 2]print(items) # [25, 49, 361] 题目006：说说Python中的浅拷贝和深拷贝。 点评：这个题目本身出现的频率非常高，但是就题论题而言没有什么技术含量。对于这种面试题，在回答的时候一定要让你的答案能够超出面试官的预期，这样才能获得更好的印象分。所以回答这个题目的要点不仅仅是能够说出浅拷贝和深拷贝的区别，深拷贝的时候可能遇到的两大问题，还要说出Python标准库对浅拷贝和深拷贝的支持，然后可以说说列表、字典如何实现拷贝操作以及如何通过序列化和反序列的方式实现深拷贝，最后还可以提到设计模式中的原型模式以及它在项目中的应用。 浅拷贝通常只复制对象本身，而深拷贝不仅会复制对象，还会递归的复制对象所关联的对象。深拷贝可能会遇到两个问题：一是一个对象如果直接或间接的引用了自身，会导致无休止的递归拷贝；二是深拷贝可能对原本设计为多个对象共享的数据也进行拷贝。Python通过copy模块中的copy和deepcopy函数来实现浅拷贝和深拷贝操作，其中deepcopy可以通过memo字典来保存已经拷贝过的对象，从而避免刚才所说的自引用递归问题；此外，可以通过copyreg模块的pickle函数来定制指定类型对象的拷贝行为。 deepcopy函数的本质其实就是对象的一次序列化和一次返回序列化，面试题中还考过用自定义函数实现对象的深拷贝操作，显然我们可以使用pickle模块的dumps和loads来做到，代码如下所示。 123import picklemy_deep_copy = lambda obj: pickle.loads(pickle.dumps(obj)) 列表的切片操作[:]相当于实现了列表对象的浅拷贝，而字典的copy方法可以实现字典对象的浅拷贝。对象拷贝其实是更为快捷的创建对象的方式。在Python中，通过构造器创建对象属于两阶段构造，首先是分配内存空间，然后是初始化。在创建对象时，我们也可以基于“原型”对象来创建新对象，通过对原型对象的拷贝（复制内存）就完成了对象的创建和初始化，这种做法更加高效，这也就是设计模式中的原型模式。在Python中，我们可以通过元类的方式来实现原型模式，代码如下所示。 1234567891011121314151617181920import copyclass PrototypeMeta(type): &quot;&quot;&quot;实现原型模式的元类&quot;&quot;&quot; def __init__(cls, *args, **kwargs): super().__init__(*args, **kwargs) # 为对象绑定clone方法来实现对象拷贝 cls.clone = lambda self, is_deep=True: \\ copy.deepcopy(self) if is_deep else copy.copy(self)class Person(metaclass=PrototypeMeta): passp1 = Person()p2 = p1.clone() # 深拷贝p3 = p1.clone(is_deep=False) # 浅拷贝 题目007：Python是如何实现内存管理的？ 点评：当面试官问到这个问题的时候，一个展示自己的机会就摆在面前了。你要先反问面试官：“你说的是官方的CPython解释器吗？”。这个反问可以展示出你了解过Python解释器的不同的实现版本，而且你也知道面试官想问的是CPython。当然，很多面试官对不同的Python解释器底层实现到底有什么差别也没有概念。所以，千万不要觉得面试官一定比你强，怀揣着这份自信可以让你更好的完成面试。 Python提供了自动化的内存管理，也就是说内存空间的分配与释放都是由Python解释器在运行时自动进行的，自动管理内存功能极大的减轻程序员的工作负担，也能够帮助程序员在一定程度上解决内存泄露的问题。以CPython解释器为例，它的内存管理有三个关键点：引用计数、标记清理、分代收集。 引用计数：对于CPython解释器来说，Python中的每一个对象其实就是PyObject结构体，它的内部有一个名为ob_refcnt 的引用计数器成员变量。程序在运行的过程中ob_refcnt的值会被更新并藉此来反映引用有多少个变量引用到该对象。当对象的引用计数值为0时，它的内存就会被释放掉。 12345typedef struct _object &#123; _PyObject_HEAD_EXTRA Py_ssize_t ob_refcnt; struct _typeobject *ob_type;&#125; PyObject; 以下情况会导致引用计数加1： 对象被创建 对象被引用 对象作为参数传入到一个函数中 对象作为元素存储到一个容器中 以下情况会导致引用计数减1： 用del语句显示删除对象引用 对象引用被重新赋值其他对象 一个对象离开它所在的作用域 持有该对象的容器自身被销毁 持有该对象的容器删除该对象 可以通过sys模块的getrefcount函数来获得对象的引用计数。引用计数的内存管理方式在遇到循环引用的时候就会出现致命伤，因此需要其他的垃圾回收算法对其进行补充。 标记清理：CPython使用了“标记-清理”（Mark and Sweep）算法解决容器类型可能产生的循环引用问题。该算法在垃圾回收时分为两个阶段：标记阶段，遍历所有的对象，如果对象是可达的（被其他对象引用），那么就标记该对象为可达；清除阶段，再次遍历对象，如果发现某个对象没有标记为可达，则就将其回收。CPython底层维护了两个双端链表，一个链表存放着需要被扫描的容器对象（姑且称之为链表A），另一个链表存放着临时不可达对象（姑且称之为链表B）。为了实现“标记-清理”算法，链表中的每个节点除了有记录当前引用计数的ref_count变量外，还有一个gc_ref变量，这个gc_ref是ref_count的一个副本，所以初始值为ref_count的大小。执行垃圾回收时，首先遍历链表A中的节点，并且将当前对象所引用的所有对象的gc_ref减1，这一步主要作用是解除循环引用对引用计数的影响。再次遍历链表A中的节点，如果节点的gc_ref值为0，那么这个对象就被标记为“暂时不可达”（GC_TENTATIVELY_UNREACHABLE）并被移动到链表B中；如果节点的gc_ref不为0，那么这个对象就会被标记为“可达“（GC_REACHABLE），对于”可达“对象，还要递归的将该节点可以到达的节点标记为”可达“；链表B中被标记为”可达“的节点要重新放回到链表A中。在两次遍历之后，链表B中的节点就是需要释放内存的节点。 分代回收：在循环引用对象的回收中，整个应用程序会被暂停，为了减少应用程序暂停的时间，Python 通过分代回收（空间换时间）的方法提高垃圾回收效率。分代回收的基本思想是：对象存在的时间越长，是垃圾的可能性就越小，应该尽量不对这样的对象进行垃圾回收。CPython将对象分为三种世代分别记为0、1、2，每一个新生对象都在第0代中，如果该对象在一轮垃圾回收扫描中存活下来，那么它将被移到第1代中，存在于第1代的对象将较少的被垃圾回收扫描到；如果在对第1代进行垃圾回收扫描时，这个对象又存活下来，那么它将被移至第2代中，在那里它被垃圾回收扫描的次数将会更少。分代回收扫描的门限值可以通过gc模块的get_threshold函数来获得，该函数返回一个三元组，分别表示多少次内存分配操作后会执行0代垃圾回收，多少次0代垃圾回收后会执行1代垃圾回收，多少次1代垃圾回收后会执行2代垃圾回收。需要说明的是，如果执行一次2代垃圾回收，那么比它年轻的代都要执行垃圾回收。如果想修改这几个门限值，可以通过gc模块的set_threshold函数来做到。 题目008：说一下你对Python中迭代器和生成器的理解。 点评：很多人面试者都会写迭代器和生成器，但是却无法准确的解释什么是迭代器和生成器。如果你也有同样的困惑，可以参考下面的回答。 迭代器是实现了迭代器协议的对象。跟其他编程语言不通，Python中没有用于定义协议或表示约定的关键字，像interface、protocol这些单词并不在Python语言的关键字列表中。Python语言通过魔法方法来表示约定，也就是我们所说的协议，而__next__和__iter__这两个魔法方法就代表了迭代器协议。可以通过for-in循环从迭代器对象中取出值，也可以使用next函数取出迭代器对象中的下一个值。生成器是迭代器的语法升级版本，可以用更为简单的代码来实现一个迭代器。 扩展：面试中经常让写生成斐波那契数列的迭代器，大家可以参考下面的代码。 12345678910111213141516class Fib(object): def __init__(self, num): self.num = num self.a, self.b = 0, 1 self.idx = 0 def __iter__(self): return self def __next__(self): if self.idx &lt; self.num: self.a, self.b = self.b, self.a + self.b self.idx += 1 return self.a raise StopIteration() 如果用生成器的语法来改写上面的代码，代码会简单优雅很多。 12345def fib(num): a, b = 0, 1 for _ in range(num): a, b = b, a + b yield a 题目009：正则表达式的match方法和search方法有什么区别？ 点评：正则表达式是字符串处理的重要工具，所以也是面试中经常考察的知识点。在Python中，使用正则表达式有两种方式，一种是直接调用re模块中的函数，传入正则表达式和需要处理的字符串；一种是先通过re模块的compile函数创建正则表达式对象，然后再通过对象调用方法并传入需要处理的字符串。如果一个正则表达式被频繁的使用，我们推荐用re.compile函数创建正则表达式对象，这样会减少频繁编译同一个正则表达式所造成的开销。 match方法是从字符串的起始位置进行正则表达式匹配，返回Match对象或None。search方法会扫描整个字符串来找寻匹配的模式，同样也是返回Match对象或None。 题目010：下面这段代码的执行结果是什么。1234def multiply(): return [lambda x: i * x for i in range(4)]print([m(100) for m in multiply()]) 运行结果： 1[300, 300, 300, 300] 上面代码的运行结果很容易被误判为[0, 100, 200, 300]。首先需要注意的是multiply函数用生成式语法返回了一个列表，列表中保存了4个Lambda函数，这4个Lambda函数会返回传入的参数乘以i的结果。需要注意的是这里有闭包（closure）现象，multiply函数中的局部变量i的生命周期被延展了，由于i最终的值是3，所以通过m(100)调列表中的Lambda函数时会返回300，而且4个调用都是如此。 如果想得到[0, 100, 200, 300]这个结果，可以按照下面几种方式来修改multiply函数。 方法一：使用生成器，让函数获得i的当前值。 1234def multiply(): return (lambda x: i * x for i in range(4))print([m(100) for m in multiply()]) 或者 12345def multiply(): for i in range(4): yield lambda x: x * iprint([m(100) for m in multiply()]) 方法二：使用偏函数，彻底避开闭包。 1234567from functools import partialfrom operator import __mul__def multiply(): return [partial(__mul__, i) for i in range(4)]print([m(100) for m in multiply()]) 题目011：Python中为什么没有函数重载？ 点评：C++、Java、C#等诸多编程语言都支持函数重载，所谓函数重载指的是在同一个作用域中有多个同名函数，它们拥有不同的参数列表（参数个数不同或参数类型不同或二者皆不同），可以相互区分。重载也是一种多态性，因为通常是在编译时通过参数的个数和类型来确定到底调用哪个重载函数，所以也被称为编译时多态性或者叫前绑定。这个问题的潜台词其实是问面试者是否有其他编程语言的经验，是否理解Python是动态类型语言，是否知道Python中函数的可变参数、关键字参数这些概念。 首先Python是解释型语言，函数重载现象通常出现在编译型语言中。其次Python是动态类型语言，函数的参数没有类型约束，也就无法根据参数类型来区分重载。再者Python中函数的参数可以有默认值，可以使用可变参数和关键字参数，因此即便没有函数重载，也要可以让一个函数根据调用者传入的参数产生不同的行为。 题目012：用Python代码实现Python内置函数max。 点评：这个题目看似简单，但实际上还是比较考察面试者的功底。因为Python内置的max函数既可以传入可迭代对象找出最大，又可以传入两个或多个参数找出最大；最为关键的是还可以通过命名关键字参数key来指定一个用于元素比较的函数，还可以通过default命名关键字参数来指定当可迭代对象为空时返回的默认值。 下面的代码仅供参考： 123456789101112131415161718192021222324def my_max(*args, key=None, default=None): &quot;&quot;&quot; 获取可迭代对象中最大的元素或两个及以上实参中最大的元素 :param args: 一个可迭代对象或多个元素 :param key: 提取用于元素比较的特征值的函数，默认为None :param default: 如果可迭代对象为空则返回该默认值，如果没有给默认值则引发ValueError异常 :return: 返回可迭代对象或多个元素中的最大元素 &quot;&quot;&quot; if len(args) == 1 and len(args[0]) == 0: if default: return default else: raise ValueError(&#x27;max() arg is an empty sequence&#x27;) items = args[0] if len(args) == 1 else args max_elem, max_value = items[0], items[0] if key: max_value = key(max_value) for item in items: value = item if key: value = key(item) if value &gt; max_value: max_elem, max_value = item, value return max_elem 题目013：写一个函数统计传入的列表中每个数字出现的次数并返回对应的字典。 点评：送人头的题目，不解释。 123456def count_letters(items): result = &#123;&#125; for item in items: if isinstance(item, (int, float)): result[item] = result.get(item, 0) + 1 return result 也可以直接使用Python标准库中collections模块的Counter类来解决这个问题，Counter是dict的子类，它会将传入的序列中的每个元素作为键，元素出现的次数作为值来构造字典。 123456from collections import Counterdef count_letters(items): counter = Counter(items) return &#123;key: value for key, value in counter.items() \\ if isinstance(key, (int, float))&#125; 题目014：使用Python代码实现遍历一个文件夹的操作。 点评：基本也是送人头的题目，只要用过os模块就应该知道怎么做。 Python标准库os模块的walk函数提供了遍历一个文件夹的功能，它返回一个生成器。 12345678import osg = os.walk(&#x27;/Users/Hao/Downloads/&#x27;)for path, dir_list, file_list in g: for dir_name in dir_list: print(os.path.join(path, dir_name)) for file_name in file_list: print(os.path.join(path, file_name)) 说明：os.path模块提供了很多进行路径操作的工具函数，在项目开发中也是经常会用到的。如果题目明确要求不能使用os.walk函数，那么可以使用os.listdir函数来获取指定目录下的文件和文件夹，然后再通过循环遍历用os.isdir函数判断哪些是文件夹，对于文件夹可以通过递归调用进行遍历，这样也可以实现遍历一个文件夹的操作。 题目015：现有2元、3元、5元共三种面额的货币，如果需要找零99元，一共有多少种找零的方式？ 点评：还有一个非常类似的题目：“一个小朋友走楼梯，一次可以走1个台阶、2个台阶或3个台阶，问走完10个台阶一共有多少种走法？”，这两个题目的思路是一样，如果用递归函数来写的话非常简单。 1234567891011from functools import lru_cache@lru_cache()def change_money(total): if total == 0: return 1 if total &lt; 0: return 0 return change_money(total - 2) + change_money(total - 3) + \\ change_money(total - 5) 说明：在上面的代码中，我们用lru_cache装饰器装饰了递归函数change_money，如果不做这个优化，上面代码的渐近时间复杂度将会是$O(3^N)$，而如果参数total的值是99，这个运算量是非常巨大的。lru_cache装饰器会缓存函数的执行结果，这样就可以减少重复运算所造成的开销，这是空间换时间的策略，也是动态规划的编程思想。 题目016：写一个函数，给定矩阵的阶数n，输出一个螺旋式数字矩阵。 例如：n &#x3D; 2，返回： 121 24 3 例如：n &#x3D; 3，返回： 1231 2 38 9 47 6 5 这个题目本身并不复杂，下面的代码仅供参考。 123456789101112131415161718192021222324252627282930313233def show_spiral_matrix(n): matrix = [[0] * n for _ in range(n)] row, col = 0, 0 num, direction = 1, 0 while num &lt;= n ** 2: if matrix[row][col] == 0: matrix[row][col] = num num += 1 if direction == 0: if col &lt; n - 1 and matrix[row][col + 1] == 0: col += 1 else: direction += 1 elif direction == 1: if row &lt; n - 1 and matrix[row + 1][col] == 0: row += 1 else: direction += 1 elif direction == 2: if col &gt; 0 and matrix[row][col - 1] == 0: col -= 1 else: direction += 1 else: if row &gt; 0 and matrix[row - 1][col] == 0: row -= 1 else: direction += 1 direction %= 4 for x in matrix: for y in x: print(y, end=&#x27;\\t&#x27;) print() 题目017：阅读下面的代码，写出程序的运行结果。123456items = [1, 2, 3, 4] print([i for i in items if i &gt; 2])print([i for i in items if i % 2])print([(x, y) for x, y in zip(&#x27;abcd&#x27;, (1, 2, 3, 4, 5))])print(&#123;x: f&#x27;item&#123;x ** 2&#125;&#x27; for x in (2, 4, 6)&#125;)print(len(&#123;x for x in &#x27;hello world&#x27; if x not in &#x27;abcdefg&#x27;&#125;)) 点评：生成式（推导式）属于Python的特色语法之一，几乎是面试必考内容。Python中通过生成式字面量语法，可以创建出列表、集合、字典。 12345[3, 4][1, 3][(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 3), (&#x27;d&#x27;, 4)]&#123;2: &#x27;item4&#x27;, 4: &#x27;item16&#x27;, 6: &#x27;item36&#x27;&#125;6 题目018：说出下面代码的运行结果。1234567891011121314class Parent: x = 1class Child1(Parent): passclass Child2(Parent): passprint(Parent.x, Child1.x, Child2.x)Child1.x = 2print(Parent.x, Child1.x, Child2.x)Parent.x = 3print(Parent.x, Child1.x, Child2.x) 点评：运行上面的代码首先输出1 1 1，这一点大家应该没有什么疑问。接下来，通过Child1.x = 2给类Child1重新绑定了属性x并赋值为2，所以Child1.x会输出2，而Parent和Child2并不受影响。执行Parent.x = 3会重新给Parent类的x属性赋值为3，由于Child2的x属性继承自Parent，所以Child2.x的值也是3；而之前我们为Child1重新绑定了x属性，那么它的x属性值不会受到Parent.x = 3的影响，还是之前的值2。 1231 1 11 2 13 2 3 题目19：说说你用过Python标准库中的哪些模块。 点评：Python标准库中的模块非常多，建议大家根据自己过往的项目经历来介绍你用过的标准库和三方库，因为这些是你最为熟悉的，经得起面试官深挖的。 模块名 介绍 sys 跟Python解释器相关的变量和函数，例如：sys.version、sys.exit() os 和操作系统相关的功能，例如：os.listdir()、os.remove() re 和正则表达式相关的功能，例如：re.compile()、re.search() math 和数学运算相关的功能，例如：math.pi、math.e、math.cos logging 和日志系统相关的类和函数，例如：logging.Logger、logging.Handler json &#x2F; pickle 实现对象序列化和反序列的模块，例如：json.loads、json.dumps hashlib 封装了多种哈希摘要算法的模块，例如：hashlib.md5、hashlib.sha1 urllib 包含了和URL相关的子模块，例如：urllib.request、urllib.parse itertools 提供各种迭代器的模块，例如：itertools.cycle、itertools.product functools 函数相关工具模块，例如：functools.partial、functools.lru_cache collections &#x2F; heapq 封装了常用数据结构和算法的模块，例如：collections.deque threading &#x2F; multiprocessing 多线程&#x2F;多进程相关类和函数的模块，例如：threading.Thread concurrent.futures &#x2F; asyncio 并发编程&#x2F;异步编程相关的类和函数的模块，例如：ThreadPoolExecutor base64 提供BASE-64编码相关函数的模块，例如：bas64.encode csv 和读写CSV文件相关的模块，例如：csv.reader、csv.writer profile &#x2F; cProfile &#x2F; pstats 和代码性能剖析相关的模块，例如：cProfile.run、pstats.Stats unittest 和单元测试相关的模块，例如：unittest.TestCase 题目20：__init__和__new__方法有什么区别？Python中调用构造器创建对象属于两阶段构造过程，首先执行__new__方法获得保存对象所需的内存空间，再通过__init__执行对内存空间数据的填充（对象属性的初始化）。__new__方法的返回值是创建好的Python对象（的引用），而__init__方法的第一个参数就是这个对象（的引用），所以在__init__中可以完成对对象的初始化操作。__new__是类方法，它的第一个参数是类，__init__是对象方法，它的第一个参数是对象。 题目21：输入年月日，判断这个日期是这一年的第几天。方法一：不使用标准库中的模块和函数。 12345678910111213def is_leap_year(year): &quot;&quot;&quot;判断指定的年份是不是闰年，平年返回False，闰年返回True&quot;&quot;&quot; return year % 4 == 0 and year % 100 != 0 or year % 400 == 0def which_day(year, month, date): &quot;&quot;&quot;计算传入的日期是这一年的第几天&quot;&quot;&quot; # 用嵌套的列表保存平年和闰年每个月的天数 days_of_month = [ [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] ] days = days_of_month[is_leap_year(year)][:month - 1] return sum(days) + date 方法二：使用标准库中的datetime模块。 123456import datetimedef which_day(year, month, date): end = datetime.date(year, month, date) start = datetime.date(year, 1, 1) return (end - start).days + 1 题目22：平常工作中用什么工具进行静态代码分析。 点评：静态代码分析工具可以从代码中提炼出各种静态属性，这使得开发者可以对代码的复杂性、可维护性和可读性有更好的了解，这里所说的静态属性包括： 代码是否符合编码规范，例如：PEP-8。 代码中潜在的问题，包括：语法错误、缩进问题、导入缺失、变量覆盖等。 代码中的坏味道。 代码的复杂度。 代码的逻辑问题。 工作中静态代码分析主要用到的是Pylint和Flake8。Pylint可以检查出代码错误、坏味道、不规范的代码等问题，较新的版本中还提供了代码复杂度统计数据，可以生成检查报告。Flake8封装了Pyflakes（检查代码逻辑错误）、McCabe（检查代码复杂性）和Pycodestyle（检查代码是否符合PEP-8规范）工具，它可以执行这三个工具提供的检查。 题目23：说一下你知道的Python中的魔术方法。 点评：魔术方法也称为魔法方法，是Python中的特色语法，也是面试中的高频问题。 魔术方法 作用 __new__、__init__、__del__ 创建和销毁对象相关 __add__、__sub__、__mul__、__div__、__floordiv__、__mod__ 算术运算符相关 __eq__、__ne__、__lt__、__gt__、__le__、__ge__ 关系运算符相关 __pos__、__neg__、__invert__ 一元运算符相关 __lshift__、__rshift__、__and__、__or__、__xor__ 位运算相关 __enter__、__exit__ 上下文管理器协议 __iter__、__next__、__reversed__ 迭代器协议 __int__、__long__、__float__、__oct__、__hex__ 类型&#x2F;进制转换相关 __str__、__repr__、__hash__、__dir__ 对象表述相关 __len__、__getitem__、__setitem__、__contains__、__missing__ 序列相关 __copy__、__deepcopy__ 对象拷贝相关 __call__、__setattr__、__getattr__、__delattr__ 其他魔术方法 题目24：函数参数*arg和**kwargs分别代表什么？Python中，函数的参数分为位置参数、可变参数、关键字参数、命名关键字参数。*args代表可变参数，可以接收0个或任意多个参数，当不确定调用者会传入多少个位置参数时，就可以使用可变参数，它会将传入的参数打包成一个元组。**kwargs代表关键字参数，可以接收用参数名=参数值的方式传入的参数，传入的参数的会打包成一个字典。定义函数时如果同时使用*args和**kwargs，那么函数可以接收任意参数。 题目25：写一个记录函数执行时间的装饰器。 点评：高频面试题，也是最简单的装饰器，面试者必须要掌握的内容。 方法一：用函数实现装饰器。 1234567891011121314from functools import wrapsfrom time import timedef record_time(func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) print(f&#x27;&#123;func.__name__&#125;执行时间: &#123;time() - start&#125;秒&#x27;) return result return wrapper 方法二：用类实现装饰器。类有__call__魔术方法，该类对象就是可调用对象，可以当做装饰器来使用。 12345678910111213141516from functools import wrapsfrom time import timeclass Record: def __call__(self, func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) print(f&#x27;&#123;func.__name__&#125;执行时间: &#123;time() - start&#125;秒&#x27;) return result return wrapper 说明：装饰器可以用来装饰类或函数，为其提供额外的能力，属于设计模式中的代理模式。 扩展：装饰器本身也可以参数化，例如上面的例子中，如果不希望在终端中显示函数的执行时间而是希望由调用者来决定如何输出函数的执行时间，可以通过参数化装饰器的方式来做到，代码如下所示。 12345678910111213141516171819from functools import wrapsfrom time import timedef record_time(output): &quot;&quot;&quot;可以参数化的装饰器&quot;&quot;&quot; def decorate(func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) output(func.__name__, time() - start) return result return wrapper return decorate 题目26：什么是鸭子类型（duck typing）？鸭子类型是动态类型语言判断一个对象是不是某种类型时使用的方法，也叫做鸭子判定法。简单的说，鸭子类型是指判断一只鸟是不是鸭子，我们只关心它游泳像不像鸭子、叫起来像不像鸭子、走路像不像鸭子就足够了。换言之，如果对象的行为跟我们的预期是一致的（能够接受某些消息），我们就认定它是某种类型的对象。 在Python语言中，有很多bytes-like对象（如：bytes、bytearray、array.array、memoryview）、file-like对象（如：StringIO、BytesIO、GzipFile、socket）、path-like对象（如：str、bytes），其中file-like对象都能支持read和write操作，可以像文件一样读写，这就是所谓的对象有鸭子的行为就可以判定为鸭子的判定方法。再比如Python中列表的extend方法，它需要的参数并不一定要是列表，只要是可迭代对象就没有问题。 说明：动态语言的鸭子类型使得设计模式的应用被大大简化。 题目27：说一下Python中变量的作用域。Python中有四种作用域，分别是局部作用域（Local）、嵌套作用域（Embedded）、全局作用域（Global）、内置作用域（Built-in），搜索一个标识符时，会按照LEGB的顺序进行搜索，如果所有的作用域中都没有找到这个标识符，就会引发NameError异常。 题目28：说一下你对闭包的理解。闭包是支持一等函数的编程语言（Python、JavaScript等）中实现词法绑定的一种技术。当捕捉闭包的时候，它的自由变量（在函数外部定义但在函数内部使用的变量）会在捕捉时被确定，这样即便脱离了捕捉时的上下文，它也能照常运行。简单的说，可以将闭包理解为能够读取其他函数内部变量的函数。正在情况下，函数的局部变量在函数调用结束之后就结束了生命周期，但是闭包使得局部变量的生命周期得到了延展。使用闭包的时候需要注意，闭包会使得函数中创建的对象不会被垃圾回收，可能会导致很大的内存开销，所以闭包一定不能滥用。 题目29：说一下Python中的多线程和多进程的应用场景和优缺点。线程是操作系统分配CPU的基本单位，进程是操作系统分配内存的基本单位。通常我们运行的程序会包含一个或多个进程，而每个进程中又包含一个或多个线程。多线程的优点在于多个线程可以共享进程的内存空间，所以线程间的通信非常容易实现；但是如果使用官方的CPython解释器，多线程受制于GIL（全局解释器锁），并不能利用CPU的多核特性，这是一个很大的问题。使用多进程可以充分利用CPU的多核特性，但是进程间通信相对比较麻烦，需要使用IPC机制（管道、套接字等）。 多线程适合那些会花费大量时间在I&#x2F;O操作上，但没有太多并行计算需求且不需占用太多内存的I&#x2F;O密集型应用。多进程适合执行计算密集型任务（如：视频编码解码、数据处理、科学计算等）、可以分解为多个并行子任务并能合并子任务执行结果的任务以及在内存使用方面没有任何限制且不强依赖于I&#x2F;O操作的任务。 扩展：Python中实现并发编程通常有多线程、多进程和异步编程三种选择。异步编程实现了协作式并发，通过多个相互协作的子程序的用户态切换，实现对CPU的高效利用，这种方式也是非常适合I&#x2F;O密集型应用的。 题目30：说一下Python 2和Python 3的区别。 点评：这种问题千万不要背所谓的参考答案，说一些自己最熟悉的就足够了。 Python 2中的print和exec都是关键字，在Python 3中变成了函数。 Python 3中没有long类型，整数都是int类型。 Python 2中的不等号&lt;&gt;在Python 3中被废弃，统一使用!=。 Python 2中的xrange函数在Python 3中被range函数取代。 Python 3对Python 2中不安全的input函数做出了改进，废弃了raw_input函数。 Python 2中的file函数被Python 3中的open函数取代。 Python 2中的/运算对于int类型是整除，在Python 3中要用//来做整除除法。 Python 3中改进了Python 2捕获异常的代码，很明显Python 3的写法更合理。 Python 3生成式中循环变量的作用域得到了更好的控制，不会影响到生成式之外的同名变量。 Python 3中的round函数可以返回int或float类型，Python 2中的round函数返回float类型。 Python 3的str类型是Unicode字符串，Python 2的str类型是字节串，相当于Python 3中的bytes。 Python 3中的比较运算符必须比较同类对象。 Python 3中定义类的都是新式类，Python 2中定义的类有新式类（显式继承自object的类）和旧式类（经典类）之分，新式类和旧式类在MRO问题上有非常显著的区别，新式类可以使用__class__属性获取自身类型，新式类可以使用__slots__魔法。 Python 3对代码缩进的要求更加严格，如果混用空格和制表键会引发TabError。 Python 3中字典的keys、values、items方法都不再返回list对象，而是返回view object，内置的map、filter等函数也不再返回list对象，而是返回迭代器对象。 Python 3标准库中某些模块的名字跟Python 2是有区别的；而在三方库方面，有些三方库只支持Python 2，有些只能支持Python 3。 题目31：谈谈你对“猴子补丁”（monkey patching）的理解。“猴子补丁”是动态类型语言的一个特性，代码运行时在不修改源代码的前提下改变代码中的方法、属性、函数等以达到热补丁（hot patch）的效果。很多系统的安全补丁也是通过猴子补丁的方式来实现的，但实际开发中应该避免对猴子补丁的使用，以免造成代码行为不一致的问题。 在使用gevent库的时候，我们会在代码开头的地方执行gevent.monkey.patch_all()，这行代码的作用是把标准库中的socket模块给替换掉，这样我们在使用socket的时候，不用修改任何代码就可以实现对代码的协程化，达到提升性能的目的，这就是对猴子补丁的应用。 另外，如果希望用ujson三方库替换掉标准库中的json，也可以使用猴子补丁的方式，代码如下所示。 12345import json, ujsonjson.__name__ = &#x27;ujson&#x27;json.dumps = ujson.dumpsjson.loads = ujson.loads 单元测试中的Mock技术也是对猴子补丁的应用，Python中的unittest.mock模块就是解决单元测试中用Mock对象替代被测对象所依赖的对象的模块。 题目32：阅读下面的代码说出运行结果。123456789101112131415161718192021class A: def who(self): print(&#x27;A&#x27;, end=&#x27;&#x27;)class B(A): def who(self): super(B, self).who() print(&#x27;B&#x27;, end=&#x27;&#x27;)class C(A): def who(self): super(C, self).who() print(&#x27;C&#x27;, end=&#x27;&#x27;)class D(B, C): def who(self): super(D, self).who() print(&#x27;D&#x27;, end=&#x27;&#x27;)item = D()item.who() 点评：这道题考查到了两个知识点： Python中的MRO（方法解析顺序）。在没有多重继承的情况下，向对象发出一个消息，如果对象没有对应的方法，那么向上（父类）搜索的顺序是非常清晰的。如果向上追溯到object类（所有类的父类）都没有找到对应的方法，那么将会引发AttributeError异常。但是有多重继承尤其是出现菱形继承（钻石继承）的时候，向上追溯到底应该找到那个方法就得确定MRO。Python 3中的类以及Python 2中的新式类使用C3算法来确定MRO，它是一种类似于广度优先搜索的方法；Python 2中的旧式类（经典类）使用深度优先搜索来确定MRO。在搞不清楚MRO的情况下，可以使用类的mro方法或__mro__属性来获得类的MRO列表。 super()函数的使用。在使用super函数时，可以通过super(类型, 对象)来指定对哪个对象以哪个类为起点向上搜索父类方法。所以上面B类代码中的super(B, self).who()表示以B类为起点，向上搜索self（D类对象）的who方法，所以会找到C类中的who方法，因为D类对象的MRO列表是D --&gt; B --&gt; C --&gt; A --&gt; object。 1ACBD 题目33：编写一个函数实现对逆波兰表达式求值，不能使用Python的内置函数。 点评：逆波兰表达式也称为“后缀表达式”，相较于平常我们使用的“中缀表达式”，逆波兰表达式不需要括号来确定运算的优先级，例如5 * (2 + 3)对应的逆波兰表达式是5 2 3 + *。逆波兰表达式求值需要借助栈结构，扫描表达式遇到运算数就入栈，遇到运算符就出栈两个元素做运算，将运算结果入栈。表达式扫描结束后，栈中只有一个数，这个数就是最终的运算结果，直接出栈即可。 12345678910111213141516171819202122232425262728293031323334353637383940import operatorclass Stack: &quot;&quot;&quot;栈（FILO）&quot;&quot;&quot; def __init__(self): self.elems = [] def push(self, elem): &quot;&quot;&quot;入栈&quot;&quot;&quot; self.elems.append(elem) def pop(self): &quot;&quot;&quot;出栈&quot;&quot;&quot; return self.elems.pop() @property def is_empty(self): &quot;&quot;&quot;检查栈是否为空&quot;&quot;&quot; return len(self.elems) == 0def eval_suffix(expr): &quot;&quot;&quot;逆波兰表达式求值&quot;&quot;&quot; operators = &#123; &#x27;+&#x27;: operator.add, &#x27;-&#x27;: operator.sub, &#x27;*&#x27;: operator.mul, &#x27;/&#x27;: operator.truediv &#125; stack = Stack() for item in expr.split(): if item.isdigit(): stack.push(float(item)) else: num2 = stack.pop() num1 = stack.pop() stack.push(operators[item](num1, num2)) return stack.pop() 题目34：Python中如何实现字符串替换操作？Python中实现字符串替换大致有两类方法：字符串的replace方法和正则表达式的sub方法。 方法一：使用字符串的replace方法。 12message = &#x27;hello, world!&#x27;print(message.replace(&#x27;o&#x27;, &#x27;O&#x27;).replace(&#x27;l&#x27;, &#x27;L&#x27;).replace(&#x27;he&#x27;, &#x27;HE&#x27;)) 方法二：使用正则表达式的sub方法。 12345import remessage = &#x27;hello, world!&#x27;pattern = re.compile(&#x27;[aeiou]&#x27;)print(pattern.sub(&#x27;#&#x27;, message)) 扩展：还有一个相关的面试题，对保存文件名的列表排序，要求文件名按照字母表和数字大小进行排序，例如对于列表filenames = [&#39;a12.txt&#39;, &#39;a8.txt&#39;, &#39;b10.txt&#39;, &#39;b2.txt&#39;, &#39;b19.txt&#39;, &#39;a3.txt&#39;] ，排序的结果是[&#39;a3.txt&#39;, &#39;a8.txt&#39;, &#39;a12.txt&#39;, &#39;b2.txt&#39;, &#39;b10.txt&#39;, &#39;b19.txt&#39;]。提示一下，可以通过字符串替换的方式为文件名补位，根据补位后的文件名用sorted函数来排序，大家可以思考下这个问题如何解决。 题目35：如何剖析Python代码的执行性能？剖析代码性能可以使用Python标准库中的cProfile和pstats模块，cProfile的run函数可以执行代码并收集统计信息，创建出Stats对象并打印简单的剖析报告。Stats是pstats模块中的类，它是一个统计对象。当然，也可以使用三方工具line_profiler和memory_profiler来剖析每一行代码耗费的时间和内存，这两个三方工具都会用非常友好的方式输出剖析结构。如果使用PyCharm，可以利用“Run”菜单的“Profile”菜单项对代码进行性能分析，PyCharm中可以用表格或者调用图（Call Graph）的方式来显示性能剖析的结果。 下面是使用cProfile剖析代码性能的例子。 example.py 12345678910111213141516171819202122232425262728293031import cProfiledef is_prime(num): for factor in range(2, int(num ** 0.5) + 1): if num % factor == 0: return False return Trueclass PrimeIter: def __init__(self, total): self.counter = 0 self.current = 1 self.total = total def __iter__(self): return self def __next__(self): if self.counter &lt; self.total: self.current += 1 while not is_prime(self.current): self.current += 1 self.counter += 1 return self.current raise StopIteration()cProfile.run(&#x27;list(PrimeIter(10000))&#x27;) 如果使用line_profiler三方工具，可以直接剖析is_prime函数每行代码的性能，需要给is_prime函数添加一个profiler装饰器，代码如下所示。 123456@profilerdef is_prime(num): for factor in range(2, int(num ** 0.5) + 1): if num % factor == 0: return False return True 安装line_profiler。 1pip install line_profiler 使用line_profiler。 1kernprof -lv example.py 运行结果如下所示。 12345678Line # Hits Time Per Hit % Time Line Contents============================================================== 1 @profile 2 def is_prime(num): 3 86624 48420.0 0.6 50.5 for factor in range(2, int(num ** 0.5) + 1): 4 85624 44000.0 0.5 45.9 if num % factor == 0: 5 6918 3080.0 0.4 3.2 return False 6 1000 430.0 0.4 0.4 return True 题目36：如何使用random模块生成随机数、实现随机乱序和随机抽样？ 点评：送人头的题目，因为Python标准库中的常用模块应该是Python开发者都比较熟悉的内容，这个问题回如果答不上来，整个面试基本也就砸锅了。 random.random()函数可以生成[0.0, 1.0)之间的随机浮点数。 random.uniform(a, b)函数可以生成[a, b]或[b, a]之间的随机浮点数。 random.randint(a, b)函数可以生成[a, b]或[b, a]之间的随机整数。 random.shuffle(x)函数可以实现对序列x的原地随机乱序。 random.choice(seq)函数可以从非空序列中取出一个随机元素。 random.choices(population, weights=None, *, cum_weights=None, k=1)函数可以从总体中随机抽取（有放回抽样）出容量为k的样本并返回样本的列表，可以通过参数指定个体的权重，如果没有指定权重，个体被选中的概率均等。 random.sample(population, k)函数可以从总体中随机抽取（无放回抽样）出容量为k的样本并返回样本的列表。 扩展：random模块提供的函数除了生成均匀分布的随机数外，还可以生成其他分布的随机数，例如random.gauss(mu, sigma)函数可以生成高斯分布（正态分布）的随机数；random.paretovariate(alpha)函数会生成帕累托分布的随机数；random.gammavariate(alpha, beta)函数会生成伽马分布的随机数。 题目37：解释一下线程池的工作原理。 点评：池化技术就是一种典型空间换时间的策略，我们使用的数据库连接池、线程池等都是池化技术的应用，Python标准库currrent.futures模块的ThreadPoolExecutor就是线程池的实现，如果要弄清楚它的工作原理，可以参考下面的内容。 线程池是一种用于减少线程本身创建和销毁造成的开销的技术，属于典型的空间换时间操作。如果应用程序需要频繁的将任务派发到线程中执行，线程池就是必选项，因为创建和释放线程涉及到大量的系统底层操作，开销较大，如果能够在应用程序工作期间，将创建和释放线程的操作变成预创建和借还操作，将大大减少底层开销。线程池在应用程序启动后，立即创建一定数量的线程，放入空闲队列中。这些线程最开始都处于阻塞状态，不会消耗CPU资源，但会占用少量的内存空间。当任务到来后，从队列中取出一个空闲线程，把任务派发到这个线程中运行，并将该线程标记为已占用。当线程池中所有的线程都被占用后，可以选择自动创建一定数量的新线程，用于处理更多的任务，也可以选择让任务排队等待直到有空闲的线程可用。在任务执行完毕后，线程并不退出结束，而是继续保持在池中等待下一次的任务。当系统比较空闲时，大部分线程长时间处于闲置状态时，线程池可以自动销毁一部分线程，回收系统资源。基于这种预创建技术，线程池将线程创建和销毁本身所带来的开销分摊到了各个具体的任务上，执行次数越多，每个任务所分担到的线程本身开销则越小。 一般线程池都必须具备下面几个组成部分： 线程池管理器：用于创建并管理线程池。 工作线程和线程队列：线程池中实际执行的线程以及保存这些线程的容器。 任务接口：将线程执行的任务抽象出来，形成任务接口，确保线程池与具体的任务无关。 任务队列：线程池中保存等待被执行的任务的容器。 题目38：举例说明什么情况下会出现KeyError、TypeError、ValueError。举一个简单的例子，变量a是一个字典，执行int(a[&#39;x&#39;])这个操作就有可能引发上述三种类型的异常。如果字典中没有键x，会引发KeyError；如果键x对应的值不是str、float、int、bool以及bytes-like类型，在调用int函数构造int类型的对象时，会引发TypeError；如果a[x]是一个字符串或者字节串，而对应的内容又无法处理成int时，将引发ValueError。 题目39：说出下面代码的运行结果。12345678910def extend_list(val, items=[]): items.append(val) return itemslist1 = extend_list(10)list2 = extend_list(123, [])list3 = extend_list(&#x27;a&#x27;)print(list1)print(list2)print(list3) 点评：Python函数在定义的时候，默认参数items的值就被计算出来了，即[]。因为默认参数items引用了对象[]，每次调用该函数，如果对items引用的列表进行了操作，下次调用时，默认参数还是引用之前的那个列表而不是重新赋值为[]，所以列表中会有之前添加的元素。如果通过传参的方式为items重新赋值，那么items将引用到新的列表对象，而不再引用默认的那个列表对象。这个题在面试中经常被问到，通常不建议使用容器类型的默认参数，像PyLint这样的代码检查工具也会对这种代码提出质疑和警告。 123[10, &#x27;a&#x27;][123][10, &#x27;a&#x27;] 题目40：如何读取大文件，例如内存只有4G，如何读取一个大小为8G的文件？很显然4G内存要一次性的加载大小为8G的文件是不现实的，遇到这种情况必须要考虑多次读取和分批次处理。在Python中读取文件可以先通过open函数获取文件对象，在读取文件时，可以通过read方法的size参数指定读取的大小，也可以通过seek方法的offset参数指定读取的位置，这样就可以控制单次读取数据的字节数和总字节数。除此之外，可以使用内置函数iter将文件对象处理成迭代器对象，每次只读取少量的数据进行处理，代码大致写法如下所示。 123with open(&#x27;...&#x27;, &#x27;rb&#x27;) as file: for data in iter(lambda: file.read(2097152), b&#x27;&#x27;): pass 在Linux系统上，可以通过split命令将大文件切割为小片，然后通过读取切割后的小文件对数据进行处理。例如下面的命令将名为filename的大文件切割为大小为512M的多个文件。 1split -b 512m filename 如果愿意， 也可以将名为filename的文件切割为10个文件，命令如下所示。 1split -n 10 filename 扩展：外部排序跟上述的情况非常类似，由于处理的数据不能一次装入内存，只能放在读写较慢的外存储器（通常是硬盘）上。“排序-归并算法”就是一种常用的外部排序策略。在排序阶段，先读入能放在内存中的数据量，将其排序输出到一个临时文件，依此进行，将待排序数据组织为多个有序的临时文件，然后在归并阶段将这些临时文件组合为一个大的有序文件，这个大的有序文件就是排序的结果。 题目41：说一下你对Python中模块和包的理解。每个Python文件就是一个模块，而保存这些文件的文件夹就是一个包，但是这个作为Python包的文件夹必须要有一个名为__init__.py的文件，否则无法导入这个包。通常一个文件夹下还可以有子文件夹，这也就意味着一个包下还可以有子包，子包中的__init__.py并不是必须的。模块和包解决了Python中命名冲突的问题，不同的包下可以有同名的模块，不同的模块下可以有同名的变量、函数或类。在Python中可以使用import或from ... import ...来导入包和模块，在导入的时候还可以使用as关键字对包、模块、类、函数、变量等进行别名，从而彻底解决编程中尤其是多人协作团队开发时的命名冲突问题。 题目42：说一下你知道的Python编码规范。 点评：企业的Python编码规范基本上是参照PEP-8或谷歌开源项目风格指南来制定的，后者还提到了可以使用Lint工具来检查代码的规范程度，面试的时候遇到这类问题，可以先说下这两个参照标准，然后挑重点说一下Python编码的注意事项。 空格的使用 使用空格来表示缩进而不要用制表符（Tab）。 和语法相关的每一层缩进都用4个空格来表示。 每行的字符数不要超过79个字符，如果表达式因太长而占据了多行，除了首行之外的其余各行都应该在正常的缩进宽度上再加上4个空格。 函数和类的定义，代码前后都要用两个空行进行分隔。 在同一个类中，各个方法之间应该用一个空行进行分隔。 二元运算符的左右两侧应该保留一个空格，而且只要一个空格就好。 标识符命名 变量、函数和属性应该使用小写字母来拼写，如果有多个单词就使用下划线进行连接。 类中受保护的实例属性，应该以一个下划线开头。 类中私有的实例属性，应该以两个下划线开头。 类和异常的命名，应该每个单词首字母大写。 模块级别的常量，应该采用全大写字母，如果有多个单词就用下划线进行连接。 类的实例方法，应该把第一个参数命名为self以表示对象自身。 类的类方法，应该把第一个参数命名为cls以表示该类自身。 表达式和语句 采用内联形式的否定词，而不要把否定词放在整个表达式的前面。例如：if a is not b就比if not a is b更容易让人理解。 不要用检查长度的方式来判断字符串、列表等是否为None或者没有元素，应该用if not x这样的写法来检查它。 就算if分支、for循环、except异常捕获等中只有一行代码，也不要将代码和if、for、except等写在一起，分开写才会让代码更清晰。 import语句总是放在文件开头的地方。 引入模块的时候，from math import sqrt比import math更好。 如果有多个import语句，应该将其分为三部分，从上到下分别是Python标准模块、第三方模块和自定义模块，每个部分内部应该按照模块名称的字母表顺序来排列。 题目43：运行下面的代码是否会报错，如果报错请说明哪里有什么样的错，如果不报错请说出代码的执行结果。123456789101112class A: def __init__(self, value): self.__value = value @property def value(self): return self.__valueobj = A(1)obj.__value = 2print(obj.value)print(obj.__value) 点评：这道题有两个考察点，一个考察点是对_和__开头的对象属性访问权限以及@property装饰器的了解，另外一个考察的点是对动态语言的理解，不需要过多的解释。 1212 扩展：如果不希望代码运行时动态的给对象添加新属性，可以在定义类时使用__slots__魔法。例如，我们可以在上面的A中添加一行__slots__ = (&#39;__value&#39;, )，再次运行上面的代码，将会在原来的第10行处产生AttributeError错误。 题目44：对下面给出的字典按值从大到小对键进行排序。123456789prices = &#123; &#x27;AAPL&#x27;: 191.88, &#x27;GOOG&#x27;: 1186.96, &#x27;IBM&#x27;: 149.24, &#x27;ORCL&#x27;: 48.44, &#x27;ACN&#x27;: 166.89, &#x27;FB&#x27;: 208.09, &#x27;SYMC&#x27;: 21.29&#125; 点评：sorted函数的高阶用法在面试的时候经常出现，key参数可以传入一个函数名或一个Lambda函数，该函数的返回值代表了在排序时比较元素的依据。 1sorted(prices, key=lambda x: prices[x], reverse=True) 题目45：说一下namedtuple的用法和作用。 点评：Python标准库的collections模块提供了很多有用的数据结构，这些内容并不是每个开发者都清楚，就比如题目问到的namedtuple，在我参加过的面试中，90%的面试者都不能准确的说出它的作用和应用场景。此外，deque也是一个非常有用但又经常被忽视的类，还有Counter、OrderedDict 、defaultdict 、UserDict等类，大家清楚它们的用法吗？ 在使用面向对象编程语言的时候，定义类是最常见的一件事情，有的时候，我们会用到只有属性没有方法的类，这种类的对象通常只用于组织数据，并不能接收消息，所以我们把这种类称为数据类或者退化的类，就像C语言中的结构体那样。我们并不建议使用这种退化的类，在Python中可以用namedtuple（命名元组）来替代这种类。 1234567from collections import namedtupleCard = namedtuple(&#x27;Card&#x27;, (&#x27;suite&#x27;, &#x27;face&#x27;))card1 = Card(&#x27;红桃&#x27;, 13)card2 = Card(&#x27;草花&#x27;, 5)print(f&#x27;&#123;card1.suite&#125;&#123;card1.face&#125;&#x27;)print(f&#x27;&#123;card2.suite&#125;&#123;card2.face&#125;&#x27;) 命名元组与普通元组一样是不可变容器，一旦将数据存储在namedtuple的顶层属性中，数据就不能再修改了，也就意味着对象上的所有属性都遵循“一次写入，多次读取”的原则。和普通元组不同的是，命名元组中的数据有访问名称，可以通过名称而不是索引来获取保存的数据，不仅在操作上更加简单，代码的可读性也会更好。 命名元组的本质就是一个类，所以它还可以作为父类创建子类。除此之外，命名元组内置了一系列的方法，例如，可以通过_asdict方法将命名元组处理成字典，也可以通过_replace方法创建命名元组对象的浅拷贝。 123456789101112class MyCard(Card): def show(self): faces = [&#x27;&#x27;, &#x27;A&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;J&#x27;, &#x27;Q&#x27;, &#x27;K&#x27;] return f&#x27;&#123;self.suite&#125;&#123;faces[self.face]&#125;&#x27;print(Card) # &lt;class &#x27;__main__.Card&#x27;&gt;card3 = MyCard(&#x27;方块&#x27;, 12)print(card3.show()) # 方块Qprint(dict(card1._asdict())) # &#123;&#x27;suite&#x27;: &#x27;红桃&#x27;, &#x27;face&#x27;: 13&#125;print(card2._replace(suite=&#x27;方块&#x27;)) # Card(suite=&#x27;方块&#x27;, face=5) 总而言之，命名元组能更好的组织数据结构，让代码更加清晰和可读，在很多场景下是元组、字典和数据类的替代品。在需要创建占用空间更少的不可变类时，命名元组就是很好的选择。 题目46：按照题目要求写出对应的函数。 要求：写一个函数，传入一个有若干个整数的列表，该列表中某个元素出现的次数超过了50%，返回这个元素。 123456789101112def more_than_half(items): temp, times = None, 0 for item in items: if times == 0: temp = item times += 1 else: if item == temp: times += 1 else: times -= 1 return temp 点评：LeetCode上的题目，在Python面试中出现过，利用元素出现次数超过了50%这一特征，出现和temp相同的元素就将计数值加1，出现和temp不同的元素就将计数值减1。如果计数值为0，说明之前出现的元素已经对最终的结果没有影响，用temp记下当前元素并将计数值置为1。最终，出现次数超过了50%的这个元素一定会被赋值给变量temp。 题目47：按照题目要求写出对应的函数。 要求：写一个函数，传入的参数是一个列表（列表中的元素可能也是一个列表），返回该列表最大的嵌套深度。例如：列表[1, 2, 3]的嵌套深度为1，列表[[1], [2, [3]]]的嵌套深度为3。 1234567def list_depth(items): if isinstance(items, list): max_depth = 1 for item in items: max_depth = max(list_depth(item) + 1, max_depth) return max_depth return 0 点评：看到题目应该能够比较自然的想到使用递归的方式检查列表中的每个元素。 题目48：按照题目要求写出对应的装饰器。 要求：有一个通过网络获取数据的函数（可能会因为网络原因出现异常），写一个装饰器让这个函数在出现指定异常时可以重试指定的次数，并在每次重试之前随机延迟一段时间，最长延迟时间可以通过参数进行控制。 方法一： 123456789101112131415161718192021from functools import wrapsfrom random import randomfrom time import sleepdef retry(*, retry_times=3, max_wait_secs=5, errors=(Exception, )): def decorate(func): @wraps(func) def wrapper(*args, **kwargs): for _ in range(retry_times): try: return func(*args, **kwargs) except errors: sleep(random() * max_wait_secs) return None return wrapper return decorate 方法二： 123456789101112131415161718192021222324from functools import wrapsfrom random import randomfrom time import sleepclass Retry(object): def __init__(self, *, retry_times=3, max_wait_secs=5, errors=(Exception, )): self.retry_times = retry_times self.max_wait_secs = max_wait_secs self.errors = errors def __call__(self, func): @wraps(func) def wrapper(*args, **kwargs): for _ in range(self.retry_times): try: return func(*args, **kwargs) except self.errors: sleep(random() * self.max_wait_secs) return None return wrapper 点评：我们不止一次强调过，装饰器几乎是Python面试必问内容，这个题目比之前的题目稍微复杂一些，它需要的是一个参数化的装饰器。 题目49：写一个函数实现字符串反转，尽可能写出你知道的所有方法。 点评：烂大街的题目，基本上算是送人头的题目。 方法一：反向切片 12def reverse_string(content): return content[::-1] 方法二：反转拼接 12def reverse_string(content): return &#x27;&#x27;.join(reversed(content)) 方法三：递归调用 1234def reverse_string(content): if len(content) &lt;= 1: return content return reverse_string(content[1:]) + content[0] 方法四：双端队列 123456from collections import dequedef reverse_string(content): q = deque() q.extendleft(content) return &#x27;&#x27;.join(q) 方法五：反向组装 1234567from io import StringIOdef reverse_string(content): buffer = StringIO() for i in range(len(content) - 1, -1, -1): buffer.write(content[i]) return buffer.getvalue() 方法六：反转拼接 12def reverse_string(content): return &#x27;&#x27;.join([content[i] for i in range(len(content) - 1, -1, -1)]) 方法七：半截交换 12345def reverse_string(content): length, content= len(content), list(content) for i in range(length // 2): content[i], content[length - 1 - i] = content[length - 1 - i], content[i] return &#x27;&#x27;.join(content) 方法八：对位交换 12345def reverse_string(content): length, content= len(content), list(content) for i, j in zip(range(length // 2), range(length - 1, length // 2 - 1, -1)): content[i], content[j] = content[j], content[i] return &#x27;&#x27;.join(content) 扩展：这些方法其实都是大同小异的，面试的时候能够给出几种有代表性的就足够了。给大家留一个思考题，上面这些方法，哪些做法的性能较好呢？我们之前提到过剖析代码性能的方法，大家可以用这些方法来检验下你给出的答案是否正确。 题目50：按照题目要求写出对应的函数。 要求：列表中有1000000个元素，取值范围是[1000, 10000)，设计一个函数找出列表中的重复元素。 1234567def find_dup(items: list): dups = [0] * 9000 for item in items: dups[item - 1000] += 1 for idx, val in enumerate(dups): if val &gt; 1: yield idx + 1000 点评：这道题的解法和计数排序的原理一致，虽然元素的数量非常多，但是取值范围[1000, 10000)并不是很大，只有9000个可能的取值，所以可以用一个能够保存9000个元素的dups列表来记录每个元素出现的次数，dups列表所有元素的初始值都是0，通过对items列表中元素的遍历，当出现某个元素时，将dups列表对应位置的值加1，最后dups列表中值大于1的元素对应的就是items列表中重复出现过的元素。 查看更多的Python面试题，请移步到我的知乎专栏《Python面试宝典》。","categories":[{"name":"Python面试","slug":"Python面试","permalink":"http://bingfly.top/categories/Python%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://bingfly.top/tags/Python/"}]},{"title":"","slug":"Python-100-Days-master/番外篇/那些年我们踩过的那些坑","date":"2024-12-12T08:38:03.476Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/那些年我们踩过的那些坑/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E9%82%A3%E4%BA%9B%E5%B9%B4%E6%88%91%E4%BB%AC%E8%B8%A9%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E5%9D%91/","excerpt":"","text":"那些年我们踩过的那些坑坑1 - 整数比较的坑在 Python 中一切都是对象，整数也是对象，在比较两个整数时有两个运算符==和is，它们的区别是： is比较的是两个整数对象的id值是否相等，也就是比较两个引用是否代表了内存中同一个地址。 ==比较的是两个整数对象的内容是否相等，使用==时其实是调用了对象的__eq__()方法。 知道了is和==的区别之后，我们可以来看看下面的代码，了解Python中整数比较有哪些坑，以CPython解释器为例，大家先看看下面的代码。 123456789101112131415161718192021222324def main(): x = y = -1 while True: x += 1 y += 1 if x is y: print(&#x27;%d is %d&#x27; % (x, y)) else: print(&#x27;Attention! %d is not %d&#x27; % (x, y)) break x = y = 0 while True: x -= 1 y -= 1 if x is y: print(&#x27;%d is %d&#x27; % (x, y)) else: print(&#x27;Attention! %d is not %d&#x27; % (x, y)) breakif __name__ == &#x27;__main__&#x27;: main() 上面代码的部分运行结果如下图所示。这个结果是因为CPython出于性能优化的考虑，把频繁使用的整数对象用一个叫small_ints的对象池缓存起来造成的。small_ints缓存的整数值被设定为[-5, 256]这个区间，也就是说，如果使用CPython解释器，在任何引用这些整数的地方，都不需要重新创建int对象，而是直接引用缓存池中的对象。如果整数不在该范围内，那么即便两个整数的值相同，它们也是不同的对象。 当然仅仅如此这个坑就不值一提了，如果你理解了上面的规则，我们就再看看下面的代码。 12345678910111213a = 257def main(): b = 257 # 第6行 c = 257 # 第7行 print(b is c) # True print(a is b) # False print(a is c) # Falseif __name__ == &quot;__main__&quot;: main() 程序的执行结果已经用注释写在代码上了。够坑吧！看上去a、b和c的值都是一样的，但是is运算的结果却不一样。为什么会出现这样的结果，首先我们来说说Python程序中的代码块。所谓代码块是程序的一个最小的基本执行单位，一个模块文件、一个函数体、一个类、交互式命令中的单行代码都叫做一个代码块。上面的代码由两个代码块构成，a = 257是一个代码块，main函数是另外一个代码块。CPython底层为了进一步提升性能还做了一个设定：对于同一个代码块中值不在small_ints缓存范围之内的整数，如果同一个代码块中已经存在一个值与其相同的整数对象，那么就直接引用该对象，否则创建新的int对象。需要大家注意的是，这条规则对数值型适用，但对字符串则需要考虑字符串的长度，这一点可以自行证明。为了验证刚刚的结论，我们可以借用dis模块（听名字就知道是进行反汇编的模块）从字节码的角度来看看这段代码。如果不理解什么是字节码，可以先看看《谈谈 Python 程序的运行原理》这篇文章。可以先用import dis导入dis模块并按照如下所示的方式修改代码。 123import disdis.dis(main) 代码的执行结果如下图所示。可以看出代码第6行和第7行，也就是main函数中的257是从同一个位置加载的，因此是同一个对象；而代码第9行的a明显是从不同的地方加载的，因此引用的是不同的对象。 如果还想对这个问题进行进一步深挖，推荐大家阅读《Python整数对象实现原理》这篇文章。 坑2 - 嵌套列表的坑Python中有一种内置的数据类型叫列表，它是一种容器，可以用来承载其他的对象（准确的说是其他对象的引用），列表中的对象可以称为列表的元素，很明显我们可以把列表作为列表中的元素，这就是所谓的嵌套列表。嵌套列表可以模拟出现实中的表格、矩阵、2D游戏的地图（如植物大战僵尸的花园）、棋盘（如国际象棋、黑白棋）等。但是在使用嵌套的列表时要小心，否则很可能遭遇非常尴尬的情况，下面是一个小例子。 12345678names = [&#x27;关羽&#x27;, &#x27;张飞&#x27;, &#x27;赵云&#x27;, &#x27;马超&#x27;, &#x27;黄忠&#x27;]subjs = [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]scores = [[0] * 3] * 5for row, name in enumerate(names): print(&#x27;请输入%s的成绩&#x27; % name) for col, subj in enumerate(subjs): scores[row][col] = float(input(subj + &#x27;: &#x27;)) print(scores) 我们希望录入5个学生3门课程的成绩，于是定义了一个有5个元素的列表，而列表中的每个元素又是一个由3个元素构成的列表，这样一个列表的列表刚好跟一个表格是一致的，相当于有5行3列，接下来我们通过嵌套的for-in循环输入每个学生3门课程的成绩。程序执行完成后我们发现，每个学生3门课程的成绩是一模一样的，而且就是最后录入的那个学生的成绩。 要想把这个坑填平，我们首先要区分对象和对象的引用这两个概念，而要区分这两个概念，还得先说说内存中的栈和堆。我们经常会听人说起“堆栈”这个词，但实际上“堆”和“栈”是两个不同的概念。众所周知，一个程序运行时需要占用一些内存空间来存储数据和代码，那么这些内存从逻辑上又可以做进一步的划分。对底层语言（如C语言）有所了解的程序员大都知道，程序中可以使用的内存从逻辑上可以为五个部分，按照地址从高到低依次是：栈（stack）、堆（heap）、数据段（data segment）、只读数据段（static area）和代码段（code segment）。其中，栈用来存储局部、临时变量，以及函数调用时保存现场和恢复现场需要用到的数据，这部分内存在代码块开始执行时自动分配，代码块执行结束时自动释放，通常由编译器自动管理；堆的大小不固定，可以动态的分配和回收，因此如果程序中有大量的数据需要处理，这些数据通常都放在堆上，如果堆空间没有正确的被释放会引发内存泄露的问题，而像Python、Java等编程语言都使用了垃圾回收机制来实现自动化的内存管理（自动回收不再使用的堆空间）。所以下面的代码中，变量a并不是真正的对象，它是对象的引用，相当于记录了对象在堆空间的地址，通过这个地址我们可以访问到对应的对象；同理，变量b是列表容器的引用，它引用了堆空间上的列表容器，而列表容器中并没有保存真正的对象，它保存的也仅仅是对象的引用。 12a = object()b = [&#x27;apple&#x27;, &#x27;pitaya&#x27;, &#x27;grape&#x27;] 知道了这一点，我们可以回过头看看刚才的程序，我们对列表进行[[0] * 3] * 5操作时，仅仅是将[0, 0, 0]这个列表的地址进行了复制，并没有创建新的列表对象，所以容器中虽然有5个元素，但是这5个元素引用了同一个列表对象，这一点可以通过id函数检查scores[0]和scores[1]的地址得到证实。所以正确的代码应该按照如下的方式进行修改。 123456789names = [&#x27;关羽&#x27;, &#x27;张飞&#x27;, &#x27;赵云&#x27;, &#x27;马超&#x27;, &#x27;黄忠&#x27;]subjs = [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]scores = [[]] * 5for row, name in enumerate(names): print(&#x27;请输入%s的成绩&#x27; % name) scores[row] = [0] * 3 for col, subj in enumerate(subjs): scores[row][col] = float(input(subj + &#x27;: &#x27;)) print(scores) 或者 123456789names = [&#x27;关羽&#x27;, &#x27;张飞&#x27;, &#x27;赵云&#x27;, &#x27;马超&#x27;, &#x27;黄忠&#x27;]subjs = [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]scores = [[0] * 3 for _ in range(5)]for row, name in enumerate(names): print(&#x27;请输入%s的成绩&#x27; % name) scores[row] = [0] * 3 for col, subj in enumerate(subjs): scores[row][col] = float(input(subj + &#x27;: &#x27;)) print(scores) 如果对内存的使用不是很理解，可以看看PythonTutor网站上提供的代码可视化执行功能，通过可视化执行，我们可以看到内存是如何分配的，从而避免在使用嵌套列表或者复制对象时可能遇到的坑。 坑3 - 访问修饰符的坑用Python做过面向对象编程的人都知道，Python的类提供了两种访问控制权限，一种是公开，一种是私有（在属性或方法前加上双下划线）。而用惯了Java或C#这类编程语言的人都知道，类中的属性（数据抽象）通常都是私有的，其目的是为了将数据保护起来；而类中的方法（行为抽象）通常都是公开的，因为方法是对象向外界提供的服务。但是Python并没有从语法层面确保私有成员的私密性，因为它只是对类中所谓的私有成员进行了命名的变换，如果知道命名的规则照样可以直接访问私有成员，请看下面的代码。 12345678910111213class Student(object): def __init__(self, name, age): self.__name = name self.__age = age def __str__(self): return self.__name + &#x27;: &#x27; + str(self.__age)stu = Student(&#x27;骆昊&#x27;, 38)print(stu._Student__name)print(stu._Student__age) Python为什么要做出这样的设定呢？用一句广为流传的格言来解释这个问题：“We are all consenting adults here”（我们都是成年人）。这句话表达了很多Python程序员的一个共同观点，那就是开放比封闭要好，我们应该自己对自己的行为负责而不是从语言层面来限制对数据或方法的访问。 所以在Python中我们实在没有必要将类中的属性或方法用双下划线开头的命名处理成私有的成员，因为这并没有任何实际的意义。如果想对属性或方法进行保护，我们建议用单下划线开头的受保护成员，虽然它也不能真正保护这些属性或方法，但是它相当于给调用者一个暗示，让调用者知道这是不应该直接访问的属性或方法，而且这样做并不影响子类去继承这些东西。 需要提醒大家注意的是，Python类中的那些魔法方法，如__str__、__repr__等，这些方法并不是私有成员哦，虽然它们以双下划线开头，但是他们也是以双下划线结尾的，这种命名并不是私有成员的命名，这一点对初学者来说真的很坑。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/英语面试","date":"2024-12-12T08:38:03.474Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/英语面试/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95/","excerpt":"","text":"英语面试以下用I表示面试官（Interviewer），用C表示面试者（Candidate）。 开场寒暄 I: Thanks for waiting. (Please follow me.) C: It’s no problem. I: How are you doing this morning? C: I’m great. &#x2F; I’m doing fine. Thank you. &#x2F; How about you? I: How did you get here? C: I took the subway here. &#x2F; I drove here. I: Glad to meet you. C: Glad to meet you. &#x2F; It’s great to finally meet you in person. (之前电话沟通过的) 正式面试人力面试 I: Can you tell me a little bit about yourself? (介绍下自己) 原则：不要谈私生活和奇怪的癖好（王者荣耀打到星耀并不值得在这里说），因为别人更想知道的是你的专业技能（qulifications）和工作经验（experience），所以重点在你之前的公司（company name）、职位（title）、时间（years）和主要职责（major responsibilities） C: Thank you for having me. My name is Dachui WANG. I’m 22 years old, and I’m single. I have a Bachelor’s Degree of Computer Science from Tsinghua University. I was a Junior Java Programmer for ABC Technologies during my college life. Then I become an intermediate Java engineer for XYZ Corporation in last two years. Programming is my everyday life and programming is where my passion is. I think I have a good knowledge of Java enterprise application developement using light-weight frameworks like Spring, Guice, Hibernate and other open source middle-ware like Dubbo, Mycat, rocketmq and so on and so forth. I love reading, travelling and playing basketball in my spare time. That’s all! Thank you! I: How would you describe your personality? (你的性格) C: I’m hard working, eager to learn, and very serious about my work. I enjoy working with other people and I love challenges. I: What do you know about our company? (你对我们公司有什么了解) (需要做功课，了解公司的状况和企业文化，该公司在这个行业中的一个状况，有什么核心业务，主要的竞争对手有哪些) C: The one thing that I like the most about our company is your core values. I think they’re very important in this industry because …(自由发挥的部分)… I personally really believe in the cause as well. Of course, I’m very interested in your products such as …(功课部分)… and the techniques behind them. I: Why are you leaving your last job? (为什么离职) C: I want to advance my career and I think this job offers more challenges and opportunities for me do to that. I: What do you see yourself in 3 or 5 years? (3-5年职业规划) C: My long term goals involve growing with the company, where I can continue to learn, to take on additional responsibilities and to contribute as much value as I can. I intend to take advantage of all of these. I: What’s your salary expectation? (期望薪资) C: My salary expectation is in line with my experience and qualifications. I believe our company will pay me and every other employee fairly. (把球踢给对方先看看对方报价是多少，如果对方非要你报价再说后面的内容) I think 15 thousands RMB or above is fitting for me to leave in Chengdu. I: Do you have any questions for me? (问面试官的问题) C: What’s the growth potential for this position? 技术面试 I: What’s difference between an interface and an abstract class? I: What are pass by reference and pass by value? I: What’s the difference between process and threads? I: Explain the available thread state in high-level. I: What’s deadlocks? How to avoid them? I: How HashMap works in Java? I: What’s the difference between ArrayList and LinkedList? (类似的问题还有很多，比如比较HashSet和TreeSet、HashMap和Hashtable) I: Tell me what you know about garbage collection in Java. I: What’re two types of exceptions in Java? I: What’s the advantage of PreparedStatement over Statement? I: What’s the use of CallableStatement? I: What does connection pool mean? I: Explain the life cycle of a Servlet. I: What’s the difference between redirect and forward? I: What’s EL? What’re implicit objects of EL? I: Tell me what you know about Spring framework and its benefits. I: What’re different types of dependency injection. I: Are singleton beans thread safe in Spring framework? I: What’re the benefits of Spring framework’s transaction management? I: Explain what’s AOP. I: What’s a proxy and how to implement proxy pattern? I: How Spring MVC works? I: What’s the working scenario of Hibernate and MyBatis? I: How to implement SOA? I: Make a brief introduction of the projects you are involved before? 上面主要是面试Java程序员的问题，但是整个流程大致如此。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/知乎问题回答","date":"2024-12-12T08:38:03.472Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/知乎问题回答/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%9F%A5%E4%B9%8E%E9%97%AE%E9%A2%98%E5%9B%9E%E7%AD%94/","excerpt":"","text":"知乎问题回答Python学习完基础语法知识后，如何进一步提高？如果你已经完成了Python基础语法的学习，想要知道接下来如何提高，那么你得先问问自己你要用Python来做什么？目前学习Python后可能的就业方向包括以下几个领域，我把每个领域需要的技术作为了一个简单的关键词摘要。 说明：以下数据参考了主要的招聘门户网站以及职友集。 职位 所需技能 招聘需求量 Python后端开发工程师 Python基础Django &#x2F; Flask &#x2F; Tornado &#x2F; SanicRESTful &#x2F; 接口文档撰写MySQL &#x2F; Redis &#x2F; MongoDB &#x2F; ElasticSearchLinux &#x2F; Git &#x2F; Scrum &#x2F; PyCharm 一般 Python爬虫开发工程师 Python基础常用标准库和三方库Scrapy &#x2F; PySpiderSelenium &#x2F; AppniumRedis &#x2F; MongoDB &#x2F; MySQL前端 &#x2F; HTTP(S) &#x2F; 抓包工具 较少 Python量化交易开发工程师 Python基础数据结构 &#x2F; 算法 &#x2F; 设计模式NoSQL（KV数据库）金融学（两融、期权、期货、股票） &#x2F; 数字货币 一般 Python数据分析工程师 &#x2F;Python机器学习工程师 统计学专业 &#x2F; 数学专业 &#x2F; 计算机专业Python基础 &#x2F; 算法设计SQL &#x2F; NoSQL &#x2F; Hive &#x2F; Hadoop &#x2F; SparkNumPy &#x2F; Scikit-Learn &#x2F; Pandas &#x2F; SeabornPyTorch &#x2F; Tensorflow &#x2F; OpenCV 大 Python自动化测试工程师 Python基础 &#x2F; 单元测试 &#x2F; 软件测试基础Linux &#x2F; Shell &#x2F; JIRA &#x2F; 禅道 &#x2F; Jenkins &#x2F; CI &#x2F; CDSelenium &#x2F; Robot Framework &#x2F; Appniumab &#x2F; sysbench &#x2F; JMeter &#x2F; LoadRunner &#x2F; QTP 大 Python自动化运维工程师 Python基础 &#x2F; Linux &#x2F; Shell Fabric &#x2F; Ansible &#x2F; PlaybookZabbix &#x2F; Saltstack &#x2F; PuppetDocker &#x2F; paramiko 大 Python云平台开发工程师 Python基础OpenStack &#x2F; CloudStackOvirt &#x2F; KVMDocker &#x2F; K8S 较少 如果弄清了自己将来要做的方向，就可以开始有针对性的学习了，下面给大家一个推荐书籍的清单。 入门读物 《Python基础教程》（Beginning Python From Novice to Professional） 《Python学习手册》（Learning Python） 《Python编程》（Programming Python） 《Python编程从入门到实践》（Python Crash Course） 《Python Cookbook》 进阶读物 《软件架构 - Python语言实现》（Software Architecture with Python） 《流畅的Python》（Fluent Python） 《Python设计模式》（Learning Python Design Patterns） 《Python高级编程》（Expert Python Programming） 《Python性能分析与优化》（Mastering Python High Performance） 数据库相关 《MySQL必知必会》（MySQL Crash Course） 《深入浅出MySQL - 数据库开发、优化与管理维护》 《MongoDB权威指南》（MongoDB: The Definitive Guide） 《Redis实战》（Redis in Action） 《Redis开发与运维》 Linux &#x2F; Shell &#x2F; Docker &#x2F; 运维 《鸟哥的Linux私房菜》 《Linux命令行与shell脚本编程大全》（Linux Command Line and Shell Scripting Bible） 《Python自动化运维:技术与最佳实践》 《第一本Docker书》（The Docker Book） 《Docker经典实例》（Docker Cookbook） Django &#x2F; Flask &#x2F; Tornado 《Django基础教程》（Tango with Django） 《轻量级Django》（Lightweight Django） 《精通Django》（Mastering Django: Core） 《Python Web开发：测试驱动方法》（Test-Driven Development with Python） 《Two Scoops of Django: Best Practice of Django 1.8》 《Flask Web开发：基于Python的Web应用开发实战》（Flask Web Development: Developing Web Applications with Python） 《深入理解Flask》（Mastering Flask） 《Introduction to Tornado》 爬虫开发 《用Python写网络爬虫》（Web Scraping with Python） 《精通Python爬虫框架Scrapy》（Learning Scrapy） 《Python网络数据采集》（Web Scraping with Python） 《Python爬虫开发与项目实战》 《Python 3网络爬虫开发实战》 数据分析 《利用Python进行数据分析》（Python for Data Analysis） 《Python数据科学手册》（Python Data Science Handbook） 《Python金融大数据分析》（Python for Finance） 《Python数据可视化编程实战》（Python Data Visualization Cookbook） 《Python数据处理》（Data Wrangling with Python） 机器学习 《Python机器学习基础教程》（Introduction to Machine Learning with Python） 《Python机器学习实践指南》（Python Machine Learning Blueprints） 《Python机器学习实践：测试驱动的开发方法》（Thoughtful Machine Learning with Python A Test Driven Approach） 《Python机器学习经典实例》（Python Machine Learning Cookbook） 《TensorFlow：实战Google深度学习框架》 其他书籍 《Pro Git》 《Selenium自动化测试 - 基于Python语言》（Learning Selenium Testing Tools with Python） 《Selenium自动化测试之道》 《Scrum敏捷软件开发》（Software Development using Scrum） 《高效团队开发 - 工具与方法》 当然学习编程，最重要的通过项目实战来提升自己的综合能力，Github上有大量的优质开源项目，其中不乏优质的Python项目。有一个名为“awesome-python-applications”的项目对这些优质的资源进行了归类并提供了传送门，大家可以了解下。如果自学能力不是那么强，可以通过网络上免费或者付费的视频课程来学习对应的知识；如果自律性没有那么强，那就只能建议花钱参加培训班了，因为花钱在有人监督的环境下学习对很多人来说确实是一个捷径，但是要记得：“师傅领进门，修行靠各人”。选择自己热爱的东西并全力以赴，不要盲目的跟风学习，这一点算是过来人的忠告吧。记得我自己刚开始进入软件开发这个行业时，有人跟我说过这么一句话，现在也分享出来与诸君共勉：“浮躁的人有两种：只观望而不学习的人，只学习而不坚持的人；浮躁的人都不是高手。”","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/用函数还是用复杂的表达式","date":"2024-12-12T08:38:03.470Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/用函数还是用复杂的表达式/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%94%A8%E5%87%BD%E6%95%B0%E8%BF%98%E6%98%AF%E7%94%A8%E5%A4%8D%E6%9D%82%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"用函数还是用复杂的表达式Perl语言的原作者Larry Wall曾经说过，伟大的程序员都有三个优点：懒惰、暴躁和自负。乍一看这三个词语没有一个是褒义词，但在程序员的世界里，这三个词有不同的意义。首先，懒惰会促使程序员去写一些省事儿的程序来辅助自己或别人更好的完成工作，这样我们就无需做那些重复和繁琐的劳动；同理能够用3行代码解决的事情，我们也绝不会写出10行代码来。其次，暴躁会让程序员主动的去完成一些你还没有提出的工作，去优化自己的代码让它更有效率，能够3秒钟完成的任务，我们绝不能容忍1分钟的等待。最后，自负会促使程序员写出可靠无误的代码，我们写代码不是为了接受批评和指责，而是为了让其他人来膜拜。 那么接下来就有一个很有意思的问题值得探讨一下，我们需要一个程序从输入的三个数中找出最大的那个数。这个程序对任何会编程的人来说都是小菜一碟，甚至不会编程的人经过10分钟的学习也能搞定。下面是用来解决这个问题的Python代码。 12345678910a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))if a &gt; b: the_max = aelse: the_max = bif c &gt; the_max: the_max = cprint(&#x27;The max is:&#x27;, the_max) 但是我们刚才说了，程序员都是懒惰的，很多程序员都会使用三元条件运算符来改写上面的代码。 123456a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))the_max = a if a &gt; b else bthe_max = c if c &gt; the_max else the_maxprint(&#x27;The max is:&#x27;, the_max) 需要说明的是，Python在2.5版本以前是没有上面代码第4行和第5行中使用的三元条件运算符的，究其原因是Guido van Rossum（Python之父）认为三元条件运算符并不能帮助 Python变得更加简洁，于是那些习惯了在C&#x2F;C++或Java中使用三元条件运算符（在这些语言中，三元条件运算符也称为“Elvis运算符”，因为?:放在一起很像著名摇滚歌手猫王Elvis的大背头）的程序员试着用and和or运算符的短路特性来模拟出三元操作符，于是在那个年代，上面的代码是这样写的。 123456a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))the_max = a &gt; b and a or bthe_max = c &gt; the_max and c or the_maxprint(&#x27;The max is:&#x27;, the_max) 但是这种做法在某些场景下是不能成立的，且看下面的代码。 123456a = 0b = -100# 下面的代码本来预期输出a的值，结果却得到了b的值# 因为a的值0在进行逻辑运算时会被视为False来处理print(True and a or b)# print(a if True else b) 所以在Python 2.5以后引入了三元条件运算符来避免上面的风险（上面代码被注释掉的最后一句话）。那么，问题又来了，上面的代码还可以写得更简短吗？答案是肯定的。 1234a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))print(&#x27;The max is:&#x27;, (a if a &gt; b else b) if (a if a &gt; b else b) &gt; c else c) 但是，这样做真的好吗？如此复杂的表达式是不是让代码变得晦涩了很多呢？我们发现，在实际开发中很多开发者都喜欢过度的使用某种语言的特性或语法糖，于是简单的多行代码变成了复杂的单行表达式，这样做真的好吗？这个问题我也不止一次的问过自己，现在我能给出的答案是下面的代码，使用辅助函数。 12345678def the_max(x, y): return x if x &gt; y else ya = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))print(&#x27;The max is:&#x27;, the_max(the_max(a, b), c)) 上面的代码中，我定义了一个辅助函数the_max用来找出参数传入的两个值中较大的那一个，于是下面的输出语句可以通过两次调用the_max函数来找出三个数中的最大值，现在代码的可读性是不是好了很多。用辅助函数来替代复杂的表达式真的是一个不错的选择，关键是比较大小的逻辑转移到这个辅助函数后不仅可以反复调用它，而且还可以进行级联操作。 当然，很多语言中比较大小的函数根本没有必要自己来实现（通常都是内置函数），Python也是如此。Python内置的max函数利用了Python对可变参数的支持，允许一次性传入多个值或者一个迭代器并找出那个最大值，所以上面讨论的问题在Python中也就是一句话的事，但是从复杂表达式到使用辅助函数简化复杂表达式这个思想是非常值得玩味的，所以分享出来跟大家做一个交流。 1234a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))c = int(input(&#x27;c = &#x27;))print(&#x27;The max is:&#x27;, max(a, b, c))","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/玩转PyCharm","date":"2024-12-12T08:38:03.468Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/玩转PyCharm/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%8E%A9%E8%BD%ACPyCharm/","excerpt":"","text":"玩转PyCharmPyCharm是由JetBrains公司开发的提供给Python专业的开发者的一个集成开发环境，它最大的优点是能够大大提升Python开发者的工作效率，为开发者集成了很多用起来非常顺手的功能，包括代码调试、高亮语法、代码跳转、智能提示、自动补全、单元测试、版本控制等等。此外，PyCharm还提供了对一些高级功能的支持，包括支持基于Django框架的Web开发。 PyCharm的下载和安装可以在JetBrains公司的官方网站找到PyCharm的下载链接，有两个可供下载的版本，一个是社区版（PyCharm CE），一个是专业版（PyCharm Professional）。社区版在Apache许可证下发布，可以免费使用；专业版在专用许可证下发布，需要购买授权后才能使用，但新用户可以试用30天。很显然，专业版提供了更为强大的功能和对企业级开发的各种支持，但是对于初学者来说，社区版已经足够强大和好用了。安装PyCharm只需要直接运行下载的安装程序，然后持续的点击“Next”（下一步）按钮就可以啦。下面是我在Windows系统下安装PyCharm的截图，安装完成后点击“Finish”（结束）按钮关闭安装向导，然后可以通过双击桌面的快捷方式来运行PyCharm。 首次使用的设置第一次使用PyCharm时，会有一个导入设置的向导，如果之前没有使用PyCharm或者没有保存过设置的就直接选择“Do not import settings”进入下一步即可，下面是我在macOS系统下第一次使用PyCharm时的截图。 专业版的PyCharm是需要激活的，强烈建议大家在条件允许的情况下支付费用来支持优秀的产品，如果不用做商业用途或者不需要使用PyCharm的高级功能，我们可以暂时选择试用30天或者使用社区版的PyCharm。如果你是一名学生，希望购买PyCharm来使用，可以看看教育优惠官方申请指南。如下图所示，我们需要点击“Evaluate”按钮来试用专业版PyCharm。 接下来是选择UI主题，可以根据个人喜好进行选择，深色的主题比较护眼而浅色的主题对比度更好。 再接下来是创建可以在“终端”或“命令行提示符”中运行PyCharm的启动脚本，当然也可以不做任何勾选，直接点击“Next: Featured plugins”按钮进入下一环节。 然后可以选择需要安装哪些插件，我们可以暂时什么都不安装，等需要的时候再来决定。 最后点击上图右下角的“Start using PyCharm”（开始使用PyCharm）就可以开启你的PyCharm之旅了。 用PyCharm创建项目启动PyCharm之后会来到一个欢迎页，在欢迎页上我们可以选择“创建新项目”（Create New Project）、“打开已有项目”（Open）和“从版本控制系统中检出项目”（Get from Version Control）。 如果选择了“Create New Project”来创建新项目就会打一个创建项目的向导页。下图所示是PyCharm专业版创建新项目的向导页，可以看出专业版支持的项目类型非常的多，而社区版只能创建纯Python项目（Pure Python），没有这一系列的选项。 接下来，我们要为项目创建专属的虚拟环境，每个Python项目最好都在自己专属的虚拟环境中运行，因为每个项目对Python解释器和三方库的需求并不相同，虚拟环境对不同的项目进行了隔离。在上图所示的界面在，我们可以选择新建虚拟环境（New environment using Virtualenv），这里的“Virtualenv”是PyCharm默认选择的创建虚拟环境的工具，我们就保留这个默认的选项就可以了。 项目创建完成后就可以开始新建各种文件来书写Python代码了，如下图所示。左侧是项目浏览器，可以看到刚才创建的项目文件夹以及虚拟环境文件夹。我们可以在项目上点击鼠标右键，选择“New”，在选择“Python File”来创建Python代码文件，下图中我们创建了两个Python文件，分别是poker_game.py和salary_system.py。当然，如果愿意，也可以使用复制粘贴的方式把其他地方的Python代码文件复制到项目文件夹下。 在工作窗口点击鼠标右键可以在上下文菜单中找到“Run”选项，例如要运行salary_system.py文件，右键菜单会显示“Run ‘salary_system’”选项，点击这个选项我们就可以运行Python代码啦，运行结果在屏幕下方的窗口可以看到，如下图所示。 常用操作和快捷键PyCharm为写Python代码提供了自动补全和高亮语法功能，这也是PyCharm作为集成开发环境（IDE）的基本功能。PyCharm的“File”菜单有一个“Settings”菜单项（macOS上是在“PyCharm”菜单的“Preferences…”菜单项），这个菜单项会打开设置窗口，可以在此处对PyCharm进行设置，如下图所示。 PyCharm的菜单项中有一个非常有用的“Code”菜单，菜单中提供了自动生成代码、自动补全代码、格式化代码、移动代码等选项，这些功能对开发者来说是非常有用的，大家可以尝试使用这些菜单项或者记住它们对应的快捷键，例如在macOS上，格式化代码这个菜单项对应的快捷键是alt+command+L。除此之外，“Refactor”菜单也非常有用，它提供了一些重构代码的选项。所谓重构是在不改变代码执行结果的前提下调整代码的结构，这也是资深程序员的一项重要技能。还有一个值得一提的菜单是“VCS”，VCS是“Version Control System”（版本控制系统）的缩写，这个菜单提供了对代码版本管理的支持。版本控制的知识会在其他的课程中为大家讲解。 下表列出了一些PyCharm中特别常用的快捷键，当然如果愿意，也可以通过设置窗口中“Keymap”菜单项自定义快捷键，PyCharm本身也针对不同的操作系统和使用习惯对快捷键进行了分组。 快捷键 作用 command + j 显示可用的代码模板 command + b 查看函数、类、方法的定义 ctrl + space 万能代码提示快捷键，一下不行按两下 command + alt + l 格式化代码 alt + enter 万能代码修复快捷键 ctrl + / 注释&#x2F;反注释代码 shift + shift 万能搜索快捷键 command + d &#x2F; command + y 复制&#x2F;删除一行代码 command + shift + - &#x2F; command + shift + + 折叠&#x2F;展开所有代码 F2 快速定位到错误代码 command+ alt + F7 查看哪些地方用到了指定的函数、类、方法 说明：Windows系统下如果使用PyCharm的默认设置，可以将上面的command键换成ctrl键即可，唯一的例外是ctrl + space那个快捷键，因为它跟Windows系统切换输入法的快捷键是冲突的，所以在Windows系统下默认没有与之对应的快捷键。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/接口文档参考示例","date":"2024-12-12T08:38:03.465Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/接口文档参考示例/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3%E5%8F%82%E8%80%83%E7%A4%BA%E4%BE%8B/","excerpt":"","text":"接口文档参考示例 用户登录 - POST /api/login/ 开发者：骆昊 版本号：v1 最后修改时间： 接口说明：登录成功后，会生成或更新用户令牌（token）。 使用帮助：测试数据库中预设了四个可供使用的账号，如下表所示。 用户名 用户口令 角色 jackfrued 123456 管理员 wangdachui 123123 普通用户 hellokitty 123123 房地产经理人 wuzetian 123456 房东 请求参数： 参数名 类型 是否必填 参数位置 说明 username 字符串 是 消息体 用户名 password 字符串 是 消息体 用户口令 响应信息： 登录成功： 12345&#123; &quot;code&quot;: 30000, &quot;message&quot;: &quot;用户登录成功&quot;, &quot;token&quot;: &quot;f83e0f624e2311e9af1f00163e02b646&quot;&#125; 登录失败： 1234&#123; &quot;code&quot;: 30001, &quot;message&quot;: &quot;用户名或密码错误&quot;&#125; 发送短信验证码 - GET /api/mobile_code/&#123;国内手机号&#125;/ 开发者：骆昊 版本号：v1 接口说明：给指定手机号发送短信验证码的接口，手机号必须是国内手机号，作为路径参数写到URL中。接口显示短信发送成功时，指定的手机号并不会收到短息，因为使用的三方短信平台赠送的测试短信已经用完了。 使用帮助：国内手机号暂不支持国际区号。 请求参数：暂无。 响应信息： 请求成功： 1234&#123; &quot;code&quot;: 10001, &quot;msg&quot;: &quot;短信验证码发送成功&quot;&#125; 两次请求间隔小于60秒： 1234&#123; &quot;code&quot;: 10002, &quot;msg&quot;: &quot;请不要在60秒以内重复发送手机验证码&quot;&#125; 手机号无效： 1234&#123; &quot;code&quot;: 10003, &quot;msg&quot;: &quot;请提供有效的手机号&quot;&#125; 短信服务平台故障： 1234&#123; &quot;code&quot;: 10004, &quot;msg&quot;: &quot;短信服务暂时无法使用&quot;&#125; 获取所有省级行政单位 - GET /api/districts/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：暂无。 请求参数：暂无。 响应信息： 12345678910[ &#123; &quot;distid&quot;: 110000, &quot;name&quot;: &quot;北京市&quot; &#125;, &#123; &quot;distid&quot;: 120000, &quot;name&quot;: &quot;天津市&quot; &#125;] 获取指定行政单位详情及其管辖的行政单位 - GET /api/districts/&#123;行政单位编号&#125;/ 开发者：骆昊 版本号：v1 接口说明：通过URL参数指定行政单位编号，如果行政单位编号为省级行政单位编号，则返回该省以及该省所辖市级行政单位的信息；如果行政单位编号为市级行政单位编号，则返回该市以及该市所辖区县的信息；如果行政单位编号为区县一级行政单位编号，则返回该区县的信息，下级行政单位cities属性值为[]。 使用帮助：数据库中除四川省外其他行政单位的“intro”数据都没有录入，该字段可能为空字符串。 请求参数：暂无。 响应信息： 12345678910111213141516171819&#123; &quot;distid&quot;: 510000, &quot;name&quot;: &quot;四川省&quot;, &quot;intro&quot;: &quot;位于中国西南地区内陆，东连重庆，南邻云南、贵州，西接西藏，北界陕西、甘肃、青海，四川省总面积48.6万平方千米，省会成都。截至2018年底，四川省下辖18个省辖市，3个自治州，17个县级市，108个县，4个自治县，54个市辖区。&quot;, &quot;cities&quot;: [ &#123; &quot;distid&quot;: 510100, &quot;name&quot;: &quot;成都市&quot; &#125;, &#123; &quot;distid&quot;: 510300, &quot;name&quot;: &quot;自贡市&quot; &#125;, &#123; &quot;distid&quot;: 510400, &quot;name&quot;: &quot;攀枝花市&quot; &#125; ]&#125; 获取热门城市 - GET /api/hotcities/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：暂无。 请求参数：暂无。 响应信息： 1234567891011121314[ &#123; &quot;distid&quot;: 110100, &quot;name&quot;: &quot;北京市&quot; &#125;, &#123; &quot;distid&quot;: 120100, &quot;name&quot;: &quot;天津市&quot; &#125;, &#123; &quot;distid&quot;: 130100, &quot;name&quot;: &quot;石家庄&quot; &#125;] 分页获取房地产经理人信息 - GET /api/agents/ 开发者：骆昊 版本号：v1 接口说明：经理人姓名按照前缀模糊匹配的方式进行处理；经理人服务星级是指经理人服务星级不得低于该星级；经理人是否持证只有0（未持证上岗）和1（持证上岗）两个选项。三个参数代表的筛选条件之间是而且的关系。返回结果为分页之后的房地产经理人信息。 使用帮助：暂无。 请求参数： 参数名 类型 是否必填 参数位置 说明 name 字符串 否 查询参数 经理人姓名 key 字符串 否 查询参数 经理人服务星级 cert 字符串 否 查询参数 经理人是否持证 page 整数 否 查询参数 页码，默认值为1 size 整数 否 查询参数 页面大小，默认值为5，最大值不超过50 响应信息： 1234567891011121314151617181920212223&#123; &quot;count&quot;: 1, &quot;next&quot;: null, &quot;previous&quot;: null, &quot;results&quot;: [ &#123; &quot;agentid&quot;: 6, &quot;estates&quot;: [ &#123; &quot;estateid&quot;: 11, &quot;name&quot;: &quot;灵芝新村&quot;, &quot;hot&quot;: 20 &#125; ], &quot;name&quot;: &quot;肖利丽&quot;, &quot;tel&quot;: &quot;13040813886&quot;, &quot;servstar&quot;: 4, &quot;realstar&quot;: 4, &quot;profstar&quot;: 4, &quot;certificated&quot;: false &#125; ]&#125; 新增房地产经理人 - POST /api/agents/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：需要登录且拥有管理员权限，用户身份令牌在请求头中提供。 请求参数： 参数名 类型 是否必填 参数位置 说明 name 字符串 是 消息体 经理人姓名 tel 字符串 是 消息体 经理人手机 servstar 整数 否 消息体 默认值0 realstar 整数 否 消息体 默认值0 profstar 整数 否 消息体 默认值0 certificated 整数 否 消息体 默认值0 token 字符串 是 请求头 用户身份认证令牌 响应信息： 新增成功 - 状态码201： 12345678910&#123; &quot;agentid&quot;: 8, &quot;estates&quot;: [], &quot;name&quot;: &quot;孙小美&quot;, &quot;tel&quot;: &quot;13800991234&quot;, &quot;servstar&quot;: 0, &quot;realstar&quot;: 0, &quot;profstar&quot;: 0, &quot;certificated&quot;: false&#125; 未提供身份认证信息 - 状态码401： 123&#123; &quot;detail&quot;: &quot;不正确的身份认证信息。&quot;&#125; 当前用户没有操作权限 - 状态码403： 123&#123; &quot;detail&quot;: &quot;您没有执行该操作的权限。&quot;&#125; 编辑房地产经理人信息 - PUT /api/agents/&#123;房地产经理人编号&#125;/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：需要登录且拥有管理员权限，用户身份令牌在请求头中提供。 请求参数： 参数名 类型 是否必填 参数位置 说明 name 字符串 是 消息体 经理人姓名 tel 字符串 是 消息体 经理人手机 servstar 整数 否 消息体 默认值0 realstar 整数 否 消息体 默认值0 profstar 整数 否 消息体 默认值0 certificated 整数 否 消息体 默认值0 token 字符串 是 请求头 用户身份认证令牌 响应信息： 更新成功 - 状态码200： 1234567891011121314151617181920212223242526&#123; &quot;agentid&quot;: 1, &quot;estates&quot;: [ &#123; &quot;estateid&quot;: 1, &quot;name&quot;: &quot;今日家园&quot;, &quot;hot&quot;: 20 &#125;, &#123; &quot;estateid&quot;: 2, &quot;name&quot;: &quot;翡翠园&quot;, &quot;hot&quot;: 30 &#125;, &#123; &quot;estateid&quot;: 3, &quot;name&quot;: &quot;万科城市花园&quot;, &quot;hot&quot;: 22 &#125; ], &quot;name&quot;: &quot;袁晓梦&quot;, &quot;tel&quot;: &quot;158173555285&quot;, &quot;servstar&quot;: 5, &quot;realstar&quot;: 4, &quot;profstar&quot;: 3, &quot;certificated&quot;: true&#125; 未提供身份认证信息 - 状态码403 - 与新增类同 当前用户没有操作权限 - 状态码403 - 与新增类同 删除房地产经理人 - DELETE /api/agents/&#123;房地产经理人编号&#125;/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用说明：暂无。 请求参数： 参数名 类型 是否必填 参数位置 说明 token 字符串 是 请求头 用户身份认证令牌 响应信息： 删除成功 - 状态码204 未提供身份认证信息 - 状态码403 - 与新增类同 当前用户没有操作权限 - 状态码403 - 与新增类同 分页获取楼盘信息 - GET /api/estates/ 开发者：骆昊 版本号：v1 接口说明：经理人姓名按照前缀模糊匹配的方式进行处理；经理人服务星级是指经理人服务星级不得低于该星级；经理人是否持证只有0（未持证上岗）和1（持证上岗）两个选项。三个参数代表的筛选条件之间是而且的关系。返回结果为分页之后的房地产经理人信息。 使用帮助：暂无。 请求参数： 参数名 类型 是否必填 参数位置 说明 name 字符串 否 查询参数 楼盘名（模糊匹配） dist 字符串 否 查询参数 楼盘所在地区编号 page 整数 否 查询参数 页码，默认值为1 size 整数 否 查询参数 页面大小，默认值为5，最大值不超过50 响应信息： 12345678910111213141516171819202122232425262728293031&#123; &quot;count&quot;: 16, &quot;next&quot;: &quot;https://120.77.222.217/api/estates/?page=2&quot;, &quot;previous&quot;: null, &quot;results&quot;: [ &#123; &quot;estateid&quot;: 6, &quot;district&quot;: &#123; &quot;distid&quot;: 440303, &quot;name&quot;: &quot;罗湖区&quot; &#125;, &quot;agents&quot;: [ &#123; &quot;agentid&quot;: 2, &quot;name&quot;: &quot;杨伟&quot;, &quot;tel&quot;: &quot;13352939550&quot;, &quot;servstar&quot;: 3 &#125;, &#123; &quot;agentid&quot;: 4, &quot;name&quot;: &quot;郭志鹏&quot;, &quot;tel&quot;: &quot;13686810707&quot;, &quot;servstar&quot;: 4 &#125; ], &quot;name&quot;: &quot;幸福里&quot;, &quot;hot&quot;: 300, &quot;intro&quot;: &quot;&quot; &#125; ]&#125; 新增楼盘 - POST /api/estates/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：需要登录且拥有管理员权限，用户身份令牌在请求头中提供。 请求参数： 参数名 类型 是否必填 参数位置 说明 name 字符串 是 消息体 楼盘名称 hot 整数 否 消息体 楼盘热度，默认值0 intro 字符串 否 消息体 楼盘介绍，默认值空字符串 distid 整数 是 消息体 楼盘所在地区编号 token 字符串 是 请求头 用户身份认证令牌 响应信息： 新增成功 - 状态码201： 1234567&#123; &quot;estateid&quot;: 17, &quot;district&quot;: 510107, &quot;name&quot;: &quot;世纪锦苑&quot;, &quot;hot&quot;: 100, &quot;intro&quot;: &quot;&quot;&#125; 未提供身份认证信息 - 状态码403： 123&#123; &quot;detail&quot;: &quot;请提供有效的身份认证信息&quot;&#125; 当前用户没有操作权限 - 状态码403： 123&#123; &quot;detail&quot;: &quot;You do not have permission to perform this action.&quot;&#125; 编辑楼盘信息 - PUT /api/estates/&#123;楼盘编号&#125; 删除楼盘信 - DELETE /api/estates/&#123;楼盘编号&#125; 获取所有户型信息 - GET /api/housetypes/ 新增户型 - POST /api/housetypes/ 编辑户型信息 - PUT /api/housetypes/&#123;户型编号&#125; 删除户型 - DELETE /api/housetypes/&#123;户型编号&#125; 分页获取房源信息 - GET /api/houseinfos/ 开发者：骆昊 版本号：v1 接口说明：暂无。 使用帮助：暂无。 请求参数： 参数名 类型 是否必填 参数位置 说明 title 字符串 否 查询参数 房源标题关键词 dist 整数 否 查询参数 楼盘所在地区编号 min_price 整数 否 查询参数 价格区间下限 max_price 整数 否 查询参数 价格区间上限 type 整数 否 查询参数 户型编号 page 整数 否 查询参数 页码，默认值为1 size 整数 否 查询参数 页面大小，默认值为5，最大值不超过50 响应信息： 12345678&#123; &quot;count&quot;: 7, &quot;next&quot;: &quot;http://localhost:8000/api/houseinfos/?dist=440303&amp;page=2&quot;, &quot;previous&quot;: null, &quot;results&quot;: [ ]&#125; 查看房源详情 - GET /api/houseinfos/&#123;房源编号&#125; 新增房源 - POST /api/houseinfos/ 编辑房源信息 - PUT /api/houseinfos/&#123;房源编号&#125; 删除房源 - DELETE /api/houseinfos/&#123;房源编号&#125; 随机获取指定数量的房源标签 - GET /api/tags/ 分页查看房源标签 - GET /api/tags/ 新增房源标签 - POST /api/tags/ 删除房源标签 - DELETE /api/tags/&#123;房源编号&#125; 查看房源的图片 - GET /api/houseinfos/&#123;房源编号&#125;/photos/ 为房源添加图片 - POST /api/houseinfos/&#123;房源编号&#125;/photos/ 删除房源图片 - DELETE /api/houseinfos/&#123;房源编号&#125;/photos/&#123;图片编号&#125;","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/我为什么选择了Python","date":"2024-12-12T08:38:03.463Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/我为什么选择了Python/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E6%8B%A9%E4%BA%86Python/","excerpt":"","text":"我为什么选择了Python目前，Python语言的发展势头在国内国外都是不可阻挡的，Python凭借其简单优雅的语法，强大的生态圈从众多语言中脱颖而出，如今已经是稳坐编程语言排行榜前三的位置。国内很多Python开发者都是从Java开发者跨界过来的，我自己也不例外。我简单的跟大家交代一下，我为什么选择了Python。 Python vs. Java我们通过几个例子来比较一下，做同样的事情Java和Python的代码都是怎么写的。 例子1：在终端中输出“hello, world”。 Java代码： 123456class Test &#123; public static void main(String[] args) &#123; System.out.println(&quot;hello, world&quot;); &#125;&#125; Python代码： 1print(&#x27;hello, world&#x27;) 例子2：从1到100求和。 Java代码： 12345678910class Test &#123; public static void main(String[] args) &#123; int total = 0; for (int i = 1; i &lt;= 100; i += 1) &#123; total += i; &#125; System.out.println(total); &#125;&#125; Python代码： 1print(sum(range(1, 101))) 例子3：双色球随机选号。 Java代码： 1234567891011121314151617181920212223242526272829303132333435363738import java.util.List;import java.util.ArrayList;import java.util.Collections;class Test &#123; /** * 产生[min, max)范围的随机整数 */ public static int randomInt(int min, int max) &#123; return (int) (Math.random() * (max - min) + min); &#125; public static void main(String[] args) &#123; // 初始化备选红色球 List&lt;Integer&gt; redBalls = new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= 33; ++i) &#123; redBalls.add(i); &#125; List&lt;Integer&gt; selectedBalls = new ArrayList&lt;&gt;(); // 选出六个红色球 for (int i = 0; i &lt; 6; ++i) &#123; selectedBalls.add(redBalls.remove(randomInt(0, redBalls.size()))); &#125; // 对红色球进行排序 Collections.sort(selectedBalls); // 添加一个蓝色球 selectedBalls.add(randomInt(1, 17)); // 输出选中的随机号码 for (int i = 0; i &lt; selectedBalls.size(); ++i) &#123; System.out.printf(&quot;%02d &quot;, selectedBalls.get(i)); if (i == selectedBalls.size() - 2) &#123; System.out.print(&quot;| &quot;); &#125; &#125; System.out.println(); &#125;&#125; Python代码： 12345678910111213141516from random import randint, sample# 初始化备选红色球red_balls = [x for x in range(1, 34)]# 选出六个红色球selected_balls = sample(red_balls, 6)# 对红色球进行排序selected_balls.sort()# 添加一个蓝色球selected_balls.append(randint(1, 16))# 输出选中的随机号码for index, ball in enumerate(selected_balls): print(&#x27;%02d&#x27; % ball, end=&#x27; &#x27;) if index == len(selected_balls) - 2: print(&#x27;|&#x27;, end=&#x27; &#x27;)print() 相信，看完这些例子后，你一定感受到了我选择了Python是有道理的。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/常见反爬策略及应对方案","date":"2024-12-12T08:38:03.461Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/常见反爬策略及应对方案/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E5%B8%B8%E8%A7%81%E5%8F%8D%E7%88%AC%E7%AD%96%E7%95%A5%E5%8F%8A%E5%BA%94%E5%AF%B9%E6%96%B9%E6%A1%88/","excerpt":"","text":"常见反爬策略及应对方案 构造合理的HTTP请求头。 Accept User-Agent Referer Accept-Encoding Accept-Language 检查网站生成的Cookie。 有用的插件：EditThisCookie 如何处理脚本动态生成的Cookie 抓取动态内容。 Selenium + WebDriver Chrome &#x2F; Firefox - Driver 限制爬取的速度。 处理表单中的隐藏域。 在读取到隐藏域之前不要提交表单 用RoboBrowser这样的工具辅助提交表单 处理表单中的验证码。 OCR（Tesseract） - 商业项目一般不考虑 专业识别平台 - 超级鹰 &#x2F; 云打码 123456789101112131415161718192021222324252627282930313233from hashlib import md5class ChaoClient(object): def __init__(self, username, password, soft_id): self.username = username password = password.encode(&#x27;utf-8&#x27;) self.password = md5(password).hexdigest() self.soft_id = soft_id self.base_params = &#123; &#x27;user&#x27;: self.username, &#x27;pass2&#x27;: self.password, &#x27;softid&#x27;: self.soft_id, &#125; self.headers = &#123; &#x27;Connection&#x27;: &#x27;Keep-Alive&#x27;, &#x27;User-Agent&#x27;: &#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;, &#125; def post_pic(self, im, codetype): params = &#123; &#x27;codetype&#x27;: codetype, &#125; params.update(self.base_params) files = &#123;&#x27;userfile&#x27;: (&#x27;captcha.jpg&#x27;, im)&#125; r = requests.post(&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;, data=params, files=files, headers=self.headers) return r.json()if __name__ == &#x27;__main__&#x27;: client = ChaoClient(&#x27;用户名&#x27;, &#x27;密码&#x27;, &#x27;软件ID&#x27;) with open(&#x27;captcha.jpg&#x27;, &#x27;rb&#x27;) as file: print(client.post_pic(file, 1902)) 绕开“陷阱”。 网页上有诱使爬虫爬取的爬取的隐藏链接（陷阱或蜜罐） 通过Selenium+WebDriver+Chrome判断链接是否可见或在可视区域 隐藏身份。 代理服务 - 快代理 &#x2F; 讯代理 &#x2F; 芝麻代理 &#x2F; 蘑菇代理 &#x2F; 云代理 《爬虫代理哪家强？十大付费代理详细对比评测出炉！》 洋葱路由 - 国内需要翻墙才能使用 123456yum -y install toruseradd admin -d /home/adminpasswd adminchown -R admin:admin /home/adminchown -R admin:admin /var/run/tortor","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/一个小例子助你彻底理解协程","date":"2024-12-12T08:38:03.457Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/一个小例子助你彻底理解协程/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E4%B8%80%E4%B8%AA%E5%B0%8F%E4%BE%8B%E5%AD%90%E5%8A%A9%E4%BD%A0%E5%BD%BB%E5%BA%95%E7%90%86%E8%A7%A3%E5%8D%8F%E7%A8%8B/","excerpt":"","text":"一个小例子助你彻底理解协程协程，可能是Python中最让初学者困惑的知识点之一，它也是Python中实现并发编程的一种重要方式。Python中可以使用多线程和多进程来实现并发，这两种方式相对来说是大家比较熟悉的。事实上，还有一种实现并发的方式叫做异步编程，而协程就是实现异步编程的必要方式。 所谓协程，可以简单的理解为多个相互协作的子程序。在同一个线程中，当一个子程序阻塞时，我们可以让程序马上从一个子程序切换到另一个子程序，从而避免CPU因程序阻塞而闲置，这样就可以提升CPU的利用率，相当于用一种协作的方式加速了程序的执行。所以，我们可以言简意赅的说：协程实现了协作式并发。 接下来用一个小例子帮助大家理解什么是协作式并发，先看看下面的代码。 12345678910import timedef display(num): time.sleep(1) print(num)for num in range(10): display(num) 上面这段代码相信大家很容看懂，程序会输出0到9的数字，每隔1秒中输出一个数字，因此整个程序的执行需要大约10秒时间。值得注意的是，因为没有使用多线程或多进程，程序中只有一个执行单元，而time.sleep(1)的休眠操作会让整个线程停滞1秒钟，对于上面的代码来说，在这段时间里面CPU是完全闲置的没有做什么事情。 我们再来看看使用协程会发生什么事情。从Python 3.5开始，使用协程实现协作式编发有了更为便捷的语法，我们可以使用async来定义异步函数，可以使用await让一个阻塞的子程序将CPU让给与它协作的子程序。在Python 3.7中，asyanc和await成为了正式的关键字，让开发者有一种喜大普奔的感觉。我们先看看如何定义一个异步函数。 123456import asyncioasync def display(num): await asyncio.sleep(1) print(num) 接下来敲黑板说重点。异步函数不同于普通函数，调用普通函数会得到返回值，而调用异步函数会得到一个协程对象。我们需要将协程对象放到一个事件循环中才能达到与其他协程对象协作的效果，因为事件循环会负责处理子程序切换的操作，简单的说就是让阻塞的子程序让出CPU给可以执行的子程序。 我们先通过下面的列表生成式来代码10个协程对象，跟刚才在循环中调用display函数的道理一致。 1coroutines = [display(num) for num in range(10)] 通过下面的代码可以获取事件循环并将协程对象放入事件循环中。 123loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(coroutines))loop.close() 执行上面的代码会发现，10个分别会阻塞1秒钟的协程总共只阻塞了约1秒种的时间，这就说明协程对象一旦阻塞会将CPU让出而不是让CPU处于闲置状态，这样就大大的提升了CPU的利用率。而且我们还会注意到，0到9的数字并不是按照我们创建协程对象的顺序打印出来的，这正是我们想要的结果啊；另外，多次执行该程序会发现每次输出的结果都不太一样，这正是并发程序本身执行顺序不确定性造成的结果。 上面的例子来自于著名的“花书”（《Python高级并发编程》），为了让大家对协程的体会更加深刻，对原书的代码做了小的改动，这个例子虽然简单，但是它已经让你体会到了协作式并发的魅力。在商业项目中，如果需要使用协作式并发，还可以将系统默认的事件循环替换为uvloop提供的事件循环，这样会获得更好的性能，因为uvloop是基于著名的跨平台异步I&#x2F;O库libuv实现的。另外，如果要做基于HTTP的网络编程，三方库aiohttp是不错的选择，它基于asyncio实现了异步的HTTP服务器和客户端。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/Python编程惯例","date":"2024-12-12T08:38:03.456Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/Python编程惯例/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E7%BC%96%E7%A8%8B%E6%83%AF%E4%BE%8B/","excerpt":"","text":"Python编程惯例“惯例”这个词指的是“习惯的做法，常规的办法，一贯的做法”，与这个词对应的英文单词叫“idiom”。由于Python跟其他很多编程语言在语法和使用上还是有比较显著的差别，因此作为一个Python开发者如果不能掌握这些惯例，就无法写出“Pythonic”的代码。下面我们总结了一些在Python开发中的惯用的代码。 让代码既可以被导入又可以被执行。 1if __name__ == &#x27;__main__&#x27;: 用下面的方式判断逻辑“真”或“假”。 12if x:if not x: 好的代码： 12345name = &#x27;jackfrued&#x27;fruits = [&#x27;apple&#x27;, &#x27;orange&#x27;, &#x27;grape&#x27;]owners = &#123;&#x27;1001&#x27;: &#x27;骆昊&#x27;, &#x27;1002&#x27;: &#x27;王大锤&#x27;&#125;if name and fruits and owners: print(&#x27;I love fruits!&#x27;) 不好的代码： 12345name = &#x27;jackfrued&#x27;fruits = [&#x27;apple&#x27;, &#x27;orange&#x27;, &#x27;grape&#x27;]owners = &#123;&#x27;1001&#x27;: &#x27;骆昊&#x27;, &#x27;1002&#x27;: &#x27;王大锤&#x27;&#125;if name != &#x27;&#x27; and len(fruits) &gt; 0 and owners != &#123;&#125;: print(&#x27;I love fruits!&#x27;) 善于使用in运算符。 12if x in items: # 包含for x in items: # 迭代 好的代码： 123name = &#x27;Hao LUO&#x27;if &#x27;L&#x27; in name: print(&#x27;The name has an L in it.&#x27;) 不好的代码： 123name = &#x27;Hao LUO&#x27;if name.find(&#x27;L&#x27;) != -1: print(&#x27;This name has an L in it!&#x27;) 不使用临时变量交换两个值。 1a, b = b, a 用序列构建字符串。 好的代码： 123chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;.join(chars)print(name) # jackfrued 不好的代码： 12345chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;for char in chars: name += charprint(name) # jackfrued EAFP优于LBYL。 EAFP - Easier to Ask Forgiveness than Permission. LBYL - Look Before You Leap. 好的代码： 123456d = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;try: value = int(d[&#x27;x&#x27;]) print(value)except (KeyError, TypeError, ValueError): value = None 不好的代码： 1234567d = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;if &#x27;x&#x27; in d and isinstance(d[&#x27;x&#x27;], str) \\ and d[&#x27;x&#x27;].isdigit(): value = int(d[&#x27;x&#x27;]) print(value)else: value = None 使用enumerate进行迭代。 好的代码： 123fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]for index, fruit in enumerate(fruits): print(index, &#x27;:&#x27;, fruit) 不好的代码： 12345fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]index = 0for fruit in fruits: print(index, &#x27;:&#x27;, fruit) index += 1 用生成式生成列表。 好的代码： 123data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10]print(result) # [60, 45, 33] 不好的代码： 123456data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3)print(result) # [60, 45, 33] 用zip组合键和值来创建字典。 好的代码： 1234keys = [&#x27;1001&#x27;, &#x27;1002&#x27;, &#x27;1003&#x27;]values = [&#x27;骆昊&#x27;, &#x27;王大锤&#x27;, &#x27;白元芳&#x27;]d = dict(zip(keys, values))print(d) 不好的代码： 123456keys = [&#x27;1001&#x27;, &#x27;1002&#x27;, &#x27;1003&#x27;]values = [&#x27;骆昊&#x27;, &#x27;王大锤&#x27;, &#x27;白元芳&#x27;]d = &#123;&#125;for i, key in enumerate(keys): d[key] = values[i]print(d) 说明：这篇文章的内容来自于网络，有兴趣的读者可以阅读原文。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/Python数据分析师面试题","date":"2024-12-12T08:38:03.453Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/Python数据分析师面试题/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"Python数据分析师面试题基础知识部分编程能力部分商业项目部分 近期公司的X指标出现了明显的下滑，说说你会如果系统化的分析指标下滑的原因。 公司对App进行了版本迭代，对X功能做出了调整，请说明你会如何评估改版的效果。 公司对App做了一次营销拉新活动，请说明你会如何评估本次拉新活动的效果。 请说说你在设计数据报表时一般会考虑哪些问题。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/Python容器使用小技巧","date":"2024-12-12T08:38:03.450Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/Python容器使用小技巧/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8%E5%B0%8F%E6%8A%80%E5%B7%A7/","excerpt":"","text":"Python容器类型使用小技巧Python中提供了非常丰富的容器型数据类型，大家最为熟悉的有list、tuple、set、dict等。下面为大家分享一些使用这些类型的小技巧，希望帮助大家写出更加Pythonic的代码。 从字典中取最大假设字典对象对应的变量名为my_dict。 取出最大值 1max(my_dict.values()) 取值最大值的键 1max(my_dict, key=my_dict.get) 取出最大值的键和值 1max(my_dict.items(), key=lambda x: x[1]) 或 123import operatormax(my_dict.items(), key=operator.itemgetter(1)) 说明：上面用到了operator模块的itemgetter函数，这个函数的的作用如下所示。在上面的代码中，itemgetter帮我们获取到了二元组中的第2个元素。 123456789def itemgetter(*items): if len(items) == 1: item = items[0] def g(obj): return obj[item] else: def g(obj): return tuple(obj[item] for item in items) return g 统计列表元素出现次数假设列表对象对应的变量名为my_list。 1&#123;x: my_list.count(x) for x in set(my_list)&#125; 或 123from itertools import groupby&#123;key: len(list(group)) for key, group in groupby(sorted(my_list))&#125; 说明：groupby函数会将相邻相同元素分到一个组中，所以先用sorted函数排序就是为了将相同的元素放到一起。 或 123from collections import Counterdict(Counter(my_list)) 截断列表元素假设列表对象对应的变量名为my_list，通常大家会想到用下面的方式来截断列表。 12my_list = my_list[:i]my_list = my_list[j:] 然而，更好的方式使用下面的操作，大家可以认真想想为什么。 12del my_list[i:]del my_list[:j] 按最长列表实现zip操作Python的内置函数zip可以产生一个生成器对象，该生成器对象将两个或多个可迭代对象的元素组装到一起，如下所示。 1list(zip(&#x27;abc&#x27;, [1, 2, 3, 4])) 执行上面的代码会得到一个如下所示的列表，相信大家也注意到了，列表中元素的个数是由zip函数中长度最小的可迭代对象决定的，所以下面的列表中只有3个元素。 1[(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 3)] 如果希望由zip函数中长度最大的可迭代对象来决定最终迭代出的元素个数，可以试一试itertools模块的zip_longest函数，其用法如下所示。 123from itertools import zip_longestlist(zip_longest(&#x27;abc&#x27;, [1, 2, 3, 4])) 上面的代码创建出的列表对象如下所示。 1[(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 3), (None, 4)] 快速拷贝一个列表如果希望快速拷贝一个列表对象，可以通过切片操作来实现，但是切片操作仅实现了浅拷贝，简单的说就是切片创建了新的列表对象，但是新列表中的元素是和之前的列表共享的。如果希望实现深拷贝，可以使用copy模块的deepcopy函数。 浅拷贝 1thy_list = my_list[:] 或 123import copythy_list = copy.copy(my_list) 深拷贝 123import copythy_list = copy.deepcopy(my_list) 对两个或多个列表对应元素进行操作Python内置函数中的map函数可以对一个可迭代对象中的元素进行“映射”操作，这个函数在批量处理数据时非常有用。但是很多人都不知道，这个函数还可以作用于多个可迭代对象，通过传入的函数对多个可迭代对象中的对应元素进行处理，如下所示。 123my_list = [11, 13, 15, 17]thy_list = [2, 4, 6, 8, 10]list(map(lambda x, y: x + y, my_list, thy_list)) 上面的操作会得到如下所示的列表。 1[13, 17, 21, 25] 当然，同样的操作也可以用zip函数配合列表生成式来完成。 123my_list = [11, 13, 15, 17]thy_list = [2, 4, 6, 8, 10][x + y for x, y in zip(my_list, thy_list)] 处理列表中的空值和零值假设列表对象对应的变量名为my_list，如果列表中有空值（None）和零值，我们可以用下面的方式去掉空值和零值。 1list(filter(bool, my_list)) 对应的列表生成式语法如下所示。 1[x for x in my_list if x] 从嵌套列表中抽取指定列假设my_list是一个如下所示的嵌套列表，该嵌套列表可以用来表示数学上的矩阵，如果要取出矩阵第一列的元素构成一个列表，我们可以这样写。 1234567my_list = [ [1, 1, 2, 2], [5, 6, 7, 8], [3, 3, 4, 4],]col1, *_ = zip(*my_list)list(col1) 这里我们会得到一个如下所示的列表，刚好是矩阵的第一列。 1[1, 5, 3] 以此类推，如果想取出矩阵第二列的元素构成一个列表，可以用如下所示的方法。 12_, col2, *_ = zip(*my_list)list(col2) 至此，如果要实现矩阵的转置操作，我们也可以按照上面的思路写出下面的代码。 1[list(x) for x in zip(*my_list)] 经过上面的操作，我们会得到如下所示的列表。 1234[[1, 5, 3], [1, 6, 3], [2, 7, 4], [2, 8, 4]] 小结不知道上面的内容有没有触及到大家的知识盲区，如果有的话欢迎在评论区留言讨论。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/Python参考书籍","date":"2024-12-12T08:38:03.448Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/Python参考书籍/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E5%8F%82%E8%80%83%E4%B9%A6%E7%B1%8D/","excerpt":"","text":"Python参考书籍入门读物 《Python基础教程》（Beginning Python From Novice to Professional） 《Python学习手册》（Learning Python） 《Python编程》（Programming Python） 《Python Cookbook》 《Python程序设计》（Python Programming: An Introduction to Computer Science） 《Modern Python Cookbook》 进阶读物 《Python核心编程》（Core Python Applications Programming） 《流畅的Python》（Fluent Python） 《Effective Python：编写高质量Python代码的59个有效方法》（Effective Python 59 Specific Ways to Write Better Python） 《Python设计模式》（Learning Python Design Patterns） 《Python高级编程》（Expert Python Programming） 《Python性能分析与优化》（Mastering Python High Performance） Web框架 《Django基础教程》（Tango with Django） 《轻量级Django》（Lightweight Django） 《Python Web开发：测试驱动方法》（Test-Driven Development with Python） 《Web Development with Django Cookbook》 《Test-Driven Development with Django》 《Django Project Blueprints 》 《Flask Web开发：基于Python的Web应用开发实战》（Flask Web Development: Developing Web Applications with Python） 《深入理解Flask》（Mastering Flask） 爬虫开发 《用Python写网络爬虫》（Web Scraping with Python） 《精通Python爬虫框架Scrapy》（Learning Scrapy） 《Python网络数据采集》（Web Scraping with Python） 《Python爬虫开发与项目实战》 《Python 3网络爬虫开发实战》 数据分析 《利用Python进行数据分析》（Python for Data Analysis） 《Python数据科学手册》（Python Data Science Handbook） 《Python金融大数据分析》（Python for Finance） 《Python数据可视化编程实战》（Python Data Visualization Cookbook） 《Python数据处理》（Data Wrangling with Python） 机器学习 《Python机器学习基础教程》（Introduction to Machine Learning with Python） 《Python机器学习实践指南》（Python Machine Learning Blueprints） 《Python Machine Learning Case Studies》 《Python机器学习实践：测试驱动的开发方法》（Thoughtful Machine Learning with Python A Test Driven Approach） 《Python机器学习经典实例》（Python Machine Learning Cookbook） 《TensorFlow：实战Google深度学习框架》","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/Python之禅的最佳翻译","date":"2024-12-12T08:38:03.446Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/Python之禅的最佳翻译/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/Python%E4%B9%8B%E7%A6%85%E7%9A%84%E6%9C%80%E4%BD%B3%E7%BF%BB%E8%AF%91/","excerpt":"","text":"Zen of Python（Python之禅） Beautiful is better than ugly. （优美比丑陋好） Explicit is better than implicit.（清晰比晦涩好） Simple is better than complex.（简单比复杂好） Complex is better than complicated.（复杂比错综复杂好） Flat is better than nested.（扁平比嵌套好） Sparse is better than dense.（稀疏比密集好） Readability counts.（可读性很重要） Special cases aren’t special enough to break the rules.（特殊情况也不应该违反这些规则） Although practicality beats purity.（但现实往往并不那么完美） Errors should never pass silently.（异常不应该被静默处理） Unless explicitly silenced.（除非你希望如此） In the face of ambiguity, refuse the temptation to guess.（遇到模棱两可的地方，不要胡乱猜测） There should be one– and preferably only one –obvious way to do it.（肯定有一种通常也是唯一一种最佳的解决方案） Although that way may not be obvious at first unless you’re Dutch.（虽然这种方案并不是显而易见的，因为你不是那个荷兰人[^1]） Now is better than never.（现在开始做比不做好） Although never is often better than *right* now.（不做比盲目去做好[^2]） If the implementation is hard to explain, it’s a bad idea.（如果一个实现方案难于理解，它通常不是一个好的方案） If the implementation is easy to explain, it may be a good idea.（如果一个实现方案易于理解，它很有可能是一个好的方案） Namespaces are one honking great idea – let’s do more of those!（命名空间非常有用，我们应当多加利用） [^1]:这里指的是 Python 之父 Guido van Rossumm。[^2]:极限编程中的YAGNI原则","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/番外篇/PEP8风格指南","date":"2024-12-12T08:38:03.444Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/番外篇/PEP8风格指南/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/PEP8%E9%A3%8E%E6%A0%BC%E6%8C%87%E5%8D%97/","excerpt":"","text":"PEP 8风格指南PEP是Python Enhancement Proposal的缩写，通常翻译为“Python增强提案”。每个PEP都是一份为Python社区提供的指导Python往更好的方向发展的技术文档，其中的第8号增强提案（PEP 8）是针对Python语言编订的代码风格指南。尽管我们可以在保证语法没有问题的前提下随意书写Python代码，但是在实际开发中，采用一致的风格书写出可读性强的代码是每个专业的程序员应该做到的事情，也是每个公司的编程规范中会提出的要求，这些在多人协作开发一个项目（团队开发）的时候显得尤为重要。我们可以从Python官方网站的PEP 8链接中找到该文档，下面我们对该文档的关键部分做一个简单的总结。 空格的使用 使用空格来表示缩进而不要用制表符（Tab）。这一点对习惯了其他编程语言的人来说简直觉得不可理喻，因为绝大多数的程序员都会用Tab来表示缩进，但是要知道Python并没有像C&#x2F;C++或Java那样的用花括号来构造一个代码块的语法，在Python中分支和循环结构都使用缩进来表示哪些代码属于同一个级别，鉴于此Python代码对缩进以及缩进宽度的依赖比其他很多语言都强得多。在不同的编辑器中，Tab的宽度可能是2、4或8个字符，甚至是其他更离谱的值，用Tab来表示缩进对Python代码来说可能是一场灾难。 和语法相关的每一层缩进都用4个空格来表示。 每行的字符数不要超过79个字符，如果表达式因太长而占据了多行，除了首行之外的其余各行都应该在正常的缩进宽度上再加上4个空格。 函数和类的定义，代码前后都要用两个空行进行分隔。 在同一个类中，各个方法之间应该用一个空行进行分隔。 二元运算符的左右两侧应该保留一个空格，而且只要一个空格就好。 标识符命名PEP 8倡导用不同的命名风格来命名Python中不同的标识符，以便在阅读代码时能够通过标识符的名称来确定该标识符在Python中扮演了怎样的角色（在这一点上，Python自己的内置模块以及某些第三方模块都做得并不是很好）。 变量、函数和属性应该使用小写字母来拼写，如果有多个单词就使用下划线进行连接。 类中受保护的实例属性，应该以一个下划线开头。 类中私有的实例属性，应该以两个下划线开头。 类和异常的命名，应该每个单词首字母大写。 模块级别的常量，应该采用全大写字母，如果有多个单词就用下划线进行连接。 类的实例方法，应该把第一个参数命名为self以表示对象自身。 类的类方法，应该把第一个参数命名为cls以表示该类自身。 表达式和语句在Python之禅（可以使用import this查看）中有这么一句名言：“There should be one– and preferably only one –obvious way to do it.”，翻译成中文是“做一件事应该有而且最好只有一种确切的做法”，这句话传达的思想在PEP 8中也是无处不在的。 采用内联形式的否定词，而不要把否定词放在整个表达式的前面。例如if a is not b就比if not a is b更容易让人理解。 不要用检查长度的方式来判断字符串、列表等是否为None或者没有元素，应该用if not x这样的写法来检查它。 就算if分支、for循环、except异常捕获等中只有一行代码，也不要将代码和if、for、except等写在一起，分开写才会让代码更清晰。 import语句总是放在文件开头的地方。 引入模块的时候，from math import sqrt比import math更好。 如果有多个import语句，应该将其分为三部分，从上到下分别是Python标准模块、第三方模块和自定义模块，每个部分内部应该按照模块名称的字母表顺序来排列。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/公开课/文档/第06次公开课-算法入门系列2-在水一方/算法入门系列2 - 在水一方","date":"2024-12-12T08:38:03.370Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/公开课/文档/第06次公开课-算法入门系列2-在水一方/算法入门系列2 - 在水一方/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E5%85%AC%E5%BC%80%E8%AF%BE/%E6%96%87%E6%A1%A3/%E7%AC%AC06%E6%AC%A1%E5%85%AC%E5%BC%80%E8%AF%BE-%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%972-%E5%9C%A8%E6%B0%B4%E4%B8%80%E6%96%B9/%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%972%20-%20%E5%9C%A8%E6%B0%B4%E4%B8%80%E6%96%B9/","excerpt":"","text":"算法入门系列2 - 在水一方在第一次的公开课中，我们讲到了穷举法。穷举法也被称为暴力搜索法，今天我们要讲的回溯法就是暴力搜索法的一种。接下来我们讲到的很多算法跟“递归”这个概念有或多或少的关系，所以我们先说说“递归”。 现实中的递归从前有座山，山里有座庙，庙里有个老和尚，正在给小和尚讲故事呢！故事是什么呢？从前有座山，山里有座庙，庙里有个老和尚，正在给小和尚讲故事呢！故事是什么呢？从前有座山，山里有座庙，庙里有个老和尚，正在给小和尚讲故事呢！故事是什么呢？…… 野比大雄在房间里，用时光电视看着未来的情况。电视画面中，野比大雄在房间里，用时光电视看着未来的情况。电视画面中，野比大雄在房间里，用时光电视看着未来的情况…… 阶乘的递归定义：$$0! &#x3D; 1$$，$$n!&#x3D;n*(n-1)!$$ ，使用被定义对象的自身来为其下定义称为递归定义。 德罗斯特效应是递归的一种视觉形式。图中女性手持的物体中有一幅她本人手持同一物体的小图片，进而小图片中还有更小的一幅她手持同一物体的图片…… 递归的应用在程序中，一个函数如果直接或者间接的调用了自身，我们就称之为递归函数。 写递归函数有两个要点： 收敛条件 - 什么时候结束递归。 递归公式 - 每一项与前一项（前N项）的关系。 例子1：求阶乘。 1234def fac(num): if num == 0: return 1 return num * fac(num - 1) Python对递归的深度加以了限制（默认1000层函数调用），如果想突破这个限制，可以使用下面的方法。 123import syssys.setrecursionlimit(10000) 例子2：爬楼梯 - 楼梯有n个台阶，一步可以走1阶、2阶或3阶，走完n个台阶共有多少种不同的走法。 123456def climb(num): if num == 0: return 1 elif num &lt; 0: return 0 return climb(num - 1) + climb(num - 2) + climb(num - 3) 注意：上面的递归函数性能会非常的差，因为时间复杂度是几何级数级的。 优化后的代码。 12345678910from functools import lru_cache@lru_cache()def climb(num): if num == 0: return 1 elif num &lt; 0: return 0 return climb(num - 1) + climb(num - 2) + climb(num - 3) 不使用的递归的代码。 12345def climb(num): a, b, c = 1, 2, 4 for _ in range(num - 1): a, b, c = b, c, a + b + c return a 重点：有更好的办法的时候，请不要考虑递归。 回溯法回溯法是暴力搜索法中的一种。对于某些计算问题而言，回溯法是一种可以找出所有（或一部分）解的一般性算法，尤其适用于约束满足问题（在解决约束满足问题时，我们逐步构造更多的候选解，并且在确定某一部分候选解不可能补全成正确解之后放弃继续搜索这个部分候选解本身及其可以拓展出的子候选解，转而测试其他的部分候选解）。 经典案例例子1：迷宫寻路。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&quot;&quot;&quot;迷宫寻路&quot;&quot;&quot;import randomimport sysWALL = -1ROAD = 0ROWS = 10COLS = 10def find_way(maze, i=0, j=0, step=1): &quot;&quot;&quot;走迷宫&quot;&quot;&quot; if 0 &lt;= i &lt; ROWS and 0 &lt;= j &lt; COLS and maze[i][j] == 0: maze[i][j] = step if i == ROWS - 1 and j == COLS - 1: print(&#x27;=&#x27; * 20) display(maze) sys.exit(0) find_way(maze, i + 1, j, step + 1) find_way(maze, i, j + 1, step + 1) find_way(maze, i - 1, j, step + 1) find_way(maze, i, j - 1, step + 1) maze[i][j] = ROADdef reset(maze): &quot;&quot;&quot;重置迷宫&quot;&quot;&quot; for i in range(ROWS): for j in range(COLS): num = random.randint(1, 10) maze[i][j] = WALL if num &gt; 7 else ROAD maze[0][0] = maze[ROWS - 1][COLS - 1] = ROADdef display(maze): &quot;&quot;&quot;显示迷宫&quot;&quot;&quot; for row in maze: for col in row: if col == -1: print(&#x27;■&#x27;, end=&#x27; &#x27;) elif col == 0: print(&#x27;□&#x27;, end=&#x27; &#x27;) else: print(f&#x27;&#123;col&#125;&#x27;.ljust(2), end=&#x27;&#x27;) print()def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; maze = [[0] * COLS for _ in range(ROWS)] reset(maze) display(maze) find_way(maze) print(&#x27;没有出路!!!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码用随机放置围墙的方式来生成迷宫，更好的生成迷宫的方式请参考《简单的使用回溯法生成 Tile Based 迷宫》一文。 例子2：骑士巡逻 - 国际象棋中的骑士（马），按照骑士的移动规则走遍整个棋盘的每一个方格，而且每个方格只能够经过一次。 123456789101112131415161718192021222324252627282930313233343536373839404142&quot;&quot;&quot;骑士巡逻&quot;&quot;&quot;import sysSIZE = 8def display(board): &quot;&quot;&quot;显示棋盘&quot;&quot;&quot; for row in board: for col in row: print(f&#x27;&#123;col&#125;&#x27;.rjust(2, &#x27;0&#x27;), end=&#x27; &#x27;) print()def patrol(board, i=0, j=0, step=1): &quot;&quot;&quot;巡逻&quot;&quot;&quot; if 0 &lt;= i &lt; SIZE and 0 &lt;= j &lt; SIZE and board[i][j] == 0: board[i][j] = step if step == SIZE * SIZE: display(board) sys.exit(0) patrol(board, i + 1, j + 2, step + 1) patrol(board, i + 2, j + 1, step + 1) patrol(board, i + 2, j - 1, step + 1) patrol(board, i + 1, j - 2, step + 1) patrol(board, i - 1, j - 2, step + 1) patrol(board, i - 2, j - 1, step + 1) patrol(board, i - 2, j + 1, step + 1) patrol(board, i - 1, j + 2, step + 1) board[i][j] = 0def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; board = [[0] * SIZE for _ in range(SIZE)] patrol(board)if __name__ == &#x27;__main__&#x27;: main() 例子3：八皇后 - 如何能够在8×8的国际象棋棋盘上放置八个皇后，使得任何一个皇后都无法直接吃掉其他的皇后？为了达到此目的，任两个皇后都不能处于同一条横行、纵行或斜线上。 说明：这个问题太经典了，网上有大把的答案，留给大家自己搞定。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/公开课/文档/第05次公开课-算法入门系列1-周而复始/算法入门系列1-周而复始","date":"2024-12-12T08:38:03.337Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/公开课/文档/第05次公开课-算法入门系列1-周而复始/算法入门系列1-周而复始/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E5%85%AC%E5%BC%80%E8%AF%BE/%E6%96%87%E6%A1%A3/%E7%AC%AC05%E6%AC%A1%E5%85%AC%E5%BC%80%E8%AF%BE-%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%971-%E5%91%A8%E8%80%8C%E5%A4%8D%E5%A7%8B/%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%971-%E5%91%A8%E8%80%8C%E5%A4%8D%E5%A7%8B/","excerpt":"","text":"算法入门系列课程1 - 周而复始算法概述 什么是算法？ 解决问题的正确方法和具体的实施步骤。 例子1：如何在两栋相距50m的大楼的两个房间牵一条线（两个房间都有窗）？ 养一只鸟（如鸽子），将线送过去 用很长的杆子将线递过去 用无人机（遥控飞行器）将线送过去 如何评价这些方法的好坏？少花钱，不费事！ 例子2：大教室里坐了几百名学生一起听课，如何快速的统计学生人数？ 例子3：向列表容器中逆向插入100000个元素。 方法1： 1234nums = []for i in range(100000): nums.append(i)nums.reverse() 方法2： 123nums = []for i in range(100000): nums.insert(0, i) 例子3：生成Fibonacci数列（前100个Fibonacci数）。 方法1 - 递推： 1234a, b = 0, 1for num in range(1, 101): a, b = b, a + b print(f&#x27;&#123;num&#125;: &#123;a&#125;&#x27;) 方法2 - 递归： 12345678def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2)for num in range(1, 101): print(f&#x27;&#123;num&#125;: &#123;fib(num)&#125;&#x27;) 方法3 - 改进的递归： 123456def fib(num, temp=&#123;&#125;): if num in (1, 2): return 1 elif num not in temp: temp[num] = fib(num - 1) + fib(num - 2) return temp[num] 方法4 - 改进的递归： 12345678from functools import lru_cache@lru_cache()def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2) 如何评价算法的好坏？ 渐近时间复杂度和渐近空间复杂度。 大O符号的意义？ 表示一个函数相对于输入规模的增长速度，也可以称之为函数的数量级。 大O符号 说明 例子 $$O(c)$$ 常量时间复杂度 布隆过滤器 &#x2F; 哈希存储 $$O(log_2n)$$ 对数时间复杂度 二分查找（折半查找） $$O(n)$$ 线性时间复杂度 顺序查找 &#x2F; 桶排序 $$O(n*log_2n)$$ 对数线性时间复杂度 高级排序算法（归并排序、快速排序） $$O(n^2)$$ 平方时间复杂度 简单排序算法（选择排序、插入排序、冒泡排序） $$O(n^3)$$ 立方时间复杂度 Floyd算法 &#x2F; 矩阵乘法运算 $$O(2^n)$$ 几何级数时间复杂度 汉诺塔 $$O(n!)$$ 阶乘时间复杂度 旅行经销商问题 穷举法在计算机科学中，穷举法或者暴力搜索法是一个非常非常直观的解决问题的方法，这种方法通过一项一项的列举解决方案所有可能的候选项以及检查每个候选项是否符合问题的描述，最终得到问题的解。 虽然暴力搜索很容易实现，并且如果解决方案存在它就一定能够找到，但是它的代价是和候选方案的数量成比例的，由于这一点，在很多实际问题中，消耗的代价会随着问题规模的增加而快速地增长。因此，当问题规模有限或当存在可用于将候选解决方案的集合减少到可管理大小时，就可以使用暴力搜索。另外，当实现方法的简单度比速度更重要的时候，也可以考虑使用这种方法。 经典例子 百钱百鸡问题：公鸡5元一只，母鸡3元一只，小鸡1元三只，用100元买一百只鸡，问公鸡、母鸡、小鸡各有多少只？ 12345for x in range(21): for y in range(34): z = 100 - x - y if z % 3 == 0 and 5 * x + 3 * y + z // 3 == 100: print(x, y, z) 五人分鱼问题：ABCDE五人在某天夜里合伙捕鱼，最后疲惫不堪各自睡觉。第二天A第一个醒来，他将鱼分为5份，扔掉多余的1条，拿走了属于自己的一份；B第二个醒来，也将鱼分为5份，扔掉多余的1条，拿走属于自己的一份；然后C、D、E依次醒来，也按同样的方式分鱼，问他们至少捕了多少条鱼？ 1234567891011121314fish = 6while True: total = fish enough = True for _ in range(5): if (total - 1) % 5 == 0: total = (total - 1) // 5 * 4 else: enough = False break if enough: print(fish) break fish += 5 暴力破解口令： 12345678910111213import reimport PyPDF2with open(&#x27;Python_Tricks_encrypted.pdf&#x27;, &#x27;rb&#x27;) as pdf_file_stream: reader = PyPDF2.PdfFileReader(pdf_file_stream) with open(&#x27;dictionary.txt&#x27;, &#x27;r&#x27;) as txt_file_stream: file_iter = iter(lambda: txt_file_stream.readline(), &#x27;&#x27;) for word in file_iter: word = re.sub(r&#x27;\\s&#x27;, &#x27;&#x27;, word) if reader.decrypt(word): print(word) break","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/公开课/文档/第04次公开课-好玩的Python/好玩的Python","date":"2024-12-12T08:38:03.291Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/公开课/文档/第04次公开课-好玩的Python/好玩的Python/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E5%85%AC%E5%BC%80%E8%AF%BE/%E6%96%87%E6%A1%A3/%E7%AC%AC04%E6%AC%A1%E5%85%AC%E5%BC%80%E8%AF%BE-%E5%A5%BD%E7%8E%A9%E7%9A%84Python/%E5%A5%BD%E7%8E%A9%E7%9A%84Python/","excerpt":"","text":"好玩的Python因为下面的代码都非常简单，简单到直接使用Python的交互式环境就能完成。当然，官方Python自带的交互式环境比较难用，推荐大家使用ipython，可以使用下面的命令来安装ipython，安装成功后键入ipython命令就能进入交互式环境。 1pip install ipython 或 1pip3 install ipython ipython最直观的优点： 可以用?或者??来获取帮助。 可以用!调用系统命令。 可以使用Tab键自动补全。 可以使用魔法指令，如：%timeit。 没有工具用代码也能P图 安装pillow三方库。 PIL（Python Imaging Library）是Python平台事实上的图像处理标准库了。PIL功能非常强大，而API却非常简单易用。但是PIL仅支持到Python 2.7，而且很多年都没有人维护了，于是一群志愿者在PIL的基础上创建了兼容的版本，名字叫Pillow，除了支持Python 3.x还加入了很多有用且有趣的新特性。 1pip install pillow 或 1pip3 install pillow 加载图片。 1234from PIL import Imagechiling = Image.open(&#x27;chiling.jpg&#x27;)chiling.show() 使用滤镜。 1234from PIL import ImageFilterchiling.filter(ImageFilter.EMBOSS).show()chiling.filter(ImageFilter.CONTOUR).show() 图像剪裁和粘贴。 123456rect = 220, 690, 265, 740 watch = chiling.crop(rect)watch.show()blured_watch = watch.filter(ImageFilter.GaussianBlur(4))chiling.paste(blured_watch, (220, 690))chiling.show() 生成镜像。 12chiling2 = chiling.transpose(Image.FLIP_LEFT_RIGHT)chiling2.show() 生成缩略图。 123width, height = chiling.sizewidth, height = int(width * 0.4), int(height * 0.4)chiling.thumbnail((width, height)) 合成图片。 12345frame = Image.open(&#x27;frame.jpg&#x27;)frame.show()frame.paste(chiling, (210, 150))frame.paste(chiling2, (522, 150))frame.show() 上面的知识在Python-100-Days项目的第15天中也有对应的内容。 向微信好友群发祝福视频 安装itchat三方库。 itchat是一个开源的微信个人号接口，使用Python调用微信从未如此简单。 1pip install itchat 或 1pip3 install itchat 登录微信。 123import itchatitchat.auto_login() 说明：用自己的微信扫描屏幕上出现的二维码就完成了登录操作，登录之后才能获取自己的好友信息以及发送消息给自己的好友。 查找自己的朋友。 123456friends_list = itchat.get_friends(update=True)print(len(friends_list))luohao = friends_list[0]props = [&#x27;NickName&#x27;, &#x27;Signature&#x27;, &#x27;Sex&#x27;]for prop in props: print(luohao[prop]) 说明：friends_list相当于是一个列表，列表中的第一个元素是自己。 随机选出5个朋友，获得他们的用户名、昵称、签名。 123456lucky_friends = random.sample(friends_list[1:], 5) props = [&#x27;NickName&#x27;, &#x27;Signature&#x27;, &#x27;City&#x27;]for friend in lucky_friends: for prop in props: print(friend[prop] or &#x27;没有此项信息&#x27;) print(&#x27;-&#x27; * 80) 给朋友发送文字消息。 1itchat.send_msg(&#x27;急需一个红包来拯救堕落的灵魂！！！&#x27;, toUserName=&#x27;@8e06606db03f0e28d0ff884083f727e6&#x27;) 群发视频给幸运的朋友们。 1234lucky_friends = random.sample(friends_list[1:], 5) for friend in lucky_friends: username = friend[&#x27;UserName&#x27;] itchat.send_video(&#x27;/Users/Hao/Desktop/my_test_video.mp4&#x27;, toUserName=username) 利用itchat还能做很多事情，比如有好友给自己发了消息又撤回了，如果想查看这些被撤回的消息，itchat就可以做到（注册一个接收消息的钩子函数，请参考CSDN上的一篇文章）；再比如，有时候我们想知道某个好友有没有把我们删除或者拉入黑名单，也可以利用itchat封装的群聊功能，非好友和黑名单用户不会被拉入群聊，通过创建群聊函数的返回值就可以判定你和指定的人之间的关系。 不用客户端查看热点新闻 安装requests库。（点击常看官方文档） 1pip install requests 或 1pip3 install requests 爬取新闻数据或者通过API接口获取新闻数据。 123import requestsresp = requests.get(&#x27;http://api.tianapi.com/allnews/?key=请使用自己申请的Key&amp;col=7&amp;num=50&#x27;) 说明：上面使用了天行数据提供的数据接口，需要的话可以自行去天行数据的网站注册开通，调用接口的时候要填写注册成功后系统分配给你的key。 使用反序列化将JSON字符串解析为字典并获取新闻列表。 123import jsonnewslist = json.loads(resp.text)[&#x27;newslist&#x27;] 对新闻列表进行循环遍历，找到感兴趣的新闻，例如：华为。 123456for news in newslist: title = news[&#x27;title&#x27;] url = news[&#x27;url&#x27;] if &#x27;华为&#x27; in title: print(title) print(url) 调用短信网关发送短信到手机上，告知关注的新闻标题并给出链接。 1234567891011121314151617import repattern = re.compile(r&#x27;https*:\\/\\/[^\\/]*\\/(?P&lt;url&gt;.*)&#x27;) matcher = pattern.match(url)if matcher: url = matcher.group(&#x27;url&#x27;) resp = requests.post( url=&#x27;http://sms-api.luosimao.com/v1/send.json&#x27;, auth=(&#x27;api&#x27;, &#x27;key-请使用你自己申请的Key&#x27;), data=&#123; &#x27;mobile&#x27;: &#x27;13548041193&#x27;, &#x27;message&#x27;: f&#x27;发现一条您可能感兴趣的新闻 - &#123;title&#125;，详情点击https://news.china.com/&#123;url&#125; 查看。【Python小课】&#x27; &#125;, timeout=10, verify=False ) 说明：上面的代码使用了螺丝帽提供的短信网关服务，利用短信网关发送短信是需要支付费用的，但是一般的平台都会提供若干条免费的测试短信。发送短信必须遵守平台的规则，违规的短信是无法发送的。上面发短信时使用的短信模板（“发现一条您可能感兴趣的新闻 - ###，详情点击https://news.china.com/### 查看。”）和短信签名（“【Python小课】”）需要登录螺丝帽管理平台进行配置，如果不清楚如何配置，可以联系平台的客服人员进行咨询。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/公开课/文档/年薪50W+的Python程序员如何写代码/年薪50W+的Python程序员如何写代码","date":"2024-12-12T08:38:03.036Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/公开课/文档/年薪50W+的Python程序员如何写代码/年薪50W+的Python程序员如何写代码/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E5%85%AC%E5%BC%80%E8%AF%BE/%E6%96%87%E6%A1%A3/%E5%B9%B4%E8%96%AA50W+%E7%9A%84Python%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%86%99%E4%BB%A3%E7%A0%81/%E5%B9%B4%E8%96%AA50W+%E7%9A%84Python%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%86%99%E4%BB%A3%E7%A0%81/","excerpt":"","text":"年薪50W+的Python程序员如何写代码为什么要用Python写代码没有对比就没有伤害 很多互联网和移动互联网企业对开发效率的要求高于对执行效率的要求。 例子1：hello, worldC的版本： 123456#include &lt;stdio.h&gt;int main() &#123; printf(&quot;hello, world\\n&quot;); return 0;&#125; Java的版本： 123456class Example01 &#123; public static void main(String[] args) &#123; System.out.println(&quot;hello, world&quot;); &#125;&#125; Python的版本： 1print(&#x27;hello, world&#x27;) 例子2：1-100求和C的版本： 12345678910#include &lt;stdio.h&gt;int main() &#123; int total = 0; for (int i = 1; i &lt;= 100; ++i) &#123; total += i; &#125; printf(&quot;%d\\n&quot;, total); return 0;&#125; Python的版本： 1print(sum(range(1, 101))) 例子3：创建和初始化数组（列表）Java的版本： 12345678910111213141516import java.util.Arrays;public class Example03 &#123; public static void main(String[] args) &#123; boolean[] values = new boolean[10]; Arrays.fill(values, true); System.out.println(Arrays.toString(values)); int[] numbers = new int[10]; for (int i = 0; i &lt; numbers.length; ++i) &#123; numbers[i] = i + 1; &#125; System.out.println(Arrays.toString(numbers)); &#125;&#125; Python的版本： 1234values = [True] * 10print(values)numbers = [x for x in range(1, 11)]print(numbers) 例子4：双色球随机选号Java的版本： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.util.List;import java.util.ArrayList;import java.util.Collections;import java.util.Scanner;class Example03 &#123; /** * 产生[min, max)范围的随机整数 */ public static int randomInt(int min, int max) &#123; return (int) (Math.random() * (max - min) + min); &#125; /** * 输出一组双色球号码 */ public static void display(List&lt;Integer&gt; balls) &#123; for (int i = 0; i &lt; balls.size(); ++i) &#123; System.out.printf(&quot;%02d &quot;, balls.get(i)); if (i == balls.size() - 2) &#123; System.out.print(&quot;| &quot;); &#125; &#125; System.out.println(); &#125; /** * 生成一组随机号码 */ public static List&lt;Integer&gt; generate() &#123; List&lt;Integer&gt; redBalls = new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= 33; ++i) &#123; redBalls.add(i); &#125; List&lt;Integer&gt; selectedBalls = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 6; ++i) &#123; selectedBalls.add(redBalls.remove(randomInt(0, redBalls.size()))); &#125; Collections.sort(selectedBalls); selectedBalls.add(randomInt(1, 17)); return selectedBalls; &#125; public static void main(String[] args) &#123; try (Scanner sc = new Scanner(System.in)) &#123; System.out.print(&quot;机选几注: &quot;); int num = sc.nextInt(); for (int i = 0; i &lt; num; ++i) &#123; display(generate()); &#125; &#125; &#125;&#125; Python的版本： 123456789101112131415161718192021222324from random import randint, sampledef generate(): &quot;&quot;&quot;生成一组随机号码&quot;&quot;&quot; red_balls = [x for x in range(1, 34)] selected_balls = sample(red_balls, 6) selected_balls.sort() selected_balls.append(randint(1, 16)) return selected_ballsdef display(balls): &quot;&quot;&quot;输出一组双色球号码&quot;&quot;&quot; for index, ball in enumerate(balls): print(f&#x27;&#123;ball:0&gt;2d&#125;&#x27;, end=&#x27; &#x27;) if index == len(balls) - 2: print(&#x27;|&#x27;, end=&#x27; &#x27;) print()num = int(input(&#x27;机选几注: &#x27;))for _ in range(num): display(generate()) 温馨提示：珍爱生命，远离任何形式的赌博。 例子5：实现一个简单的HTTP服务器。Java的版本： 说明：JDK 1.6以前，需要通过套接字编程来实现，具体又可以分为多线程和NIO两种做法。JDK 1.6以后，可以使用com.sun.net.httpserver包提供的HttpServer类来实现。 12345678910111213141516171819202122232425262728import com.sun.net.httpserver.HttpExchange;import com.sun.net.httpserver.HttpHandler;import com.sun.net.httpserver.HttpServer;import java.io.IOException;import java.io.OutputStream;import java.net.InetSocketAddress;class Example05 &#123; public static void main(String[] arg) throws Exception &#123; HttpServer server = HttpServer.create(new InetSocketAddress(8000), 0); server.createContext(&quot;/&quot;, new RequestHandler()); server.start(); &#125; static class RequestHandler implements HttpHandler &#123; @Override public void handle(HttpExchange exchange) throws IOException &#123; String response = &quot;&lt;h1&gt;hello, world&lt;/h1&gt;&quot;; exchange.sendResponseHeaders(200, 0); try (OutputStream os = exchange.getResponseBody()) &#123; os.write(response.getBytes()); &#125; &#125; &#125;&#125; Python的版本： 12345678910111213from http.server import HTTPServer, SimpleHTTPRequestHandlerclass RequestHandler(SimpleHTTPRequestHandler): def do_GET(self): self.send_response(200) self.end_headers() self.wfile.write(&#x27;&lt;h1&gt;hello, world&lt;/h1&gt;&#x27;.encode())server = HTTPServer((&#x27;&#x27;, 8000), RequestHandler)server.serve_forever() 或 1python3 -m http.server 8000 一行Python代码可以做什么 很多时候，你的问题只需一行Python代码就能解决。 1234567891011121314# 一行代码实现求阶乘函数fac = lambda x: __import__(&#x27;functools&#x27;).reduce(int.__mul__, range(1, x + 1), 1)# 一行代码实现求最大公约数函数gcd = lambda x, y: y % x and gcd(y % x, x) or x# 一行代码实现判断素数的函数is_prime = lambda x: x &gt; 1 and not [f for f in range(2, int(x ** 0.5) + 1) if x % f == 0]# 一行代码实现快速排序quick_sort = lambda items: len(items) and quick_sort([x for x in items[1:] if x &lt; items[0]]) + [items[0]] + quick_sort([x for x in items[1:] if x &gt; items[0]]) or items# 生成FizzBuzz列表[&#x27;Fizz&#x27;[x % 3 * 4:] + &#x27;Buzz&#x27;[x % 5 * 4:] or x for x in range(1, 101)] 设计模式从未如此简单 Python是动态类型语言，大量的设计模式在Python中被简化或弱化。 思考：如何优化下面的代码。 1234def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2) 代理模式在Python中可以通过内置的或自定义的装饰器来实现。 123456789101112from functools import lru_cache@lru_cache()def fib(num): if num in (1, 2): return 1 return fib(num - 1) + fib(num - 2)for n in range(1, 121): print(f&#x27;&#123;n&#125;: &#123;fib(n)&#125;&#x27;) 说明：通过Python标准库functools模块的lru_cache装饰器为fib函数加上缓存代理，缓存函数执行的中间结果，优化代码的性能。 单例模式在Python中可以通过自定义的装饰器或元类来实现。 123456789101112131415from functools import wrapsfrom threading import RLockdef singleton(cls): instances = &#123;&#125; lock = RLock() @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: with lock: if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] 说明：需要实现单例模式的类只需要添加上面的装饰器即可。 原型模式在Python中可以通过元类来实现。 123456789import copyclass PrototypeMeta(type): def __init__(cls, *args, **kwargs): super().__init__(*args, **kwargs) cls.clone = lambda self, is_deep=True: \\ copy.deepcopy(self) if is_deep else copy.copy(self) 说明：通过元类给指定了metaclass=PrototypeMeta的类添加一个clone方法实现对象克隆，利用Python标准库copy模块的copy和deepcopy分别实现浅拷贝和深拷贝。 数据采集和数据分析从未如此简单 网络数据采集是Python最擅长的领域之一。 例子：获取豆瓣电影“Top250”。 123456789101112131415import randomimport timeimport requestsfrom bs4 import BeautifulSoupfor page in range(10): resp = requests.get( url=f&#x27;https://movie.douban.com/top250?start=&#123;25 * page&#125;&#x27;, headers=&#123;&#x27;User-Agent&#x27;: &#x27;BaiduSpider&#x27;&#125; ) soup = BeautifulSoup(resp.text, &quot;lxml&quot;) for elem in soup.select(&#x27;a &gt; span.title:nth-child(1)&#x27;): print(elem.text) time.sleep(random.random() * 5) 利用NumPy、Pandas、Matplotlib可以轻松实现数据分析和可视化。 写出Python代码的正确姿势 用Python写代码就要写出Pythonic的代码。 姿势1：选择结构的正确姿势跨界开发者的代码： 12345name = &#x27;jackfrued&#x27;fruits = [&#x27;apple&#x27;, &#x27;orange&#x27;, &#x27;grape&#x27;]owners = &#123;&#x27;name&#x27;: &#x27;骆昊&#x27;, &#x27;age&#x27;: 40, &#x27;gender&#x27;: True&#125;if name != &#x27;&#x27; and len(fruits) &gt; 0 and len(owners.keys()) &gt; 0: print(&#x27;Jackfrued love fruits.&#x27;) Pythonic的代码： 12345name = &#x27;jackfrued&#x27;fruits = [&#x27;apple&#x27;, &#x27;orange&#x27;, &#x27;grape&#x27;]owners = &#123;&#x27;name&#x27;: &#x27;骆昊&#x27;, &#x27;age&#x27;: 40, &#x27;gender&#x27;: True&#125;if name and fruits and owners: print(&#x27;Jackfrued love fruits.&#x27;) 姿势2：交换两个变量的正确姿势跨界开发者的代码： 123temp = aa = bb = temp 或 123a = a ^ bb = a ^ ba = a ^ b Pythonic的代码： 1a, b = b, a 姿势3：用序列组装字符串的正确姿势跨界开发者的代码： 1234chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;for char in chars: name += char Pythonic的代码： 12chars = [&#x27;j&#x27;, &#x27;a&#x27;, &#x27;c&#x27;, &#x27;k&#x27;, &#x27;f&#x27;, &#x27;r&#x27;, &#x27;u&#x27;, &#x27;e&#x27;, &#x27;d&#x27;]name = &#x27;&#x27;.join(chars) 姿势4：遍历列表的正确姿势跨界开发者的代码： 12345fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]index = 0for fruit in fruits: print(index, &#x27;:&#x27;, fruit) index += 1 Pythonic的代码： 123fruits = [&#x27;orange&#x27;, &#x27;grape&#x27;, &#x27;pitaya&#x27;, &#x27;blueberry&#x27;]for index, fruit in enumerate(fruits): print(index, &#x27;:&#x27;, fruit) 姿势5：创建列表的正确姿势跨界开发者的代码： 12345data = [7, 20, 3, 15, 11]result = []for i in data: if i &gt; 10: result.append(i * 3) Pythonic的代码： 12data = [7, 20, 3, 15, 11]result = [num * 3 for num in data if num &gt; 10] 姿势6：确保代码健壮性的正确姿势跨界开发者的代码： 1234567data = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;if &#x27;x&#x27; in data and isinstance(data[&#x27;x&#x27;], (str, int, float)) \\ and data[&#x27;x&#x27;].isdigit(): value = int(data[&#x27;x&#x27;]) print(value)else: value = None Pythonic的代码： 123456data = &#123;&#x27;x&#x27;: &#x27;5&#x27;&#125;try: value = int(data[&#x27;x&#x27;]) print(value)except (KeyError, TypeError, ValueError): value = None 使用Lint工具检查你的代码规范阅读下面的代码，看看你能看出哪些地方是有毛病的或者说不符合Python的编程规范的。 1234567891011121314151617181920212223242526272829303132333435363738from enum import *@uniqueclass Suite (Enum): SPADE, HEART, CLUB, DIAMOND = range(4)class Card(object): def __init__(self,suite,face ): self.suite = suite self.face = face def __repr__(self): suites=&#x27;♠♥♣♦&#x27; faces=[&#x27;&#x27;,&#x27;A&#x27;,&#x27;2&#x27;,&#x27;3&#x27;,&#x27;4&#x27;,&#x27;5&#x27;,&#x27;6&#x27;,&#x27;7&#x27;,&#x27;8&#x27;,&#x27;9&#x27;,&#x27;10&#x27;,&#x27;J&#x27;,&#x27;Q&#x27;,&#x27;K&#x27;] return f&#x27;&#123;suites[self.suite.value]&#125;&#123;faces[self.face]&#125;&#x27;import randomclass Poker(object): def __init__(self): self.cards =[Card(suite, face) for suite in Suite for face in range(1, 14)] self.current=0 def shuffle (self): self.current=0 random.shuffle(self.cards) def deal (self): card = self.cards[self.current] self.current+=1 return card def has_next (self): if self.current&lt;len(self.cards): return True return Falsep = Poker()p.shuffle()print(p.cards) PyLint的安装和使用Pylint是Python代码分析工具，它分析Python代码中的错误，查找不符合代码风格标准（默认使用的代码风格是 PEP 8）和有潜在问题的代码。 12pip install pylintpylint [options] module_or_package Pylint输出格式如下所示。 模块名:行号:列号: 消息类型 消息 消息类型有以下几种： C - 惯例：违反了Python编程惯例（PEP 8）的代码。 R - 重构：写得比较糟糕需要重构的代码。 W - 警告：代码中存在的不影响代码运行的问题。 E - 错误：代码中存在的影响代码运行的错误。 F - 致命错误：导致Pylint无法继续运行的错误。 Pylint命令的常用参数： --disable=&lt;msg ids&gt;或-d &lt;msg ids&gt;：禁用指定类型的消息。 --errors-only或-E：只显示错误。 --rcfile=&lt;file&gt;：指定配置文件。 --list-msgs：列出Pylint的消息清单。 --generate-rcfile：生成配置文件的样例。 --reports=&lt;y_or_n&gt;或-r &lt;y_or_n&gt;：是否生成检查报告。 使用Profile工具剖析你的代码性能cProfile模块example01.py 12345678910111213141516171819202122232425262728293031import cProfiledef is_prime(num): for factor in range(2, int(num ** 0.5) + 1): if num % factor == 0: return False return Trueclass PrimeIter: def __init__(self, total): self.counter = 0 self.current = 1 self.total = total def __iter__(self): return self def __next__(self): if self.counter &lt; self.total: self.current += 1 while not is_prime(self.current): self.current += 1 self.counter += 1 return self.current raise StopIteration() cProfile.run(&#x27;list(PrimeIter(10000))&#x27;) 执行结果： 123456789101112114734 function calls in 0.573 secondsOrdered by: standard namencalls tottime percall cumtime percall filename:lineno(function) 1 0.006 0.006 0.573 0.573 &lt;string&gt;:1(&lt;module&gt;) 1 0.000 0.000 0.000 0.000 example.py:14(__init__) 1 0.000 0.000 0.000 0.000 example.py:19(__iter__) 10001 0.086 0.000 0.567 0.000 example.py:22(__next__)104728 0.481 0.000 0.481 0.000 example.py:5(is_prime) 1 0.000 0.000 0.573 0.573 &#123;built-in method builtins.exec&#125; 1 0.000 0.000 0.000 0.000 &#123;method &#x27;disable&#x27; of &#x27;_lsprof.Profiler&#x27; objects&#125; ####line_profiler 给需要剖析时间性能的函数加上一个profile装饰器，这个函数每行代码的执行次数和时间都会被剖析。 example02.py 1234567891011121314151617181920212223242526272829@profiledef is_prime(num): for factor in range(2, int(num ** 0.5) + 1): if num % factor == 0: return False return Trueclass PrimeIter: def __init__(self, total): self.counter = 0 self.current = 1 self.total = total def __iter__(self): return self def __next__(self): if self.counter &lt; self.total: self.current += 1 while not is_prime(self.current): self.current += 1 self.counter += 1 return self.current raise StopIteration()list(PrimeIter(1000)) 安装和使用line_profiler三方库。 123456789101112131415161718pip install line_profilerkernprof -lv example.pyWrote profile results to example02.py.lprofTimer unit: 1e-06 sTotal time: 0.089513 sFile: example02.pyFunction: is_prime at line 1 # Hits Time Per Hit % Time Line Contents============================================================== 1 @profile 2 def is_prime(num): 3 86624 43305.0 0.5 48.4 for factor in range(2, int(num ** 0.5) + 1): 4 85624 42814.0 0.5 47.8 if num % factor == 0: 5 6918 3008.0 0.4 3.4 return False 6 1000 386.0 0.4 0.4 return True ####memory_profiler 给需要剖析内存性能的函数加上一个profile装饰器，这个函数每行代码的内存使用情况都会被剖析。 example03.py 123456789@profiledef eat_memory(): items = [] for _ in range(1000000): items.append(object()) return itemseat_memory() 安装和使用memory_profiler三方库。 12345678910111213pip install memory_profilerpython3 -m memory_profiler example.pyFilename: example03.pyLine # Mem usage Increment Line Contents================================================ 1 38.672 MiB 38.672 MiB @profile 2 def eat_memory(): 3 38.672 MiB 0.000 MiB items = [] 4 68.727 MiB 0.000 MiB for _ in range(1000000): 5 68.727 MiB 1.797 MiB items.append(object()) 6 68.727 MiB 0.000 MiB return items 如何构建综合职业素养学习总结 了解全局 确定范围 定义目标 寻找资源 创建学习计划 筛选资源 开始学习，浅尝辄止（YAGNI） 动手操作，边学边玩 全面掌握，学以致用 乐为人师，融会贯通 时间管理 提升专注力 充分利用碎片时间 使用番茄工作法 时间是怎么浪费掉的 任何行动都比不采取行动好 好书推荐 职业规划：《软技能 - 代码之外的生存指南》 吴军系列：《浪潮之巅》、《硅谷之谜》、《数学之美》、…… 时间管理：《成为一个更高效的人》、《番茄工作法图解》","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/99.面试中的公共问题","date":"2024-12-12T08:38:02.787Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/99.面试中的公共问题/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/99.%E9%9D%A2%E8%AF%95%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E9%97%AE%E9%A2%98/","excerpt":"","text":"面试中的公共问题计算机基础 TCP&#x2F;IP模型相关问题。 建议阅读阮一峰的《互联网协议入门（一）》和《互联网协议入门（二）》。 HTTP和HTTPS相关问题。 建议阅读阮一峰的《HTTP 协议入门》和《SSL&#x2F;TLS协议运行机制的概述》。 Linux常用命令和服务。 进程和线程之间的关系。什么时候用多线程？什么时候用多进程？。 关系型数据库相关问题（ACID、事务隔离级别、锁、SQL优化）。 非关系型数据库相关问题（CAP&#x2F;BASE、应用场景）。 Python基础 开发中用过哪些标准库和三方库。 标准库：sys &#x2F; os &#x2F; re &#x2F; math &#x2F; random &#x2F; logging &#x2F; json &#x2F; pickle &#x2F; shelve &#x2F; socket &#x2F; datetime &#x2F; hashlib &#x2F; configparser &#x2F; urllib &#x2F; itertools &#x2F; collections &#x2F; functools &#x2F; threading &#x2F; multiprocess &#x2F; timeit &#x2F; atexit &#x2F; abc &#x2F; asyncio &#x2F; base64 &#x2F; concurrent.futures &#x2F; copy &#x2F; csv &#x2F; operator &#x2F; enum &#x2F; heapq &#x2F; http &#x2F; profile &#x2F; pstats &#x2F; ssl &#x2F; unittest &#x2F; uuid 三方库：openpyxl &#x2F; xlrd &#x2F; xlwt &#x2F; PyPDF2 &#x2F; ReportLab &#x2F; PyYAML &#x2F; jieba &#x2F; pillow &#x2F; requests &#x2F; urllib3 &#x2F; responses &#x2F; aiohttp &#x2F; BeautifulSoup4 &#x2F; lxml &#x2F; pyquery &#x2F; PyMySQL &#x2F; psycopg2 &#x2F; redis &#x2F; PyMongo &#x2F; Peewee &#x2F; SQLAlchemy &#x2F; alipay &#x2F; PyJWT &#x2F; itsdangerous &#x2F; celery &#x2F; flower &#x2F; elasticsearch-dsl-py &#x2F; PyCrypto &#x2F; Paramiko &#x2F; logbook &#x2F; nose &#x2F; pytest &#x2F; coverage &#x2F; Selenium &#x2F; lineprofiler &#x2F; memoryprofiler &#x2F; matplotlib &#x2F; pygal &#x2F; OpenCV 装饰器的作用、原理和实现。 使用过哪些魔法方法。 建议阅读《Python魔术方法指南》。 生成式、生成器、迭代器的编写。 列表、集合、字典的底层实现。 垃圾回收相关问题。 并发编程的相关问题。 协程和异步I&#x2F;O相关知识。 Django和Flask MVC架构（MTV）解决了什么问题。 中间件的执行流程以及如何自定义中间件。 REST数据接口如何设计（URL、域名、版本、过滤、状态码、安全性）。 建议阅读阮一峰的《RESTful API设计指南》。 使用ORM框架实现CRUD操作的相关问题。 如何实现多条件组合查询 &#x2F; 如何执行原生的SQL &#x2F; 如何避免N+1查询问题 如何执行异步任务和定时任务。 如何实现页面缓存和查询缓存？缓存如何预热？ 爬虫相关 Scrapy框架的组件和数据处理流程。 爬取的目的（项目中哪些地方需要用到爬虫的数据）。 使用的工具（抓包、下载、清理、存储、分析、可视化）。 数据的来源（能够轻松的列举出10个网站）。 数据的构成（抓取的某个字段在项目中有什么用）。 反反爬措施（限速、请求头、Cookie池、代理池、Selenium WebDriver、RoboBrowser、TOR、OCR）。 数据的体量（最后抓取了多少数据，多少W条数据或多少个G的数据）。 后期数据处理（持久化、数据补全、归一化、格式化、转存、分类）。 数据分析 科学运算函数库（SciPy和NumPy常用运算）。 数据分析库（Pandas中封装的常用算法）。 常用的模型及对应的场景（分类、回归、聚类）。 提取了哪些具体的指标。 如何评价模型的优劣。 每种模型实际操作的步骤，对结果如何评价。 项目相关 项目团队构成以及自己在团队中扮演的角色（在项目中的职责）。 项目的业务架构（哪些模块及子模块）和技术架构（移动端、PC端、后端技术栈）。 软件控制管理相关工具（版本控制、问题管理、持续集成）。 核心业务实体及其属性，实体与实体之间的关系。 用到哪些依赖库，依赖库主要解决哪方面的问题。 项目如何部署上线以及项目的物理架构（Nginx、Gunicorn&#x2F;uWSGI、Redis、MongoDB、MySQL、Supervisor等）。 如何对项目进行测试，有没有做过性能调优。 项目中遇到的困难有哪些，如何解决的。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/98.项目部署上线和性能调优","date":"2024-12-12T08:38:02.784Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/98.项目部署上线和性能调优/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"项目部署上线指南准备上线 上线前的检查工作。 1python manage.py check --deploy 将DEBUG设置为False并配置ALLOWED_HOSTS。 12DEBUG = FalseALLOWED_HOSTS = [&#x27;*&#x27;] 安全相关的配置。 123456789101112131415161718192021# 保持HTTPS连接的时间SECURE_HSTS_SECONDS = 3600SECURE_HSTS_INCLUDE_SUBDOMAINS = TrueSECURE_HSTS_PRELOAD = True# 自动重定向到安全连接SECURE_SSL_REDIRECT = True# 避免浏览器自作聪明推断内容类型SECURE_CONTENT_TYPE_NOSNIFF = True# 避免跨站脚本攻击SECURE_BROWSER_XSS_FILTER = True# COOKIE只能通过HTTPS进行传输SESSION_COOKIE_SECURE = TrueCSRF_COOKIE_SECURE = True# 防止点击劫持攻击手段 - 修改HTTP协议响应头# 当前网站是不允许使用&lt;iframe&gt;标签进行加载的X_FRAME_OPTIONS = &#x27;DENY&#x27; 敏感信息放到环境变量或文件中。 123456SECRET_KEY = os.environ[&#x27;SECRET_KEY&#x27;]DB_USER = os.environ[&#x27;DB_USER&#x27;]DB_PASS = os.environ[&#x27;DB_PASS&#x27;]REDIS_AUTH = os.environ[&#x27;REDIS_AUTH&#x27;] 更新服务器Python环境到3.x 说明：如果需要清除之前的安装，就删除对应的文件和文件夹即可 安装底层依赖库。 1yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel libdb4-devel libpcap-devel xz-devel libffi-devel libxml2 下载Python源代码。 1wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xz 验证下载文件。 1md5sum Python-3.7.6.tar.xz 解压缩和解归档。 12xz -d Python-3.7.6.tar.xztar -xvf Python-3.7.6.tar 执行安装前的配置（生成Makefile文件）。 12cd Python-3.7.6./configure --prefix=/usr/local/python37 --enable-optimizations 构建和安装。 1make &amp;&amp; make install 配置PATH环境变量（用户或系统环境变量）并激活。 12vim ~/.bash_profilevim /etc/profile 12345... 此处省略上面的代码...export PATH=$PATH:/usr/local/python37/bin... 此处省略下面的代码... 12source ~/.bash_profilesource /etc/profile 注册软链接（符号链接）- 这一步不是必须的，但通常会比较有用。 1ln -s /usr/local/python37/bin/python3 /usr/bin/python3 测试Python环境是否更新成功（安装Python 3一定不能破坏原来的Python 2）。 12python3 --versionpython --version 项目目录结构假设项目文件夹为project，下面的五个子目录分别是：code、conf、logs、stat和venv分别用来保存项目的代码、配置文件、日志文件、静态资源和虚拟环境。其中，conf目录下的子目录cert中保存了配置HTTPS需要使用的证书和密钥；code目录下的项目代码可以通过版本控制工具从代码仓库中检出；虚拟环境可以通过工具（如：venv、virtualenv、pyenv等）进行创建。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364project├── code│ └── fangtx│ ├── api│ ├── common│ ├── fangtx│ ├── forum│ ├── rent│ ├── user│ ├── manage.py│ ├── README.md│ ├── static│ └── templates├── conf│ ├── cert│ │ ├── 214915882850706.key│ │ └── 214915882850706.pem│ ├── nginx.conf│ └── uwsgi.ini├── logs│ ├── access.log│ ├── error.log│ └── uwsgi.log├── stat│ └── css│ └── images│ └── js└── venv ├── bin │ ├── activate │ ├── activate.csh │ ├── activate.fish │ ├── celery │ ├── celerybeat │ ├── celeryd │ ├── celeryd-multi │ ├── coverage │ ├── coverage3 │ ├── coverage-3.7 │ ├── django-admin │ ├── django-admin.py │ ├── easy_install │ ├── easy_install-3.7 │ ├── pip │ ├── pip3 │ ├── pip3.7 │ ├── __pycache__ │ ├── pyrsa-decrypt │ ├── pyrsa-decrypt-bigfile │ ├── pyrsa-encrypt │ ├── pyrsa-encrypt-bigfile │ ├── pyrsa-keygen │ ├── pyrsa-priv2pub │ ├── pyrsa-sign │ ├── pyrsa-verify │ ├── python -&gt; python3 │ ├── python3 -&gt; /usr/bin/python3 │ └── uwsgi ├── include ├── lib │ └── python3.7 ├── lib64 -&gt; lib ├── pip-selfcheck.json └── pyvenv.cfg 下面以阿里云为例，简单说明如何为项目注册域名、解析域名以及购买权威机构颁发的证书。 注册域名。 域名备案。 域名解析。 购买证书。 可以使用类似于sftp的工具将证书上传到conf/cert目录，然后使用git克隆项目代码到code目录。 12cd codegit clone &lt;url&gt; 回到项目目录，创建并激活虚拟环境。 12python3 -m venv venvsource venv/bin/activate 重建项目依赖项。 1pip install -r code/teamproject/requirements.txt uWSGI的配置 安装uWSGI。 1pip install uwsgi 修改uWSGI的配置文件（/root/project/conf/uwsgi.ini）。 123456789101112131415161718192021[uwsgi]# 配置前导路径base=/root/project# 配置项目名称name=teamproject# 守护进程master=true# 进程个数processes=4# 虚拟环境pythonhome=%(base)/venv# 项目地址chdir=%(base)/code/%(name)# 指定python解释器pythonpath=%(pythonhome)/bin/python# 指定uwsgi文件module=%(name).wsgi# 通信的地址和端口(自己服务器的IP地址和端口)socket=172.18.61.250:8000# 日志文件地址logto=%(base)/logs/uwsgi.log 说明：可以先将“通信的地址和端口”项等号前面改为http来进行测试，如果没有问题再改回 成socket，然后通过Nginx来实现项目的“动静分离”（静态资源交给Nginx处理，动态内容交给 uWSGI处理）。按照下面的方式可以启动uWSGI服务器。 启动服务器。 1nohup uwsgi --ini conf/uwsgi.ini &amp; Nginx的配置 安装Nginx。 1yum -y install nginx 修改全局配置文件（/etc/nginx/nginx.conf）。 1234567891011121314151617181920212223242526272829303132333435363738394041# 配置用户user nginx;# 工作进程数(建议跟CPU的核数量一致)worker_processes auto;# 错误日志error_log /var/log/nginx/error.log;# 进程文件pid /run/nginx.pid;# 包含其他的配置include /usr/share/nginx/modules/*.conf;# 工作模式(多路IO复用方式)和连接上限events &#123; use epoll; worker_connections 1024;&#125;# HTTP服务器相关配置http &#123; # 日志格式 log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; # 访问日志 access_log /var/log/nginx/access.log main; # 开启高效文件传输模式 sendfile on; # 用sendfile传输文件时有利于改善性能 tcp_nopush on; # 禁用Nagle来解决交互性问题 tcp_nodelay on; # 客户端保持连接时间 keepalive_timeout 30; types_hash_max_size 2048; # 包含MIME类型的配置 include /etc/nginx/mime.types; # 默认使用二进制流格式 default_type application/octet-stream; # 包含其他配置文件 include /etc/nginx/conf.d/*.conf; # 包含项目的Nginx配置文件 include /root/project/conf/*.conf;&#125; 编辑局部配置文件（/root/project/conf/nginx.conf）。 1234567891011121314151617181920212223242526272829303132333435server &#123; listen 80; server_name _; access_log /root/project/logs/access.log; error_log /root/project/logs/error.log; location / &#123; include uwsgi_params; uwsgi_pass 172.18.61.250:8000; &#125; location /static/ &#123; alias /root/project/stat/; expires 30d; &#125;&#125;server &#123; listen 443; server_name _; ssl on; access_log /root/project/logs/access.log; error_log /root/project/logs/error.log; ssl_certificate /root/project/conf/cert/214915882850706.pem; ssl_certificate_key /root/project/conf/cert/214915882850706.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; include uwsgi_params; uwsgi_pass 172.18.61.250:8000; &#125; location /static/ &#123; alias /root/project/static/; expires 30d; &#125;&#125; 到此为止，我们可以启动Nginx来访问我们的应用程序，HTTP和HTTPS都是没有问题的，如果Nginx已经运行，在修改配置文件后，我们可以用下面的命令重新启动Nginx。 重启Nginx服务器。 1nginx -s reload 或 1systemctl restart nginx 说明：可以对Django项目使用python manage.py collectstatic命令将静态资源收集到指定目录下，要做到这点只需要在项目的配置文件settings.py中添加STATIC_ROOT配置即可。 负载均衡配置下面的配置中我们使用Nginx实现负载均衡，为另外的三个Nginx服务器（通过Docker创建）提供反向代理服务。 123docker run -d -p 801:80 --name nginx1 nginx:latestdocker run -d -p 802:80 --name nginx2 nginx:latestdocker run -d -p 803:80 --name nginx3 nginx:latest 123456789101112131415161718192021222324252627282930313233343536373839404142434445user root;worker_processes auto;error_log /var/log/nginx/error.log;pid /run/nginx.pid;include /usr/share/nginx/modules/*.conf;events &#123; worker_connections 1024;&#125;# 为HTTP服务配置负载均衡http &#123; upstream xx &#123; server 192.168.1.100 weight=2; server 192.168.1.101 weight=1; server 192.168.1.102 weight=1; &#125; server &#123; listen 80 default_server; listen [::]:80 default_server; listen 443 ssl; listen [::]:443 ssl; ssl on; access_log /root/project/logs/access.log; error_log /root/project/logs/error.log; ssl_certificate /root/project/conf/cert/214915882850706.pem; ssl_certificate_key /root/project/conf/cert/214915882850706.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / &#123; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_buffering off; proxy_pass http://fangtx; &#125; &#125;&#125; 说明：Nginx在配置负载均衡时，默认使用WRR（加权轮询算法），除此之外还支持ip_hash、fair（需要安装upstream_fair模块）和url_hash算法。此外，在配置upstream模块时可以指定服务器的状态值，包括：backup（备份机器，其他服务器不可用时才将请求分配到该机器）、down、fail_timeout（请求失败达到max_fails后的暂停服务时间）、max_fails（允许请求失败的次数）和weight（轮询的权重）。 Keepalived当使用Nginx进行负载均衡配置时，要考虑负载均衡服务器宕机的情况。为此可以使用Keepalived来实现负载均衡主机和备机的热切换，从而保证系统的高可用性。Keepalived的配置还是比较复杂，通常由专门做运维的人进行配置，一个基本的配置可以参照《Keepalived的配置和使用》。 MySQL主从复制下面还是基于Docker来演示如何配置MySQL主从复制。我们事先准备好MySQL的配置文件以及保存MySQL数据和运行日志的目录，然后通过Docker的数据卷映射来指定容器的配置、数据和日志文件的位置。 1234567891011121314root└── mysql ├── master │ ├── conf | └── data └── slave-1 | ├── conf | └── data └── slave-2 | ├── conf | └── data └── slave-3 ├── conf └── data MySQL的配置文件（master和slave的配置文件需要不同的server-id）。 12345678910111213[mysqld]pid-file=/var/run/mysqld/mysqld.pidsocket=/var/run/mysqld/mysqld.sockdatadir=/var/lib/mysqllog-error=/var/log/mysql/error.logserver-id=1log-bin=/var/log/mysql/mysql-bin.logexpire_logs_days=30max_binlog_size=256Msymbolic-links=0# slow_query_log=ON# slow_query_log_file=/var/log/mysql/slow.log# long_query_time=1 创建和配置master。 123456docker run -d -p 3306:3306 --name mysql-master \\-v /root/mysql/master/conf:/etc/mysql/mysql.conf.d \\-v /root/mysql/master/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 mysql:5.7docker exec -it mysql-master /bin/bash 12345678910111213141516171819202122232425262728mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.7.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; grant replication slave on *.* to &#x27;slave&#x27;@&#x27;%&#x27; identified by &#x27;iamslave&#x27;;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000003 | 590 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec)mysql&gt; quitByeexit 上面创建Docker容器时使用的-v参数（--volume）表示映射数据卷，冒号前是宿主机的目录，冒号后是容器中的目录，这样相当于将宿主机中的目录挂载到了容器中。 备份主表中的数据（如果需要的话）。 1mysql&gt; flush table with read lock; 1mysqldump -u root -p 123456 -A -B &gt; /root/backup/mysql/mybak$(date +&quot;%Y%m%d%H%M%S&quot;).sql 1mysql&gt; unlock table; 创建和配置slave。 12345678910111213141516171819docker run -d -p 3308:3306 --name mysql-slave-1 \\-v /root/mysql/slave-1/conf:/etc/mysql/mysql.conf.d \\-v /root/mysql/slave-1/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\--link mysql-master:mysql-master mysql:5.7docker run -d -p 3309:3306 --name mysql-slave-2 \\-v /root/mysql/slave-2/conf:/etc/mysql/mysql.conf.d \\-v /root/mysql/slave-2/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\--link mysql-master:mysql-master mysql:5.7docker run -d -p 3310:3306 --name mysql-slave-3 \\-v /root/mysql/slave-3/conf:/etc/mysql/mysql.conf.d \\-v /root/mysql/slave-3/data:/var/lib/mysql \\-e MYSQL_ROOT_PASSWORD=123456 \\--link mysql-master:mysql-master mysql:5.7docker exec -it mysql-slave-1 /bin/bash 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.7.23-log MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; reset slave;Query OK, 0 rows affected (0.02 sec)mysql&gt; change master to master_host=&#x27;mysql-master&#x27;, master_user=&#x27;slave&#x27;, master_password=&#x27;iamslave&#x27;, master_log_file=&#x27;mysql-bin.000003&#x27;, master_log_pos=590;Query OK, 0 rows affected, 2 warnings (0.03 sec)mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)mysql&gt; show slave status\\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: mysql57 Master_User: slave Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 590 Relay_Log_File: f352f05eb9d0-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 590 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 30c38043-ada1-11e8-8fa1-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version:1 row in set (0.00 sec)mysql&gt; quitByeexit 接下来可以如法炮制配置出slave2和slave3，这样就可以搭建起一个“一主带三从”的主从复制环境。上面创建创建容器时使用的--link参数用来配置容器在网络上的主机名（网络地址别名）。 配置好主从复制后，写数据的操作应该master上执行，而读数据的操作应该在slave上完成。为此，在Django项目中需要配置DATABASE_ROUTERS并通过自定义的主从复制路由类来实现读写分离操作，如下所示： 1234DATABASE_ROUTERS = [ # 此处省略其他配置 &#x27;common.routers.MasterSlaveRouter&#x27;,] 12345678910111213141516171819202122232425262728293031class MasterSlaveRouter(object): &quot;&quot;&quot;主从复制路由&quot;&quot;&quot; @staticmethod def db_for_read(model, **hints): &quot;&quot;&quot; Attempts to read auth models go to auth_db. &quot;&quot;&quot; return random.choice((&#x27;slave1&#x27;, &#x27;slave2&#x27;, &#x27;slave3&#x27;)) @staticmethod def db_for_write(model, **hints): &quot;&quot;&quot; Attempts to write auth models go to auth_db. &quot;&quot;&quot; return &#x27;default&#x27; @staticmethod def allow_relation(obj1, obj2, **hints): &quot;&quot;&quot; Allow relations if a model in the auth app is involved. &quot;&quot;&quot; return None @staticmethod def allow_migrate(db, app_label, model_name=None, **hints): &quot;&quot;&quot; Make sure the auth app only appears in the &#x27;auth_db&#x27; database. &quot;&quot;&quot; return True 上面的内容参考了Django官方文档的DATABASE_ROUTERS配置，对代码进行了适当的调整。 Docker事实上，项目上线中最为麻烦的事情就是配置软件运行环境，环境的差异会给软件的安装和部署带来诸多的麻烦，而Docker正好可以解决这个问题。关于Docker在之前的文档中我们已经介绍过了，接下来我们对Docker的知识做一些必要的补充。 创建镜像文件。 将容器保存成镜像： 1docker commit -m &quot;...&quot; -a &quot;jackfrued&quot; &lt;container-name&gt; jackfrued/&lt;image-name&gt; 使用Dockerfile构建镜像： 1234567891011121314151617181920# 指定基础镜像文件FROM centos:latest# 指定维护者信息MAINTAINER jackfrued# 执行命令RUN yum -y install gccRUN cd ~RUN mkdir -p project/codeRUN mkdir -p project/logs# 拷贝文件COPY ...# 暴露端口EXPOSE ...# 在容器启动时执行命令CMD ~/init.sh 1docker build -t jackfrued/&lt;image-name&gt; . 镜像的导入和导出。 12docker save -o &lt;file-name&gt;.tar &lt;image-name&gt;:&lt;version&gt;docker load -i &lt;file-name&gt;.tar 推送到DockerHub服务器。 123docker tag &lt;image-name&gt;:&lt;version&gt; jackfrued/&lt;name&gt;docker logindocker push jackfrued/&lt;name&gt; 容器之间的通信。 1docker run --link &lt;container-name&gt;:&lt;alias-name&gt; 如果我们能够在Docker中完成项目的部署，并且将整个部署好的容器打包成镜像文件进行分发和安装，这样就可以解决项目在多个节点上进行部署时可能遇到的麻烦，而且整个部署可以在很短的时间内完成。 SupervisorSupervisor是一个用Python写的进程管理工具，可以很方便的用来在类Unix系统下启动、重启（自动重启程序）和关闭进程，目前Supervisor暂时还没有提供对Python 3的支持，可以通过Python 2来安装和运行Supervisor，再通过Supervisor来管理Python 3的程序。 提示：还有一个和Supervisor功能类似的工具名为Circus，支持Python 3。 安装Supervisor。 123virtualenv -p /usr/bin/python venvsource venv/bin/activatepip install supervisor 查看Supervisor的配置文件。 1vim /etc/supervisord.conf 12345678; 此处省略上面的代码; The [include] section can just contain the &quot;files&quot; setting. This; setting can list multiple files (separated by whitespace or; newlines). It can also contain wildcards. The filenames are; interpreted as relative to this file. Included files *cannot*; include files themselves.[include]files = supervisord.d/*.ini 可以看出自定义的管理配置代码可以放在/etc/supervisord.d目录中，并且文件名以ini作为后缀即可。 编写自己的配置文件fangtx.ini并放在/etc/supervisord.d目录中。 1234567891011121314151617181920212223242526272829[program:project]command=uwsgi --ini /root/project/conf/uwsgi.inistopsignal=QUITautostart=trueautorestart=trueredirect_stderr=true[program:celery]; Set full path to celery program if using virtualenvcommand=/root/project/venv/bin/celery -A fangtx workeruser=rootnumprocs=1stdout_logfile=/var/log/supervisor/celery.logstderr_logfile=/var/log/supervisor/celery_error.logautostart=trueautorestart=truestartsecs=10; Need to wait for currently executing tasks to finish at shutdown.; Increase this if you have very long running tasks.;stopwaitsecs = 600; When resorting to send SIGKILL to the program to terminate it; send SIGKILL to its whole process group instead,; taking care of its children as well.killasgroup=true; Set Celery priority higher than default (999); so, if rabbitmq is supervised, it will start first.priority=1000 启动Supervisor。 1supervisorctl -c /etc/supervisord.conf 其他服务 常用开源软件。 功能 开源方案 版本控制工具 Git、Mercurial、SVN 缺陷管理 Redmine、Mantis 负载均衡 Nginx、LVS、HAProxy 邮件服务 Postfix、Sendmail HTTP服务 Nginx、Apache 消息队列 RabbitMQ、ZeroMQ、Redis、Kafka 文件系统 FastDFS 基于位置服务（LBS） MongoDB、Redis 监控服务 Nagios、Zabbix 关系型数据库 MySQL、PostgreSQL 非关系型数据库 MongoDB、Redis、Cassandra、TiDB 搜索引擎 ElasticSearch、Solr 缓存服务 Mamcached、Redis 常用云服务。 功能 可用的云服务 团队协作工具 Teambition、钉钉 代码托管平台 Github、Gitee、CODING 邮件服务 SendCloud 云存储（CDN） 七牛、OSS、LeanCloud、Bmob、又拍云、S3 移动端推送 极光、友盟、百度 即时通信 环信、融云 短信服务 云片、极光、Luosimao、又拍云 第三方登录 友盟、ShareSDK 网站监控和统计 阿里云监控、监控宝、百度云观测、小鸟云","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/97.电商网站技术要点剖析","date":"2024-12-12T08:38:02.780Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/97.电商网站技术要点剖析/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/97.%E7%94%B5%E5%95%86%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E8%A6%81%E7%82%B9%E5%89%96%E6%9E%90/","excerpt":"","text":"电商网站技术要点剖析商业模式 B2B - 商家对商家，交易双方都是企业（商家），最典型的案例就是阿里巴巴。 C2C - 个人对个人，例如：淘宝、人人车。 B2C - 商家对个人，例如：唯品会，聚美优品。 C2B - 个人对商家，先有消费者提出需求，后有商家按需求组织生产，例如： 尚品宅配。 O2O - 线上到线下，将线下的商务机会与互联网结合，让互联网成为线下交易的平台，例如：美团外卖、饿了么。 B2B2C - 商家对商家对个人，例如：天猫、京东。 需求要点 用户端 首页（商品分类、广告轮播、滚动快讯、瀑布加载、推荐、折扣、热销、……） 用户（登录（第三方登录）、注册、注销、自服务（个人信息、浏览历史、收货地址、……）） 商品（分类、列表、详情、搜索、热门搜索、搜索历史、添加到购物车、收藏、关注、评论、……） 购物车（查看、编辑（修改数量、删除商品、清空）） 订单（提交订单（支付）、历史订单、订单详情、订单评价、……） 管理端 核心业务实体的CRUD 定时任务（周期性和非周期性，如处理未支付订单、采集数据对异常事件报警、……） 报表功能（导入导出Excel、PDF等以及前端ECharts统计图表展示） 权限控制（RBAC、白名单、黑名单、……） 业务流转（如发起退款流程，常用流程引擎有：Activity、Airflow、Spiff等） 三方服务（接入地图、短信、物流、支付、实名认证、天气、监控、云存储、……） 物理模型设计首先要搞清楚两个概念：SPU（Standard Product Unit）和SKU（Stock Keeping Unit）。 SPU：iPhone 6s SKU：iPhone 6s 64G 土豪金 第三方登录第三方登录是指利用第三方网站（通常是知名社交网站）的账号进行登录验证（主要是通过知名第三方网站获取到用户相关信息），比如国内的 QQ、微博，国外的Google、Facebook等。第三方登录大部分都是使用OAuth协议，它是一个关于授权的开放网络标准（数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌，用来代替密码，供第三方应用使用），得到了广泛的应用，目前通常使用的是2.0版本。关于OAuth的基础知识，可以阅读阮一峰老师的《理解OAuth 2.0》。关于令牌和密码的区别，我们可以简单总结出以下三点差异： 令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。 令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。 令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。 所以，通过令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是OAuth协议的优势。 OAuth 2.0授权流程 用户打开客户端以后，客户端要求用户（资源所有者）给予授权。 用户（资源所有者）同意给予客户端授权。 客户端使用上一步获得的授权，向认证服务器申请访问令牌。 认证服务器对客户端进行认证以后，发放访问令牌。 客户端使用访问令牌向资源服务器申请获取资源。 资源服务器确认访问令牌无误，同意向客户端开放资源。 如果使用微博登录进行接入，其具体步骤可以参考微博开放平台上的“微博登录接入”文档。使用QQ登录进行接入，需要首先注册成为QQ互联开发者并通过审核，具体的步骤可以参考QQ互联上的“接入指南”，具体的步骤可以参考“网站开发流程”。 提示：在Gitbook上面有一本名为《Django博客入门》的书以Github为例介绍了第三方账号登录，有兴趣的可以自行阅读。 通常电商网站在使用第三方登录时，会要求与网站账号进行绑定或者根据获取到的第三方账号信息（如：手机号）自动完成账号绑定。 缓存预热和查询缓存缓存预热所谓缓存预热，是指在启动服务器时将数据提前加载到缓存中，为此可以在Django应用的apps.py模块中编写AppConfig的子类并重写ready()方法，代码如下所示。 1234567891011121314151617181920212223import pymysqlfrom django.apps import AppConfigfrom django.core.cache import cacheSELECT_PROVINCE_SQL = &#x27;select distid, name from tb_district where pid is null&#x27;class CommonConfig(AppConfig): name = &#x27;common&#x27; def ready(self): conn = pymysql.connect(host=&#x27;1.2.3.4&#x27;, port=3306, user=&#x27;root&#x27;, password=&#x27;pass&#x27;, database=&#x27;db&#x27;, charset=&#x27;utf8&#x27;, cursorclass=pymysql.cursors.DictCursor) try: with conn.cursor() as cursor: cursor.execute(SELECT_PROVINCE_SQL) provinces = cursor.fetchall() cache.set(&#x27;provinces&#x27;, provinces) finally: conn.close() 接下来，还需要在应用的__init__.py中编写下面的代码。 1default_app_config = &#x27;common.apps.CommonConfig&#x27; 或者在项目的settings.py文件中注册应用。 12345INSTALLED_APPS = [ ... &#x27;common.apps.CommonConfig&#x27;, ...] 查询缓存自定义装饰器实现查询结果的缓存。 12345678910111213141516171819202122232425from pickle import dumps, loadsfrom django.core.cache import cachesMODEL_CACHE_KEY = &#x27;project:modelcache:%s&#x27;def my_model_cache(key, section=&#x27;default&#x27;, timeout=None): &quot;&quot;&quot;实现模型缓存的装饰器&quot;&quot;&quot; def wrapper1(func): def wrapper2(*args, **kwargs): real_key = &#x27;%s:%s&#x27; % (MODEL_CACHE_KEY % key, &#x27;:&#x27;.join(map(str, args))) serialized_data = caches[section].get(real_key) if serialized_data: data = loads(serialized_data) else: data = func(*args, **kwargs) cache.set(real_key, dumps(data), timeout=timeout) return data return wrapper2 return wrapper1 123@my_model_cache(key=&#x27;provinces&#x27;)def get_all_provinces(): return list(Province.objects.all()) 购物车实现问题一：已登录用户的购物车放在哪里？未登录用户的购物车放在哪里？ 12345678910111213141516171819202122232425262728293031323334353637383940414243class CartItem(object): &quot;&quot;&quot;购物车中的商品项&quot;&quot;&quot; def __init__(self, sku, amount=1, selected=False): self.sku = sku self.amount = amount self.selected = selected @property def total(self): return self.sku.price * self.amountclass ShoppingCart(object): &quot;&quot;&quot;购物车&quot;&quot;&quot; def __init__(self): self.items = &#123;&#125; self.index = 0 def add_item(self, item): if item.sku.id in self.items: self.items[item.sku.id].amount += item.amount else: self.items[item.sku.id] = item def remove_item(self, sku_id): if sku_id in self.items: self.items.remove(sku_id) def clear_all_items(self): self.items.clear() @property def cart_items(self): return self.items.values() @property def cart_total(self): total = 0 for item in self.items.values(): total += item.total return total 已登录用户的购物车可以放在数据库中（可以先在Redis中缓存）；未登录用户的购物车可以保存在Cookie、localStorage或sessionStorage中（减少服务器端内存开销）。 12345&#123; &#x27;1001&#x27;: &#123;sku: &#123;...&#125;, &#x27;amount&#x27;: 1, &#x27;selected&#x27;: True&#125;, &#x27;1002&#x27;: &#123;sku: &#123;...&#125;, &#x27;amount&#x27;: 2, &#x27;selected&#x27;: False&#125;, &#x27;1003&#x27;: &#123;sku: &#123;...&#125;, &#x27;amount&#x27;: 3, &#x27;selected&#x27;: True&#125;,&#125; 1234request.get_signed_cookie(&#x27;cart&#x27;)cart_base64 = base64.base64encode(pickle.dumps(cart))response.set_signed_cookie(&#x27;cart&#x27;, cart_base64) 问题二：用户登录之后，如何合并购物车？（目前电商应用的购物车几乎都做了持久化处理，主要是方便在多个终端之间共享数据） 集成支付功能问题一：支付信息如何持久化？（必须保证每笔交易都有记录） 问题二：如何接入支付宝？（接入其他平台基本类似） 蚂蚁金服开放平台。 入驻平台。 开发者中心。 文档中心。 SDK集成 - PYPI链接。 API列表。 配置文件： 123ALIPAY_APPID = &#x27;......&#x27;ALIPAY_URL = &#x27;https://openapi.alipaydev.com/gateway.do&#x27;ALIPAY_DEBUG = False 获得支付链接（发起支付）： 1234567891011121314151617181920212223242526# 创建调用支付宝的对象alipay = AliPay( # 在线创建应用时分配的ID appid=settings.ALIPAY_APPID, app_notify_url=None, # 自己应用的私钥 app_private_key_path=os.path.join( os.path.dirname(os.path.abspath(__file__)), &#x27;keys/app_private_key.pem&#x27;), # 支付宝的公钥 alipay_public_key_path=os.path.join( os.path.dirname(os.path.abspath(__file__)), &#x27;keys/alipay_public_key.pem&#x27;), sign_type=&#x27;RSA2&#x27;, debug=settings.ALIPAY_DEBUG)# 调用获取支付页面操作order_info = alipay.api_alipay_trade_page_pay( out_trade_no=&#x27;...&#x27;, total_amount=&#x27;...&#x27;, subject=&#x27;...&#x27;, return_url=&#x27;http://...&#x27;)# 生成完整的支付页面URLalipay_url = settings.ALIPAY_URL + &#x27;?&#x27; + order_inforeturn JsonResponse(&#123;&#x27;alipay_url&#x27;: alipay_url&#125;) 通过上面返回的链接可以进入支付页面，支付完成后会自动跳转回上面代码中设定好的项目页面，在该页面中可以获得订单号（out_trade_no）、支付流水号（trade_no）、交易金额（total_amount）和对应的签名（sign）并请求后端验证和保存交易结果，代码如下所示： 123456789101112131415161718192021# 创建调用支付宝的对象alipay = AliPay( # 在线创建应用时分配的ID appid=settings.ALIPAY_APPID, app_notify_url=None, # 自己应用的私钥 app_private_key_path=os.path.join( os.path.dirname(os.path.abspath(__file__)), &#x27;keys/app_private_key.pem&#x27;), # 支付宝的公钥 alipay_public_key_path=os.path.join( os.path.dirname(os.path.abspath(__file__)), &#x27;keys/alipay_public_key.pem&#x27;), sign_type=&#x27;RSA2&#x27;, debug=settings.ALIPAY_DEBUG)# 请求参数（假设是POST请求）中包括订单号、支付流水号、交易金额和签名params = request.POST.dict()# 调用验证操作if alipay.verify(params, params.pop(&#x27;sign&#x27;)): # 对交易进行持久化操作 支付宝的支付API还提供了交易查询、交易结算、退款、退款查询等一系列的接口，可以根据业务需要进行调用，此处不再进行赘述。 秒杀和超卖 秒杀：秒杀是通常意味着要在很短的时间处理极高的并发，系统在短时间需要承受平时百倍以上的流量，因此秒杀架构是一个比较复杂的问题，其核心思路是流量控制和性能优化，需要从前端（通过JavaScript实现倒计时、避免重复提交和限制频繁刷新）到后台各个环节的配合。流量控制主要是限制只有少部分流量进入服务后端（毕竟最终只有少部分用户能够秒杀成功），同时在物理架构上使用缓存（一方面是因为读操作多写操作少；另外可以将库存放在Redis中，利用DECR原语实现减库存；同时也可以利用Redis来进行限流，道理跟限制频繁发送手机验证码是一样的）和消息队列（消息队列最为重要的作用就是“削峰”和“上下游节点解耦合”）来进行优化；此外还要采用无状态服务设计，这样才便于进行水平扩展（通过增加设备来为系统扩容）。 超卖现象：比如某商品的库存为1，此时用户1和用户2并发购买该商品，用户1提交订单后该商品的库存被修改为0，而此时用户2并不知道的情况下提交订单，该商品的库存再次被修改为-1这就是超卖现象。解决超卖现象有三种常见的思路： 悲观锁控制：查询商品数量的时候就用select ... for update对数据加锁，这样的话用户1查询库存时，用户2因无法读取库存数量被阻塞，直到用户1提交或者回滚了更新库存的操作后才能继续，从而解决了超卖问题。但是这种做法对并发访问量很高的商品来说性能太过糟糕，实际开发中可以在库存小于某个值时才考虑加锁，但是总的来说这种做法不太可取。 乐观锁控制：查询商品数量不用加锁，更新库存的时候设定商品数量必须与之前查询数量相同才能更新，否则说明其他事务已经更新了库存，必须重新发出请求。 尝试减库存：将上面的查询（select）和更新（update）操作合并为一条SQL操作，更新库存的时候，在where筛选条件中加上库存&gt;=购买数量或库存-购买数量&gt;=0的条件，这种做法要求事务隔离级别为读提交（read committed）。 提示：有兴趣的可以自己在知乎上看看关于这类问题的讨论。 静态资源管理静态资源的管理可以自己架设文件服务器或者分布式文件服务器（FastDFS），但是一般的项目中没有必要这样做而且效果未必是最好的，我们建议使用云存储服务来管理网站的静态资源，国内外的云服务提供商如亚马逊、阿里云、七牛、LeanCloud、Bmob等都提供了非常优质的云存储服务，而且价格也是一般公司可以接受的，具体的操作可以参考官方文档，例如：阿里云的对象存储 OSS开发人员指南。 全文检索方案选择 使用数据库的模糊查询功能 - 效率低，每次需要全表扫描，不支持分词。 使用数据库的全文检索功能 - MySQL 5.6以前只适用于MyISAM引擎，检索操作和其他的DML操作耦合在数据库中，可能导致检索操作非常缓慢，数据量达到百万级性能显著下降，查询时间很长。 使用开源搜索引擎 - 索引数据和原始数据分离，可以使用ElasticSearch或Solr来提供外置索引服务，如果不考虑高并发的全文检索需求，纯Python的Whoosh也可以考虑。 ElasticSearchElasticSearch既是一个分布式文档数据库又是一个高可扩展的开源全文搜索和分析引擎，它允许存储、搜索和分析大量的数据，并且这个过程是近实时的。它通常被用作底层引擎和技术，为复杂的搜索功能和要求提供动力，大家熟知的维基百科、Stack-Overflow、Github都使用了ElasticSearch。 ElasticSearch的底层是开源搜索引擎Lucene，但是直接用Lucene会非常麻烦，必须自己编写代码去调用它的接口而且只支持Java语言。ElasticSearch相当于对Lucene进行了一次全面的封装，提供了REST风格的API接口，通过基于HTTP协议的访问方式屏蔽了编程语言的差异。ElasticSearch会为数据构建倒排索引，但是ElasticSearch内置的分词器对中文分词的支持几乎为零，因此需要通过安装elasticsearch-analysis-ik插件来提供中文分词服务。 ElasticSearch的安装和配置可以参考《ElasticSearch之Docker安装》。除了ElasticSearch之外，也可以使用Solr、Whoosh等来提供搜索引擎服务，基本上Django项目中可以考虑如下几种方案： haystack（django-haystack &#x2F; drf-haystack） + whoosh + Jieba haystack （django-haystack &#x2F; drf-haystack）+ elasticsearch requests + elasticsearch django-elasticsearch-dsl ####安装和使用ElasticSearch 使用Docker安装ElasticSearch。 12docker pull elasticsearch:7.6.0docker run -d -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; --name es elasticsearch:7.6.0 说明：上面创建容器时通过-e参数指定了使用单机模式和Java虚拟机最小最大可用堆空间的大小，堆空间大小可以根据服务器实际能够提供给ElasticSearch的内存大小来决定，默认为2G。 创建数据库。 请求：PUT - http://1.2.3.4:9200/demo/ 响应： 12345&#123; &quot;acknowledged&quot;: true, &quot;shards_acknowledged&quot;: true, &quot;index&quot;: &quot;demo&quot;&#125; 查看创建的数据库。 请求：GET - http://1.2.3.4:9200/demo/ 响应： 123456789101112131415161718&#123; &quot;demo&quot;: &#123; &quot;aliases&quot;: &#123;&#125;, &quot;mappings&quot;: &#123;&#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;creation_date&quot;: &quot;1552213970199&quot;, &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;uuid&quot;: &quot;ny3rCn10SAmCsqW6xPP1gw&quot;, &quot;version&quot;: &#123; &quot;created&quot;: &quot;6050399&quot; &#125;, &quot;provided_name&quot;: &quot;demo&quot; &#125; &#125; &#125;&#125; 插入数据。 请求：POST - http://1.2.3.4:9200/demo/goods/1/ 请求头：Content-Type: application&#x2F;json 参数： 123456789&#123; &quot;no&quot;: &quot;5089253&quot;, &quot;title&quot;: &quot;Apple iPhone X (A1865) 64GB 深空灰色 移动联通电信4G手机&quot;, &quot;brand&quot;: &quot;Apple&quot;, &quot;name&quot;: &quot;Apple iPhone X&quot;, &quot;product&quot;: &quot;中国大陆&quot;, &quot;resolution&quot;: &quot;2436 x 1125&quot;, &quot;intro&quot;: &quot;一直以来，Apple都心存一个设想，期待能够打造出这样一部iPhone：它有整面屏幕，能让你在使用时，完全沉浸其中，仿佛忘了它的存在。它是如此智能，哪怕轻轻一瞥，都能得到它心有灵犀的回应。而这个设想，终于随着iPhone X的到来成为了现实。现在，就跟未来见个面吧。&quot;&#125; 响应： 1234567891011121314&#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 4, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 3, &quot;_primary_term&quot;: 1&#125; 删除数据。 请求：DELETE - http://1.2.3.4:9200/demo/goods/1/ 响应： 1234567891011121314&#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;deleted&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 1, &quot;_primary_term&quot;: 1&#125; 更新数据。 请求：PUT - http://1.2.3.4:9200/demo/goods/1/_update 请求头：Content-Type: application&#x2F;json 参数： 1234567891011&#123; &quot;doc&quot;: &#123; &quot;no&quot;: &quot;5089253&quot;, &quot;title&quot;: &quot;Apple iPhone X (A1865) 64GB 深空灰色 移动联通电信4G手机&quot;, &quot;brand&quot;: &quot;Apple(苹果)&quot;, &quot;name&quot;: &quot;Apple iPhone X&quot;, &quot;product&quot;: &quot;美国&quot;, &quot;resolution&quot;: &quot;2436 x 1125&quot;, &quot;intro&quot;: &quot;一直以来，Apple都心存一个设想，期待能够打造出这样一部iPhone：它有整面屏幕，能让你在使用时，完全沉浸其中，仿佛忘了它的存在。它是如此智能，哪怕轻轻一瞥，都能得到它心有灵犀的回应。而这个设想，终于随着iPhone X的到来成为了现实。现在，就跟未来见个面吧。&quot; &#125;&#125; 响应： 1234567891011121314&#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 10, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 9, &quot;_primary_term&quot;: 1&#125; 查询数据。 请求：GET - http://1.2.3.4:9200/demo/goods/1/ 响应： 123456789101112131415161718&#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 10, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;doc&quot;: &#123; &quot;no&quot;: &quot;5089253&quot;, &quot;title&quot;: &quot;Apple iPhone X (A1865) 64GB 深空灰色 移动联通电信4G手机&quot;, &quot;brand&quot;: &quot;Apple(苹果)&quot;, &quot;name&quot;: &quot;Apple iPhone X&quot;, &quot;product&quot;: &quot;美国&quot;, &quot;resolution&quot;: &quot;2436 x 1125&quot;, &quot;intro&quot;: &quot;一直以来，Apple都心存一个设想，期待能够打造出这样一部iPhone：它有整面屏幕，能让你在使用时，完全沉浸其中，仿佛忘了它的存在。它是如此智能，哪怕轻轻一瞥，都能得到它心有灵犀的回应。而这个设想，终于随着iPhone X的到来成为了现实。现在，就跟未来见个面吧。&quot; &#125; &#125;&#125; 配置中文分词和拼音插件 进入Docker容器的plugins目录。 1docker exec -it es /bin/bash 下载和ElasticSearch版本对应的ik和pinyin插件。 12345678910111213yum install -y wgetcd plugins/mkdir ikcd ikwget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.0/elasticsearch-analysis-ik-7.6.0.zipunzip elasticsearch-analysis-ik-7.6.0.ziprm -f elasticsearch-analysis-ik-7.6.0.zipcd ..mkdir pinyincd pinyinwget https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v7.6.0/elasticsearch-analysis-pinyin-7.6.0.zipunzip elasticsearch-analysis-pinyin-7.6.0.ziprm -f elasticsearch-analysis-pinyin-7.6.0.zip 退出容器，重启ElasticSearch。 1docker restart es 测试中文分词效果。 请求：POST - http://1.2.3.4:9200/_analyze 请求头：Content-Type: application&#x2F;json 参数： 1234&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;中国男足在2022年卡塔尔世界杯预选赛中勇夺小组最后一名&quot;&#125; 响应： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;中国&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;男足&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;在&quot;, &quot;start_offset&quot;: 4, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;2022年&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 10, &quot;type&quot;: &quot;TYPE_CQUAN&quot;, &quot;position&quot;: 3 &#125;, &#123; &quot;token&quot;: &quot;卡塔尔&quot;, &quot;start_offset&quot;: 10, &quot;end_offset&quot;: 13, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 4 &#125;, &#123; &quot;token&quot;: &quot;世界杯&quot;, &quot;start_offset&quot;: 13, &quot;end_offset&quot;: 16, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 5 &#125;, &#123; &quot;token&quot;: &quot;预选赛&quot;, &quot;start_offset&quot;: 16, &quot;end_offset&quot;: 19, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 6 &#125;, &#123; &quot;token&quot;: &quot;中&quot;, &quot;start_offset&quot;: 19, &quot;end_offset&quot;: 20, &quot;type&quot;: &quot;CN_CHAR&quot;, &quot;position&quot;: 7 &#125;, &#123; &quot;token&quot;: &quot;勇夺&quot;, &quot;start_offset&quot;: 20, &quot;end_offset&quot;: 22, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 8 &#125;, &#123; &quot;token&quot;: &quot;小组&quot;, &quot;start_offset&quot;: 22, &quot;end_offset&quot;: 24, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 9 &#125;, &#123; &quot;token&quot;: &quot;最后&quot;, &quot;start_offset&quot;: 24, &quot;end_offset&quot;: 26, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 10 &#125;, &#123; &quot;token&quot;: &quot;一名&quot;, &quot;start_offset&quot;: 26, &quot;end_offset&quot;: 28, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 11 &#125; ]&#125; 测试拼音分词效果。 请求：POST - http://1.2.3.4:9200/_analyze 请求头：Content-Type: application&#x2F;json 参数： 1234&#123; &quot;analyzer&quot;: &quot;pinyin&quot;, &quot;text&quot;: &quot;张学友&quot;&#125; 响应： 1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;zhang&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 0, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;zxy&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 0, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;xue&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 0, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;you&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 0, &quot;type&quot;: &quot;word&quot;, &quot;position&quot;: 2 &#125; ]&#125; 全文检索功能可以通过GET或者POST请求进行搜索，下面演示了搜索有“未来”关键词商品。 GET - http://120.77.222.217:9200/demo/goods/_search?q=未来 注意：URL中的中文应该要处理成百分号编码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; &quot;took&quot;: 19, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 0.73975396, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 0.73975396, &quot;_source&quot;: &#123; &quot;doc&quot;: &#123; &quot;no&quot;: &quot;5089253&quot;, &quot;title&quot;: &quot;Apple iPhone X (A1865) 64GB 深空灰色 移动联通电信4G手机&quot;, &quot;brand&quot;: &quot;Apple(苹果)&quot;, &quot;name&quot;: &quot;Apple iPhone X&quot;, &quot;product&quot;: &quot;美国&quot;, &quot;resolution&quot;: &quot;2436*1125&quot;, &quot;intro&quot;: &quot;一直以来，Apple都心存一个设想，期待能够打造出这样一部iPhone：它有整面屏幕，能让你在使用时，完全沉浸其中，仿佛忘了它的存在。它是如此智能，哪怕轻轻一瞥，都能得到它心有灵犀的回应。而这个设想，终于随着iPhone X的到来成为了现实。现在，就跟未来见个面吧。&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot;: &quot;demo&quot;, &quot;_type&quot;: &quot;goods&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 0.68324494, &quot;_source&quot;: &#123; &quot;no&quot;: &quot;42417956432&quot;, &quot;title&quot;: &quot;小米9 透明尊享版 手机 透明尊享 全网通(12GB + 256GB)&quot;, &quot;brand&quot;: &quot;小米（MI）&quot;, &quot;name&quot;: &quot;小米（MI）小米9透明&quot;, &quot;product&quot;: &quot;中国大陆&quot;, &quot;resolution&quot;: &quot;2340*1080&quot;, &quot;intro&quot;: &quot;全面透明机身，独特科幻机甲风，来自未来的设计。&quot; &#125; &#125; ] &#125;&#125; URL中可用的搜索参数如下表所示： 参数 说明 q 查询字符串 analyzer 分析查询字符串使用的分词器 analyze_wildcard 通配符或者前缀查询是否被分析，默认为false default_operator 多个条件之间的关系，默认为OR，可以修改为AND explain 在返回的结果中包含评分机制的解释 fields 只返回索引中指定的列，多个列中间用逗号隔开 sort 排序参考的字段，可以用:asc和:desc来指定升序和降序 timeout 超时时间 from 匹配结果的开始值，默认为0 size 匹配结果的条数，默认为10 POST - http://120.77.222.217:9200/demo/goods/_search 请求头：Content-Type: application&#x2F;json 参数： 1234567&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;type&quot;: &quot;&quot; &#125; &#125;&#125; POST搜索是基于DSL的。 Django对接ElasticSearchPython对接ElasticSearch的第三方库是HayStack，在Django项目中可以使用django-haystack，通过HayStack可以在不修改代码对接多种搜索引擎服务。 1pip install django-haystack elasticsearch 配置文件： 12345678910111213141516171819INSTALLED_APPS = [ ... &#x27;haystack&#x27;, ...]HAYSTACK_CONNECTIONS = &#123; &#x27;default&#x27;: &#123; # 引擎配置 &#x27;ENGINE&#x27;: &#x27;haystack.backends.elasticsearch_backend.ElasticsearchSearchEngine&#x27;, # 搜索引擎服务的URL &#x27;URL&#x27;: &#x27;http://1.2.3.4:9200&#x27;, # 索引库的名称 &#x27;INDEX_NAME&#x27;: &#x27;goods&#x27;, &#125;,&#125;# 添加/删除/更新数据时自动生成索引HAYSTACK_SIGNAL_PROCESSOR = &#x27;haystack.signals.RealtimeSignalProcessor&#x27; 索引类： 1234567891011from haystack import indexesclass GoodsIndex(indexes.SearchIndex, indexes.Indexable): text = indexes.CharField(document=True, use_template=True) def get_model(self): return Goods def index_queryset(self, using=None): return self.get_model().objects.all() 编辑text字段的模板（需要放在templates&#x2F;search&#x2F;indexes&#x2F;demo&#x2F;goods_text.txt）： 12&#123;&#123;object.title&#125;&#125;&#123;&#123;object.intro&#125;&#125; 配置URL： 1234urlpatterns = [ # ... url(&#x27;search/&#x27;, include(&#x27;haystack.urls&#x27;)),] 生成初始索引： 1python manage.py rebuild_index 说明：可以参考《Django Haystack 全文检索与关键词高亮》一文来更深入的了解基于Haystack的全文检索操作。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/96.软件测试和自动化测试","date":"2024-12-12T08:38:02.778Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/96.软件测试和自动化测试/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/96.%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%92%8C%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/","excerpt":"","text":"软件测试和自动化测试软件测试概述软件测试是一种用来促进鉴定软件的正确性、完整性、安全性和品质的过程，也就是在规定的条件下对程序进行操作以发现程序中的错误，衡量软件的品质并对其是否能满足设计要求进行评估的过程。 测试的方法黑盒测试：测试应用程序的功能，而不是其内部结构或运作。测试者不需具备应用程序的代码、内部结构和编程语言的专门知识。测试者只需知道什么是系统应该做的事，即当键入一个特定的输入，可得到一定的输出。测试案例是依应用系统应该做的功能，照规范、规格或要求等设计。测试者选择有效输入和无效输入来验证是否正确的输出。此测试方法可适合大部分的软件测试，例如集成测试和系统测试。 白盒测试：测试应用程序的内部结构或运作，而不是测试应用程序的功能（即黑箱测试）。在白箱测试时，以编程语言的角度来设计测试案例。测试者输入数据验证数据流在程序中的流动路径，并确定适当的输出，类似测试电路中的节点。 由于时间和成本的约束，软件测试中一个最为关键的问题就是：“在所有可能的测试用例中，哪个子集能发现最多的错误？”。所以在设计测试用例时，白盒测试看重程序逻辑覆盖的程度（语句覆盖、条件覆盖、分支覆盖），黑盒测试可以使用等价类划分、边界值分析、因果图分析、错误猜测等方法来设计测试用例。 测试的种类（阶段）单元测试：对软件组成单元进行测试，其目的是检验软件基本组成单位的正确性，测试的对象是软件设计的最小单位 - 函数。 集成测试：将程序模块采用适当的集成策略组装起来，对系统的接口及集成后的功能进行正确性检测的测试工作。其主要目的是检查软件单位之间的接口是否正确，集成测试的对象是已经经过单元测试的模块。 系统测试：系统测试主要包括功能测试、界面测试、可靠性测试、易用性测试、性能测试。 回归测试：为了检测代码修改而引入的错误所进行的测试活动。回归测试是软件维护阶段的重要工作，有研究表明，回归测试带来的耗费占软件生命周期的1&#x2F;3总费用以上。 测试驱动开发（敏捷测试）测试驱动开发包括以下三个步骤： 为未实现的新功能或者改进编写自动化测试。 提供通过所有定义的测试的最小代码量。 重构代码以满足所需的质量标准。 测试驱动开发的好处在于可以有效的防止软件回归以及提供更有质量的代码。还有就是验收测试应该由客户来进行，客户通过对使用场景来设计验收测试，对应用程序是否满足他们的要求进行客观、公正的确认。能够通过单元测试、甚至是系统测试的功能未必能够通过客户的验收测试。 互联网应用和移动应用的测试互联网应用的测试策略： 表示层测试（内容测试、站点结构测试、用户环境（浏览器、操作系统等）） 业务层测试（性能、数据验证、事务、外部服务） 持久层测试（响应时间、数据完整性、容错性） 移动应用的测试策略： 真机测试 基于模拟器的测试 单元（模块）测试Python的标准库里有为编写单元测试而准备的unittest模块，执行测试时建议使用pytest或nose2。pytest是一款能够自动搜索并执行测试的测试执行工具，并且会输出详细的错误报告。关于单元测试可以看看《Python必会的单元测试框架 - unittest》。 可以安装testfixtures库来辅助单元测试，它整合了多种典型配置器，提供了生成目录、更改系统日期、生成mock对象的功能模块，这些模块能够帮助我们将单元测试与单元测试所依赖的环境分离开。mock 是将测试对象所依赖的对象替换为虚拟对象的库，在测试的时候，我们可以为虚拟对象指定其在被调用时的返回值以及是否发生异常等。 tox能便捷地为我们准备好执行测试所需的环境。tox会在多个virtualenv环境中搭建测试 环境，然后在这些环境中执行测试并显示结果。它能够把测试工具的选项及环境变量等内容统 一起来，所以我们只需执行tox命令即能轻松完成所需的测试。 自动化测试UI自动化测试桌面端 - PyAutoGui移动端 - AppniumWeb端 - SeleniumSelenium是实现Web应用程序的功能测试以及集成测试自动化的浏览器驱动测试工具群。和使用浏览器的用户相同，Selenium可以在浏览器进行的鼠标操作、在表单中输入文字、验证表单的值等，利用这一点就可以将手动操作变成自动化操作。 Selenium优点 自动化测试用例制作简单。Selenium提供了Selenium IDE工具，该工具可以捕获鼠标、键盘的操作，然后通过重放功能来重复这些操作，这样就可以简单的制作测试用例。 支持多种浏览器和操作系统。 Selenium的组件 Selenium IDE Selenium Remote Control Selenium WebDriver 与持续集成工具协作 持续集成指的是频繁的将代码集成到主干。它的好处主要有两个： 快速发现错误。每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 防止分支大幅偏离主干。如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是代码集成到主干之前，必须通过自动化测试，只要有一个测试用例失败，就不能集成。编程大师Martin Fowler曾经说过：“持续集成并不能消除Bug，而是让它们非常容易发现和改正。” 可以在Jenkins中安装“Seleniumhq Plugin”插件，这样就可以将Selenium IDE制作的测试用例保存为HTML格式并提供给Jenkins来使用，基本步骤是： 在执行测试的机器上，从版本控制系统中下载测试套件和测试用例。 在执行测试的机器上下载Selenium Server。 从Jenkins的“系统管理”中选择“插件管理”来安装“Seleniumhq Plugin”。 在Jenkins的“系统管理”中选择“系统设置”并配置“Selenium Remote Control”下的“HTMLSuite Runner”。 新建测试用的Jenkins任务并进行配置，配置的内容包括：浏览器、起始URL、测试套件和测试结果输出文件。 配置完成后，就可以执行Jenkins的“立即构建”了。 除了Selenium之外，WebTest、Splinter和RobotFramework也是Web端测试的选择，其中WebTest可以对WSGI应用执行模拟请求并获取结果，基本上所有WSGI应用的测试都可以用它；Splinter是对Selenium的二次封装，使用上更加方便简单。 接口测试自动化测试 requests HttpRunner PyRestTest 其他方面的自动化测试 Locust pythem 测试相关工具 PostMan AB JMeter LoadRunner Benchmark Factory WAS","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/94.网络API接口设计","date":"2024-12-12T08:38:02.775Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/94.网络API接口设计/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/94.%E7%BD%91%E7%BB%9CAPI%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"网络API接口设计目前许多的Web应用和移动应用都使用了前后端分离的开发模式，前后端分离简单的说就是前端或移动端通过网络API接口和后台进行交互，获得接口中提供的数据并负责用户界面的渲染。API是应用程序的编程接口的缩写，网络API通常指的是基于一个URL（统一资源定位符）可以访问到的资源，也就是说通过这个URL我们就可以请求服务器对某个资源进行操作并返回操作的结果。大家可以想想，网络API接口不也是一种封装吗，简单的说就是将复杂的业务逻辑隐藏在简单的API接口中。 URL的通用格式如下所示： 1协议://用户名:口令@主机:端口/路径1/.../路径N/资源名 说明：URL中的用户名（有可能不需要提供用户名）、口令（有可能不需要提供口令）、端口（有可能使用默认端口）、路径（资源有可能直接位于根路径/下）并不是必需的部分，可以根据需要进行设置。 网络API通常基于HTTP或HTTPS进行访问，基于HTTP&#x2F;HTTPS最大的好处就在于访问起来非常的简单方便，而且可以跨语言、跨应用进行访问和互操作。 设计原则关键问题为移动端或者PC端设计网络API接口一个非常重要的原则是：根据业务实体而不是用户界面或操作来设计API接口。如果API接口的设计是根据用户的操作或者界面上的功能设置来设计，随着需求的变更，用户界面也会进行调整，需要的数据也在发生变化，那么后端开发者就要不停的调整API，或者给一个API设计出多个版本，这些都会使项目的开发和维护成本增加。我们可以将业务实体理解为服务器提供的资源，而URL就是资源的定位符（标识符），这种方式是最为简单自然的。对于相对复杂的用户操作，我们可以提供一个“门面”（设计模式中的“门面模式”），通过该“门面”把多个接口的功能组装起来即可。 下面是某个网站开放API的接口，可以看出API的设计是围绕业务实体来进行的，而且都做到了“见名知意”。 评论 comments&#x2F;show 获取某条微博的评论列表 comments&#x2F;by_me 自己的评论列表 comments&#x2F;to_me 收到的评论列表 comments&#x2F;mentions @了自己的评论列表 comments&#x2F;create 创建一条评论 comments&#x2F;destroy 删除一条评论 comments&#x2F;reply 回复一条评论 需要说明的是，上面的API接口并不是REST风格的。REST是一种网络应用架构风格，被认为最适合分布式的网络应用。关于REST的知识，可以阅读阮一峰的《理解RESTful架构》以及《RESTful API设计指南》，当然这两篇文章大家也要批判的阅读，因为上面阐述的观点并不完全正确，有些内容甚至是自相矛盾的。 API接口返回的数据通常都是JSON或XML格式，XML这种数据格式目前基本已经被弃用了。对于JSON格式的数据，我们需要做到不要返回null这的值，因为这样的值一旦处置失当，会给前端和移动端开发带来不必要的麻烦（因为开发者有可能会使用强类型语言）。要解决这个问题可以从源头入手，在设计数据库的时候，尽量给每个字段都加上“not null”约束或者设置合理的默认值约束。 其他问题 更新提示问题：设计一个每次使用系统首先要访问的API，该API会向移动端返回系统更新的相关信息，这样就可以提升用户更新App了。 版本升级问题：API版本升级时应该考虑对低版本的兼容，同时要让新版本和旧版本都能够被访问，可以在URL中包含版本信息或者在将版本号放在HTTP(S)协议头部，关于这个问题有很多的争论，有兴趣的可以看看stack overflow上面对这个问题的讨论。 图片尺寸问题：移动端对于一张图片可能需要不同的尺寸，可以在获取图片时传入尺寸参数并获取对应的资源；更好的做法是直接使用云存储或CDN（直接提供了图片缩放的功能），这样可以加速对资源的访问。 文档撰写下面以设计评论接口为例，简单说明接口文档应该如何撰写。 首先，我们可以定义全局返回状态码。 返回码 返回信息 说明 10000 获取评论成功 10001 创建评论成功 10002 无法创建评论 创建评论时因违反审核机制而无法创建 10003 评论已被删除 查看评论时评论因不和谐因素已被删除 10004 …… …… 获取文章评论。 GET /articles/&#123;article-id&#125;/comments/ 开发者：王大锤 最后更新时间：2018年8月10日 标签：v 1.0 接口说明：获取指定文章的所有评论 使用帮助：默认返回20条数据，需要在请求头中设置身份标识（key） 请求参数： 参数名 类型 是否必填 参数位置 说明 page 整数 否 查询参数 页码，默认值1 size 整数 否 查询参数 每次获取评论数量（10~100），默认值20 key 字符串 是 请求头 用户的身份标识 响应信息： 123456789101112131415161718192021222324&#123; &quot;code&quot;: 10000, &quot;message&quot;: &quot;获取评论成功&quot;, &quot;page&quot;: 1, &quot;size&quot;: 10, &quot;totalPage&quot;: 35, &quot;contents&quot;: [ &#123; &quot;userId&quot;: 1700095, &quot;nickname&quot;: &quot;王大锤&quot;, &quot;pubDate&quot;: &quot;2018年7月31日&quot;, &quot;content&quot;: &quot;小编是不是有病呀&quot;, /* ... */ &#125;, &#123; &quot;userId&quot;, 1995322, &quot;nickname&quot;: &quot;白元芳&quot;, &quot;pubDate&quot;: &quot;2018年8月2日&quot;, &quot;content&quot;: &quot;楼上说得好&quot;, /* ... */ &#125; ] /* ... */&#125; 新增文章评论。 POST /articles/&#123;article-id&#125;/comments 开发者：王大锤 最后更新时间：2018年8月10日 标签：v 1.0 接口说明：为指定的文章创建评论 使用帮助：暂无 请求参数： 参数名 类型 是否必填 参数位置 说明 userId 字符串 是 消息体 用户ID key 字符串 是 请求头 用户的令牌 content 字符串 是 消息体 评论的内容 响应信息： 12345678910&#123; &quot;code&quot;: 10001, &quot;message&quot;: &quot;创建评论成功&quot;, &quot;comment&quot;: &#123; &quot;pubDate&quot;: &quot;2018年7月31日&quot;, &quot;content&quot;: &quot;小编是不是有病呀&quot; /* ... */ &#125; /* ... */&#125; 提示：如果没有接口文档撰写经验，可以使用在线接口文档编辑平台RAP2或YAPI来进行接口文档撰写。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/93.MySQL性能优化","date":"2024-12-12T08:38:02.773Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/93.MySQL性能优化/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/93.MySQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"","text":"MySQL性能优化基本原则想要发挥 MySQL 的最佳性能，需要遵循 3 个基本使用原则。 让MySQL回归存储的基本职能：MySQL 数据库只用于数据的存储，不进行数据的复杂计算，不承载业务逻辑，确保存储和计算分离； 查询数据时，尽量单表查询，减少跨库查询和多表关联； 杜绝大事务、大 SQL、大批量、大字段等一系列性能杀手。 大事务：运行步骤较多，涉及的表和字段较多，容易造成资源的争抢，甚至形成死锁。一旦事务回滚，会导致资源占用时间过长。 大 SQL：复杂的SQL意味着过多的表的关联，MySQL 数据库处理关联超过3张表以上的SQL时，占用资源多，性能低下。 大批量：多条SQL一次性执行完成，可以减少一条条执行SQL产生的额外开销，但必须确保进行充分的测试，并且在业务低峰时段或者非业务时段执行。 大字段：blob、text类型的大字段要尽量少用，必须要用时，尽量与主业务表分离，减少对这类字段的检索和更新。 建库建表 必须指定默认存储引擎为 InnoDB，并且禁用 MyISAM 存储引擎，随着 MySQL 8.0 版本的发布，所有的数据字典表都已经转换成了 InnoDB，MyISAM 存储引擎已成为了历史。 默认字符集 UTF8mb4，以前版本的 UTF8 是 UTF8mb3，未包含个别特殊字符，新版本的 UTF8mb4 包含所有字符，官方强烈建议使用此字符集。 关闭区分大小写功能。设置参数lower_case_table_names的值为1，即可关闭区分大小写功能，即大写字母 T 和小写字母 t 一样。 存储过程、触发器、视图、event等功能尽量在程序中实现，一方面是为了存储和计算分离，另一方面是因为这些功能非常不完整，调试、排错、监控都非常困难，相关数据字典也不完善，存在潜在的风险。一般在生产数据库中，禁止使用。 单个数据库实例表数量控制在2000个以内。 InnoDB表的注意事项 主键列使用unsigned整数，可以使用auto_increment，但是要禁止手动更新主键。 每个列都必须添加comment注释。 在建表时必须显示指定engine。 表必备三字段：xxx_id、 xxx_create、 xxx_modified。其中xxx_id为主键，类型unsigned整数类型（例如：int unsigned）；xxx_create、xxx_modified的类型均为datetime类型，分别记录该条数据的创建时间、修改时间。 所有字段必须指定not null，为空值指定default值，因为MySQL难以优化null值，含null值的复合索引会失效，最终导致查询效率低。 单张表的字段数尽量空值在50个字段以内，如果字段过多可以考虑垂直拆分。 禁用enum和set类型，因为这样的类型兼容性不好且性能较差。 大文件不应该使用blob类型而是保存它们的路径，blob和text这样的类型会导致处理性能下降，全表扫描代价大大增加。 对货币等对精度敏感的数据，应该使用定点数（decimal）而不是浮点数（float）。 保存IP地址不要用char(15)，应该使用int unsigned，可以使用inet_aton和inet_ntoa函数实现整数和IP地址的转换。 使用索引在前面《关系型数据库MySQL》一文中，我们已经讲到过索引的相关知识，这里我们做一个简单的回顾。 索引的设计原则 创建索引的列并不一定是select操作中要查询的列，最适合做索引的列是出现在where子句中经常用作筛选条件或连表子句中作为表连接条件的列。 具有唯一性的列，索引效果好；重复值较多的列，索引效果差。 如果为字符串类型创建索引，最好指定一个前缀长度，创建短索引。短索引可以减少磁盘I&#x2F;O而且在做比较时性能也更好，更重要的是MySQL底层的高速索引缓存能够缓存更多的键值。 创建一个包含N列的复合索引（多列索引）时，相当于是创建了N个索引，此时应该利用最左前缀进行匹配。 不要过度使用索引。索引并不是越多越好，索引需要占用额外的存储空间而且会影响写操作的性能（插入、删除、更新数据时索引也需要更新）。MySQL在生成执行计划时，要考虑各个索引的使用，这个也是需要耗费时间的。 要注意可能使索引失效的场景，例如：模糊查询使用了前置通配符、使用负向条件进行查询等。 使用过程过程，通常也称之为存储过程，它是事先编译好存储在数据库中的一组SQL的集合。调用存储过程可以简化应用程序开发人员的工作，减少与数据库服务器之间的通信，对于提升数据操作的性能是有帮助的，这些我们在之前的《关系型数据库MySQL》一文中已经提到过。 数据分区MySQL支持做数据分区，通过分区可以存储更多的数据、优化查询，获得更大的吞吐量并快速删除过期的数据。关于这个知识点建议大家看看MySQL的官方文档。数据分区有以下几种类型： RANGE分区：基于连续区间范围，把数据分配到不同的分区。 1234567891011121314CREATE TABLE tb_emp ( eno INT NOT NULL, ename VARCHAR(20) NOT NULL, job VARCHAR(10) NOT NULL, hiredate DATE NOT NULL, dno INT NOT NULL)PARTITION BY RANGE( YEAR(hiredate) ) ( PARTITION p0 VALUES LESS THAN (1960), PARTITION p1 VALUES LESS THAN (1970), PARTITION p2 VALUES LESS THAN (1980), PARTITION p3 VALUES LESS THAN (1990), PARTITION p4 VALUES LESS THAN MAXVALUE); LIST分区：基于枚举值的范围，把数据分配到不同的分区。 HASH分区 &#x2F; KEY分区：基于分区个数，把数据分配到不同的分区。 123456789CREATE TABLE tb_emp ( eno INT NOT NULL, ename VARCHAR(20) NOT NULL, job VARCHAR(10) NOT NULL, hiredate DATE NOT NULL, dno INT NOT NULL)PARTITION BY HASH(dno)PARTITIONS 4; SQL优化 定位低效率的SQL语句 - 慢查询日志。 查看慢查询日志相关配置 1234567891011121314mysql&gt; show variables like &#x27;slow_query%&#x27;;+---------------------------+----------------------------------+| Variable_name | Value |+---------------------------+----------------------------------+| slow_query_log | OFF || slow_query_log_file | /mysql/data/localhost-slow.log |+---------------------------+----------------------------------+mysql&gt; show variables like &#x27;long_query_time&#x27;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+ 创建慢查询日志文件并修改所有者。 12touch /var/log/mysqld-slow.logchown mysql /var/log/mysqld-slow.log 修改全局慢查询日志配置。 123mysql&gt; set global slow_query_log_file=&#x27;/var/log/mysqld-slow.log&#x27;mysql&gt; set global slow_query_log=&#x27;ON&#x27;; mysql&gt; set global long_query_time=1; 或者直接修改MySQL配置文件启用慢查询日志。 1234[mysqld]slow_query_log=ONslow_query_log_file=/var/log/mysqld-slow.loglong_query_time=1 注意：修改了配置文件需要重启MySQL，CentOS上对应的命令是systemctl restart mysqld。 通过explain了解SQL的执行计划。例如： 12345678910111213explain select ename, job, sal from tb_emp where dno=20\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_emp type: refpossible_keys: fk_emp_dno key: fk_emp_dno key_len: 5 ref: const rows: 7 Extra: NULL1 row in set (0.00 sec) select_type：查询类型（SIMPLE - 简单查询、PRIMARY - 主查询、UNION - 并集、SUBQUERY - 子查询）。 table：输出结果集的表。 type：访问类型（ALL - 全表查询性能最差、index、range、ref、eq_ref、const、NULL）。 possible_keys：查询时可能用到的索引。 key：实际使用的索引。 key_len：索引字段的长度。 rows：扫描的行数，行数越少肯定性能越好。 extra：额外信息。 通过show profiles和show profile for query分析SQL。 MySQL从5.0.37开始支持剖面系统来帮助用户了解SQL执行性能的细节，可以通过下面的方式来查看MySQL是否支持和开启了剖面系统。 12select @@have_profiling;select @@profiling; 如果没有开启剖面系统，可以通过下面的SQL来打开它。 1set profiling=1; 接下来就可以通过剖面系统来了解SQL的执行性能，例如： 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; select count(*) from tb_emp;+----------+| count(*) |+----------+| 14 |+----------+1 row in set (0.00 sec)mysql&gt; show profiles;+----------+------------+-----------------------------+| Query_ID | Duration | Query |+----------+------------+-----------------------------+| 1 | 0.00029600 | select count(*) from tb_emp |+----------+------------+-----------------------------+1 row in set, 1 warning (0.00 sec)mysql&gt; show profile for query 1;+----------------------+----------+| Status | Duration |+----------------------+----------+| starting | 0.000076 || checking permissions | 0.000007 || Opening tables | 0.000016 || init | 0.000013 || System lock | 0.000007 || optimizing | 0.000005 || statistics | 0.000012 || preparing | 0.000010 || executing | 0.000003 || Sending data | 0.000070 || end | 0.000012 || query end | 0.000008 || closing tables | 0.000012 || freeing items | 0.000032 || cleaning up | 0.000013 |+----------------------+----------+15 rows in set, 1 warning (0.00 sec) 优化CRUD操作。 优化insert语句 在insert语句后面跟上多组值进行插入在性能上优于分开insert。 如果有多个连接向同一个表插入数据，使用insert delayed可以获得更好的性能。 如果要从一个文本文件装载数据到表时，使用load data infile比insert性能好得多。 优化order by语句 如果where子句的条件和order by子句的条件相同，而且排序的顺序与索引的顺序相同，如果还同时满足排序字段都是升序或者降序，那么只靠索引就能完成排序。 优化group by语句 在使用group by子句分组时，如果希望避免排序带来的开销，可以用order by null禁用排序。 优化嵌套查询 MySQL从4.1开始支持嵌套查询（子查询），这使得可以将一个查询的结果当做另一个查询的一部分来使用。在某些情况下，子查询可以被更有效率的连接查询取代，因为在连接查询时MySQL不需要在内存中创建临时表来完成这个逻辑上需要多个步骤才能完成的查询。 优化or条件 如果条件之间是or关系，则只有在所有条件都用到索引的情况下索引才会生效。 优化分页查询 分页查询时，一个比较头疼的事情是如同limit 1000, 20，此时MySQL已经排序出前1020条记录但是仅仅返回第1001到1020条记录，前1000条实际都用不上，查询和排序的代价非常高。一种常见的优化思路是在索引上完成排序和分页的操作，然后根据返回的结果做表连接操作来得到最终的结果，这样可以避免出现全表查询，也避免了外部排序。 12select * from tb_emp order by ename limit 10000, 20;select * from tb_emp t1 inner join (select eno from tb_emp order by ename limit 10000, 20) t2 on t1.eno=t2.eno; 上面的代码中，第2行SQL是优于第1行SQL的，当然我们的前提是已经在ename字段上创建了索引。 使用SQL提示 USE INDEX：建议MySQL使用指定的索引。 IGNORE INDEX：建议MySQL忽略掉指定的索引。 FORCE INDEX：强制MySQL使用指定的索引。 配置优化可以使用下面的命令来查看MySQL服务器配置参数的默认值。 1234show variables;show variables like &#x27;key_%&#x27;;show variables like &#x27;%cache%&#x27;;show variables like &#x27;innodb_buffer_pool_size&#x27;; 通过下面的命令可以了解MySQL服务器运行状态值。 12345show status;show status like &#x27;com_%&#x27;;show status like &#x27;innodb_%&#x27;;show status like &#x27;connections&#x27;;show status like &#x27;slow_queries&#x27;; 调整max_connections：MySQL最大连接数量，默认151。在Linux系统上，如果内存足够且不考虑用户等待响应时间这些问题，MySQL理论上可以支持到万级连接，但是通常情况下，这个值建议控制在1000以内。 调整back_log：TCP连接的积压请求队列大小，通常是max_connections的五分之一，最大不能超过900。 调整table_open_cache：这个值应该设置为max_connections的N倍，其中N代表每个连接在查询时打开的表的最大个数。 调整innodb_lock_wait_timeout：该参数可以控制InnoDB事务等待行锁的时间，默认值是50ms，对于反馈响应要求较高的应用，可以将这个值调小避免事务长时间挂起；对于后台任务，可以将这个值调大来避免发生大的回滚操作。 调整innodb_buffer_pool_size：InnoDB数据和索引的内存缓冲区大小，以字节为单位，这个值设置得越高，访问表数据需要进行的磁盘I&#x2F;O操作就越少，如果可能甚至可以将该值设置为物理内存大小的80%。 架构优化 通过拆分提高表的访问效率。 垂直拆分 水平拆分 逆范式理论。数据表设计的规范程度称之为范式（Normal Form），要提升表的规范程度通常需要将大表拆分为更小的表，范式级别越高数据冗余越小，而且在插入、删除、更新数据时出问题的可能性会大幅度降低，但是节省了空间就意味着查询数据时可能花费更多的时间，原来的单表查询可能会变成连表查询。为此，项目实践中我们通常会进行逆范式操作，故意降低范式级别增加冗余来减少查询的时间开销。 1NF：列不能再拆分 2NF：所有的属性都依赖于主键 3NF：所有的属性都直接依赖于主键（消除传递依赖） BCNF：消除非平凡多值依赖 使用中间表提高统计查询速度。 使用insert into 中间表 select ... where ...这样的语句先将需要的数据筛选出来放到中间表中，然后再对中间表进行统计，避免不必要的运算和处理。 主从复制和读写分离，具体内容请参考《项目部署上线和性能调优》。 配置MySQL集群。 说明：本章内容参考了网易出品的《深入浅出MySQL》一书，该书和《高性能MySQL》一样，都对MySQL进行了深入细致的讲解，虽然总体感觉后者更加高屋建瓴，但是前者也算得上是提升MySQL技能的佳作（作者的文字功底稍显粗糙，深度也不及后者），建议有兴趣的读者可以阅读这两本书。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/92.Docker容器技术详解","date":"2024-12-12T08:38:02.771Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/92.Docker容器技术详解/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/92.Docker%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Docker容器技术详解Docker是基于Go语言开发的开源应用容器引擎，遵从Apache Licence 2.0协议，可以让开发者打包应用以及应用的依赖包到一个可移植的容器中，然后发布到各种发行版本的Linux系统上。 Docker简介软件开发中最为麻烦的事情可能就是配置环境了。由于用户使用的操作系统具有多样性，即便使用跨平台的开发语言（如Java和Python）都不能保证代码能够在各种平台下都可以正常的运转，而且在不同的环境下我们安装的软件需要依赖的软件包也是不一样的。 那么问题来了，我们安装软件的时候可不可以把软件运行的环境一并安装？我们是不是可以把原始环境一模一样地复制过来呢？ 虚拟机（virtual machine）就是带环境安装的一种解决方案，它可以在一种操作系统里面运行另一种操作系统，比如在Windows系统里面运行Linux系统，在macOS上运行Windows，而应用程序对此毫无感知。使用过虚拟机的人都知道，虚拟机用起来跟真实系统一模一样，而对于虚拟机的宿主系统来说，虚拟机就是一个普通文件，不需要了就删掉，对宿主系统或者其他的程序并没有影响。但是虚拟机通常会占用较多的系统资源，启动和关闭也非常的缓慢，总之用户体验并没有想象中的那么好。 Docker属于对Linux容器技术（LXC）的一种封装（利用了Linux的namespace和cgroup技术），它提供了简单易用的容器使用接口，是目前最流行的 Linux 容器解决方案。Docker将应用程序与该程序的依赖打包在一个文件里面，运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。下图是虚拟机和容器的对比，左边是传统的虚拟机，右边是Docker。 目前，Docker主要用于几下几个方面： 提供一次性的环境。 提供弹性的云服务（利用Docker很容易实现扩容和收缩）。 实践微服务架构（隔离真实环境在容器中运行多个服务）。 安装Docker下面以CentOS为例讲解如何安装Docker，使用Ubuntu、macOS或Windows的用户可以通过点击对应的链接了解这些平台下如何进行安装。 确定操作系统内核版本（CentOS 7要求64位，内核版本3.10+；CentOS 6要求64位，内核版本2.6+）。 1uname -r 更新系统底层的库文件（建议一定要执行，否则在使用Docker时可能会出现莫名其妙的问题）。 1yum update 移除可能存在的旧的Docker版本。 12yum list installed | grep dockeryum erase -y docker docker-common docker-engine 安装yum工具包和依赖项。 1yum install -y yum-utils device-mapper-persistent-data lvm2 通过yum工具包添加yum源（安装Docker-ce的源）。 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 在CentOS下使用yum安装Docker-ce并启动。 12yum -y install docker-cesystemctl start docker 查看Docker的信息和版本。 12docker versiondocker info 接下来可以通过下载镜像和创建容器来看看Docker是否可以运转起来。可以使用下面的命令从Docker的镜像仓库下载名为hello-world的镜像文件。 1docker pull hello-world 查看所有镜像文件。 1docker images 12REPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/hello-world latest fce289e99eb9 7 months ago 1.84 kB 通过镜像文件创建并运行容器。 1docker container run --name mycontainer hello-world 说明：其中mycontainer是我们给容器起的名字，跟在--name参数之后；hello-world就是我们刚才下载的镜像文件。 1234567891011121314151617181920Hello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/ 如果要删除这个容器，可以使用下面的命令。 1docker container rm mycontainer 在删除容器之后，我们还可以删除刚才下载的镜像文件。 1docker rmi hello-world 说明：如果要在Ubuntu（内核版本3.10+）下面安装和启动Docker，可以按照如下的步骤进行。 123apt updateapt install docker-ceservice docker start 国内用户可以通过更换Ubuntu软件下载源来提升下载速度，具体请参照清华大学开源软件镜像站上的《Ubuntu镜像使用帮助》。 安装Docker后，由于直接访问dockerhub下载镜像会非常缓慢，建议将服务器更换为国内镜像，可以通过修改 /etc/docker/daemon.json 文件来做到。一般的云服务器会有自己专属的镜像，就不需要手动修改了。 123456&#123; &quot;registry-mirrors&quot;: [ &quot;http://hub-mirror.c.163.com&quot;, &quot;https://registry.docker-cn.com&quot; ]&#125; 使用Docker想要玩转Docker，最简单的办法就是马上用Docker创建一些自己学习和工作中需要用到的容器，下面我们带着大家一起来创建这些容器。 运行NginxNginx是高性能的Web服务器，同时也是做反向代理服务器的上佳选择。使用Docker可以非常简单的创建一个运行Nginx的容器，命令如下所示。 1docker container run -d -p 80:80 --rm --name mynginx nginx 说明：上面的参数-d表示容器在后台运行（不产生输出到Shell）并显示容器的ID；-p是用来映射容器的端口到宿主机的端口，冒号前面是宿主机的端口，冒号后面是容器内部使用的端口；--rm表示容器停止后自动删除容器，例如执行命令docker container stop mynginx后，容器就不复存在了；--name后面的mynginx是自定义的容器名字；在创建容器的过程中，需要用到nginx的镜像文件，镜像文件的下载是自动完成的，如果没有指定版本号，默认是最新版本（latest）。 如果需要将自己的Web项目（页面）部署到Nginx上，可以使用容器拷贝命令将指定路径下所有的文件和文件夹拷贝到容器的指定目录中。 1docker container cp /root/web/index.html mynginx:/usr/share/nginx/html 如果不愿意拷贝文件也可以在创建容器时通过数据卷操作--volume将指定的文件夹映射到容器的某个目录中，例如将Web项目的文件夹直接映射到/usr/share/nginx/html目录。我们先通过下面的命令让刚才创建的容器停止运行。 1docker container stop mynginx 然后用下面的命令重新创建容器。 1docker container run -d -p 80:80 --rm --name mynginx --volume /root/docker/nginx/html:/usr/share/nginx/html nginx 说明：上面创建容器和拷贝文件的命令中，container是可以省略的，也就是说docker container run和docker run是一样的，而docker container cp和docker cp是一样的。此外，命令中的--volume也可以缩写为-v，就如同-d是--detach的缩写，-p是--publish的缩写。$PWD代表宿主系统当前文件夹，这些对于使用过Unix或者Linux系统的人来说，应该是很容易理解的。 要查看运行中的容器，可以使用下面的命令。 1docker ps 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3c38d2476384 nginx &quot;nginx -g &#x27;daemon ...&quot; 4 seconds ago Up 4 seconds 0.0.0.0:80-&gt;80/tcp mynginx 要启动和停止容器，可以使用下面的命令。 12docker start mynginxdocker stop mynginx 由于在创建容器时使用了--rm选项，容器在停止时会被移除，当我们使用下面的命令查看所有容器时，应该已经看不到刚才的mynginx容器了。 1docker container ls -a 如果在创建容器时没有指定--rm选项，那么也可以使用下面的命令来删除容器。 1docker rm mynginx 要删除正在运行中的容器，需要使用-f选项。 1docker rm -f mynginx 运行MySQL我们再来尝试用Docker安装一台MySQL服务器，首先可以先检查一下有没有MySQL的镜像文件。 1docker search mysql 123INDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATEDdocker.io docker.io/mysql MySQL is a ... 8486 [OK]... 说明：上面查询结果的列依次代表索引、镜像名、镜像描述、用户评价、是否官方镜像、自动构建。 下载MySQL镜像并指定镜像的版本号。 1docker pull mysql:5.7 如果需要查看已经下载的镜像文件，可以使用下面的命令。 1docker images 123REPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/nginx latest e445ab08b2be 2 weeks ago 126 MBdocker.io/mysql 5.7 f6509bac4980 3 weeks ago 373 MB 创建并运行MySQL容器。 1docker run -d -p 3306:3306 --name mysql57 -v /root/docker/mysql/conf:/etc/mysql/mysql.conf.d -v /root/docker/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 注意：上面创建容器时我们又一次使用了数据卷操作，那是因为通常容器是随时创建随时删除的，而数据库中的数据却是需要保留下来的。 上面的两个数据卷操作一个是映射了MySQL配置文件所在的文件夹，一个是映射了MySQL数据所在的文件夹，这两个数据卷操作非常重要。我们可以将MySQL的配置文件放在$PWD/mysql/conf目录下，配置文件的具体内容如下所示： 12345678910[mysqld]pid-file=/var/run/mysqld/mysqld.pidsocket=/var/run/mysqld/mysqld.sockdatadir=/var/lib/mysqllog-error=/var/log/mysql/error.logserver-id=1log-bin=/var/log/mysql/mysql-bin.logexpire_logs_days=30max_binlog_size=256Msymbolic-links=0 如果安装了MySQL 8.x版本（目前的最新版本），在使用客户端工具连接服务器时可能会遇到error 2059: Authentication plugin &#39;caching_sha2_password&#39; cannot be loaded的问题，这是因为MySQL 8.x默认使用了名为“caching_sha2_password”的机制对用户口令进行了更好的保护，但是如果客户端工具不支持新的认证方式，连接就会失败。解决这个问题有两种方式：一是升级客户端工具来支持MySQL 8.x的认证方式；二是进入容器，修改MySQL的用户口令认证方式。下面是具体的步骤，我们先用docker exec命令进入容器的交互式环境，假设运行MySQL 8.x的容器名字叫mysql8x。 1docker exec -it mysql8x /bin/bash 进入容器的交互式Shell之后，可以首先利用MySQL的客户端工具连接MySQL服务器。 12345678910mysql -u root -pEnter password:Your MySQL connection id is 16Server version: 8.0.12 MySQL Community Server - GPLCopyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.mysql&gt; 接下来通过SQL来修改用户口令就可以了。 1alter user &#x27;root&#x27;@&#x27;%&#x27; identified with mysql_native_password by &#x27;123456&#x27; password expire never; 当然，如果愿意你也可以查看一下用户表检查是否修改成功。 123456789use mysql;select user, host, plugin, authentication_string from user where user=&#x27;root&#x27;;+------+-----------+-----------------------+-------------------------------------------+| user | host | plugin | authentication_string |+------+-----------+-----------------------+-------------------------------------------+| root | % | mysql_native_password | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 || root | localhost | mysql_native_password | *6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9 |+------+-----------+-----------------------+-------------------------------------------+2 rows in set (0.00 sec) 在完成上面的步骤后，现在即便不更新客户端工具也可以连接MySQL 8.x了。 运行Redis接下来我们试一试运行多个容器并让多个容器之间通过网络通信。我们创建4个Redis容器来实现一主三从的主从复制结构。 1234docker run -d -p 6379:6379 --name redis-master redisdocker run -d -p 6380:6379 --name redis-slave-1 --link redis-master:redis-master redis redis-server --replicaof redis-master 6379docker run -d -p 6381:6379 --name redis-slave-2 --link redis-master:redis-master redis redis-server --replicaof redis-master 6379docker run -d -p 6382:6379 --name redis-slave-3 --link redis-master:redis-master redis redis-server --replicaof redis-master 6379 上面的命令中，--link参数用于给容器创建网络别名，因为三台从机（slave）需要通过网络连接自己的主机（master）。虽然，我们可以通过docker inspect --format &#39;&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;&#39; &lt;container-ID&gt;命令来查看到容器的IP地址，但是由于容器的即装即用性，容器的IP地址有可能会发生变化，如果直接使用IP地址，在容器重启后就可能会因为IP地址的变化导致从机无法连接到主机。使用--link参数创建网络别名就是为了在启动Redis服务器时在redis-server后面的--replicaof参数后使用这个别名而不是IP地址。 接下来我们进入名为redis-master的容器，看看主从复制的配置是否成功。 1docker exec -it redis-master /bin/bash 通过redis-cli启动命令行工具。 12345678910111213141516redis-cli127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:3slave0:ip=172.17.0.4,port=6379,state=online,offset=1988,lag=0slave1:ip=172.17.0.5,port=6379,state=online,offset=1988,lag=1slave2:ip=172.17.0.6,port=6379,state=online,offset=1988,lag=1master_replid:94703cfa03c3ddc7decc74ca5b8dd13cb8b113eamaster_replid2:0000000000000000000000000000000000000000master_repl_offset:1988second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:1988 运行GitLabGitLab是由GitLab Inc.开发的Git仓库管理工具，具有wiki、问题跟踪、持续集成等一系列的功能，分为社区版和企业版。通过Docker提供的虚拟化容器，我们可以安装社区版的Docker。因为GitLab需要使用SSH协议进行安全连接，我们要暴露容器的22端口，所以可以先将宿主机SSH连接的22端口修改为其他端口（如：12345），然后再进行后续的操作。 1vim /etc/ssh/sshd_config 将其中定义端口的那行代码去掉注释并将端口修改为12345。 1Port 12345 重新启动sshd服务。 1systemctl restart sshd 提示：修改端口后应该确保防火墙上也开启对应的端口，否则无法使用SSH连接到Linux服务器。 创建需要用于数据卷映射操作的文件夹。 1mkdir -p /root/gitlab/&#123;config,logs,data&#125; 基于gitlab/gitlab-ce镜像创建容器，并暴露80端口（HTTP连接）和22端口（SSH连接）。 1docker run -d -p 80:80 -p 22:22 --name gitlab -v /root/gitlab/config:/etc/gitlab -v /root/gitlab/logs:/var/log/gitlab -v /root/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce 说明：GitLab的启动比较缓慢，创建好容器后可能需要等待一段时间才能通过浏览器来进行访问。 首次进入GitLab访问界面会提示我们修改管理员密码，设置好管理员密码后就可以在登录界面输入用户名root和刚才设置的密码登录到管理员控制台，在使用上还是非常简单和人性化的。 构建镜像通过上面的讲解，我们已经掌握了如何通过官方提供的镜像来创建容器。当然如果愿意，我们也可以用配置好的容器来生成镜像。简而言之，Docker镜像是由文件系统叠加而成的，系统的最底层是bootfs，相当于就是Linux内核的引导文件系统；接下来第二层是rootfs，这一层可以是一种或多种操作系统（如Debian或Ubuntu文件系统），Docker中的rootfs是只读状态的；Docker利用联合挂载技术将各层文件系统叠加到一起，最终的文件系统会包含有底层的文件和目录，这样的文件系统就是一个镜像。 之前我们讲过了如何查找、列出镜像和拉取（下载）镜像，接下来看看构建镜像的两种方式： 使用docker commit命令。（不推荐） 使用docker build命令和Dockerfile文件。 使用commit命令构建镜像为了演示如何构建镜像，我们先使用Ubuntu镜像来定制一个容器，命令如下所示。 1docker run --name myubuntu -it ubuntu /bin/bash 在容器中执行下面的命令来安装Apache服务器并退出容器。 123apt -y upgradeapt -y install apache2exit 我们将这个容器作为一个定制的Web服务器保存起来，当需要这样一台Web服务器的时候，就没有必要重新创建容器并安装Apache了。 首先我们通过下面的命令查看容器的ID。 1docker container ls -a 123docker container ls -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES014bdb321612 ubuntu &quot;/bin/bash&quot; 5 minutes ago Exited (0) myubuntu 提交定制的容器。 1docker commit 014bdb321612 jackfrued/mywebserver 查看镜像文件。 1docker images 12REPOSITORY TAG IMAGE ID CREATED SIZEjackfrued/mywebserver latest 795b294d265a 14 seconds ago 189 MB 生成镜像文件以后，后面就可以利用刚才创建的镜像文件来创建新的容器。 使用Dockerfile构建镜像Dockerfile使用DSL（Domain Specific Language）来构建一个Docker镜像，只要编辑好了Dockerfile文件，就可以使用docker build命令来构建一个新的镜像。 我们先创建一个名为myapp的文件夹来保存项目代码和Dockerfile的文件，如下所示： 1234567[ECS-root temp]# tree myappmyapp├── api│ ├── app.py│ ├── requirements.txt│ └── start.sh└── Dockerfile 其中api是Flask项目的文件夹，其中包括了项目代码、依赖项以及启动脚本等文件，具体内容如下所示： app.py文件： 1234567891011121314151617from flask import Flaskfrom flask_restful import Resource, Apifrom flask_cors import CORSapp = Flask(__name__)CORS(app, resources=&#123;r&#x27;/api/*&#x27;: &#123;&#x27;origins&#x27;: &#x27;*&#x27;&#125;&#125;)api = Api(app)class Product(Resource): def get(self): products = [&#x27;Ice Cream&#x27;, &#x27;Chocolate&#x27;, &#x27;Coca Cola&#x27;, &#x27;Hamburger&#x27;] return &#123;&#x27;products&#x27;: products&#125;api.add_resource(Product, &#x27;/api/products&#x27;) requirements.txt文件： 1234flaskflask-restfulflask-corsgunicorn start.sh文件： 12#!/bin/bashexec gunicorn -w 4 -b 0.0.0.0:8000 app:app 提示：需要给start.sh文件以执行权限，可以使用chmod 755 start.sh命令来做到。 Dockerfile文件： 1234567891011121314# 指定基础镜像FROM python:3.7# 指定镜像的维护者MAINTAINER jackfrued &quot;jackfrued@126.com&quot;# 将指定文件添加到容器中指定的位置ADD api/* /root/api/# 设置工作目录WORKDIR /root/api# 执行命令(安装Flask项目的依赖项)RUN pip install -r requirements.txt -i https://pypi.doubanio.com/simple/# 容器启动时要执行的命令ENTRYPOINT [&quot;./start.sh&quot;]# 暴露端口EXPOSE 8000 我们来解释一下上面的Dockerfile文件。Dockerfile文件通过特殊的指令来指定基础镜像（FROM指令）、创建容器后需要指定的命令（RUN指令）以及需要暴露的端口（EXPOSE）等信息。我们稍后会专门为大家介绍这些Dockfile中的指令。 接下来我们可以使用docker build命令来创建镜像，如下所示。 1docker build -t &quot;jackfrued/myapp&quot; . 提示：上面的命令最后面的. 千万不要漏掉了哦，它表示从当前路径下寻找Dockerfile。 通过下面的命令可以查看创建好的镜像。 1docker images 12REPOSITORY TAG IMAGE ID CREATED SIZEjackfrued/myapp latest 6d6f026a7896 5 seconds ago 930 MB 如果想知道镜像文件是如何创建出来的，可以使用下面的命令。 1docker history jackfrued/myapp 12345678IMAGE CREATED CREATED BY SIZE COMMENT6d6f026a7896 31 seconds ago /bin/sh -c #(nop) EXPOSE 8000/tcp 0 B 3f7739173a79 31 seconds ago /bin/sh -c #(nop) ENTRYPOINT [&quot;./start.sh&quot;] 0 B 321e6bf09bf1 32 seconds ago /bin/sh -c pip install -r requirements.txt... 13 MB 2f9bf2c89ac7 37 seconds ago /bin/sh -c #(nop) WORKDIR /root/api 0 B 86119afbe1f8 37 seconds ago /bin/sh -c #(nop) ADD multi:4b76f9c9dfaee8... 870 B 08d465e90d4d 3 hours ago /bin/sh -c #(nop) MAINTAINER jackfrued &quot;j... 0 B fbf9f709ca9f 12 days ago /bin/sh -c #(nop) CMD [&quot;python3&quot;] 0 B 使用该镜像来创建容器运行Web服务器。 1docker run -d -p 8000:8000 --name myapp jackfrued/myapp 如果希望将上面创建的镜像文件放到dockerhub仓库中，可以按照如下所示的步骤进行操作。 通过下面的命令登录到dockerhub。 1docker login 输入用户名和口令进行登录。 1234Login with your Docker ID to push and pull images from Docker Hub. If you don&#x27;t have a Docker ID, head over to https://hub.docker.com to create one.Username: jackfruedPassword: Login Succeeded 通过下面的命令将镜像推到仓库中。 1docker push jackfrued/webserver Dockerfile指令想了解Dockerfile的指令可以查看官方提供的参考手册，下面我们为大家介绍一些常用的指令。 FROM：设置基础镜像，必须是Dockerfile中的第一条指令。 1FROM &lt;镜像名&gt; [AS &lt;别名&gt;] 或 1FROM &lt;镜像名&gt;[:&lt;标签&gt;] [AS &lt;别名&gt;] RUN：指定构建镜像时要执行的命令。 1RUN &lt;命令&gt; [参数1], [参数2], ... 或 1RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;, ...] CMD：指定构建镜像后要执行的命令。 1CMD &lt;命令&gt; [参数1], [参数2], ... 或 1CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;, ...] 说明：Docker不同于虚拟机，容器本身就是一个进程，容器中的应用应该位于前台运行。CMD命令相当于就是用来指定容器主进程（创建容器后要在前台执行的程序）的，如果主进程结束了，容器也就停止运行了。所以在容器中启动Nginx不能使用service nginx start或是systemctl start nginx而是要通过CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]让它在前台运行。 ENTRYPOINT：和CMD类似，也可以执行命令，但docker run命令行中指定的任何参数都会被当做参数再次传给ENTRYPOINT指令中的命令，这就使得我们可以构建一个镜像，它既可以运行一个默认的命令，也支持通过docker run命令行为该命令指定可覆盖的参数选项。 1ENTRYPOINT &lt;命令&gt; [参数1], [参数2], ... 或 1ENTRYPOINT [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;, ...] WORKDIR：在通过镜像创建新容器时，在容器内部创建一个工作目录，ENTRYPOINT和CMD指定的程序会在这个目录下执行。在使用docker run命令时可以通过-w参数来覆盖由WORKDIR指定的工作目录。例如： 1WORKDIR /opt/webapp 1docker run -w /usr/share/webapp ... ENV：在创建镜像时设置环境变量。在使用docker run命令时，可以通过-e参数来修改环境变量的设置。例如： 1ENV DEFAULT_PORT=8080 1docker run -e &quot;DEFAULT_PORT=8000&quot; ... USER：指定镜像会以什么用户身份去运行。例如： 1USER nginx VOLUME：在创建容器时添加一个数据卷的挂载点。通过数据卷操作可以实现容器间数据的共享和重用，对卷所作的修改可以马上生效而不需要重新启动容器，我们之前创建容器时使用--volume参数就是为了实现数据卷的映射操作。 1VOLUME [&quot;/路径1&quot;, &quot;/路径2/子路径2.1/&quot;, ...] ADD：将构建目录下的文件和文件夹复制到镜像中，如果是压缩文件和归档文件，ADD命令会对这些文件进行解压缩解归档的操作。 1ADD [--chown=&lt;用户&gt;:&lt;用户组&gt;] &lt;源文件&gt; &lt;目标文件&gt; COPY：非常类似于ADD，但不会主动对文件进行提取操作。 LABEL：为Docker镜像添加一些元数据，在使用docker inspect命令时会看到这些元数据。 1LABEL version=&quot;1.0.0&quot; location=&quot;Chengdu&quot; ONBUILD：为镜像添加触发器，当一个镜像被用作其他镜像的基础镜像，触发器将会被执行。例如： 12ONBUILD ADD . /app/srcONBUILD RUN cd /app/src &amp;&amp; make 多容器管理我们的项目可能会使用了多个容器，容器多了之后管理容器的工作就会变得麻烦。如果要对多个容器进行自动配置使得容器可以相互协作甚至实现复杂的调度，这就需要进行容器编排。Docker原生对容器编排的支持非常弱，但是可以通过社区提供的工具来实现容器编排。 Docker Compose可以通过安装Docker Compose工具来实现基于YAML文件的容器编排，YAML文件会定义一系列的容器以及容器运行时的属性，Docker Compose会根据这些配置来管理容器。 安装Docker Compose。 12curl -L &quot;https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 说明：如果没有curl工具，在CentOS下可以先通过包管理工具yum安装curl再执行上面的命令。 当然我们也可以使用Python的包管理工具pip来安装Docker Compose，命令如下所示。 1pip3 install -U docker-compose 使用Docker Compose。 我们在刚才的Flask项目中引入缓存，然后再利用Flask提供的数据接口为前端页面提供数据，使用Vue.js进行页面渲染并将静态页面部署在Nginx服务器上。项目文件夹结构如下所示： 1234567891011[ECS-root ~]# tree temptemp├── docker-compose.yml├── html│ └── index.html└── myapp ├── api │ ├── app.py │ ├── requirements.txt │ └── start.sh └── Dockerfile 修改后的app.py文件代码如下所示： 1234567891011121314151617181920212223242526from pickle import dumps, loadsfrom flask import Flaskfrom flask_restful import Resource, Apifrom flask_cors import CORSfrom redis import Redisapp = Flask(__name__)CORS(app, resources=&#123;r&#x27;/api/*&#x27;: &#123;&#x27;origins&#x27;: &#x27;*&#x27;&#125;&#125;)api = Api(app)redis = Redis(host=&#x27;redis-master&#x27;, port=6379)class Product(Resource): def get(self): data = redis.get(&#x27;products&#x27;) if data: products = loads(data) else: products = [&#x27;Ice Cream&#x27;, &#x27;Chocolate&#x27;, &#x27;Coca Cola&#x27;, &#x27;Hamburger&#x27;] redis.set(&#x27;products&#x27;, dumps(products)) return &#123;&#x27;products&#x27;: products&#125;api.add_resource(Product, &#x27;/api/products&#x27;) html文件夹用来保存静态页面，稍后我们会通一个运行Nginx的容器来向浏览器提供静态页面。index.html文件的内容如下所示： 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;app&quot;&gt; &lt;h2&gt;产品列表&lt;/h2&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt;&#123;&#123; product &#125;&#125;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;script src=&quot;https://cdn.bootcss.com/vue/2.6.10/vue.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [] &#125;, created() &#123; fetch(&#x27;http://1.2.3.4:8000/api/products&#x27;) .then(resp =&gt; resp.json()) .then(json =&gt; &#123;this.products = json.products&#125;) &#125; &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 接下来，我们要通过docker-compose.yml文件来创建三个容器并指明容器之间的依赖关系。 123456789101112131415161718version: &#x27;3&#x27;services: api-server: build: ./myapp ports: - &#x27;8000:8000&#x27; links: - redis-master web-server: image: nginx ports: - &#x27;80:80&#x27; volumes: - ./html:/usr/share/nginx/html redis-master: image: redis expose: - &#x27;6379&#x27; 有了这个YAML文件，我们就可以使用docker-compose命令来创建容器运行项目，其命令如下所示： 12345678910111213141516171819202122[ECS-root temp]# docker-compose upCreating network &quot;temp_default&quot; with the default driverCreating temp_web-server_1 ... doneCreating temp_redis-master_1 ... doneCreating temp_api-server_1 ... doneAttaching to temp_redis-master_1, temp_web-server_1, temp_api-server_1redis-master_1 | 1:C 05 Dec 2019 11:57:26.828 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Ooredis-master_1 | 1:C 05 Dec 2019 11:57:26.828 # Redis version=5.0.6, bits=64, commit=00000000, modified=0, pid=1, just startedredis-master_1 | 1:C 05 Dec 2019 11:57:26.828 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.confredis-master_1 | 1:M 05 Dec 2019 11:57:26.830 * Running mode=standalone, port=6379.redis-master_1 | 1:M 05 Dec 2019 11:57:26.831 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.redis-master_1 | 1:M 05 Dec 2019 11:57:26.831 # Server initializedredis-master_1 | 1:M 05 Dec 2019 11:57:26.831 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add &#x27;vm.overcommit_memory = 1&#x27; to /etc/sysctl.conf and then reboot or run the command &#x27;sysctl vm.overcommit_memory=1&#x27; for this to take effect.redis-master_1 | 1:M 05 Dec 2019 11:57:26.831 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command &#x27;echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled&#x27; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.redis-master_1 | 1:M 05 Dec 2019 11:57:26.831 * Ready to accept connectionsapi-server_1 | [2019-12-05 11:57:27 +0000] [1] [INFO] Starting gunicorn 20.0.4api-server_1 | [2019-12-05 11:57:27 +0000] [1] [INFO] Listening at: http://0.0.0.0:8000 (1)api-server_1 | [2019-12-05 11:57:27 +0000] [1] [INFO] Using worker: syncapi-server_1 | [2019-12-05 11:57:27 +0000] [8] [INFO] Booting worker with pid: 8api-server_1 | [2019-12-05 11:57:27 +0000] [9] [INFO] Booting worker with pid: 9api-server_1 | [2019-12-05 11:57:27 +0000] [10] [INFO] Booting worker with pid: 10api-server_1 | [2019-12-05 11:57:27 +0000] [11] [INFO] Booting worker with pid: 11 要停止容器的运行，可以使用下面的命令。 1docker-compose down Kubernetes（K8S）实际的生产环境中常常需要部署和管理多个协同工作的容器，docker compose解决了多容器创建和管理的问题，但是实际项目中，我们还需要Kubernetes（以下都简称为K8S）来提供一个跨主机集群的容器调度平台。K8S可以进行自动化容器的部署、扩展和操作，从而提供以容器为中心的基础架构。该项目是谷歌在2014年启动的项目，建立在谷歌公司十余年运维经验的基础之上，而且谷歌自己的应用也是运行在容器上的。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/91.团队项目开发的问题和解决方案","date":"2024-12-12T08:38:02.769Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/91.团队项目开发的问题和解决方案/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/91.%E5%9B%A2%E9%98%9F%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"团队项目开发的问题和解决方案个人开发和团队开发这两个词相信对大家来说并不陌生。所谓个人开发就是一个人把控产品的所有内容；而团队开发则是由多个人组团并完成产品的开发。要实施团队开发以下几点是不可或缺的： 对开发过程中的各种事件（例如：谁到什么时间完成了什么事情）进行管理和共享。 在团队内部共享各类工作成果以及新的知识技巧等。 管理工作成果的变更，既要防止成果被破坏，又要保证各个成员利用现有成果并行作业。 证明团队开发出的软件在任何时候都是可以正常运行的。 使用自动化的工作流程，让团队成员能够正确的实施开发、测试和部署。 团队项目开发常见问题团队开发相较于个人开发，容易遇到以下几个方面的问题。 问题1：传统的沟通方式无法确定处理的优先级例如：使用邮件进行沟通可能出现邮件数量太多导致重要的邮件被埋没，无法管理状态，不知道哪些问题已经解决，哪些问题尚未处理，如果用全文检索邮件的方式来查询相关问题效率过于低下。 解决方案：使用缺陷管理工具。 问题2：没有能够用于验证的环境例如：收到项目正式环境中发生的故障报告后，需要还原正式环境需要花费很长的时间。 解决方法：实施持续交付。 问题3：用别名目录管理项目分支解决方法：实施版本控制。 问题4：重新制作数据库非常困难例如：正式环境和开发环境中数据库表结构不一致或者某个表列的顺序不一致。 解决方法：实施版本控制。 问题5：不运行系统就无法察觉问题例如：解决一个bug可能引入其他的bug或者造成系统退化，不正确的使用版本系统覆盖了其他人的修改，修改的内容相互发生了干扰，如果问题不能尽早发现，那么等过去几个月后再想追溯问题就非常麻烦了。 解决方法：实施持续集成，将团队成员的工作成果经常、持续的进行构建和测试。 问题6：覆盖了其他成员修正的代码解决方法：实施版本控制。 问题7：无法实施代码重构例如：在实施代码重构（在不影响代码产生的结果的前提下对代码内部的构造进行调整）时可能引发退化。 解决方法：大量的可重用的测试并实施持续集成。 问题8：不知道bug的修正日期无法追踪退化解决方法：版本控制系统、缺陷管理系统和持续集成之间需要交互，最好能够和自动化部署工具集成到一起来使用。 问题9：发布过程太复杂解决方法：实施持续交付。 基于对上述问题的阐述和分析，我们基本上可以得到以下的结论，在团队开发中版本控制、缺陷管理和持续集成都是非常重要且不可或缺的。 版本控制针对上面提到的一系列问题，我们可以得出一个简单的结论，版本控制是实施团队开发的首要前提，必须通过版本控制对产品研发过程中产生的各种信息进行管理，这些内容包括： 代码。 需求和设计的相关文档。 数据库模式和初始数据。 配置文件。 库的依赖关系定义。 Git简介 Git是诞生于2005年的一个开源分布式版本控制系统，最初是Linus Torvalds（Linux之父） 为了帮助管理Linux内核开发而开发的一个版本控制软件。Git与常用的版本控制工具Subversion等不同，它采用了分布式版本控制的方式，在没有中央服务器支持的环境下也能够实施版本控制。 对于有使用Subversion（以下简称为SVN）经验的人来说，Git和SVN的共同点是摒弃了传统的基于锁定模式的版本控制（早期的CVS和VSS使用了锁定模式，当一个开发者编辑一个文件时会锁定该文件，其他开发者在此期间无法编辑该文件），采用了更有效率的基于合并模式的版本控制，而二者的区别在于： Git是分布式的，SVN是集中式的，SVN需要中央服务器的支持才能工作。 Git把内容按元数据方式存储，而SVN是按文件，即把文件的元信息隐藏在一个.svn文件夹里。 Git分支和SVN的分支不同，SVN对分支的处理是相当“狗血”的。 Git没有一个全局版本号，但是可以自己维护一个版本标签。 Git的内容完整性要优于SVN，Git的内容存储使用的是SHA-1哈希算法。这能确保代码内容的完整性，确保在遇到磁盘故障和网络问题时降低对版本库的破坏。 总而言之，Git真的非常棒！！！ 安装Git可以在Git官方网站找到适合自己系统的Git下载链接并进行安装，macOS和Windows平台下安装Git都非常简单，Linux下如果要安装官方最新的版本，建议通过官方提供的Git源代码进行构建安装，步骤如下所示（以CentOS为例）。 下载Git源代码压缩文件。 1wget https://mirrors.edge.kernel.org/pub/software/scm/git/git-2.23.0.tar.xz 解压缩和解归档。 12xz -d git-2.23.0.tar.xztar -xvf git-2.23.0.tar 安装底层依赖库。 1yum -y install libcurl-devel 说明：没有这个依赖库，git的网络功能将无法执行。 安装前的配置。 12cd git-2.23.0./configure --prefix=/usr/local 构建和安装。 1make &amp;&amp; make install 安装成功后可以在终端中键入下面的命令检查自己的Git版本。 1git --version 如果之前完全没有接触过Git，可以先阅读《git - 简易指南》来对Git有一个大致的了解。 Git本地操作可以使用下面的命令将一个文件夹变成Git仓库。 1git init 当你完成了上述操作后，本地目录就变成了下面的样子，下图左边是你的工作区（正在操作的工作目录），而右边是你的本地仓库，中间是工作区和本地仓库之间的暂存区（也称为缓存区）。 提示：用ls -la查看所有文件会发现在执行完上面的命令后，文件夹下多了一个名为.git的隐藏文件夹，这个就是本地的Git版本仓库。 通过git add可以将指定的文件或所有文件添加到暂存区。 12git add &lt;file&gt;git add . 这个时候使用下面的命令可以查看工作区、暂存区和本地仓库的状态。 1git status 提示：如果不希望将文件添加到暂存区，可以按照提示，使用git rm --cached &lt;file&gt;命令将文件从暂存区放回到工作区。 如果这个时候对工作区的文件又进行了修改使得工作区和暂存区的内容并不相同了，再次执行git status可以看到哪个或哪些文件被修改了，如果希望用暂存区的内容恢复工作区，可以使用下面的命令。 12git restore &lt;file&gt;git restore . 注意：上面的命令目前仍然处于试验性阶段，在Git较早的版本中对应的命令是git checkout -- &lt;file&gt;。由于git checkout这个命令还可以用于切换分支，容易引起混淆，所以Git最新版本中将这个命令的两项功能分别赋予两个新的命令，一个就是上面的git restore，另一个是git switch。 如果第一次使用Git，需要配置用户名和邮箱，然后才能将代码提交到仓库。 12git config --global user.name &quot;jackfrued&quot;git config --global user.email &quot;jackfrued@126.com&quot; 提示：可以用git config --list来查看Git的配置信息。 通过下面的命令可以将暂存区的内容纳入本地仓库， 1git commit -m &#x27;本次提交的说明&#x27; 可以通过git log查看每次提交对应的日志。 12git loggit log --graph --oneline --abbrev-commit Git服务器概述Git不像SVN那样一定需要中央服务器才能工作，上面我们演示的版本控制操作都是在本地执行的，但是对于企业开发多人协作这样的场景还是需要中央服务器的支持。通常，企业可以选择使用代码托管平台（如GitHub）或自己搭建Git私服的方式来建立中央服务器（版本仓库），当然大多数的企业更倾向于后者。Github创办于2008年4月，目前是全世界最大的代码托管平台，支持企业用户（可以创建私有仓库，私有仓库内容不对外界公开）和普通用户（受限的使用私有仓库，不受限的使用公开仓库，公开仓库内容对他人可见）。Github上面代码库惊人的增长速度证明了它是非常成功的，在2018年6月被微软以75亿美元的天价收购。 国内也有不少类似Github的代码托管平台，最有名的当属码云和CODING，目前码云和CODING对注册用户都提供了受限的使用私有仓库的功能，支持Pull Request（一种对话机制，可以在提交你的工作成果时让相关人员或团队注意到这件事情），同时还提供了对缺陷管理、Webhook等功能支持，这些使得版本控制系统还具备了缺陷管理和持续集成的能力。当然，很多公司都不愿意将自己的商业代码托管于别人的平台，这样的公司可以用Gitlab来搭建公司内部的Git私服，具体的做法在下一章为大家介绍。 这里我们直接以码云为例来说明使用Git服务器的一些注意事项。首先需要在码云上注册账号，当然也可以使用第三方登录（github账号、微信账号、新浪微博账号、CSDN账号等），登录成功后就可以创建项目，创建项目几乎是“傻瓜式”的，无需赘述，我们只对几个地方加以说明。 创建项目时不建议勾选如下图所示的这些选项，编程语言可以暂时不做选择，而.gitignore模板也可以稍后自己编写或者通过更专业的工具（如：http://gitignore.io/网站）自动生成。 添加项目成员。创建项目后，可以在项目的“设置”或“管理”中找到“成员管理”功能，这样就可以将其他开发者设置为项目团队的成员，项目成员通常分为“所有者”、“管理者”、“普通成员”和“受限成员”几种角色。 项目的分支。创建项目后，项目只有一个默认的master分支，应该将该分支设置为“保护分支”来避免项目管理者之外的成员修改该分支（不可直接提交）。当然，如果需要我们也可以在线创建新的代码分支。 设置公钥实现免密访问。在项目的“设置”或“管理”中我们还可以找到“部署公钥管理”的选项，通过添加部署公钥，可以通过SSH（安全远程连接）的方式访问服务器而不用每次输入用户名和口令。可以使用ssh-keygen命令来创建密钥对。 1ssh-keygen -t rsa -b 2048 -C &quot;your_email@example.com&quot; 说明：上面命令生成的密钥对在~/.ssh目录下，公钥文件默认的名字为id_rsa.pub，可以通过cat id_rsa.pub来查看自己的公钥。Windows用户在安装Git工具后，可以通过Git Bash来输入上面的命令。 Git远程操作拥有了Git服务器之后，我们就可以通过Git的远程操作将自己的工作成果推到服务器的仓库中，也可以将他人的工作成果从服务器仓库更新到本地。我们以刚才在码云上创建的仓库（仓库名为python）为例来说明如何进行远程操作。可以在如下所示的页面上找到仓库的地址（URL），如果配置了SSH Key就使用SSH方式访问仓库，否则就用HTTPS方式，后者需要在进行远程操作时提供用户名和口令。 添加远程仓库（Git服务器）。 1git remote add origin git@gitee.com:jackfrued/python.git 其中git@gitee.com:jackfrued/python.git是上图中显示的仓库的URL，而前面的origin是替代这个冗长的URL的字符串，简单的说origin就是服务器上仓库的别名（如果有多个Git服务器，这个简短的名字也会有多个）。可以用git remote -v来查看已经指定的Git服务，也可以用git remote remove来删除指定的Git服务器。 将本地代码（工作成果）推送到远程仓库。 1git push -u origin master:master 其中，-u是--set-upstream的缩写，用来指定推送的服务器仓库，后面的origin就是刚才给仓库起的简短的别名，冒号前面的master是本地分支名，冒号后面的master是远程分支名，如果本地分支master已经和远程分支master建立过关联，则冒号以及后面的部分可以省略。 从远程仓库取回代码。 1git pull origin master Git分支操作 创建和切换分支。下面的命令创建了名为dev 的分支并切换到该分支。 12git branch &lt;branch-name&gt;git switch &lt;branch-name&gt; 或 1git switch -c &lt;branch-name&gt; 注意：在之前的Git版本中，切换分支使用git checkout &lt;branch-name&gt;命令，也可以通过git checkout -b &lt;branch-name&gt;来创建并切换分支。git switch命令目前仍然处于试验性阶段，但很明显这个命令更加清晰的表达了它要做的事情。 关联远程分支。例如：如果当前所在的分支还没有关联到远程分支，可以使用下面的命令为它们建立关联。 1git branch --set-upstream-to origin/develop 如果需要为指定的分支关联远程分支，可以如下操作。 1git branch --set-upstream-to origin/develop &lt;branch-name&gt; 提示：上面的操作假设Git服务器上存在名为develop的分支，--set-upstream-to可以缩写为-u。 当然，在创建分支时，如果使用了--track参数，也可以直接指定与本地分支关联的远程分支，如下所示。 1git branch --track &lt;branch-name&gt; origin/develop 如果需要解除本地分支与远程分支的关联，可以使用下面的命令。 1git branch --unset-upstream &lt;branch-name&gt; 分支合并。例如在dev分支上完成开发任务之后，如果希望将dev分支上的成果合并到master，可以先切回到master分支然后使用git merge来做分支合并，合并的结果如下图右上方所示。 12git switch mastergit merge --no-ff dev 使用git merge合并分支时，默认使用Fast Forward合并，这意味着如果删除了分支，分支上的信息就全都丢掉了，如果希望将分支上的历史版本保留下来，可以使用--no-ff参数来禁用Fast Forward。 在合并分支时，没有冲突的部分Git会做自动合并。如果发生了冲突（如dev和master分支上都修改了同一个文件），会看到CONFLICT (content): Merge conflict in &lt;filename&gt;. Automatic merge failed; fix conflicts and then commit the result（自动合并失败，修复冲突之后再次提交）的提示，这个时候我们可以用git diff来查看产生冲突的内容。解决冲突通常需要当事人当面沟通之后才能决定保留谁的版本，冲突解决后需要重新提交代码。 分支变基。分支合并操作可以将多个分支上的工作成果最终合并到一个分支上，但是再多次合并操作之后，分支可能会变得非常的混乱和复杂，为了解决这个问题，可以使用git rebase操作来实现分支变基。如下图所示，当我们希望将master和dev上的工作成果统一到一起的时候，也可以使用变基操作。 123git rebase mastergit switch mastergit merge dev 当我们在dev分支执行git rebase命令时，将首先计算dev分支和master分支的差集，然后应用该差集到dev分支，最后我们切回到master分支并执行操作合并，这样就看到了如上图右下方所示的干净的分支。 删除分支。删除分支可以使用git branch加上-d参数，如果分支上的工作成果还没有合并，那么在删除分支时会看到error: The branch &#39;&lt;branch-name&gt;&#39; is not fully merged.这样的错误提示。如果希望强行删除分支，可以使用-D参数。删除分支的操作如下所示。 1234git branch -d &lt;branch-name&gt;error: The branch &#x27;&lt;branch-name&gt;&#x27; is not fully merged.If you are sure you want to delete it, run &#x27;git branch -D &lt;branch-name&gt;&#x27;.git branch -D &lt;branch-name&gt; 如果要删除远程分支，可以使用下面的命令，但是请慎重的操作。 12git branch -r -d origin/developgit push origin :develop 或者 1git push origin --delete develop Git其他操作 git fetch：下载远程仓库的所有变动，可以将远程仓库下载到一个临时分支，然后再根据需要进行合并操作，git fetch命令和git merge命令可以看作是之前讲的git pull命令的分解动作。 12git fetch origin master:tempgit merge temp git diff：常用于比较工作区和仓库、暂存区与仓库、两个分支之间有什么差别。 git stash：将当前工作区和暂存区发生的变动放到一个临时的区域，让工作区变干净。这个命令适用于手头工作还没有提交，但是突然有一个更为紧急的任务（如线上bug需要修正）需要去处理的场景。 123git stashgit stash listgit stash pop git reset：回退到指定的版本。该命令主要有三个参数，如下图所示。 git cherry-pick：挑选某个分支的单次提交并作为一个新的提交引入到你当前分支上。 git revert：撤回提交信息。 git tag：经常用于查看或新增一个标签。 Git工作流程（分支管理策略）既然Git是团队开发必备的工具，那么在团队协作时就必须有一个规范的工作流程，这样才能让团队高效的工作，让项目顺利的进展下去，否则工具再厉害但团队成员各自为战，冲突就会无处不在，协作更加无从谈起。我们仍然以刚才码云上创建的python项目为例，来说明Git的分支管理策略。 Github-flow 克隆服务器上的代码到本地。 1git clone git@gitee.com:jackfrued/python.git 创建并切换到自己的分支。 1git switch -c &lt;branch-name&gt; 或 1git checkout -b &lt;branch-name&gt; 在自己的分支上开发并在本地做版本控制。 将自己的分支（工作成果）推到服务器。 1git push origin &lt;branch-name&gt; 在线发起一次合并请求（通常称之为Pull Request，有的地方称为Merge Request），请求将自己的工作成果合并到master分支，合并之后可以删除该分支。 上面这种分支管理策略就是被称为github-flow或PR的流程，它非常简单容易理解，只需要注意以下几点： master的内容都是可以进行发布的内容（不能直接在master上进行修改）。 开发时应该以master为基础建立新分支（日常开发任务在自己的分支上进行）。 分支先在本地实施版本控制，然后以同名分支定期向服务器进行push操作。 开发任务完成后向master发送合并请求。 合并请求通过审查之后合并到master，并从master向正式环境发布。 当然，github-flow的缺点也很明显，master分支默认就是当前的线上代码，但是有的时候工作成果合并到master分支，并不代表它就能立刻发布，这样就会导致线上版本落后于master分支。 Git-flow除了上述的github-flow分支管理策略外，还有一种名为git-flow的分支管理策略，它也是大多数公司愿意使用的一套流程。Git-flow借鉴了中央集权型版本控制系统的长处，为团队内部统一建立、合并和关闭分支的方法，如下图所示。 在这种模式下，项目有两个长线分支，分别是master和develop，其他都是临时的的辅助分支，包括feature（开发特定功能的分支，开发结束后合并到develop）、release（从develop分离出来的为发布做准备的分支，发布结束后合并到master和develop）和hotfix（产品发布后出现问题时紧急建立的分支，直接从master分离，问题修复后合并到master并打上标签，同时还要合并到develop来避免将来的版本遗漏了这个修复工作，如果此时有正在发布中的release分支，还要合并到release分支）。具体的实施过程如下所示： 最开始的时候只有master和develop分支，如上图左侧所示。 从develop分支创建feature分支（上图右上），工作完成后将工作成果合并到develop分支（上图右中）。 创建feature分支： 1git switch -c feature/user develop 或 1git checkout -b feature/user develop 接下来就是在feature分支上进行开发并实施版本控制，这一段如何操作我们就不再赘述了。工作完成后，将feature分支合并到develop分支： 1234git checkout developgit merge --no-ff feature/usergit branch -d feature/usergit push origin develop 从develop分支创建release分支，发布结束后合并回master和develop分支。 创建release分支： 12345git checkout -b release-0.1 developgit push -u origin release-0.1... ... ...git pullgit commit -a -m &quot;............&quot; 将release分支合并回master和develop分支： 123456789101112git checkout mastergit merge --no-ff release-0.1git pushgit checkout developgit merge --no-ff release-0.1git pushgit branch -d release-0.1git push --delete release-0.1git tag v0.1 mastergit push --tags 从master分支创建hotfix分支，在修复bug后合并到develop和master分支（上图右下）。 创建hotfix分支： 12345git checkout -b hotfix-0.1.1 mastergit push -u origin hotfix-0.1.1... ... ...git pullgit commit -a -m &quot;............&quot; 将hotfix分支合并回develop和master分支。 123456789101112git checkout mastergit merge --no-ff hotfix-0.1.1git pushgit checkout developgit merge --no-ff hotfix-0.1.1git pushgit branch -d hotfix-0.1.1git push --delete hotfix-0.1.1git tag v0.1.1 mastergit push --tags Git-flow流程比较容易控制各个分支的状况，但是在运用上github-flow要复杂得多，因此实际使用的时候通常会安装名为gitflow的命令行工具（Windows环境的Git自带了该工具）或者使用图形化的Git工具（如：SmartGit、SourceTree等）来简化操作，具体的可以参考《git-flow 的工作流程》一文，因为这篇文章写得已经很好了，本文不再进行赘述。 缺陷管理没有好的团队管理工具必然导致项目进展不顺利，任务管理困难，而引入缺陷管理系统正好可以解决这些问题，通常一个缺陷管理系统都包含了以下的功能： 任务管理（包括必须做什么、谁来做、什么时候完成、现在处于什么状态等）。 直观而且可以检索过去发生的各种问题。 能够对信息进行统一的管理和共享。 能够生成各类报表。 能够关联到其他系统，具有可扩展性。 禅道禅道是国产的专业项目管理软件，它不仅仅是缺陷管理工具，它提供了完整软件生命周期管理功能，支持Scrum敏捷开发，能够实现需求管理、缺陷管理、任务管理等一系列的功能，而且拥有强大的扩展机制和丰富的功能插件。可以从禅道的官方网站提供的下载链接来下载禅道，推荐使用一键安装包。 下面仍然以CentOS Linux为例，讲解如何利用官方提供的一键安装包来安装禅道。 1234cd /optwget http://dl.cnezsoft.com/zentao/pro8.5.2/ZenTaoPMS.pro8.5.2.zbox_64.tar.gzgunzip ZenTaoPMS.pro8.5.2.zbox_64.tar.gztar -xvf ZenTaoPMS.pro8.5.2.zbox_64.tar 我们在/opt目录下（官方推荐使用这个目录）下载了禅道的归档压缩文件，并进行了解压缩和解归档的操作，完成上述步骤后，会看到一个名为zbox的文件夹。一键安装包中内置了Apache、MySQL、PHP等应用，也就是说这些都不需要单独安装部署了，接下来我们通过下面的命令来启动禅道。 12/opt/zbox/zbox -ap 8080 -mp 3307/opt/zbox/zbox start 说明：上面使用zbox文件夹下的zbox命令，其中-ap是为了指定Apache服务器使用的端口，-mp是为了指定MySQL数据库使用的端口，这里使用3307端口是为了避开服务器上可能已经存在的MySQL服务的3306端口；start表示启动服务，stop可以用来停止服务。此外，需要打开防火墙8080端口以便访问禅道，注意数据库的端口决不能暴露给公网。 打开浏览器，输入服务器的公网IP地址就可以访问禅道，如果愿意，也可以通过DNS解析绑定一个域名来进行访问，禅道的首页如下图所示，默认的管理员是admin，口令是123456。 第一次使用禅道时，建议通过点击用户名，然后通过“帮助”菜单的“新手教程”来迅速了解禅道。官方网站的文档链接中提供了视频教程，初学者也可以通过视频教程来上手。 对敏捷开发以及敏捷闭环工具不是特别了解的，可以参考《基于JIRA的Scrum敏捷开发的项目管理》一文。 GitLab常用的代码托管平台和之前提到的Git私服Gitlab都提供了缺陷管理的功能，当我们要报告一个bug时，可以在如下图所示的界面创建一个新的问题票（issue ticket）。填写的内容包括： **[必填]**出现问题的软件版本号、具体的使用环境（如操作系统）等相关信息。 **[必填]**能够稳定重现该问题的相关步骤。 **[必填]**描述此处期待的行为和实际的行为。 **[可选]**你对这个bug的看法（产生bug的原因是什么）。 如上图所示，我们在创建问题票时，还需要将问题指派给处理问题的人，如果不清楚应该由谁来修复这个bug，就指派给项目管理者，除此之外还要指定问题的优先级（十分紧急、紧急、普通、不紧急等）、问题的标签（功能缺陷、新特性、改进增强、前瞻研究等）、里程碑（通过里程碑可以将问题与某些特定的项目节点关联起来，之后可以查看每一个里程碑的进展，可以基于软件版本号来建立里程碑，也可以基于迭代周期来建立里程碑）以及需要在哪个时间点以前修复等信息。 有些敏捷团队使用问题票来管理产品的需求，称之为“问题驱动开发”（TiDD），也就是说新功能的开发是通过创建问题票来驱动的，具体的步骤包括：建立问题票、指定责任人、开发、提交、Push到代码库。如果要创建一个和需求相关的问题票，应该要填写以下的内容： **[必填]**简短的描述需求，并用它作为标题。 **[必填]**这个需求是解决什么问题的。 **[必填]**这个需求对软件现有功能会造成什么影响。 **[必填]**这个需求应该实现什么样的功能。 **[必填]**这个需求是否依赖其他模块提供相关支持。 **[可选]**这个需求有哪些实现方式。 **[可选]**这些可选的实现方式分别有哪些优缺点。 其他产品除了禅道和GitLab之外，JIRA、Redmine、Backlog等也是不错的缺陷管理系统。目前，这些系统大都不仅仅提供了缺陷管理的功能，更多的时候它们可以作为敏捷闭环工具来使用，关于敏捷闭环工具这个话题，请大家参考《基于JIRA的Scrum敏捷开发的项目管理》一文。 持续集成为了快速的产出高质量的软件，在团队开发中持续集成（CI）是一个非常重要的环节。所谓CI，就是一种让计算机自动任意次重复编译、测试、汇报等工作的方法，通过CI可以帮助开发者提早发现问题，降低各种人为失误给项目带来的风险。按照经典的软件过程模型（瀑布模型），集成的工作一般要等到所有的开发工作都结束后才能开始，但这个时候如果发现了问题，修复问题的代价是非常具体的。基本上，集成实施得越晚，代码量越大，解决问题就越困难。持续集成将版本控制、自动化构建、代码测试融入到一起，让这些工作变得自动化和可协作。由于其频繁重复整个开发流程（在指定时间内多次pull源代码并运行测试代码），所以能帮助开发者提早发现问题。 在所有的CI工具中，Jenkins和TravisCI是最具有代表性的，前者是基于 Java的开源CI工具，后者是新晋的在线CI工具，下图是Jenkins的工作面板。 持续集成对于编译型语言的意义更大，对于Python这样的解释型语言，更多的时候是用于对接版本控制系统触发自动化测试并产生相应的报告，类似的功能也可以通过配置Webhook来完成。如果要通过Docker这样的虚拟化容器进行项目打包部署或者通过K8S进行容器管理，可以在持续集成平台安装对应的插件来支持这些功能。码云甚至可以直接对接钉钉开放平台使用钉钉机器人来向项目相关人员发送即时消息。GitLab也对CI和CD（持续交付）提供了支持，具体内容请大家参考《GitLab CI&#x2F;CD基础教程》。 说明： 关于敏捷开发的相关内容，有兴趣的读者可以阅读知乎上的《这才是敏捷开发》一文。 本章中的部分插图来自于网易云课堂《人人都会用Git》课程（免费哟），在此表示感谢。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day91-100/100.Python面试题实录","date":"2024-12-12T08:38:02.767Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day91-100/100.Python面试题实录/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day91-100/100.Python%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AE%9E%E5%BD%95/","excerpt":"","text":"Python面试题实录 温馨提示：请访问我的另一个项目“Python面试宝典”。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/90.PyTorch实战","date":"2024-12-12T08:38:02.736Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/90.PyTorch实战/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/90.PyTorch%E5%AE%9E%E6%88%98/","excerpt":"","text":"PyTorch实战","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/89.PyTorch概述","date":"2024-12-12T08:38:02.733Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/89.PyTorch概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/89.PyTorch%E6%A6%82%E8%BF%B0/","excerpt":"","text":"PyTorch入门","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/88.深度学习入门","date":"2024-12-12T08:38:02.731Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/88.深度学习入门/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/88.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/","excerpt":"","text":"Tensorflow入门","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/87.回归分析","date":"2024-12-12T08:38:02.729Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/87.回归分析/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/87.%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/","excerpt":"","text":"回归分析","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/86.K-均值聚类","date":"2024-12-12T08:38:02.727Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/86.K-均值聚类/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/86.K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB/","excerpt":"","text":"K-均值聚类","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/85.支持向量机","date":"2024-12-12T08:38:02.725Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/85.支持向量机/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/85.%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","excerpt":"","text":"支持向量机","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/84.贝叶斯分类","date":"2024-12-12T08:38:02.724Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/84.贝叶斯分类/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/84.%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB/","excerpt":"","text":"贝叶斯分类","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/83.推荐系统实战-1","date":"2024-12-12T08:38:02.721Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/83.推荐系统实战-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/83.%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98-1/","excerpt":"","text":"推荐系统实战(1)","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/83.决策树","date":"2024-12-12T08:38:02.719Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/83.决策树/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/83.%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"","text":"决策树","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/82.k最近邻分类","date":"2024-12-12T08:38:02.715Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/82.k最近邻分类/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/82.k%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB/","excerpt":"","text":"k最近邻分类$k$最近邻（简称kNN，k-Nearest Neighbor）是Cover和Hart在1968年提出的一种简单的监督学习算法，可用于字符识别、文本分类、图像识别等领域。kNN的工作机制非常简单：给定测试样本，基于某种距离度量（如：欧式距离、曼哈顿距离等）找出训练集中与其最接近的$k$个训练样本，然后基于这$k$个“最近邻居”的信息来进行预测。对于分类任务，可以在$k$个最近邻居中选择出现次数最多的类别标签作为预测的结果；对于回归任务，可以使用$k$个最近邻居实际输出（目标值）的平均值作为预测的结果，当然也可以根据距离的远近进行加权平均，距离越近的样本权重值就越大。 案例：电影分类预测k值的选择和交叉检验k值的选择对于kNN算法的结果有非常显著的影响。下面用李航博士的《统计学习方法》一书中的叙述，来对k值的选择加以说明。 如果选择较小的$k$值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差会减小，只有与输入实例较近（相似的）训练实例才会对预测结果起作用；但缺点是“学习”的估计误差会增大，预测结果会对近邻的实例点非常敏感，如果近邻的实例点刚好是噪声，预测就会出错。换句话说，$k$值的减小就意味着整体模型变得复杂，容易发生过拟合。 如果选择较大的$k$值，就相当于用较大的邻域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测起作用，使预测发生错误。对于$k&#x3D;N$的极端情况（其中$N$代表所有的训练实例的数量），那么无论输入实例是什么，都会预测它属于训练实例中最多的类，很显然，这样的模型完全忽略了训练实例中大量的有用信息，是不可取的。 实际应用中，$k$的取值通常都比较小，可以通过交叉检验的方式来选择较好的$k$值。 算法优缺点优点： 简单有效 重新训练代价低 适合类域交叉样本 适合大样本分类 缺点： 惰性学习 输出的可解释性不强 不擅长处理不均衡样本 计算量比较大","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day81-90/81.人工智能和机器学习概述","date":"2024-12-12T08:38:02.713Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day81-90/81.人工智能和机器学习概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day81-90/81.%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","excerpt":"","text":"人工智能和机器学习概述所谓“人工智能”通常是泛指让机器具有像人一样的智慧的技术，其目的是让机器像人一样能够感知、思考和解决问题；而“机器学习”通常是指让计算机通过学习现有的数据，实现认知的更新和进步。显然，机器学习是实现人工智能的一种途径，这也是我们的课程要讨论的内容。现如今，“机器学习”和“大数据”可以说是最时髦的两个词汇，而在弱人工智能阶段，无论是“机器学习”还是“大数据”最终要解决的问题本质上是一样的，就是让计算机将纷繁复杂的数据处理成有用的信息，这样就可以发掘出数据带来的意义以及隐藏在数据背后的规律，简单的说就是用现有的数据对将来的状况做出预测和判断。 在讨论机器学习相关内容之前，我们先按照问题的“输入”和“输出”对用计算机求解的问题进行一个分类，如下所示： 输入的信息是精确的，要求输出最优解。 输入的信息是精确的，无法找到最优解，只能获得满意解。 输入的信息是模糊的，要求输出最优解。 输入的信息是模糊的，无法找到最优解，只能获得满意解。 在上面的四大类问题中，第1类问题是计算机最擅长解决的，这类问题其实就是“数值计算”和“逻辑推理”方面的问题，而传统意义上的人工智能也就是利用逻辑推理来解决问题（如早期的“人机对弈”）。一直以来，我们都习惯于将计算机称为“电脑”，而基于“冯诺依曼”体系结构的“电脑”实际上只是实现了“人脑”理性思维这部分的功能，而且在这一点上“电脑”的表现通常是优于“人脑”的；但是“人脑”在处理模糊输入信息时表现出来的强大处理能力，在很多场景下“电脑”是难以企及的。所以我们研究机器学习的算法，就是要解决在输入模糊信息时让计算机给出满意解甚至是最优解的问题。 人类通过记忆和归纳这两种方式进行学习，通过记忆可以积累单个事实，使用归纳可以从旧的事实推导出新的事实。所以机器学习其实是一种训练，让计算机通过这种训练能够学会根据数据隐含模式进行合理推断的能力，其基本流程如下所示： 观察一组实例，通常称为训练数据，它们可以表示某种统计现象的不完整信息; 对观测到的实例进行扩展，并使用推断技术对扩展过程建模; 使用这个模型对未知实例进行预测。 基本概念监督学习和非监督学习监督学习是从给定的训练数据集中学习得到一个函数，当新的数据到来时，可以根据这个函数预测结果，监督学习的训练集包括输入和输出，也可以说是特征和目标。监督学习的目标是由人来标注的，而非监督学习的数据没有类别信息，训练集也没有人为标注结果，通过无监督学习可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据中的信息 。 特征向量和特征工程距离度量 欧氏距离 $$d &#x3D; \\sqrt{\\sum_{k&#x3D;1}^n(x_{1k}-x_{2k})^2}$$ 曼哈顿距离 $$d &#x3D; \\sum_{k&#x3D;1}^n \\mid {x_{1k}-x_{2k}} \\mid$$ 切比雪夫距离 $$d &#x3D; max(\\mid x_{1k}-x_{2k} \\mid)$$ 闵可夫斯基距离 当$p&#x3D;1$时，就是曼哈顿距离 当$p&#x3D;2$时，就是欧式距离 当$p \\to \\infty$时，就是切比雪夫距离 $$d &#x3D; \\sqrt[p]{\\sum_{k&#x3D;1}^n \\mid x_{1k}-x_{2k} \\mid ^p}$$ 余弦距离 $$ cos(\\theta) &#x3D; \\frac{\\sum_{k&#x3D;1}^n x_{1k}x_{2k}}{\\sqrt{\\sum_{k&#x3D;1}^n x_{1k}^2} \\sqrt{\\sum_{k&#x3D;1}^n x_{2k}^2}} $$ 机器学习的定义和应用领域根据上面的论述，我们可以给“机器学习”下一个正式的定义：机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。即使对于机器学习这个概念不那么熟悉，但是机器学习的成果已经广泛渗透到了生产生活的各个领域，下面的这些场景对于你来说一定不陌生。 场景1：搜索引擎会根据搜索和使用习惯，优化下一次搜索的结果。 场景2：电商网站会根据你的访问历史自动推荐你可能感兴趣的商品。 场景3：金融类产品会通过你最近的金融活动信息综合评定你的贷款申请。 场景4：视频和直播平台会自动识别图片和视频中有没有不和谐的内容。 场景5：智能家电和智能汽车会根据你的语音指令做出相应的动作。 简单的总结一下，机器学习可以应用到但不限于以下领域： 计算机视觉。计算机视觉是指机器感知环境的能力，目前在物体检测和人脸识别这两个领域已经非常成熟且产生了大量的应用。 刷脸支付 涂鸦识别 自然语言处理（NLP）。自然语言处理是目前机器学习中一个非常热门的分支，具体的又可以分为三类应用场景。其中文本挖掘主要是对文本进行分类，包括句法分析、情绪分析和垃圾信息检测等；而机器翻译和语音识别相信不用太多的解释大家也都清楚。 文本挖掘 机器翻译 语音识别 机器人。机器人可以分为固定机器人和移动机器人两大类。固定机器人通常被用于工业生产，例如用于装配流水线。常见的移动机器人应用有货运机器人、空中机器人和自动载具。机器人需要软硬件的协作才能实现最优的作业，其中硬件包含传感器、反应器和控制器等，而软件主要是实现感知能力，包括定位、测绘、目标检测和识别等。 机甲大师 扫地机器人 机器学习实施步骤实现机器学习的一般步骤： 数据收集 数据准备 数据分析 训练算法 测试算法 应用算法 Scikit-learn介绍 Scikit-learn源于Google Summer of Code项目，由David Cournapeau在2007年发起，它提供了机器学习可能用到的工具，包括数据预处理、监督学习（分类、回归）、非监督学习（聚类）、模型选择、降维等。 官网地址：https://scikit-learn.org/stable/index.html 安装方法：pip install scikit-learn","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/79.数据可视化-2","date":"2024-12-12T08:38:02.419Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/79.数据可视化-2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/79.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-2/","excerpt":"","text":"数据可视化-2本章我们尝试用 matplotlib 来绘制一些高阶统计图表。正如前面所说的，大家可以通过 matplotlib 官方网站上提供的文档和示例来学习如何使用 matplotlib 并绘制出更加高级的统计图表；尤其是在定制一些比较复杂的图表时，我们建议大家直接找到官网提供的示例，然后只需要做出相应的修改，就可以绘制出自己想要的图表。这种“拷贝+修改”的做法应该会大大提高你的工作效率，因为大多数时候，你的代码跟官网上的代码就仅仅是数据有差别而已，没有必要去做重复乏味的事情。 气泡图气泡图可以用来了解三个变量之间的关系，通过比较气泡位置和大小来分析数据维度之间的相关性。例如在我们之前绘制的月收入和网购支出的散点图中，我们已经发现了二者的正相关关系，如果我们引入第三个变量网购次数，那么我们就需要使用气泡图来进行展示。 代码： 12345678910income = np.array([5550, 7500, 10500, 15000, 20000, 25000, 30000, 40000])outcome = np.array([800, 1800, 1250, 2000, 1800, 2100, 2500, 3500])nums = np.array([5, 3, 10, 5, 12, 20, 8, 10])# 通过scatter函数的s参数和c参数分别控制面积和颜色plt.scatter(income, outcome, s=nums * 30, c=nums, cmap=&#x27;Reds&#x27;)# 显示颜色条plt.colorbar()# 显示图表plt.show() 输出： 面积图面积图又叫堆叠折线图，是在折线图的基础上，对折线以下的区域进行颜色填充（展示面积），用于在连续间隔或时间跨度上展示数值，一般用来显示趋势和对比数值，不同颜色的填充可以让多个面积块之间的对比和趋势更好的突显。下面的例子中，我们用面积图来展示从周一到周日花在睡觉、吃饭、工作和玩耍上的时间。 代码： 1234567891011121314plt.figure(figsize=(8, 4))days = np.arange(7)sleeping = [7, 8, 6, 6, 7, 8, 10]eating = [2, 3, 2, 1, 2, 3, 2]working = [7, 8, 7, 8, 6, 2, 3]playing = [8, 5, 9, 9, 9, 11, 9]# 绘制堆叠折线图plt.stackplot(days, sleeping, eating, working, playing)# 定制横轴刻度plt.xticks(days, labels=[f&#x27;星期&#123;x&#125;&#x27; for x in &#x27;一二三四五六日&#x27;])# 定制图例plt.legend([&#x27;睡觉&#x27;, &#x27;吃饭&#x27;, &#x27;工作&#x27;, &#x27;玩耍&#x27;], fontsize=10)# 显示图表plt.show() 输出： 雷达图雷达图通常用来比较多个定量数据，用于查看哪些变量具有相似的值。 雷达图也可用于查看数据集中哪些变量的值比较低，哪些变量的值比较高，是显示性能或表现的理想选择。经常观看篮球、足球比赛的读者应该对雷达图非常熟悉，例如在 NBA 的转播中就经常使用雷达图来展示球员的各项数据。雷达图的本质折线图，只不过将折线图映射到了极坐标系。在绘制雷达图时，需要让折线闭合，简单的说就是首尾相连，下面是绘制雷达图的代码。 代码： 12345678910111213141516171819202122labels = np.array([&#x27;速度&#x27;, &#x27;力量&#x27;, &#x27;经验&#x27;, &#x27;防守&#x27;, &#x27;发球&#x27;, &#x27;技术&#x27;])# 马龙和水谷隼的数据malong_values = np.array([93, 95, 98, 92, 96, 97])shuigu_values = np.array([30, 40, 65, 80, 45, 60])angles = np.linspace(0, 2 * np.pi, labels.size, endpoint=False)# 多加一条数据让图形闭合malong_values = np.append(malong_values, malong_values[0])shuigu_values = np.append(shuigu_values, shuigu_values[0])angles = np.append(angles, angles[0])# 创建画布plt.figure(figsize=(4, 4), dpi=120)# 创建坐标系ax = plt.subplot(projection=&#x27;polar&#x27;)# 绘图和填充plt.plot(angles, malong_values, color=&#x27;r&#x27;, linewidth=2, label=&#x27;马龙&#x27;)plt.fill(angles, malong_values, color=&#x27;r&#x27;, alpha=0.3)plt.plot(angles, shuigu_values, color=&#x27;g&#x27;, linewidth=2, label=&#x27;水谷隼&#x27;)plt.fill(angles, shuigu_values, color=&#x27;g&#x27;, alpha=0.2)# 显示图例ax.legend()# 显示图表plt.show() 输出： 玫瑰图玫瑰图是映射在极坐标下的柱状图，由弗罗伦斯·南丁格尔（Florence Nightingale）所发明，当年是南丁格尔用来呈现战地医院季节性死亡率的一种图表。由于半径和面积的关系是平方的关系，南丁格尔玫瑰图会将数据的比例大小夸大，尤其适合对比大小相近的数值，同时由于圆形有周期的特性，所以南丁格尔玫瑰图也适用于表示一个周期内的时间概念，比如星期、月份。 代码： 1234567891011121314151617group1 = np.random.randint(20, 50, 4)group2 = np.random.randint(10, 60, 4)x = np.array([f&#x27;A组-Q&#123;i&#125;&#x27; for i in range(1, 5)] + [f&#x27;B组-Q&#123;i&#125;&#x27; for i in range(1, 5)])y = np.array(group1.tolist() + group2.tolist())# 玫瑰花瓣的角度和宽度theta = np.linspace(0, 2 * np.pi, x.size, endpoint=False)width = 2 * np.pi / x.size# 生成8种随机颜色colors = np.random.rand(8, 3)# 将柱状图投影到极坐标ax = plt.subplot(projection=&#x27;polar&#x27;)# 绘制柱状图plt.bar(theta, y, width=width, color=colors, bottom=0)# 设置网格ax.set_thetagrids(theta * 180 / np.pi, x, fontsize=10)# 显示图表plt.show() 输出： 3D图表matplotlib 还可以用于绘制3D图，具体的内容大家可以参考官方文档，下面我们用一段简单的代码为大家展示如何绘制3D图表。 代码： 1234567891011121314from mpl_toolkits.mplot3d import Axes3Dfig = plt.figure(figsize=(8, 4), dpi=120)# 创建3D坐标系并添加到画布上ax = Axes3D(fig, auto_add_to_figure=False)fig.add_axes(ax)x = np.arange(-2, 2, 0.1)y = np.arange(-2, 2, 0.1)x, y = np.meshgrid(x, y)z = (1 - y ** 5 + x ** 5) * np.exp(-x ** 2 - y ** 2)# 绘制3D曲面ax.plot_surface(x, y, z)# 显示图表plt.show() 输出： 需要指出的是， JupyterLab 中渲染的3D图并不是真正的3D图，因为你没有办法调整观察者的视角，也没有办法旋转或者缩放。如果想要看到真正的3D效果，需要在将图表渲染到 Qt 窗口中，为此我们可以先安装名为 PyQt6 的三方库，如下所示。 1%pip install PyQt6 然后，我们使用魔法指令让 JupyterLab 将图表渲染到 Qt 窗口中。 1%matplotlib qt 在完成上面的操作后，我们可以重新运行刚才绘制3D图的代码，看到如下所示的窗口。在这个窗口中，我们可以通过鼠标对3D进行旋转、缩放，我们有可以选中图表的一部分数据进行观测，是不是非常的酷。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/78.数据可视化-1","date":"2024-12-12T08:38:02.418Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/78.数据可视化-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/78.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-1/","excerpt":"","text":"数据可视化-1在完成了对数据的透视之后，我们可以将数据透视的结果通过可视化的方式呈现出来，简单的说，就是将数据变成漂亮的统计图表，因为人类对颜色和形状会更加敏感，然后再进一步解读数据背后隐藏的商业价值。在之前的课程中，我们已经为大家展示过用使用Series或DataFrame对象的plot方法生成可视化图表的操作，本章我们为大家讲解这个绘图方法的基石，它就是大名鼎鼎的 matplotlib 库。 在讲解 matplotlib 之前，请大家先看看下面这张图，它给出了常用的图表类型及其应用场景。我们在选择统计图表时，如果不知道做出怎样的选择最合适，相信这张图就能帮到你。简单的说，看趋势折线图，比数据柱状图，定关系散点图，查占比饼状图，看分布直方图，找离群箱线图。 导入和配置之前的课程中，我们为大家讲解过如何安装和导入 matplotlib 库，如果不确定是否已经安装了 matplotlib，可以使用下面的魔法指令尝试安装或升级你的 matplotlib。 1%pip install -U matplotlib 为了解决 matplotlib 图表中文显示的问题，我们需要修改pyplot模块的rcParams配置参数，具体的操作如下所示。 1234import matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;].insert(0, &#x27;SimHei&#x27;)plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False 说明：上面代码中的SimHei是字体名称，大家可以通过百度云盘下载并安装该字体，链接地址：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g?pwd=e7b4。你可以尝试安装其他的中文字体，安装之后如果不知道字体叫什么名字，可以到用户主目录下名为`.matplotlib`的文件夹中找到`fontlist-v330.json`文件，打开后可以看到字体文件的路径和字体的名称等信息。需要注意的是，使用中文字体后坐标轴上的负号将会无法显示，需要将`axes.unicode_minus`参数设置为`False`，这样才能让坐标轴上的负号正常显示。 通过下面的魔法指令，我们可以在绘图时生成矢量图（SVG - Scalable Vector Graphics），矢量图的特点是不会因为放大、缩小或旋转等操作而失真，看起来会舒服很多。 1%config InlineBackend.figure_format=&#x27;svg&#x27; 创建画布pyplot模块的figure函数可以用来创建画布，创建画布时，可以通过figsize参数指定画布的尺寸（默认值是[6.4, 4.8]）；可以通过dpi参数设置绘图的分辨率，因为dpi代表了每英寸的像素点数量。除此之外，还可以通过facecolor参数设置画布的背景色。figure函数的返回值是一个Figure对象，它代表了绘图使用的画布，我们可以基于画布来创建绘图使用的坐标系。 1plt.figure(figsize=(8, 4), dpi=120, facecolor=&#x27;darkgray&#x27;) 创建坐标系可以直接使用pyplot模块的subplot函数来创建坐标系，该函数会返回Axes对象。subplot的前三个参数分别用来指定整个画布分成几行几列以及当前坐标系的索引，这三个参数的默认值都是1。如果没有创建坐标系，我们绘图时会使用画布上默认的也是唯一的一个坐标系；如果需要在画布上创建多个坐标系，就可以使用该函数。当然，我们也可以通过上面创建的Figure对象的add_subplot方法或add_axes方法来创建坐标系，前者跟subplot函数的作用一致，后者会产生嵌套的坐标系。 1plt.subplot(2, 2, 1) 绘制图表折线图在绘图时，如果没有先调用figure函数和subplot函数，我们将使用默认的画布和坐标系，如果要绘制折线图，可以使用pyplot模块的plot函数，并指定横轴和纵轴的数据。折线图最适合用来观察数据的趋势，尤其是当横坐标代表时间的情况下。我们可以使用plot函数的color参数来定制折线的颜色，可以使用marker参数来定制数据点的标记（例如：*表示五角星，^表示三角形，o表示小圆圈等），可以使用linestyle参数来定制折线的样式（例如：-表示实线，--表示虚线，:表示点线等），可以使用linewidth参数来定制折线的粗细。 下面的代码绘制了一条正弦曲线，其中marker=&#39;*&#39;会将数据点的标记设置为五角星形状，而color=&#39;red&#39;会将折线绘制为红色。 代码： 1234567891011import numpy as npx = np.linspace(-2 * np.pi, 2 * np.pi, 120)y = np.sin(x)# 创建画布plt.figure(figsize=(8, 4), dpi=120)# 绘制折线图plt.plot(x, y, linewidth=2, marker=&#x27;*&#x27;, color=&#x27;red&#x27;)# 显示绘图plt.show() 输出： 如果要在一个坐标系上同时绘制正弦和余弦曲线，可以对上面的代码稍作修改。 代码： 1234567891011121314x = np.linspace(-2 * np.pi, 2 * np.pi, 120)y1, y2 = np.sin(x), np.cos(x)plt.figure(figsize=(8, 4), dpi=120)plt.plot(x, y1, linewidth=2, marker=&#x27;*&#x27;, color=&#x27;red&#x27;)plt.plot(x, y2, linewidth=2, marker=&#x27;^&#x27;, color=&#x27;blue&#x27;)# 定制图表上的标注（annotate函数的参数如果不理解可以先不管它）plt.annotate(&#x27;sin(x)&#x27;, xytext=(0.5, -0.75), xy=(0, -0.25), fontsize=12, arrowprops=&#123; &#x27;arrowstyle&#x27;: &#x27;-&gt;&#x27;, &#x27;color&#x27;: &#x27;darkgreen&#x27;, &#x27;connectionstyle&#x27;: &#x27;angle3, angleA=90, angleB=0&#x27;&#125;)plt.annotate(&#x27;cos(x)&#x27;, xytext=(-3, 0.75), xy=(-1.25, 0.5), fontsize=12, arrowprops=&#123; &#x27;arrowstyle&#x27;: &#x27;-&gt;&#x27;, &#x27;color&#x27;: &#x27;darkgreen&#x27;, &#x27;connectionstyle&#x27;: &#x27;arc3, rad=0.35&#x27;&#125;)plt.show() 输出： 如果要使用两个坐标系分别绘制正弦和余弦，可以用上面提到的subplot函数来创建坐标系，然后再绘图。 代码： 12345678plt.figure(figsize=(8, 4), dpi=120)# 创建坐标系（第1个图）plt.subplot(2, 1, 1)plt.plot(x, y1, linewidth=2, marker=&#x27;*&#x27;, color=&#x27;red&#x27;)# 创建坐标系（第2个图）plt.subplot(2, 1, 2)plt.plot(x, y2, linewidth=2, marker=&#x27;^&#x27;, color=&#x27;blue&#x27;)plt.show() 输出： 当然也可以像下面这么做，大家可以运行代码看看跟上面的图有什么区别。 123456plt.figure(figsize=(8, 4), dpi=120)plt.subplot(1, 2, 1)plt.plot(x, y1, linewidth=2, marker=&#x27;*&#x27;, color=&#x27;red&#x27;)plt.subplot(1, 2, 2)plt.plot(x, y2, linewidth=2, marker=&#x27;^&#x27;, color=&#x27;blue&#x27;)plt.show() 然后，再试一试下面这个代码，看看运行效果如何。 123456789fig = plt.figure(figsize=(10, 4), dpi=120)plt.plot(x, y1, linewidth=2, marker=&#x27;*&#x27;, color=&#x27;red&#x27;)# 用Figure对象的add_axes方法在现有坐标系中嵌套一个新的坐标系，该方法的参数是一个四元组，# 代表了新坐标系在原坐标系中的位置，前两个值是左下角的位置，后两个值是坐标系的宽度和高度ax = fig.add_axes((0.595, 0.6, 0.3,0.25))ax.plot(x, y2, marker=&#x27;^&#x27;, color=&#x27;blue&#x27;)ax = fig.add_axes((0.155, 0.2, 0.3,0.25))ax.plot(x, y2, marker=&#x27;^&#x27;, color=&#x27;green&#x27;)plt.show() 散点图散点图可以帮助我们了解两个变量的关系，如果需要了解三个变量的关系，可以将散点图升级为气泡图。下面的代码中，x和y两个数组分别表示每个月的收入和每个月网购的支出，如果我们想了解x和y是否存在相关关系，就可以绘制如下所示的散点图。 代码： 123456x = np.array([5550, 7500, 10500, 15000, 20000, 25000, 30000, 40000])y = np.array([800, 1800, 1250, 2000, 1800, 2100, 2500, 3500])plt.figure(figsize=(6, 4), dpi=120)plt.scatter(x, y)plt.show() 输出： 柱状图在对比数据的差异时，柱状图是非常棒的选择，我们可以使用pyplot模块的bar函数来生成柱状图，也可以使用barh函数来生成水平柱状图（也称为“条状图”）。我们先为柱状图准备一些数据，代码如下所示。 123x = np.arange(4)y1 = np.random.randint(20, 50, 4)y2 = np.random.randint(10, 60, 4) 绘制柱状图的代码。 代码： 123456789plt.figure(figsize=(6, 4), dpi=120)# 通过横坐标的偏移，让两组数据对应的柱子分开，width参数控制柱子的粗细，label参数为柱子添加标签plt.bar(x - 0.1, y1, width=0.2, label=&#x27;销售A组&#x27;)plt.bar(x + 0.1, y2, width=0.2, label=&#x27;销售B组&#x27;)# 定制横轴的刻度plt.xticks(x, labels=[&#x27;Q1&#x27;, &#x27;Q2&#x27;, &#x27;Q3&#x27;, &#x27;Q4&#x27;])# 定制显示图例plt.legend()plt.show() 输出： 如果想绘制堆叠柱状图，可以对上面的代码稍作修改，如下所示。 代码： 1234567labels = [&#x27;Q1&#x27;, &#x27;Q2&#x27;, &#x27;Q3&#x27;, &#x27;Q4&#x27;]plt.figure(figsize=(6, 4), dpi=120)plt.bar(labels, y1, width=0.4, label=&#x27;销售A组&#x27;)# 注意：堆叠柱状图的关键是将之前的柱子作为新柱子的底部，可以通过bottom参数指定底部数据，新柱子绘制在底部数据之上plt.bar(labels, y2, width=0.4, bottom=y1, label=&#x27;销售B组&#x27;)plt.legend(loc=&#x27;lower right&#x27;)plt.show() 输出： 饼状图饼状图通常简称为饼图，是一个将数据划分为几个扇形区域的统计图表，它主要用于描述数量、频率等之间的相对关系。在饼图中，每个扇形区域的大小就是其所表示的数量的比例，这些扇形区域合在一起刚好是一个完整的饼。在需要展示数据构成的场景下，饼状图、树状图和瀑布图是不错的选择，我们可以使用pyplot模块的pie函数来绘制饼图，代码如下所示。 代码： 12345678910111213141516171819202122232425262728data = np.random.randint(100, 500, 7)labels = [&#x27;苹果&#x27;, &#x27;香蕉&#x27;, &#x27;桃子&#x27;, &#x27;荔枝&#x27;, &#x27;石榴&#x27;, &#x27;山竹&#x27;, &#x27;榴莲&#x27;]plt.figure(figsize=(5, 5), dpi=120)plt.pie( data, # 自动显示百分比 autopct=&#x27;%.1f%%&#x27;, # 饼图的半径 radius=1, # 百分比到圆心的距离 pctdistance=0.8, # 颜色（随机生成） colors=np.random.rand(7, 3), # 分离距离 # explode=[0.05, 0, 0.1, 0, 0, 0, 0], # 阴影效果 # shadow=True, # 字体属性 textprops=dict(fontsize=8, color=&#x27;black&#x27;), # 楔子属性（生成环状饼图的关键） wedgeprops=dict(linewidth=1, width=0.35), # 标签 labels=labels)# 定制图表的标题plt.title(&#x27;水果销售额占比&#x27;)plt.show() 输出： 说明：大家可以试一试将上面代码中被注释的部分恢复，看看有什么样的效果。 直方图在统计学中，直方图是一种展示数据分布情况的图形，是一种二维统计图表，它的两个坐标分别是统计样本和该样本对应的某个属性的度量。下面的数据是某学校100名男学生的身高，如果我们想知道数据的分布，就可以使用直方图。 123456789101112heights = np.array([ 170, 163, 174, 164, 159, 168, 165, 171, 171, 167, 165, 161, 175, 170, 174, 170, 174, 170, 173, 173, 167, 169, 173, 153, 165, 169, 158, 166, 164, 173, 162, 171, 173, 171, 165, 152, 163, 170, 171, 163, 165, 166, 155, 155, 171, 161, 167, 172, 164, 155, 168, 171, 173, 169, 165, 162, 168, 177, 174, 178, 161, 180, 155, 155, 166, 175, 159, 169, 165, 174, 175, 160, 152, 168, 164, 175, 168, 183, 166, 166, 182, 174, 167, 168, 176, 170, 169, 173, 177, 168, 172, 159, 173, 185, 161, 170, 170, 184, 171, 172]) 可以使用pyplot模块的hist函数来绘制直方图，其中bins参数代表了我们使用的分箱方式（身高从150厘米到190厘米，每5厘米为一个分箱），代码如下所示。 代码： 12345678plt.figure(figsize=(6, 4), dpi=120)# 绘制直方图plt.hist(heights, bins=np.arange(145, 196, 5), color=&#x27;darkcyan&#x27;)# 定制横轴标签plt.xlabel(&#x27;身高&#x27;)# 定制纵轴标签plt.ylabel(&#x27;概率密度&#x27;)plt.show() 输出： 绘制直方图时，如果将hist函数的density参数修改为True，同时将cumulative参数也修改为True，那么一方面纵轴会显示为概率密度，而图表会绘制概率的累计分布，如下所示。 代码： 12345678plt.figure(figsize=(6, 4), dpi=120)# 绘制直方图plt.hist(heights, bins=np.arange(145, 196, 5), color=&#x27;darkcyan&#x27;, density=True, cumulative=True)# 定制横轴标签plt.xlabel(&#x27;身高&#x27;)# 定制纵轴标签plt.ylabel(&#x27;概率&#x27;)plt.show() 输出： 箱线图箱线图又叫箱型图或盒须图，是一种用于展示一组数据分散情况的统计图表，如下所示。因图形如箱子，而且在上下四分位数之外有线条像胡须延伸出去而得名。在箱线图中，箱子的上边界是上四分位数（$Q_3$）的位置，箱子的下边界是下四分位数（$Q_1$）的位置，箱子中间的线条是中位数（$Q_2$）的位置，而箱子的长度就是四分位距离（IQR）。除此之外，箱子上方线条的边界是最大值，箱子下方线条的边界是最小值，这两条线之外的点就是离群值（outlier）。所谓离群值，是指数据小于$Q_1 - 1.5 \\times IQR$或数据大于$Q_3 + 1.5 \\times IQR$的值，公式中的1.5还可以替换为3来发现极端离群值（extreme outlier），而介于1.5到3之间的离群值通常称之为适度离群值（mild outlier）。 可以使用pyplot模块的boxplot函数来绘制箱线图，代码如下所示。 代码： 123456789101112131415# 数组中有47个[0, 100)范围的随机数data = np.random.randint(0, 100, 47)# 向数组中添加三个可能是离群点的数据data = np.append(data, 160)data = np.append(data, 200)data = np.append(data, -50)plt.figure(figsize=(6, 4), dpi=120)# whis参数的默认值是1.5，将其设置为3可以检测极端离群值，showmeans=True表示在图中标记均值的位置plt.boxplot(data, whis=1.5, showmeans=True, notch=True)# 定制纵轴的取值范围plt.ylim([-100, 250])# 定制横轴的刻度plt.xticks([1], labels=[&#x27;data&#x27;])plt.show() 输出： 说明：由于数据是随机生成的，大家运行上面的代码生成的图表可能跟我这里并不相同，以实际运行结果为准。 显示和保存图表可以使用pyplot模块的show函数来显示绘制的图表，我们在上面的代码中使用过这个函数。如果希望保存图表，可以使用savefig函数。需要注意的是，如果要同时显示和保存图表，应该先执行savefig函数，再执行show函数，因为在调用show函数时，图表已经被释放，位于show函数之后的savefig保存的只是一个空白的区域。 12plt.savefig(&#x27;chart.png&#x27;)plt.show() 其他图表使用 matplotlib，我们还可以绘制出其他的统计图表（如：雷达图、玫瑰图、热力图等），但实际工作中，使用频率最高的几类图表我们在上面已经为大家完整的展示出来了。此外，matplotlib 还有很多对统计图表进行定制的细节，例如定制坐标轴、定制图表上的文字和标签等。如果想了解如何用 matplotlib 绘制和定制更多的统计图表，可以直接查看 matplotlib 官方网站上的文档和示例，在下一个章节我们会为大家做一个简要的介绍。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/77.深入浅出pandas-6","date":"2024-12-12T08:38:02.415Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/77.深入浅出pandas-6/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/77.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-6/","excerpt":"","text":"深入浅出pandas-6我们再来看看Index类型，它为Series和DataFrame对象提供了索引服务，有了索引我们就可以排序数据（sort_index方法）、对齐数据（在运算和合并数据时非常重要）并实现对数据的快速检索（索引运算）。由于DataFrame类型表示的是二维数据，所以它的行和列都有索引，分别是index和columns。Index类型的创建的比较简单，通常给出data、dtype和name三个参数即可，分别表示作为索引的数据、索引的数据类型和索引的名称。由于Index本身也是一维的数据，索引它的方法和属性跟Series非常类似，你可以尝试创建一个Index对象，然后尝试一下之前学过的属性和方法在Index类型上是否生效。接下来，我们主要看看Index的几种子类型。 范围索引范围索引是由具有单调性的整数构成的索引，我们可以通过RangeIndex构造器来创建范围索引，也可以通过RangeIndex类的类方法from_range来创建范围索引，代码如下所示。 代码： 1234sales_data = np.random.randint(400, 1000, 12)index = pd.RangeIndex(1, 13, name=&#x27;月份&#x27;)ser = pd.Series(data=sales_data, index=index)ser 输出： 1234567891011121314月份1 7032 7053 5574 9435 9616 6157 7888 9859 92110 95111 87412 609dtype: int64 分类索引分类索引是由定类尺度构成的索引。如果我们需要通过索引将数据分组，然后再进行聚合操作，分类索引就可以派上用场。分类索引还有一个名为reorder_categories的方法，可以给索引指定一个顺序，分组聚合的结果会按照这个指定的顺序进行呈现，代码如下所示。 代码： 12345678sales_data = [6, 6, 7, 6, 8, 6]index = pd.CategoricalIndex( data=[&#x27;苹果&#x27;, &#x27;香蕉&#x27;, &#x27;苹果&#x27;, &#x27;苹果&#x27;, &#x27;桃子&#x27;, &#x27;香蕉&#x27;], categories=[&#x27;苹果&#x27;, &#x27;香蕉&#x27;, &#x27;桃子&#x27;], ordered=True)ser = pd.Series(data=sales_data, index=index)ser 输出： 1234567苹果 6香蕉 6苹果 7苹果 6桃子 8香蕉 6dtype: int64 基于索引分组数据，然后使用sum进行求和。 1ser.groupby(level=0).sum() 输出： 1234苹果 19香蕉 12桃子 8dtype: int64 指定索引的顺序。 12ser.index = index.reorder_categories([&#x27;香蕉&#x27;, &#x27;桃子&#x27;, &#x27;苹果&#x27;])ser.groupby(level=0).sum() 输出： 1234香蕉 12桃子 8苹果 19dtype: int64 多级索引Pandas 中的MultiIndex类型用来表示层次或多级索引。可以使用MultiIndex类的类方法from_arrays、from_product、from_tuples等来创建多级索引，我们给大家举几个例子。 代码： 123tuples = [(1, &#x27;red&#x27;), (1, &#x27;blue&#x27;), (2, &#x27;red&#x27;), (2, &#x27;blue&#x27;)]index = pd.MultiIndex.from_tuples(tuples, names=[&#x27;no&#x27;, &#x27;color&#x27;])index 输出： 12345MultiIndex([(1, &#x27;red&#x27;), (1, &#x27;blue&#x27;), (2, &#x27;red&#x27;), (2, &#x27;blue&#x27;)], names=[&#x27;no&#x27;, &#x27;color&#x27;]) 代码： 123arrays = [[1, 1, 2, 2], [&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;red&#x27;, &#x27;blue&#x27;]]index = pd.MultiIndex.from_arrays(arrays, names=[&#x27;no&#x27;, &#x27;color&#x27;])index 输出： 12345MultiIndex([(1, &#x27;red&#x27;), (1, &#x27;blue&#x27;), (2, &#x27;red&#x27;), (2, &#x27;blue&#x27;)], names=[&#x27;no&#x27;, &#x27;color&#x27;]) 代码： 123sales_data = np.random.randint(1, 100, 4)ser = pd.Series(data=sales_data, index=index)ser 输出： 123456no color1 red 43 blue 312 red 55 blue 75dtype: int64 代码： 1ser.groupby(&#x27;no&#x27;).sum() 输出： 1234no1 742 130dtype: int64 代码： 1ser.groupby(level=1).sum() 输出： 1234colorblue 106red 98dtype: int64 代码： 1234567stu_ids = np.arange(1001, 1006)semisters = [&#x27;期中&#x27;, &#x27;期末&#x27;]index = pd.MultiIndex.from_product((stu_ids, semisters), names=[&#x27;学号&#x27;, &#x27;学期&#x27;])courses = [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]scores = np.random.randint(60, 101, (10, 3))df = pd.DataFrame(data=scores, columns=courses, index=index)df 输出： 123456789101112 语文 数学 英语学号 学期 1001 期中 93 77 60 期末 93 98 841002 期中 64 78 71 期末 70 71 971003 期中 72 88 97 期末 99 100 631004 期中 80 71 61 期末 91 62 721005 期中 82 95 67 期末 84 78 86 根据第一级索引分组数据，按照期中成绩占25%，期末成绩占75% 的方式计算每个学生每门课的成绩。 代码： 1df.groupby(level=0).agg(lambda x: x.values[0] * 0.25 + x.values[1] * 0.75) 输出： 1234567 语文 数学 英语学号 1001 93.00 92.75 78.001002 68.50 72.75 90.501003 92.25 97.00 71.501004 88.25 64.25 69.251005 83.50 82.25 81.25 间隔索引间隔索引顾名思义是使用固定的间隔范围充当索引，我们通常会使用interval_range函数来创建间隔索引，代码如下所示。 代码： 12index = pd.interval_range(start=0, end=5)index 输出： 1IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]], dtype=&#x27;interval[int64, right]&#x27;) IntervalIndex有一个名为contains的方法，可以检查范围内是否包含了某个元素，如下所示。 代码： 1index.contains(1.5) 输出： 1array([False, True, False, False, False]) IntervalIndex还有一个名为overlaps的方法，可以检查一个范围跟其他的范围是否有重叠，如下所示。 代码： 1index.overlaps(pd.Interval(1.5, 3.5)) 输出： 1array([False, True, True, True, False]) 如果希望间隔范围是左闭右开的状态，可以在创建间隔索引时通过closed=&#39;left&#39;来做到；如果希望两边都是关闭状态，可以将close参数的值赋值为both，代码如下所示。 代码： 12index = pd.interval_range(start=0, end=5, closed=&#x27;left&#x27;)index 输出： 1IntervalIndex([[0, 1), [1, 2), [2, 3), [3, 4), [4, 5)], dtype=&#x27;interval[int64, left]&#x27;) 代码： 12index = pd.interval_range(start=pd.Timestamp(&#x27;2022-01-01&#x27;), end=pd.Timestamp(&#x27;2022-01-04&#x27;), closed=&#x27;both&#x27;)index 输出： 1IntervalIndex([[2022-01-01, 2022-01-02], [2022-01-02, 2022-01-03], [2022-01-03, 2022-01-04]], dtype=&#x27;interval[datetime64[ns], both]&#x27;) 日期时间索引DatetimeIndex应该是众多索引中最复杂最重要的一种索引，我们通常会使用date_range()函数来创建日期时间索引，该函数有几个非常重要的参数start、end、periods、freq、tz，分别代表起始日期时间、结束日期时间、生成周期、采样频率和时区。我们先来看看如何创建DatetimeIndex对象，再来讨论它的相关运算和操作，代码如下所示。 代码： 1pd.date_range(&#x27;2021-1-1&#x27;, &#x27;2021-6-30&#x27;, periods=10) 输出： 1234DatetimeIndex([&#x27;2021-01-01&#x27;, &#x27;2021-01-21&#x27;, &#x27;2021-02-10&#x27;, &#x27;2021-03-02&#x27;, &#x27;2021-03-22&#x27;, &#x27;2021-04-11&#x27;, &#x27;2021-05-01&#x27;, &#x27;2021-05-21&#x27;, &#x27;2021-06-10&#x27;, &#x27;2021-06-30&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=None) 代码： 1pd.date_range(&#x27;2021-1-1&#x27;, &#x27;2021-6-30&#x27;, freq=&#x27;W&#x27;) 说明：freq=W表示采样周期为一周，它会默认星期日是一周的开始；如果你希望星期一表示一周的开始，你可以将其修改为freq=W-MON；你也可以试着将该参数的值修改为12H，M，Q等，看看会发生什么，相信你不难猜到它们的含义。 输出： 12345678DatetimeIndex([&#x27;2021-01-03&#x27;, &#x27;2021-01-10&#x27;, &#x27;2021-01-17&#x27;, &#x27;2021-01-24&#x27;, &#x27;2021-01-31&#x27;, &#x27;2021-02-07&#x27;, &#x27;2021-02-14&#x27;, &#x27;2021-02-21&#x27;, &#x27;2021-02-28&#x27;, &#x27;2021-03-07&#x27;, &#x27;2021-03-14&#x27;, &#x27;2021-03-21&#x27;, &#x27;2021-03-28&#x27;, &#x27;2021-04-04&#x27;, &#x27;2021-04-11&#x27;, &#x27;2021-04-18&#x27;, &#x27;2021-04-25&#x27;, &#x27;2021-05-02&#x27;, &#x27;2021-05-09&#x27;, &#x27;2021-05-16&#x27;, &#x27;2021-05-23&#x27;, &#x27;2021-05-30&#x27;, &#x27;2021-06-06&#x27;, &#x27;2021-06-13&#x27;, &#x27;2021-06-20&#x27;, &#x27;2021-06-27&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=&#x27;W-SUN&#x27;) DatatimeIndex可以跟DateOffset类型进行运算，这一点很好理解，以为我们可以设置一个时间差让时间向前或向后偏移，具体的操作如下所示。 代码： 12index = pd.date_range(&#x27;2021-1-1&#x27;, &#x27;2021-6-30&#x27;, freq=&#x27;W&#x27;)index - pd.DateOffset(days=2) 输出： 12345678DatetimeIndex([&#x27;2021-01-01&#x27;, &#x27;2021-01-08&#x27;, &#x27;2021-01-15&#x27;, &#x27;2021-01-22&#x27;, &#x27;2021-01-29&#x27;, &#x27;2021-02-05&#x27;, &#x27;2021-02-12&#x27;, &#x27;2021-02-19&#x27;, &#x27;2021-02-26&#x27;, &#x27;2021-03-05&#x27;, &#x27;2021-03-12&#x27;, &#x27;2021-03-19&#x27;, &#x27;2021-03-26&#x27;, &#x27;2021-04-02&#x27;, &#x27;2021-04-09&#x27;, &#x27;2021-04-16&#x27;, &#x27;2021-04-23&#x27;, &#x27;2021-04-30&#x27;, &#x27;2021-05-07&#x27;, &#x27;2021-05-14&#x27;, &#x27;2021-05-21&#x27;, &#x27;2021-05-28&#x27;, &#x27;2021-06-04&#x27;, &#x27;2021-06-11&#x27;, &#x27;2021-06-18&#x27;, &#x27;2021-06-25&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=None) 代码： 1index + pd.DateOffset(hours=2, minutes=10) 输出： 1234567891011121314DatetimeIndex([&#x27;2021-01-03 02:10:00&#x27;, &#x27;2021-01-10 02:10:00&#x27;, &#x27;2021-01-17 02:10:00&#x27;, &#x27;2021-01-24 02:10:00&#x27;, &#x27;2021-01-31 02:10:00&#x27;, &#x27;2021-02-07 02:10:00&#x27;, &#x27;2021-02-14 02:10:00&#x27;, &#x27;2021-02-21 02:10:00&#x27;, &#x27;2021-02-28 02:10:00&#x27;, &#x27;2021-03-07 02:10:00&#x27;, &#x27;2021-03-14 02:10:00&#x27;, &#x27;2021-03-21 02:10:00&#x27;, &#x27;2021-03-28 02:10:00&#x27;, &#x27;2021-04-04 02:10:00&#x27;, &#x27;2021-04-11 02:10:00&#x27;, &#x27;2021-04-18 02:10:00&#x27;, &#x27;2021-04-25 02:10:00&#x27;, &#x27;2021-05-02 02:10:00&#x27;, &#x27;2021-05-09 02:10:00&#x27;, &#x27;2021-05-16 02:10:00&#x27;, &#x27;2021-05-23 02:10:00&#x27;, &#x27;2021-05-30 02:10:00&#x27;, &#x27;2021-06-06 02:10:00&#x27;, &#x27;2021-06-13 02:10:00&#x27;, &#x27;2021-06-20 02:10:00&#x27;, &#x27;2021-06-27 02:10:00&#x27;], dtype=&#x27;datetime64[ns]&#x27;, freq=None) 如果Series对象或DataFrame对象使用了DatetimeIndex类型的索引，此时我们可以通过asfreq()方法指定一个时间频率来实现对数据的抽样，我们仍然以之前讲过的百度股票数据为例，给大家做一个演示。 代码： 123baidu_df = pd.read_excel(&#x27;data/2022年股票数据.xlsx&#x27;, sheet_name=&#x27;BIDU&#x27;, index_col=&#x27;Date&#x27;)baidu_df.sort_index(inplace=True)baidu_df.asfreq(&#x27;5D&#x27;) 输出： 大家可能注意到了，每5天抽取1天有可能会抽中非交易日，那么对应的列都变成了空值，为了解决这个问题，在使用asfreq方法时可以通过method参数来指定一种填充空值的方法，可以将相邻的交易日的数据填入进来。 代码： 1baidu_df.asfreq(&#x27;5D&#x27;, method=&#x27;ffill&#x27;) 输出： 当使用DatetimeIndex索引时，我们也可以通过resample()方法基于时间对数据进行重采样，相当于根据时间周期对数据进行了分组操作，分组之后还可以进行聚合统计，代码如下所示。 代码： 1baidu_df.resample(&#x27;1M&#x27;).mean() 输出： 代码： 1baidu_df.resample(&#x27;1M&#x27;).agg([&#x27;mean&#x27;, &#x27;std&#x27;]) 输出： 提示：不知大家是否注意到，上面输出的DataFrame 的列索引是一个MultiIndex对象。你可以访问上面的DataFrame对象的columns属性看看。 如果要实现日期时间的时区转换，我们可以先用tz_localize()方法将日期时间本地化，代码如下所示。 代码： 12baidu_df = baidu_df.tz_localize(&#x27;Asia/Chongqing&#x27;)baidu_df 输出： 在对时间本地化以后，我们再使用tz_convert()方法就可以实现转换时区，代码如下所示。 代码： 1baidu_df.tz_convert(&#x27;America/New_York&#x27;) 输出： 如果你的数据使用了DatetimeIndex类型的索引，那么你就很有可能要对数据进行时间序列分析，关于时间序列分析的方法和模型并不是本章节要探讨的内容，我们在其他的专栏中为大家分享。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/76.深入浅出pandas-5","date":"2024-12-12T08:38:02.414Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/76.深入浅出pandas-5/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/76.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-5/","excerpt":"","text":"深入浅出pandas-5我们再来补充一些使用DataFrame做数据分析时会使用到的操作，这些操作不仅常见而且也非常重要。 计算同比环比我们之前讲过一个统计月度销售额的例子，我们可以通过groupby方法做分组聚合，也可以通过pivot_table生成透视表，如下所示。 123456sales_df = pd.read_excel(&#x27;data/2020年销售数据.xlsx&#x27;)sales_df[&#x27;月份&#x27;] = sales_df.销售日期.dt.monthsales_df[&#x27;销售额&#x27;] = sales_df.售价 * sales_df.销售数量result_df = sales_df.pivot_table(index=&#x27;月份&#x27;, values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;)result_df.rename(columns=&#123;&#x27;销售额&#x27;: &#x27;本月销售额&#x27;&#125;, inplace=True)result_df 输出： 1234567891011121314 本月销售额月份 1 54098552 46084553 41649724 39967705 32390056 28179367 35013048 29481899 263296010 237538511 238528312 1691973 在得到月度销售额之后，如果我们需要计算月环比，这里有两种方案。第一种方案是我们可以使用shift方法对数据进行移动，将上一个月的数据与本月数据对齐，然后通过(本月销售额 - 上月销售额) / 上月销售额来计算月环比，代码如下所示。 12result_df[&#x27;上月销售额&#x27;] = result_df.本月销售额.shift(1)result_df 输出： 1234567891011121314 本月销售额 上月销售额月份 1 5409855 NaN2 4608455 5409855.03 4164972 4608455.04 3996770 4164972.05 3239005 3996770.06 2817936 3239005.07 3501304 2817936.08 2948189 3501304.09 2632960 2948189.010 2375385 2632960.011 2385283 2375385.012 1691973 2385283.0 在上面的例子中，shift方法的参数为1表示将数据向下移动一个单元，当然我们可以使用参数-1将数据向上移动一个单元。相信大家能够想到，如果我们有更多年份的数据，我们可以将参数设置为12，这样就可以计算今年的每个月与去年的每个月之间的同比。 12345result_df[&#x27;环比&#x27;] = (result_df.本月销售额 - result_df.上月销售额) / result_df.上月销售额result_df.style.format( formatter=&#123;&#x27;上月销售额&#x27;: &#x27;&#123;:.0f&#125;&#x27;, &#x27;环比&#x27;: &#x27;&#123;:.2%&#125;&#x27;&#125;, na_rep=&#x27;--------&#x27;) 输出： 1234567891011121314 本月销售额 上月销售额 环比月份 1 5409855 -------- -------- 2 4608455 5409855 -14.81% 3 4164972 4608455 -9.62%4 3996770 4164972 -4.04%5 3239005 3996770 -18.96%6 2817936 3239005 -13.00%7 3501304 2817936 24.25%8 2948189 3501304 -15.80%9 2632960 2948189 -10.69%10 2375385 2632960 -9.78%11 2385283 2375385 0.42%12 1691973 2385283 -29.07% 说明：使用 JupyterLab 时，可以通过DataFrame对象的style属性在网页中对其进行渲染，上面的代码通过Styler对象的format方法将环比格式化为百分比进行显示，此外还指定了将空值替换为--------。 更为简单的第二种方案是直接使用pct_change方法计算变化的百分比，我们先将之前的上月销售额和环比列删除掉。 1result_df.drop(columns=[&#x27;上月销售额&#x27;, &#x27;环比&#x27;], inplace=True) 接下来，我们使用DataFrame对象的pct_change方法完成环比的计算。值得一提的是，pct_change方法有一个名为periods的参数，它的默认值是1，计算相邻两项数据变化的百分比，这不就是我们想要的环比吗？如果我们有很多年的数据，在计算时把这个参数的值修改为12，就可以得到相邻两年的月同比。 12result_df[&#x27;环比&#x27;] = result_df.pct_change()result_df 窗口计算DataFrame对象的rolling方法允许我们将数据置于窗口中，然后用函数对窗口中的数据进行运算和处理。例如，我们获取了某只股票近期的数据，想制作5日均线和10日均线，那么就需要先设置窗口再进行运算。我们先用如下所示的代码读取2022年百度的股票数据，数据文件可以通过下面的链接来获取。 123baidu_df = pd.read_excel(&#x27;data/2022年股票数据.xlsx&#x27;, sheet_name=&#x27;BIDU&#x27;)baidu_df.sort_index(inplace=True)baidu_df 输出： 上面的DataFrame有Open、High、Low、Close、Volume五个列，分别代表股票的开盘价、最高价、最低价、收盘价和成交量，接下来我们对百度的股票数据进行窗口计算。 1baidu_df.rolling(5).mean() 输出： 我们也可以在Series上使用rolling设置窗口并在窗口内完成运算，例如我们可以对上面的百度股票收盘价（Close列）计算5日均线和10日均线，并使用merge函数将其组装到一个DataFrame对象中并绘制出双均线图，代码如下所示。 123456close_ma5 = baidu_df.Close.rolling(5).mean()close_ma10 = baidu_df.Close.rolling(10).mean()result_df = pd.merge(close_ma5, close_ma10, left_index=True, right_index=True)result_df.rename(columns=&#123;&#x27;Close_x&#x27;: &#x27;MA5&#x27;, &#x27;Close_y&#x27;: &#x27;MA10&#x27;&#125;, inplace=True)result_df.plot(kind=&#x27;line&#x27;, figsize=(10, 6))plt.show() 输出： 相关性判定在统计学中，我们通常使用协方差（covariance）来衡量两个随机变量的联合变化程度。如果变量 $X$ 的较大值主要与另一个变量 $Y$ 的较大值相对应，而两者较小值也相对应，那么两个变量倾向于表现出相似的行为，协方差为正。如果一个变量的较大值主要对应于另一个变量的较小值，则两个变量倾向于表现出相反的行为，协方差为负。简单的说，协方差的正负号显示着两个变量的相关性。方差是协方差的一种特殊情况，即变量与自身的协方差。 $$cov(X,Y) &#x3D; E((X - \\mu)(Y - \\upsilon)) &#x3D; E(X \\cdot Y) - \\mu\\upsilon$$ 如果 $X$ 和 $Y$ 是统计独立的，那么二者的协方差为0，这是因为在 $X$ 和 $Y$ 独立的情况下： $$E(X \\cdot Y) &#x3D; E(X) \\cdot E(Y) &#x3D; \\mu\\upsilon$$ 协方差的数值大小取决于变量的大小，通常是不容易解释的，但是正态形式的协方差可以显示两变量线性关系的强弱。在统计学中，皮尔逊积矩相关系数就是正态形式的协方差，它用于度量两个变量 $X$ 和 $Y$ 之间的相关程度（线性相关），其值介于-1到1之间。 $$\\frac {cov(X, Y)} {\\sigma_{X}\\sigma_{Y}}$$ 估算样本的协方差和标准差，可以得到样本皮尔逊系数，通常用希腊字母 $\\rho$ 表示。 $$\\rho &#x3D; \\frac {\\sum_{i&#x3D;1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})} {\\sqrt{\\sum_{i&#x3D;1}^{n}(X_i - \\bar{X})^2} \\sqrt{\\sum_{i&#x3D;1}^{n}(Y_i - \\bar{Y})^2}}$$ 我们用 $\\rho$ 值判断指标的相关性时遵循以下两个步骤。 判断指标间是正相关、负相关，还是不相关。 当 $ \\rho \\gt 0 $，认为变量之间是正相关，也就是两者的趋势一致。 当 $ \\rho \\lt 0 $，认为变量之间是负相关，也就是两者的趋势相反。 当 $ \\rho \\approx 0 $，认为变量之间是不相关的，但并不代表两个指标是统计独立的。 判断指标间的相关程度。 当 $ \\rho $ 的绝对值在 $ [0.6,1] $ 之间，认为变量之间是强相关的。 当 $ \\rho $ 的绝对值在 $ [0.1,0.6) $ 之间，认为变量之间是弱相关的。 当 $ \\rho $ 的绝对值在 $ [0,0.1) $ 之间，认为变量之间没有相关性。 皮尔逊相关系数适用于： 两个变量之间是线性关系，都是连续数据。 两个变量的总体是正态分布，或接近正态的单峰分布。 两个变量的观测值是成对的，每对观测值之间相互独立。 这里，我们顺便说一下，如果两组变量并不是来自于正态总体的连续值，我们该如何判断相关性呢？对于定序尺度（等级），我们可以使用斯皮尔曼秩相关系数，其计算公式如下所示：$$r_{s}&#x3D;1-{\\frac {6\\sum d_{i}^{2}}{n(n^{2}-1)}}$$其中，$d_{i}&#x3D;\\operatorname {R} (X_{i})-\\operatorname {R} (Y_{i})$，即每组观测中两个变量的等级差值，$n$为观测样本数。 对于定类尺度（类别），我们可以使用卡方检验的方式来判定其是否相关。其实很多时候，连续值也可以通过分箱的方式处理成离散的等级或类别，然后使用斯皮尔曼秩相关系数或卡方检验的方式来判定相关性。 DataFrame对象的cov方法和corr方法分别用于计算协方差和相关系数，corr方法有一个名为method的参数，其默认值是pearson，表示计算皮尔逊相关系数；除此之外，还可以指定kendall或spearman来计算肯德尔系数或斯皮尔曼秩相关系数。 我们从名为boston_house_price.csv的文件中获取著名的波士顿房价数据集来创建一个DataFrame。 12boston_df = pd.read_csv(&#x27;data/boston_house_price.csv&#x27;)boston_df 输出： 说明：上面代码中使用了相对路径来访问 CSV 文件，也就是说 CSV 文件在当前工作路径下名为data的文件夹中。如果需要上面例子中的 CSV 文件，可以通过下面的百度云盘地址进行获取。链接：https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g?pwd=e7b4，提取码：e7b4。 可以看出，该数据集中包含了诸多影响房价的特征，包括犯罪率、一氧化氮浓度、平均房间数、低收入人群占比等，其中PRICE代表房价，具体情况如下所示。 接下来，我们将其中可以视为来自于正态总体的连续值，通过corr方法计算皮尔逊相关系数，看看哪些跟房价是正相关或负相关的关系，代码如下所示。 1boston_df[[&#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;PTRATIO&#x27;, &#x27;LSTAT&#x27;, &#x27;PRICE&#x27;]].corr() 输出： 可以看出，平均房间数（RM）跟房价有较强的正相关性，而低收入人群占比（LSTAT）跟房价之间存在明显的负相关性。 斯皮尔曼秩相关系数对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级数据，或者是由连续变量转化成等级的数据，不论两个变量的总体分布形态、样本容量的大小如何，都可以用斯皮尔曼等级相关系数来进行研究。我们可以通过下面的方式对部分特征进行预处理，然后计算斯皮尔曼秩相关系数。 1234567boston_df[&#x27;CRIM&#x27;] = boston_df.CRIM.apply(lambda x: x // 5 if x &lt; 25 else 5).map(int)boston_df[&#x27;ZN&#x27;] = pd.qcut(boston_df.ZN, q=[0, 0.75, 0.8, 0.85, 0.9, 0.95, 1], labels=np.arange(6))boston_df[&#x27;AGE&#x27;] = (boston_df.AGE // 20).map(int)boston_df[&#x27;DIS&#x27;] = (boston_df.DIS // 2.05).map(int)boston_df[&#x27;B&#x27;] = (boston_df.B // 66).map(int)boston_df[&#x27;PRICE&#x27;] = pd.qcut(boston_df.PRICE, q=[0, 0.15, 0.3, 0.5, 0.7, 0.85, 1], labels=np.arange(6))boston_df[[&#x27;CRIM&#x27;, &#x27;ZN&#x27;, &#x27;AGE&#x27;, &#x27;DIS&#x27;, &#x27;B&#x27;, &#x27;PRICE&#x27;]].corr(method=&#x27;spearman&#x27;) 输出： 可以看出，房价跟犯罪率（CRIM）和房龄（AGE）之间存在较为明显的负相关关系，跟住房用地尺寸（ZN）存在微弱的正相关关系。相关性可以帮助我们在实际工作中找到业务抓手，即找到那些能够影响或改变工作结果的相关因素。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/75.深入浅出pandas-4","date":"2024-12-12T08:38:02.412Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/75.深入浅出pandas-4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/75.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-4/","excerpt":"","text":"深入浅出pandas-4数据透视经过前面的学习，我们已经将数据准备就绪而且变成了我们想要的样子，接下来就是最为重要的数据透视阶段了。当我们拿到一大堆数据的时候，如何从数据中迅速的解读出有价值的信息，把繁杂的数据变成容易解读的统计图表并再此基础上产生业务洞察，这就是数据分析要解决的核心问题。 获取描述性统计信息首先，我们可以获取数据的描述性统计信息，通过描述性统计信息，我们可以了解数据的集中趋势和离散趋势。 例如，我们有如下所示的学生成绩表。 12345scores = np.random.randint(50, 101, (5, 3))names = (&#x27;关羽&#x27;, &#x27;张飞&#x27;, &#x27;赵云&#x27;, &#x27;马超&#x27;, &#x27;黄忠&#x27;)courses = (&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;)df = pd.DataFrame(data=scores, columns=courses, index=names)df 输出： 123456 语文 数学 英语关羽 96 72 73张飞 72 70 97赵云 74 51 79马超 100 54 54黄忠 89 100 88 我们可以通过DataFrame对象的方法mean、max、min、std、var等方法分别获取每个学生或每门课程的平均分、最高分、最低分、标准差、方差等信息，也可以直接通过describe方法直接获取描述性统计信息，代码如下所示。 计算每门课程成绩的平均分。 1df.mean() 输出： 1234语文 86.2数学 69.4英语 78.2dtype: float64 计算每个学生成绩的平均分。 1df.mean(axis=1) 输出： 123456关羽 80.333333张飞 79.666667赵云 68.000000马超 69.333333黄忠 92.333333dtype: float64 计算每门课程成绩的方差。 1df.var() 输出： 1234语文 161.2数学 379.8英语 265.7dtype: float64 说明：通过方差可以看出，数学成绩波动最大，两极分化可能更严重。 获取每门课程的描述性统计信息。 1df.describe() 输出： 123456789 语文 数学 英语count 5.000000 5.000000 5.000000mean 86.200000 69.400000 78.200000std 12.696456 19.488458 16.300307min 72.000000 51.000000 54.00000025% 74.000000 54.000000 73.00000050% 89.000000 70.000000 79.00000075% 96.000000 72.000000 88.000000max 100.000000 100.000000 97.000000 排序和取头部值如果需要对数据进行排序，可以使用DataFrame对象的sort_values方法，该方法的by参数可以指定根据哪个列或哪些列进行排序，而ascending参数可以指定升序或是降序。例如，下面的代码展示了如何将学生表按语文成绩排降序。 1df.sort_values(by=&#x27;语文&#x27;, ascending=False) 输出： 123456 语文 数学 英语马超 100 54 54关羽 96 72 73黄忠 89 100 88赵云 74 51 79张飞 72 70 97 如果DataFrame数据量很大，排序将是一个非常耗费时间的操作。有的时候我们只需要获得排前N名或后N名的数据，这个时候其实没有必要对整个数据进行排序，而是直接利用堆结构找出Top-N的数据。DataFrame的nlargest和nsmallest方法就提供对Top-N操作的支持，代码如下所示。 找出语文成绩前3名的学生信息。 1df.nlargest(3, &#x27;语文&#x27;) 输出： 1234 语文 数学 英语马超 100 54 54关羽 96 72 73黄忠 89 100 88 找出数学成绩最低的3名学生的信息。 1df.nsmallest(3, &#x27;数学&#x27;) 输出： 1234 语文 数学 英语赵云 74 51 79马超 100 54 54张飞 72 70 97 分组聚合我们先从之前使用过的 Excel 文件中读取2020年销售数据，然后再为大家演示如何进行分组聚合操作。 12df = pd.read_excel(&#x27;data/2020年销售数据.xlsx&#x27;)df.head() 输出： 123456 销售日期 销售区域 销售渠道 销售订单 品牌 售价 销售数量0 2020-01-01 上海 拼多多 182894-455 八匹马 99 831 2020-01-01 上海 抖音 205635-402 八匹马 219 292 2020-01-01 上海 天猫 205654-021 八匹马 169 853 2020-01-01 上海 天猫 205654-519 八匹马 169 144 2020-01-01 上海 天猫 377781-010 皮皮虾 249 61 如果我们要统计每个销售区域的销售总额，可以先通过“售价”和“销售数量”计算出销售额，为DataFrame添加一个列，代码如下所示。 12df[&#x27;销售额&#x27;] = df[&#x27;售价&#x27;] * df[&#x27;销售数量&#x27;]df.head() 输出： 123456 销售日期 销售区域 销售渠道 销售订单 品牌 售价 销售数量 销售额0 2020-01-01 上海 拼多多 182894-455 八匹马 99 83 82171 2020-01-01 上海 抖音 205635-402 八匹马 219 29 63512 2020-01-01 上海 天猫 205654-021 八匹马 169 85 143653 2020-01-01 上海 天猫 205654-519 八匹马 169 14 23664 2020-01-01 上海 天猫 377781-010 皮皮虾 249 61 15189 然后再根据“销售区域”列对数据进行分组，这里我们使用的是DataFrame对象的groupby方法。分组之后，我们取“销售额”这个列在分组内进行求和处理，代码和结果如下所示。 1df.groupby(&#x27;销售区域&#x27;).销售额.sum() 输出： 123456789销售区域上海 11610489北京 12477717安徽 895463广东 1617949江苏 2304380浙江 687862福建 10178227Name: 销售额, dtype: int64 如果我们要统计每个月的销售总额，我们可以将“销售日期”作为groupby&#96;方法的参数，当然这里需要先将“销售日期”处理成月，代码和结果如下所示。 1df.groupby(df[&#x27;销售日期&#x27;].dt.month).销售额.sum() 输出： 1234567891011121314销售日期1 54098552 46084553 41649724 39967705 32390056 28179367 35013048 29481899 263296010 237538511 238528312 1691973Name: 销售额, dtype: int64 接下来我们将难度升级，统计每个销售区域每个月的销售总额，这又该如何处理呢？事实上，groupby方法的第一个参数可以是一个列表，列表中可以指定多个分组的依据，大家看看下面的代码和输出结果就明白了。 1df.groupby([&#x27;销售区域&#x27;, df[&#x27;销售日期&#x27;].dt.month]).销售额.sum() 输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950销售区域 销售日期上海 1 1679125 2 1689527 3 1061193 4 1082187 5 841199 6 785404 7 863906 8 734937 9 1107693 10 412108 11 825169 12 528041北京 1 1878234 2 1807787 3 1360666 4 1205989 5 807300 6 1216432 7 1219083 8 645727 9 390077 10 671608 11 678668 12 596146安徽 4 341308 5 554155广东 3 388180 8 469390 9 365191 11 395188江苏 4 537079 7 841032 10 710962 12 215307浙江 3 248354 8 439508福建 1 1852496 2 1111141 3 1106579 4 830207 5 1036351 6 816100 7 577283 8 658627 9 769999 10 580707 11 486258 12 352479Name: 销售额, dtype: int64 如果希望统计出每个区域的销售总额以及每个区域单笔金额的最高和最低，我们可以在DataFrame或Series对象上使用agg方法并指定多个聚合函数，代码和结果如下所示。 1df.groupby(&#x27;销售区域&#x27;).销售额.agg([&#x27;sum&#x27;, &#x27;max&#x27;, &#x27;min&#x27;]) 输出： 123456789 sum max min销售区域 上海 11610489 116303 948北京 12477717 133411 690安徽 895463 68502 1683广东 1617949 120807 990江苏 2304380 114312 1089浙江 687862 90909 3927福建 10178227 87527 897 如果希望自定义聚合后的列的名字，可以使用如下所示的方法。 1df.groupby(&#x27;销售区域&#x27;).销售额.agg(销售总额=&#x27;sum&#x27;, 单笔最高=&#x27;max&#x27;, 单笔最低=&#x27;min&#x27;) 输出： 123456789 销售总额 单笔最高 单笔最低销售区域 上海 11610489 116303 948北京 12477717 133411 690安徽 895463 68502 1683广东 1617949 120807 990江苏 2304380 114312 1089浙江 687862 90909 3927福建 10178227 87527 897 如果需要对多个列使用不同的聚合函数，例如“统计每个销售区域销售额的总和以及销售数量的最低值和最高值”，我们可以按照下面的方式来操作。 123df.groupby(&#x27;销售区域&#x27;)[[&#x27;销售额&#x27;, &#x27;销售数量&#x27;]].agg(&#123; &#x27;销售额&#x27;: &#x27;sum&#x27;, &#x27;销售数量&#x27;: [&#x27;max&#x27;, &#x27;min&#x27;]&#125;) 输出： 12345678910 销售额 销售数量 sum max min销售区域 上海 11610489 100 10北京 12477717 100 10安徽 895463 98 16广东 1617949 98 10江苏 2304380 100 11浙江 687862 95 20福建 10178227 100 10 透视表和交叉表上面的例子中，“统计每个销售区域每个月的销售总额”会产生一个看起来很长的结果，在实际工作中我们通常把那些行很多列很少的表成为“窄表”，如果我们不想得到这样的一个“窄表”，可以使用DataFrame的pivot_table方法或者是pivot_table函数来生成透视表。透视表的本质就是对数据进行分组聚合操作，根据 A 列对 B 列进行统计，如果大家有使用 Excel 的经验，相信对透视表这个概念一定不会陌生。例如，我们要“统计每个销售区域的销售总额”，那么“销售区域”就是我们的 A 列，而“销售额”就是我们的 B 列，在pivot_table函数中分别对应index和values参数，这两个参数都可以是单个列或者多个列。 1pd.pivot_table(df, index=&#x27;销售区域&#x27;, values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;) 输出： 123456789 销售额销售区域 上海 11610489北京 12477717安徽 895463广东 1617949江苏 2304380浙江 687862福建 10178227 注意：上面的结果操作跟之前用groupby的方式得到的结果有一些区别，groupby操作后，如果对单个列进行聚合，得到的结果是一个Series对象，而上面的结果是一个DataFrame 对象。 如果要统计每个销售区域每个月的销售总额，也可以使用pivot_table函数，代码如下所示。 12df[&#x27;月份&#x27;] = df[&#x27;销售日期&#x27;].dt.monthpd.pivot_table(df, index=[&#x27;销售区域&#x27;, &#x27;月份&#x27;], values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;) 上面的操作结果是一个DataFrame，但也是一个长长的“窄表”，如果希望做成一个行比较少列比较多的“宽表”，可以将index参数中的列放到columns参数中，代码如下所示。 1pd.pivot_table(df, index=&#x27;销售区域&#x27;, columns=&#x27;月份&#x27;, values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;, fill_value=0) 说明：pivot_table函数的fill_value=0会将空值处理为0。 输出： 使用pivot_table函数时，还可以通过添加margins和margins_name参数对分组聚合的结果做一个汇总，具体的操作和效果如下所示。 1pd.pivot_table(df, index=&#x27;销售区域&#x27;, columns=&#x27;月份&#x27;, values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;, fill_value=0, margins=True, margins_name=&#x27;总计&#x27;) 输出： 交叉表就是一种特殊的透视表，它不需要先构造一个DataFrame对象，而是直接通过数组或Series对象指定两个或多个因素进行运算得到统计结果。例如，我们要统计每个销售区域的销售总额，也可以按照如下所示的方式来完成，我们先准备三组数据。 1sales_area, sales_month, sales_amount = df[&#x27;销售区域&#x27;], df[&#x27;月份&#x27;], df[&#x27;销售额&#x27;] 使用crosstab函数生成交叉表。 1pd.crosstab(index=sales_area, columns=sales_month, values=sales_amount, aggfunc=&#x27;sum&#x27;).fillna(0).astype(&#x27;i8&#x27;) 说明：上面的代码使用了DataFrame对象的fillna方法将空值处理为0，再使用astype方法将数据类型处理成整数。 数据呈现一图胜千言，我们对数据进行透视的结果，最终要通过图表的方式呈现出来，因为图表具有极强的表现力，能够让我们迅速的解读数据中隐藏的价值。和Series一样，DataFrame对象提供了plot方法来支持绘图，底层仍然是通过matplotlib库实现图表的渲染。关于matplotlib的内容，我们在下一个章节进行详细的探讨，这里我们只简单的讲解plot方法的用法。 例如，我们想通过一张柱状图来比较“每个销售区域的销售总额”，可以直接在透视表上使用plot方法生成柱状图。我们先导入matplotlib.pyplot模块，通过修改绘图的参数使其支持中文显示。 123import matplotlib.pyplot as pltplt.rcParams[&#x27;font.sans-serif&#x27;] = &#x27;FZJKai-Z03S&#x27; 说明：上面的FZJKai-Z03S是我电脑上已经安装的一种支持中文的字体的名称，字体的名称可以通过查看用户主目录下.matplotlib文件夹下名为fontlist-v330.json的文件来获得，而这个文件在执行上面的命令后就会生成。 使用魔法指令配置生成矢量图。 1%config InlineBackend.figure_format = &#x27;svg&#x27; 绘制“每个销售区域销售总额”的柱状图。 1234temp = pd.pivot_table(df, index=&#x27;销售区域&#x27;, values=&#x27;销售额&#x27;, aggfunc=&#x27;sum&#x27;)temp.plot(figsize=(8, 4), kind=&#x27;bar&#x27;)plt.xticks(rotation=0)plt.show() 说明：上面的第3行代码会将横轴刻度上的文字旋转到0度，第4行代码会显示图像。 输出： 如果要绘制饼图，可以修改plot方法的kind参数为pie，然后使用定制饼图的参数对图表加以定制，代码如下所示。 1234567891011temp.sort_values(by=&#x27;销售额&#x27;, ascending=False).plot( figsize=(6, 6), kind=&#x27;pie&#x27;, y=&#x27;销售额&#x27;, ylabel=&#x27;&#x27;, autopct=&#x27;%.2f%%&#x27;, pctdistance=0.8, wedgeprops=dict(linewidth=1, width=0.35), legend=False)plt.show() 输出：","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/72.深入浅出pandas-1","date":"2024-12-12T08:38:02.406Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/72.深入浅出pandas-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/72.%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BApandas-1/","excerpt":"","text":"深入浅出pandas-1Pandas 是 Wes McKinney 在2008年开发的一个强大的分析结构化数据的工具集。Pandas 以 NumPy 为基础（实现数据存储和运算），提供了专门用于数据分析的类型、方法和函数，对数据分析和数据挖掘提供了很好的支持；同时 pandas 还可以跟数据可视化工具 matplotlib 很好的整合在一起，非常轻松愉快的实现数据可视化呈现。 Pandas 核心的数据类型是Series（数据系列）、DataFrame（数据窗&#x2F;数据框），分别用于处理一维和二维的数据，除此之外，还有一个名为Index的类型及其子类型，它们为Series和DataFrame提供了索引功能。日常工作中DataFrame使用得最为广泛，因为二维的数据结构刚好可以对应有行有列的表格。Series和DataFrame都提供了大量的处理数据的方法，数据分析师以此为基础，可以实现对数据的筛选、合并、拼接、清洗、预处理、聚合、透视和可视化等各种操作。 创建Series对象Pandas 库中的Series对象可以用来表示一维数据结构，但是多了索引和一些额外的功能。Series类型的内部结构包含了两个数组，其中一个用来保存数据，另一个用来保存数据的索引。我们可以通过列表或数组创建Series对象，代码如下所示。 代码： 12345import numpy as npimport pandas as pdser1 = pd.Series(data=[120, 380, 250, 360], index=[&#x27;一季度&#x27;, &#x27;二季度&#x27;, &#x27;三季度&#x27;, &#x27;四季度&#x27;])ser1 说明：Series构造器中的data参数表示数据，index参数表示数据的索引，相当于数据对应的标签。 输出： 12345一季度 120二季度 380三季度 250四季度 360dtype: int64 通过字典创建Series对象。 代码： 12ser2 = pd.Series(&#123;&#x27;一季度&#x27;: 320, &#x27;二季度&#x27;: 180, &#x27;三季度&#x27;: 300, &#x27;四季度&#x27;: 405&#125;)ser2 说明：通过字典创建Series对象时，字典的键就是数据的标签（索引），键对应的值就是数据。 输出： 12345一季度 320二季度 180三季度 300四季度 405dtype: int64 Series对象的运算标量运算我们尝试给刚才的ser1每个季度加上10，代码如下所示。 代码： 12ser1 += 10ser1 输出： 12345一季度 130二季度 390三季度 260四季度 370dtype: int64 矢量运算我们尝试把ser1和ser2对应季度的数据加起来，代码如下所示。 代码： 1ser1 + ser2 输出： 12345一季度 450二季度 570三季度 560四季度 775dtype: int64 索引运算普通索引跟数组一样，Series对象也可以进行索引和切片操作，不同的是Series对象因为内部维护了一个保存索引的数组，所以除了可以使用整数索引检索数据外，还可以通过自己设置的索引（标签）获取对应的数据。 使用整数索引。 代码： 1ser1[2] 输出： 1260 使用自定义索引。 代码： 1ser1[&#x27;三季度&#x27;] 输出： 1260 代码： 12ser1[&#x27;一季度&#x27;] = 380ser1 输出： 12345一季度 380二季度 390三季度 260四季度 370dtype: int64 切片索引Series对象的切片操作跟列表、数组类似，通过给出起始和结束索引，从原来的Series对象中取出或修改部分数据，这里也可以使用整数索引和自定义的索引，代码如下所示。 代码： 1ser2[1:3] 输出： 123二季度 180三季度 300dtype: int64 代码： 1ser2[&#x27;二季度&#x27;:&#x27;四季度&#x27;] 输出： 1234二季度 180三季度 300四季度 405dtype: int64 提示：在使用自定义索引进行切片时，结束索引对应的元素也是可以取到的。 代码： 12ser2[1:3] = 400, 500ser2 输出： 12345一季度 320二季度 400三季度 500四季度 405dtype: int64 花式索引代码： 1ser2[[&#x27;二季度&#x27;, &#x27;四季度&#x27;]] 输出： 123二季度 400四季度 405dtype: int64 代码： 12ser2[[&#x27;二季度&#x27;, &#x27;四季度&#x27;]] = 600, 520ser2 输出： 12345一季度 320二季度 600三季度 500四季度 520dtype: int64 布尔索引代码： 1ser2[ser2 &gt;= 500] 输出： 1234二季度 600三季度 500四季度 520dtype: int64 Series对象的属性和方法Series对象的属性和方法非常多，我们就捡着重要的跟大家讲吧。先看看下面的表格，它展示了Series对象常用的属性。 属性 说明 dtype &#x2F; dtypes 返回Series对象的数据类型 hasnans 判断Series对象中有没有空值 at &#x2F; iat 通过索引访问Series对象中的单个值 loc &#x2F; iloc 通过索引访问Series对象中的单个值或一组值 index 返回Series对象的索引（Index对象） is_monotonic 判断Series对象中的数据是否单调 is_monotonic_increasing 判断Series对象中的数据是否单调递增 is_monotonic_decreasing 判断Series对象中的数据是否单调递减 is_unique 判断Series对象中的数据是否独一无二 size 返回Series对象中元素的个数 values 以ndarray的方式返回Series对象中的值（ndarray对象） 我们可以通过下面的代码来了解Series对象的属性。 代码： 123456print(ser2.dtype) # 数据类型print(ser2.hasnans) # 有没有空值print(ser2.index) # 索引print(ser2.values) # 值print(ser2.is_monotonic_increasing) # 是否单调递增print(ser2.is_unique) # 是否每个值都独一无二 输出： 123456int64FalseIndex([&#x27;一季度&#x27;, &#x27;二季度&#x27;, &#x27;三季度&#x27;, &#x27;四季度&#x27;], dtype=&#x27;object&#x27;)[320 600 500 520]FalseTrue Series对象的方法很多，下面我们通过一些代码片段为大家介绍常用的方法。 统计相关Series对象支持各种获取描述性统计信息的方法。 代码： 12345678print(ser2.count()) # 计数print(ser2.sum()) # 求和print(ser2.mean()) # 求平均print(ser2.median()) # 找中位数print(ser2.max()) # 找最大print(ser2.min()) # 找最小print(ser2.std()) # 求标准差print(ser2.var()) # 求方差 输出： 1234567841940485.0510.0600320118.1806526749055713966.666666666666 Series对象还有一个名为describe()的方法，可以获得上述所有的描述性统计信息，如下所示。 代码： 1ser2.describe() 输出： 123456789count 4.000000mean 485.000000std 118.180653min 320.00000025% 455.00000050% 510.00000075% 540.000000max 600.000000dtype: float64 提示：因为describe()返回的也是一个Series对象，所以也可以用ser2.describe()[&#39;mean&#39;]来获取平均值，用ser2.describe()[[&#39;max&#39;, &#39;min&#39;]]来获取最大值和最小值。 如果Series对象有重复的值，我们可以使用unique()方法获得由独一无二的值构成的数组；可以使用nunique()方法统计不重复值的数量；如果想要统计每个值重复的次数，可以使用value_counts()方法，这个方法会返回一个Series对象，它的索引就是原来的Series对象中的值，而每个值出现的次数就是返回的Series对象中的数据，在默认情况下会按照出现次数做降序排列，如下所示。 代码： 12ser3 = pd.Series(data=[&#x27;apple&#x27;, &#x27;banana&#x27;, &#x27;apple&#x27;, &#x27;pitaya&#x27;, &#x27;apple&#x27;, &#x27;pitaya&#x27;, &#x27;durian&#x27;])ser3.value_counts() 输出： 12345apple 3pitaya 2durian 1banana 1dtype: int64 代码： 1ser3.nunique() 输出： 14 对于ser3，我们还可以用mode()方法来找出数据的众数，由于众数可能不唯一，所以mode()方法的返回值仍然是一个Series对象。 代码： 1ser3.mode() 输出： 120 appledtype: object 处理数据Series对象的isna()和isnull()方法可以用于空值的判断，notna()和notnull()方法可以用于非空值的判断，代码如下所示。 代码： 12ser4 = pd.Series(data=[10, 20, np.nan, 30, np.nan])ser4.isna() 说明：np.nan是一个IEEE 754标准的浮点小数，专门用来表示“不是一个数”，在上面的代码中我们用它来代表空值；当然，也可以用 Python 中的None来表示空值，在 pandas 中None也会被处理为np.nan。 输出： 1234560 False1 False2 True3 False4 Truedtype: bool 代码： 1ser4.notna() 输出： 1234560 True1 True2 False3 True4 Falsedtype: bool Series对象的dropna()和fillna()方法分别用来删除空值和填充空值，具体的用法如下所示。 代码： 1ser4.dropna() 输出： 12340 10.01 20.03 30.0dtype: float64 代码： 1ser4.fillna(value=40) # 将空值填充为40 输出： 1234560 10.01 20.02 40.03 30.04 40.0dtype: float64 代码： 1ser4.fillna(method=&#x27;ffill&#x27;) # 用空值前面的非空值填充 输出： 1234560 10.01 20.02 20.03 30.04 30.0dtype: float64 需要提醒大家注意的是，dropna()和fillna()方法都有一个名为inplace的参数，它的默认值是False，表示删除空值或填充空值不会修改原来的Series对象，而是返回一个新的Series对象。如果将inplace参数的值修改为True，那么删除或填充空值会就地操作，直接修改原来的Series对象，此时方法的返回值是None。后面我们会接触到的很多方法，包括DataFrame对象的很多方法都会有这个参数，它们的意义跟这里是一样的。 Series对象的mask()和where()方法可以将满足或不满足条件的值进行替换，如下所示。 代码： 12ser5 = pd.Series(range(5))ser5.where(ser5 &gt; 0) 输出： 1234560 NaN1 1.02 2.03 3.04 4.0dtype: float64 代码： 1ser5.where(ser5 &gt; 1, 10) 输出： 1234560 101 102 23 34 4dtype: int64 代码： 1ser5.mask(ser5 &gt; 1, 10) 输出： 1234560 01 12 103 104 10dtype: int64 Series对象的duplicated()方法可以帮助我们找出重复的数据，而drop_duplicates()方法可以帮我们删除重复数据。 代码： 1ser3.duplicated() 输出： 123456780 False1 False2 True3 False4 True5 True6 Falsedtype: bool 代码： 1ser3.drop_duplicates() 输出： 123450 apple1 banana3 pitaya6 duriandtype: object Series对象的apply()和map()方法非常重要，它们可以通过字典或者指定的函数来处理数据，把数据映射或转换成我们想要的样子。这两个方法在数据准备阶段非常重要，我们先来试一试这个名为map的方法。 代码： 12ser6 = pd.Series([&#x27;cat&#x27;, &#x27;dog&#x27;, np.nan, &#x27;rabbit&#x27;])ser6 输出： 123450 cat1 dog2 NaN3 rabbitdtype: object 代码： 1ser6.map(&#123;&#x27;cat&#x27;: &#x27;kitten&#x27;, &#x27;dog&#x27;: &#x27;puppy&#x27;&#125;) 说明：通过字典给出的映射规则对数据进行处理。 输出： 123450 kitten1 puppy2 NaN3 NaNdtype: object 代码： 1ser6.map(&#x27;I am a &#123;&#125;&#x27;.format, na_action=&#x27;ignore&#x27;) 说明：将指定字符串的format方法作用到数据系列的数据上，忽略掉所有的空值。 输出： 123450 I am a cat1 I am a dog2 NaN3 I am a rabbitdtype: object 我们创建一个新的Series对象， 12ser7 = pd.Series([20, 21, 12], index=[&#x27;London&#x27;, &#x27;New York&#x27;, &#x27;Helsinki&#x27;])ser7 输出： 1234London 20New York 21Helsinki 12dtype: int64 代码： 1ser7.apply(np.square) 说明：将求平方的函数作用到数据系列的数据上，也可以将参数np.square替换为lambda x: x ** 2。 输出： 1234London 400New York 441Helsinki 144dtype: int64 代码： 1ser7.apply(lambda x, value: x - value, args=(5, )) 注意：上面apply方法中的lambda函数有两个参数，第一个参数是数据系列中的数据，而第二个参数需要我们传入，所以我们给apply方法增加了args参数，用于给lambda函数的第二个参数传值。 输出： 1234London 15New York 16Helsinki 7dtype: int64 取头部值和排序Series对象的sort_index()和sort_values()方法可以用于对索引和数据的排序，排序方法有一个名为ascending的布尔类型参数，该参数用于控制排序的结果是升序还是降序；而名为kind的参数则用来控制排序使用的算法，默认使用了quicksort，也可以选择mergesort或heapsort；如果存在空值，那么可以用na_position参数空值放在最前还是最后，默认是last，代码如下所示。 代码： 12345ser8 = pd.Series( data=[35, 96, 12, 57, 25, 89], index=[&#x27;grape&#x27;, &#x27;banana&#x27;, &#x27;pitaya&#x27;, &#x27;apple&#x27;, &#x27;peach&#x27;, &#x27;orange&#x27;])ser8.sort_values() # 按值从小到大排序 输出： 1234567pitaya 12peach 25grape 35apple 57orange 89banana 96dtype: int64 代码： 1ser8.sort_index(ascending=False) # 按索引从大到小排序 输出： 1234567pitaya 12peach 25orange 89grape 35banana 96apple 57dtype: int64 如果要从Series对象中找出元素中最大或最小的“Top-N”，我们不需要对所有的值进行排序的，可以使用nlargest()和nsmallest()方法来完成，如下所示。 代码： 1ser8.nlargest(3) # 值最大的3个 输出： 1234banana 96orange 89apple 57dtype: int64 代码： 1ser8.nsmallest(2) # 值最小的2个 输出： 123pitaya 12peach 25dtype: int64 绘制图表Series对象有一个名为plot的方法可以用来生成图表，如果选择生成折线图、饼图、柱状图等，默认会使用Series对象的索引作为横坐标，使用Series对象的数据作为纵坐标。下面我们创建一个Series对象并基于它绘制柱状图，代码如下所示。 代码： 12345678910111213import matplotlib.pyplot as pltser9 = pd.Series(&#123;&#x27;Q1&#x27;: 400, &#x27;Q2&#x27;: 520, &#x27;Q3&#x27;: 180, &#x27;Q4&#x27;: 380&#125;)# 通过plot方法的kind指定图表类型为柱状图ser9.plot(kind=&#x27;bar&#x27;)# 定制纵轴的取值范围plt.ylim(0, 600)# 定制横轴刻度（旋转到0度）plt.xticks(rotation=0)# 为柱子增加数据标签for i in range(ser9.size): plt.text(i, ser9[i] + 5, ser9[i], ha=&#x27;center&#x27;)plt.show() 输出： 我们也可以将其绘制为饼图，代码如下所示。 代码： 12345# plot方法的kind参数指定了图表类型为饼图# autopct会自动计算并显示百分比# pctdistance用来控制百分比到圆心的距离ser9.plot(kind=&#x27;pie&#x27;, autopct=&#x27;%.1f%%&#x27;, pctdistance=0.65)plt.show() 输出：","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/71.NumPy的应用-4","date":"2024-12-12T08:38:02.403Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/71.NumPy的应用-4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/71.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-4/","excerpt":"","text":"NumPy的应用-4向量向量（vector）也叫矢量，是一个同时具有大小和方向，且满足平行四边形法则的几何对象。与向量相对的概念叫标量或数量，标量只有大小，绝大多数情况下没有方向。我们通常用带箭头的线段来表示向量，在平面直角坐标系中的向量如下图所示。需要注意的是，向量是表达大小和方向的量，并没有规定起点和终点，所以相同的向量可以画在任意位置，例如下图中$\\boldsymbol{w}$和$\\boldsymbol{v}$两个向量并没有什么区别。 向量有很多种代数表示法，对于二维空间的向量，下面几种写法都是可以的。$$\\boldsymbol{a} &#x3D; \\langle a_1, a_2 \\rangle &#x3D; (a_1, a_2) &#x3D; \\begin{pmatrix} a_1 \\ a_2 \\end{pmatrix} &#x3D; \\begin{bmatrix} a_1 \\ a_2 \\end{bmatrix}$$向量的大小称为向量的模，它是一个标量，对于二维空间的向量，模可以通过下面的公式计算。$$\\lvert \\boldsymbol{a} \\rvert &#x3D; \\sqrt{a_{1}^{2} + a_{2}^{2}}$$注意，这里的$\\lvert \\boldsymbol{a} \\rvert$并不是绝对值，你可以将其称为向量$\\boldsymbol{a}$的二范数，这是数学中的符号重用现象。上面的写法和概念也可以推广到$n$维空间，我们通常用$\\boldsymbol{R^n}$表示$n$维空间，我们刚才说的二维空间可以记为$\\boldsymbol{R^2}$，三维空间可以记为$\\boldsymbol{R^3}$。虽然生活在三维空间的我们很难想象四维空间、五维空间是什么样子，但是这并不影响我们探讨高维空间，机器学习中，我们经常把有$n$个特征的训练样本称为一个$n$维向量。 向量的加法相同维度的向量可以相加得到一个新的向量，运算的方法是将向量的每个分量相加，如下所示。$$\\boldsymbol{u} &#x3D; \\begin{bmatrix} u_1 \\ u_2 \\ \\vdots \\ u_n \\end{bmatrix}, \\quad\\boldsymbol{v} &#x3D; \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix}, \\quad\\boldsymbol{u} + \\boldsymbol{v} &#x3D; \\begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \\vdots \\ u_n + v_n \\end{bmatrix}$$向量的加法满足“平行四边形法则”，即两个向量$\\boldsymbol{u}$和$\\boldsymbol{v}$构成了平行四边形的两条邻边，相加的结果是平行四边形的对角线，如下图所示。 向量的数乘一个向量$\\boldsymbol{v}$可以和一个标量$k$相乘，运算的方法是将向量中的每个分量与该标量相乘即可，如下所示。$$\\boldsymbol{v} &#x3D; \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix}, \\quadk \\cdot \\boldsymbol{v} &#x3D; \\begin{bmatrix} k \\cdot v_1 \\ k \\cdot v_2 \\ \\vdots \\ k \\cdot v_n \\end{bmatrix}$$我们可以用 NumPy 的数组来表示向量，向量的加法可以通过两个数组的加法来实现，向量的数乘可以通过数组和标量的乘法来实现，此处不再进行赘述。 向量的点积点积（dot product）是两个向量之间最为重要的运算之一，运算的方法是将两个向量对应分量的乘积求和，所以点积的结果是一个标量，其几何意义是两个向量的模乘以二者夹角的余弦如下所示。$$\\boldsymbol{u} &#x3D; \\begin{bmatrix} u_1 \\ u_2 \\ \\vdots \\ u_n \\end{bmatrix}, \\quad\\boldsymbol{v} &#x3D; \\begin{bmatrix} v_1 \\ v_2 \\ \\vdots \\ v_n \\end{bmatrix} \\quad \\\\boldsymbol{u} \\cdot \\boldsymbol{v} &#x3D; \\sum_{i&#x3D;1}^{n}{u_iv_i} &#x3D; \\lvert \\boldsymbol{u} \\rvert \\lvert \\boldsymbol{v} \\rvert cos\\theta$$假如我们用3维向量来表示用户对喜剧片、言情片和动作片这三类电影的偏好，我们用1到5的数字来表示喜欢的程度，其中5表示非常喜欢，4表示比较喜欢，3表示无感，2表示比较反感，1表示特别反感。那么，下面的向量表示用户非常喜欢喜剧片，特别反感言情片，对动作片不喜欢也不反感。$$\\boldsymbol{u} &#x3D; \\begin{pmatrix} 5 \\ 1 \\ 3 \\end{pmatrix}$$现在有两部电影上映了，一部属于言情喜剧片，一部属于喜剧动作片，我们把两部电影也通过3维向量的方式进行表示，如下所示。$$\\boldsymbol{m_1} &#x3D; \\begin{pmatrix} 4 \\ 5 \\ 1 \\end{pmatrix}, \\quad \\boldsymbol{m_2} &#x3D; \\begin{pmatrix} 5 \\ 1 \\ 5 \\end{pmatrix}$$如果现在我们需要向刚才的用户推荐一部电影，我们应该给他推荐哪一部呢？我们可以将代表用户的向量$\\boldsymbol{u}$和代表电影的向量$\\boldsymbol{m_1}$和$\\boldsymbol{m_2}$分别进行点积运算，再除以向量的模长，得到向量夹角的余弦值，余弦值越接近1，说明向量的夹角越接近0度，也就是两个向量的相似度越高。很显然，我们应该向用户推荐跟他观影喜好相似度更高的电影。$$cos\\theta_1 &#x3D; \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{m1}}{|\\boldsymbol{u}||\\boldsymbol{m1}|} \\approx \\frac{4 \\times 5 + 5 \\times 1 + 3 \\times 1}{5.92 \\times 6.48} \\approx 0.73 \\cos\\theta_2 &#x3D; \\frac{\\boldsymbol{u} \\cdot \\boldsymbol{m2}}{|\\boldsymbol{u}||\\boldsymbol{m2}|} \\approx \\frac{5 \\times 5 + 1 \\times 1 + 3 \\times 5}{5.92 \\times 7.14} \\approx 0.97$$大家可能会说，向量$\\boldsymbol{m_2}$代表的电影肉眼可见跟用户是更加匹配的。的确，对于一个三维向量我们凭借直觉也能够给出正确的答案，但是对于一个$n$维向量，当$n$的值非常大时，你还有信心凭借肉眼的观察和本能的直觉给出准确的答案吗？向量的点积可以通过dot函数来计算，而向量的模长可以通过 NumPy 的linalg模块中的norm函数来计算，代码如下所示。 12345u = np.array([5, 1, 3])m1 = np.array([4, 5, 1])m2 = np.array([5, 1, 5])print(np.dot(u, m1) / (np.linalg.norm(u) * np.linalg.norm(m1))) # 0.7302967433402214print(np.dot(u, m2) / (np.linalg.norm(u) * np.linalg.norm(m2))) # 0.9704311900788593 向量的叉积在二维空间，两个向量的叉积是这样定义的：$$\\boldsymbol{A} &#x3D; \\begin{pmatrix} a_{1} \\ a_{2} \\end{pmatrix}, \\quad \\boldsymbol{B} &#x3D; \\begin{pmatrix} b_{1} \\ b_{2} \\end{pmatrix} \\\\boldsymbol{A} \\times \\boldsymbol{B} &#x3D; \\begin{vmatrix} a_{1} \\quad a_{2} \\ b_{1} \\quad b_{2} \\end{vmatrix} &#x3D; a_{1}b_{2} - a_{2}b_{1}$$对于三维空间，两个向量的叉积结果是一个向量，如下所示：$$\\boldsymbol{A} &#x3D; \\begin{pmatrix} a_{1} \\ a_{2} \\ a_{3} \\end{pmatrix}, \\quad \\boldsymbol{B} &#x3D; \\begin{pmatrix} b_{1} \\ b_{2} \\ b_{3} \\end{pmatrix} \\\\boldsymbol{A} \\times \\boldsymbol{B} &#x3D; \\begin{vmatrix} \\boldsymbol{\\hat{i}} \\quad \\boldsymbol{\\hat{j}} \\quad \\boldsymbol{\\hat{k}} \\ a_{1} \\quad a_{2} \\quad a_{3} \\ b_{1} \\quad b_{2} \\quad b_{3} \\end{vmatrix} &#x3D; \\langle \\boldsymbol{\\hat{i}}\\begin{vmatrix} a_{2} \\quad a_{3} \\ b_{2} \\quad b_{3} \\end{vmatrix}, -\\boldsymbol{\\hat{j}}\\begin{vmatrix} a_{1} \\quad a_{3} \\ b_{1} \\quad b_{3} \\end{vmatrix}, \\boldsymbol{\\hat{k}}\\begin{vmatrix} a_{1} \\quad a_{2} \\ b_{1} \\quad b_{2} \\end{vmatrix} \\rangle$$因为叉积的结果是向量，所以$\\boldsymbol{A} \\times \\boldsymbol{B}$和$\\boldsymbol{B} \\times \\boldsymbol{A}$的结果并不相同，事实上：$$\\boldsymbol{A} \\times \\boldsymbol{B} &#x3D; -(\\boldsymbol{B} \\times \\boldsymbol{A})$$NumPy 中可以通过cross函数来计算向量的叉积，代码如下所示。 12print(np.cross(u, m1)) # [-14 7 21]print(np.cross(m1, u)) # [ 14 -7 -21] 行列式行列式（determinant）通常记作$det(\\boldsymbol{A})$或$|\\boldsymbol{A}|$，其中$\\boldsymbol{A}$是一个$n$阶方阵。行列式可以看做是有向面积或体积的概念在一般欧几里得空间的推广，或者说行列式描述的是一个线性变换对“体积”所造成的影响。行列式的概念最早出现在解线性方程组的过程中，十七世纪晚期，关孝和（日本江户时代的数学家）与莱布尼茨的著作中已经使用行列式来确定线性方程组解的个数以及形式；十八世纪开始，行列式开始作为独立的数学概念被研究；十九世纪以后，行列式理论进一步得到发展和完善。 行列式的性质行列式是由向量引出的，所以行列式解释的其实是向量的性质。 性质1：如果$det(\\boldsymbol{A})$中某行（或某列）的元素全部为0，那么$det(\\boldsymbol{A}) &#x3D; 0$。 性质2：如果$det(\\boldsymbol{A})$中某行（或某列）有公共因子$k$，则可以提出$k$，得到行列式$det(\\boldsymbol{A^{‘}})$，且$det(\\boldsymbol{A}) &#x3D; k \\cdot det(\\boldsymbol{A^{‘}})$。$$det(\\boldsymbol{A})&#x3D;{\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\{\\color {blue}k}a_{i1}&amp;{\\color {blue}k}a_{i2}&amp;\\dots &amp;{\\color {blue}k}a_{in}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}&#x3D;{\\color {blue}k}{\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{i1}&amp;a_{i2}&amp;\\dots &amp;a_{in}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}&#x3D;{\\color {blue}k} \\cdot det(\\boldsymbol{A^{‘}})$$ 性质3：如果$det(\\boldsymbol{A})$中某行（或某列）的每个元素是两数之和，则此行列式可拆分为两个行列式相加，如下所示。$${\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\{\\color {blue}a_{i1}}+{\\color {OliveGreen}b_{i1}}&amp;{\\color {blue}a_{i2}}+{\\color {OliveGreen}b_{i2}}&amp;\\dots &amp;{\\color {blue}a_{in}}+{\\color {OliveGreen}b_{in}}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}&#x3D;{\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\{\\color {blue}a_{i1}}&amp;{\\color {blue}a_{i2}}&amp;\\dots &amp;{\\color {blue}a_{in}}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}+{\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\{\\color {OliveGreen}b_{i1}}&amp;{\\color {OliveGreen}b_{i2}}&amp;\\dots &amp;{\\color {OliveGreen}b_{in}}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}$$性质4：如果$det(\\boldsymbol{A})$中两行（或两列）元素对应成比例，那么$det(\\boldsymbol{A}) &#x3D; 0$。 性质5：如果$det(\\boldsymbol{A})$中两行（或两列）互换得到$det(\\boldsymbol{A^{‘}})$，那么$det(\\boldsymbol{A}) &#x3D; -det(\\boldsymbol{A^{‘}})$。 性质6：将$det(\\boldsymbol{A})$中某行（或某列）的$k$倍加进另一行（或另一列）里，行列式的值不变，如下所示。$${\\begin{vmatrix}\\vdots &amp;\\vdots &amp;\\vdots &amp;\\vdots \\a_{i1}&amp;a_{i2}&amp;\\dots &amp;a_{in}\\a_{j1}&amp;a_{j2}&amp;\\dots &amp;a_{jn}\\\\vdots &amp;\\vdots &amp;\\vdots &amp;\\vdots \\\\end{vmatrix}}&#x3D;{\\begin{vmatrix}\\vdots &amp;\\vdots &amp;\\vdots &amp;\\vdots \\a_{i1}&amp;a_{i2}&amp;\\dots &amp;a_{in}\\a_{j1}{\\color {blue}+ka_{i1}}&amp;a_{j2}{\\color {blue}+ka_{i2}}&amp;\\dots &amp;a_{jn}{\\color {blue}+ka_{in}}\\\\vdots &amp;\\vdots &amp;\\vdots &amp;\\vdots \\\\end{vmatrix}}$$性质7：将行列式的行列互换，行列式的值不变，如下所示。$${\\begin{vmatrix}a_{11}&amp;a_{12}&amp;\\dots &amp;a_{1n}\\a_{21}&amp;a_{22}&amp;\\dots &amp;a_{2n}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{n1}&amp;a_{n2}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}&#x3D;{\\begin{vmatrix}a_{11}&amp;a_{21}&amp;\\dots &amp;a_{n1}\\a_{12}&amp;a_{22}&amp;\\dots &amp;a_{n2}\\\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots \\a_{1n}&amp;a_{2n}&amp;\\dots &amp;a_{nn}\\end{vmatrix}}$$性质8：方块矩阵$\\boldsymbol{A}$和$\\boldsymbol{B}$的乘积的行列式等于其行列式的乘积，即$det(\\boldsymbol{A}\\boldsymbol{B}) &#x3D; det(\\boldsymbol{A})det(\\boldsymbol{B})$。特别的，若将矩阵中的每一行都乘以常数$r$，那么行列式的值将是原来的$r^{n}$倍，即$det(r\\boldsymbol{A}) &#x3D; det(r\\boldsymbol{I_{n}} \\cdot \\boldsymbol{A}) &#x3D; r^{n}det(\\boldsymbol{A})$，其中$\\boldsymbol{I_{n}}$是$n$阶单位矩阵。 性质9：若$\\boldsymbol{A}$是可逆矩阵，那么$det(\\boldsymbol{A}^{-1}) &#x3D; (det(\\boldsymbol{A}))^{-1}$。 行列式的计算$n$阶行列式的计算公式如下所示：$$det(\\boldsymbol{A})&#x3D;\\sum_{n!} \\pm {a_{1\\alpha}a_{2\\beta} \\cdots a_{n\\omega}}$$ 对于二阶行列式，上面的公式相当于：$$\\begin{vmatrix} a_{11} \\quad a_{12} \\ a_{21} \\quad a_{22} \\end{vmatrix} &#x3D; a_{11}a_{22} - a_{12}a_{21}$$对于三阶行列式，上面的计算公式相当于：$$\\begin{vmatrix} a_{11} \\quad a_{12} \\quad a_{13} \\ a_{21} \\quad a_{22} \\quad a_{23} \\ a_{31} \\quad a_{32} \\quad a_{33} \\end{vmatrix} &#x3D; a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}$$高阶行列式可以用代数余子式（cofactor）展开成多个低阶行列式，如下所示：$$det(\\boldsymbol{A})&#x3D;a_{11}C_{11}+a_{12}C_{12}+ \\cdots +a_{1n}C_{1n} &#x3D; \\sum_{i&#x3D;1}^{n}{a_{1i}C_{1i}}$$其中，$C_{11}$是原行列式去掉$a_{11}$所在行和列之后剩余的部分构成的行列式，以此类推。 矩阵矩阵（matrix）是由一系列元素排成的矩形阵列，矩阵里的元素可以是数字、符号或数学公式。矩阵可以进行加法、减法、数乘、转置、矩阵乘法等运算，如下图所示。 值得一提的是矩阵乘法运算，该运算仅当第一个矩阵$\\boldsymbol{A}$的列数和另一个矩阵$\\boldsymbol{B}$的行数相等时才能定义。如果$\\boldsymbol{A}$是一个$m \\times n$的矩阵，$\\boldsymbol{B}$是一个$n \\times k$矩阵，它们的乘积是一个$m \\times k$的矩阵，其中元素的计算公式如下所示：$$ [\\mathbf{AB}]{i,j} &#x3D; A{i,1}B_{1,j} + A_{i,2}B_{2,j} + \\cdots + A_{i,n}B_{n,j} &#x3D; \\sum_{r&#x3D;1}^n A_{i,r}B_{r,j}$$ 例如：$$\\begin{bmatrix} 1 &amp; 0 &amp; 2 \\ -1 &amp; 3 &amp; 1 \\ \\end{bmatrix}\\times \\begin{bmatrix} 3 &amp; 1 \\ 2 &amp; 1 \\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} (1 \\times 3 + 0 \\times 2 + 2 \\times 1) &amp; (1 \\times 1 + 0 \\times 1 + 2 \\times 0) \\ (-1 \\times 3 + 3 \\times 2 + 1 \\times 1) &amp; (-1 \\times 1 + 3 \\times 1 + 1 \\times 0) \\ \\end{bmatrix} \\begin{bmatrix} 5 &amp; 1 \\ 4 &amp; 2 \\ \\end{bmatrix}$$矩阵的乘法满足结合律和对矩阵加法的分配律： 结合律： $(\\boldsymbol{AB})\\boldsymbol{C} &#x3D; \\boldsymbol{A}(\\boldsymbol{BC})$。 左分配律：$(\\boldsymbol{A} + \\boldsymbol{B})\\boldsymbol{C} &#x3D; \\boldsymbol{AC} + \\boldsymbol{BC}$。 右分配律：$\\boldsymbol{C}(\\boldsymbol{A} + \\boldsymbol{B}) &#x3D; \\boldsymbol{CA} + \\boldsymbol{CB}$。 矩阵乘法不满足交换律。一般情况下，矩阵$\\boldsymbol{A}$和$\\boldsymbol{B}$的乘积$\\boldsymbol{AB}$存在，但$\\boldsymbol{BA}$不一定存在，即便$\\boldsymbol{BA}$存在，大多数时候$\\boldsymbol{AB} \\neq \\boldsymbol{BA}$。 矩阵乘法的一个基本应用是在线性方程组上。线性方程组是方程组的一种，它符合以下的形式：$$\\begin{cases} a_{1,1}x_{1} + a_{1,2}x_{2} + \\cdots + a_{1,n}x_{n}&#x3D; b_{1} \\ a_{2,1}x_{1} + a_{2,2}x_{2} + \\cdots + a_{2,n}x_{n}&#x3D; b_{2} \\ \\vdots \\quad \\quad \\quad \\vdots \\ a_{m,1}x_{1} + a_{m,2}x_{2} + \\cdots + a_{m,n}x_{n}&#x3D; b_{m} \\end{cases}$$运用矩阵的方式，可以将线性方程组写成一个向量方程：$$\\boldsymbol{Ax} &#x3D; \\boldsymbol{b}$$其中，$\\boldsymbol{A}$是由方程组里未知数的系数排成的$m \\times n$矩阵，$\\boldsymbol{x}$是含有$n$个元素的行向量，$\\boldsymbol{b}$是含有$m$个元素的行向量。 矩阵是线性变换（保持向量加法和标量乘法的函数）的便利表达法。矩阵乘法的本质在联系到线性变换的时候最能体现，因为矩阵乘法和线性变换的合成有以下的联系，即每个$m \\times n$的矩阵$\\boldsymbol{A}$都代表了一个从$\\boldsymbol{R}^{n}$射到$\\boldsymbol{R}^{m}$的线性变换。如果无法理解上面这些内容，推荐大家看看B站上名为《线性代数的本质》的视频，相信这套视频会让你对线性代数有一个更好的认知。 下图是一个来自于维基百科的例子，图中展示了一些典型的二维实平面上的线性变换对平面向量（图形）造成的效果以及它们对应的二维矩阵，其中每个线性变换将蓝色图形映射成绿色图形；平面的原点$(0, 0)$用黑点表示。 矩阵对象NumPy 中提供了专门用于线性代数（linear algebra）的模块和表示矩阵的类型matrix，当然我们通过二维数组也可以表示一个矩阵，官方并不推荐使用matrix类而是建议使用二维数组，而且有可能在将来的版本中会移除matrix类。无论如何，利用这些已经封装好的类和函数，我们可以轻松愉快的实现很多对矩阵的操作。 我们可以通过下面的代码来创建矩阵（matrix）对象。 代码： 12m1 = np.matrix(&#x27;1 2 3; 4 5 6&#x27;)m1 说明：matrix构造器可以传入类数组对象也可以传入字符串来构造矩阵对象。 输出： 12matrix([[1, 2, 3], [4, 5, 6]]) 代码： 12m2 = np.asmatrix(np.array([[1, 1], [2, 2], [3, 3]]))m2 说明：asmatrix函数也可以用mat函数代替，这两个函数其实是同一个函数。 输出： 123matrix([[1, 1], [2, 2], [3, 3]]) 代码： 1m1 * m2 输出： 12matrix([[14, 14], [32, 32]]) 说明：注意matrix对象和ndarray对象乘法运算的差别，matrix对象的*运算是矩阵乘法运算。如果两个二维数组要做矩阵乘法运算，应该使用@运算符或matmul函数，而不是*运算符。 矩阵对象的属性如下表所示。 属性 说明 A 获取矩阵对象对应的ndarray对象 A1 获取矩阵对象对应的扁平化后的ndarray对象 I 可逆矩阵的逆矩阵 T 矩阵的转置 H 矩阵的共轭转置 shape 矩阵的形状 size 矩阵元素的个数 矩阵对象的方法跟之前讲过的ndarray数组对象的方法基本差不多，此处不再进行赘述。 线性代数模块NumPy 的linalg模块中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的函数，它们跟 MATLAB 和 R 等语言所使用的是相同的行业标准线性代数库，下面的表格列出了numpy以及linalg模块中一些常用的线性代数相关函数。 函数 说明 diag 以一维数组的形式返回方阵的对角线元素或将一维数组转换为方阵（非对角元素元素为0） matmul 矩阵乘法运算 trace 计算对角线元素的和 norm 求矩阵或向量的范数 det 计算行列式的值 matrix_rank 计算矩阵的秩 eig 计算矩阵的特征值（eigenvalue）和特征向量（eigenvector） inv 计算非奇异矩阵（$n$阶方阵）的逆矩阵 pinv 计算矩阵的摩尔-彭若斯（Moore-Penrose）广义逆 qr QR分解（把矩阵分解成一个正交矩阵与一个上三角矩阵的积） svd 计算奇异值分解（singular value decomposition） solve 解线性方程组$\\boldsymbol{Ax}&#x3D;\\boldsymbol{b}$，其中$\\boldsymbol{A}$是一个方阵 lstsq 计算$\\boldsymbol{Ax}&#x3D;\\boldsymbol{b}$的最小二乘解 下面我们简单尝试一下上面的函数，先试一试求逆矩阵。 代码： 123m3 = np.array([[1., 2.], [3., 4.]])m4 = np.linalg.inv(m3)m4 输出： 12array([[-2. , 1. ], [ 1.5, -0.5]]) 代码： 1np.around(m3 @ m4) 说明：around函数对数组元素进行四舍五入操作，默认小数点后面的位数为0。 输出： 12array([[1., 0.], [0., 1.]]) 说明：矩阵和它的逆矩阵做矩阵乘法会得到单位矩阵。 计算行列式的值。 代码： 12m5 = np.array([[1, 3, 5], [2, 4, 6], [4, 7, 9]])np.linalg.det(m5) 输出： 12 计算矩阵的秩。 代码： 1np.linalg.matrix_rank(m5) 输出： 13 求解线性方程组。$$\\begin{cases}x_1 + 2x_2 + x_3 &#x3D; 8 \\3x_1 + 7x_2 + 2x_3 &#x3D; 23 \\2x_1 + 2x_2 + x_3 &#x3D; 9\\end{cases}$$ 对于上面的线性方程组，我们可以用矩阵的形式来表示它，如下所示。$$\\boldsymbol{A} &#x3D; \\begin{bmatrix}1 &amp; 2 &amp; 1\\3 &amp; 7 &amp; 2\\2 &amp; 2 &amp; 1\\end{bmatrix}, \\quad\\boldsymbol{x} &#x3D; \\begin{bmatrix}x_1 \\x_2\\x_3\\end{bmatrix}, \\quad\\boldsymbol{b} &#x3D; \\begin{bmatrix}8 \\23\\9\\end{bmatrix}$$ $$\\boldsymbol{Ax} &#x3D; \\boldsymbol{b}$$ 线性方程组有唯一解的条件：系数矩阵$\\boldsymbol{A}$的秩等于增广矩阵$\\boldsymbol{Ab}$的秩，而且跟未知数的个数相同。 代码： 1234A = np.array([[1, 2, 1], [3, 7, 2], [2, 2, 1]])b = np.array([8, 23, 9]).reshape(-1, 1)print(np.linalg.matrix_rank(A))print(np.linalg.matrix_rank(np.hstack((A, b)))) 说明：使用数组对象的reshape方法调形时，如果其中一个参数为-1，那么该维度有多少个元素是通过数组元素个数（size属性）和其他维度的元素个数自动计算出来的。 输出： 1233 代码： 1np.linalg.solve(A, b) 输出： 123array([[1.], [2.], [3.]]) 说明：上面的结果表示，线性方程组的解为：$x_1 &#x3D; 1, x_2 &#x3D; 2, x_3 &#x3D; 3$。 下面是另一种求解线性方程组的方法，大家可以停下来思考下为什么。$$\\boldsymbol{x} &#x3D; \\boldsymbol{A}^{-1} \\cdot \\boldsymbol{b}$$代码： 1np.linalg.inv(A) @ b 输出： 123array([[1.], [2.], [3.]]) 多项式除了数组，NumPy 中还封装了用于多项式（polynomial）运算的数据类型。多项式是变量的整数次幂与系数的乘积之和，形如：$$f(x)&#x3D;a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1x^{1} + a_0x^{0}$$在 NumPy 1.4版本之前，我们可以用poly1d类型来表示多项式，目前它仍然可用，但是官方提供了新的模块numpy.polynomial，它除了支持基本的幂级数多项式外，还可以支持切比雪夫多项式、拉盖尔多项式等。 创建多项式对象创建poly1d对象，例如：$\\small{f(x)&#x3D;3x^{2}+2x+1}$。 代码： 1234p1 = np.poly1d([3, 2, 1])p2 = np.poly1d([1, 2, 3])print(p1)print(p2) 输出： 1234 23 x + 2 x + 1 21 x + 2 x + 3 多项式的操作获取多项式的系数 代码： 12print(p1.coefficients)print(p2.coeffs) 输出： 12[3 2 1][1 2 3] 两个多项式的四则运算 代码： 12print(p1 + p2)print(p1 * p2) 输出： 1234 24 x + 4 x + 4 4 3 23 x + 8 x + 14 x + 8 x + 3 带入$\\small{x}$求多项式的值 代码： 12print(p1(3))print(p2(3)) 输出： 123418 多项式求导和不定积分 代码： 12print(p1.deriv())print(p1.integ()) 输出： 12346 x + 2 3 21 x + 1 x + 1 x 求多项式的根 例如有多项式$\\small{f(x)&#x3D;x^2+3x+2}$，多项式的根即一元二次方程$\\small{x^2+3x+2&#x3D;0}$的解。 代码： 12p3 = np.poly1d([1, 3, 2])print(p3.roots) 输出： 1[-2. -1.] 如果使用numpy.polynomial模块的Polynomial类来表示多项式对象，那么对应的操作如下所示。 代码： 123456789from numpy.polynomial import Polynomialp3 = Polynomial((2, 3, 1))print(p3) # 输出多项式print(p3(3)) # 令x=3，计算多项式的值print(p3.roots()) # 计算多项式的根print(p3.degree()) # 获得多项式的次数print(p3.deriv()) # 求导print(p3.integ()) # 求不定积分 输出： 1234562.0 + 3.0·x + 1.0·x²20.0[-2. -1.]23.0 + 2.0·x0.0 + 2.0·x + 1.5·x² + 0.33333333·x³ 最小二乘解Polynomial类还有一个名为fit的类方法，它可以给多项式求最小二乘解。所谓最小二乘解（least-squares solution），是用最小二乘法通过最小化误差的平方和来寻找数据的最佳匹配函数的系数。假设多项式为$\\small{f(x)&#x3D;ax+b}$，最小二乘解就是让下面的残差平方和$\\small{RSS}$达到最小的$\\small{a}$和$\\small{b}$。$$RSS &#x3D; \\sum_{i&#x3D;0}^{k}(f(x_i) - y_i)^{2}$$例如，我们想利用收集到的月收入和网购支出的历史数据来建立一个预测模型，以达到通过某人的月收入预测他网购支出金额的目标，下面是我们收集到的收入和网购支出的数据，保存在两个数组中。 12345678910x = np.array([ 25000, 15850, 15500, 20500, 22000, 20010, 26050, 12500, 18500, 27300, 15000, 8300, 23320, 5250, 5800, 9100, 4800, 16000, 28500, 32000, 31300, 10800, 6750, 6020, 13300, 30020, 3200, 17300, 8835, 3500])y = np.array([ 2599, 1400, 1120, 2560, 1900, 1200, 2320, 800, 1650, 2200, 980, 580, 1885, 600, 400, 800, 420, 1380, 1980, 3999, 3800, 725, 520, 420, 1200, 4020, 350, 1500, 560, 500]) 我们可以先绘制散点图来了解两组数据是否具有正相关或负相关关系。正相关意味着数组x中较大的值对应到数组y中也是较大的值，而负相关则意味着数组x中较大的值对应到数组y中较小的值。 12345import matplotlib.pyplot as pltplt.figure(dpi=120)plt.scatter(x, y, color=&#x27;blue&#x27;)plt.show() 输出： 如果需要定量的研究两组数据的相关性，我们可以计算协方差或相关系数，对应的 NumPy 函数分别是cov和corrcoef。 代码： 1np.corrcoef(x, y) 输出： 12array([[1. , 0.92275889], [0.92275889, 1. ]]) 说明：相关系数是一个-1到1之间的值，越靠近1 说明正相关性越强，越靠近-1说明负相关性越强，靠近0则说明两组数据没有明显的相关性。上面月收入和网购支出之间的相关系数是0.92275889，说明二者是强正相关关系。 通过上面的操作，我们确定了收入和网购支出之前存在强正相关关系，于是我们用这些数据来创建一个回归模型，找出一条能够很好的拟合这些数据点的直线。这里，我们就可以用到上面提到的fit方法，具体的代码如下所示。 代码： 123from numpy.polynomial import PolynomialPolynomial.fit(x, y, deg=1).convert().coef 说明：deg=1说明回归模型最高次项就是1次项，回归模型形如$\\small{y&#x3D;ax+b}$；如果要生一个类似于$\\small{y&#x3D;ax^2+bx+c}$的模型，就需要设置deg=2，以此类推。 输出： 1array([-2.94883437e+02, 1.10333716e-01]) 根据上面输出的结果，我们的回归方程应该是$\\small{y&#x3D;0.110333716x-294.883437}$。我们将这个回归方程绘制到刚才的散点图上，红色的点是我们的预测值，蓝色的点是历史数据，也就是真实值。 代码： 123456import matplotlib.pyplot as pltplt.scatter(x, y, color=&#x27;blue&#x27;)plt.scatter(x, 0.110333716 * x - 294.883437, color=&#x27;red&#x27;)plt.plot(x, 0.110333716 * x - 294.883437, color=&#x27;darkcyan&#x27;)plt.show() 输出： 如果不使用Polynomial类型的fit方法，我们也可以通过 NumPy 提供的polyfit函数来完成同样的操作，有兴趣的读者可以自行研究。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/70.NumPy的应用-3","date":"2024-12-12T08:38:02.401Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/70.NumPy的应用-3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/70.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-3/","excerpt":"","text":"NumPy的应用-3数组的运算使用 NumPy 最为方便的是当需要对数组元素进行运算时，不用编写循环代码遍历每个元素，所有的运算都会自动的矢量化。简单的说就是，NumPy 中的数学运算和数学函数会自动作用于数组中的每个成员。 数组跟标量的运算NumPy 的数组可以跟一个数值进行加、减、乘、除、求模、求幂等运算，对应的运算会作用到数组的每一个元素上，如下所示。 代码： 123array1 = np.arange(1, 10)print(array1 + 10)print(array1 * 10) 输出： 12[11 12 13 14 15 16 17 18 19][10 20 30 40 50 60 70 80 90] 除了上述的运算，关系运算也是没有问题的，之前讲布尔索引的时候已经遇到过了。 代码： 12print(array1 &gt; 5)print(array1 % 2 == 0) 输出： 12[False False False False False True True True True][False True False True False True False True False] 数组跟数组的运算NumPy 的数组跟数组也可以执行算术运算和关系运算，运算会作用于两个数组对应的元素上，这就要求两个数组的形状（shape属性）要相同，如下所示。 代码： 1234array2 = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])print(array1 + array2)print(array1 * array2)print(array1 ** array2) 输出： 123[ 2 3 4 6 7 8 10 11 12][ 1 2 3 8 10 12 21 24 27][ 1 2 3 16 25 36 343 512 729] 代码： 12print(array1 &gt; array2)print(array1 % array2 == 0) 输出： 12[False True True True True True True True True][ True True True True False True False False True] 通用一元函数NumPy 中通用一元函数的参数是一个数组对象，函数会对数组进行元素级的处理，例如：sqrt函数会对数组中的每个元素计算平方根，而log2函数会对数组中的每个元素计算以2为底的对数，代码如下所示。 代码： 12print(np.sqrt(array1))print(np.log2(array1)) 输出： 1234[1. 1.41421356 1.73205081 2. 2.23606798 2.44948974 2.64575131 2.82842712 3. ][0. 1. 1.5849625 2. 2.32192809 2.5849625 2.80735492 3. 3.169925 ] 表1：通用一元函数 函数 说明 abs &#x2F; fabs 求绝对值的函数 sqrt 求平方根的函数，相当于array ** 0.5 square 求平方的函数，相当于array ** 2 exp 计算$e^x$的函数 log &#x2F; log10 &#x2F; log2 对数函数（e为底 &#x2F; 10为底 &#x2F; 2为底） sign 符号函数（1 - 正数；0 - 零；-1 - 负数） ceil &#x2F; floor 上取整 &#x2F; 下取整 isnan 返回布尔数组，NaN对应True，非NaN对应False isfinite &#x2F; isinf 判断数值是否为无穷大的函数 cos &#x2F; cosh &#x2F; sin 三角函数 sinh &#x2F; tan &#x2F; tanh 三角函数 arccos &#x2F; arccosh &#x2F; arcsin 反三角函数 arcsinh &#x2F; arctan &#x2F; arctanh 反三角函数 rint &#x2F; round 四舍五入函数 通用二元函数NumPy 中通用二元函数的参数是两个数组对象，函数会对两个数组中的对应元素进行运算，例如：maximum函数会对两个数组中对应的元素找最大值，而power函数会对两个数组中对应的元素进行求幂操作，代码如下所示。 代码： 1234array3 = np.array([[4, 5, 6], [7, 8, 9]])array4 = np.array([[1, 2, 3], [3, 2, 1]])print(np.maximum(array3, array4))print(np.power(array3, array4)) 输出： 1234[[4 5 6] [7 8 9]][[ 4 25 216] [343 64 9]] 表2：通用二元函数 函数 说明 add(x, y) &#x2F; substract(x, y) 加法函数 &#x2F; 减法函数 multiply(x, y) &#x2F; divide(x, y) 乘法函数 &#x2F; 除法函数 floor_divide(x, y) &#x2F; mod(x, y) 整除函数 &#x2F; 求模函数 allclose(x, y) 检查数组x和y元素是否几乎相等 power(x, y) 数组$x$的元素$x_i$和数组$y$的元素$y_i$，计算$x_i^{y_i}$ maximum(x, y) &#x2F; fmax(x, y) 两两比较元素获取最大值 &#x2F; 获取最大值（忽略NaN） minimum(x, y) &#x2F; fmin(x, y) 两两比较元素获取最小值 &#x2F; 获取最小值（忽略NaN） dot(x, y) 点积运算（数量积，通常记为$\\cdot$，用于欧几里得空间（Euclidean space）） inner(x, y) 内积运算（内积的含义要高于点积，点积相当于是内积在欧几里得空间$\\mathbb{R}^n$的特例，而内积可以推广到赋范向量空间，只要它满足平行四边形法则即可） cross(x, y) 叉积运算（向量积，通常记为$\\times$，运算结果是一个向量） outer(x, y) 外积运算（张量积，通常记为$\\bigotimes$，运算结果通常是一个矩阵） intersect1d(x, y) 计算x和y的交集，返回这些元素构成的有序数组 union1d(x, y) 计算x和y的并集，返回这些元素构成的有序数组 in1d(x, y) 返回由判断x 的元素是否在y中得到的布尔值构成的数组 setdiff1d(x, y) 计算x和y的差集，返回这些元素构成的数组 setxor1d(x, y) 计算x和y的对称差，返回这些元素构成的数组 说明：关于向量和矩阵的运算，我们在下一个章节加以说明。 广播机制上面数组运算的例子中，两个数组的形状（shape属性）是完全相同的，我们再来研究一下，两个形状不同的数组是否可以直接做二元运算或使用通用二元函数进行运算，请看下面的例子。 代码： 123array5 = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]])array6 = np.array([1, 2, 3])array5 + array6 输出： 1234array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]) 代码： 12array7 = np.array([[1], [2], [3], [4]])array5 + array7 输出： 1234array([[1, 1, 1], [3, 3, 3], [5, 5, 5], [7, 7, 7]]) 通过上面的例子，我们发现形状不同的数组仍然有机会进行二元运算，但这不代表任意形状的数组都可以进行二元运算。简单的说，只有两个数组后缘维度相同或者后缘维度不同但其中一个数组后缘维度为1时，广播机制才会被触发。通过广播机制，NumPy 将两个原本形状不相同的数组变成形状相同，才能进行二元运算。所谓后缘维度，指的是数组形状（shape属性）从后往前看对应的部分，我们举例说明。 上图中，一个数组的形状是(4, 3)，另一个数组的形状是(3, )，从后往前看对应的部分都是3，属于后缘维度相同，可以应用广播机制，第二个数组会沿着缺失元素那个轴的方向去广播自己，最终让两个数组形状达成一致。 上图中，一个数组的形状是(3, 4, 2)，另一个数组的形状是(4, 2)，从后往前看对应的部分都是(4, 2)，属于后缘维度相同，可以应用广播机制，第二个数组会沿着缺失元素那个轴的方向去广播自己，最终让两个数组形状达成一致。 上图中，一个数组的形状是(4, 3)，另一个数组的形状是(4, 1)，这是后缘维度不相同的情况，但是第二个数组跟第一个数组不同的地方为1，第二个数组可以沿着为1 的那个轴广播自己，最终让两个数组形状达成一致。 思考：一个3行1列的二维数组和一个1行3列的二维数组能够执行加法运算吗？ 其他常用函数除了上面讲到的函数外，NumPy 中还提供了很多用于处理数组的函数，ndarray对象的很多方法也可以通过调用函数来实现，下表给出了一些常用的函数。 表3：NumPy其他常用函数 函数 说明 unique 去除数组重复元素，返回唯一元素构成的有序数组 copy 返回拷贝数组得到的数组 sort 返回数组元素排序后的拷贝 split &#x2F; hsplit &#x2F; vsplit 将数组拆成若干个子数组 stack &#x2F; hstack &#x2F; vstack 将多个数组堆叠成新数组 concatenate 沿着指定的轴连接多个数组构成新数组 append &#x2F; insert 向数组末尾追加元素 &#x2F; 在数组指定位置插入元素 argwhere 找出数组中非0元素的位置 extract &#x2F; select &#x2F; where 按照指定的条件从数组中抽取或处理数组元素 flip 沿指定的轴翻转数组中的元素 fromregex 通过读取文件和正则表达式解析获取数据创建数组对象 repeat &#x2F; tile 通过对元素的重复来创建新数组 roll 沿指定轴对数组元素进行移位 resize 重新调整数组的大小 place &#x2F; put 将数组中满足条件的元素&#x2F;指定的元素替换为指定的值 partition 用选定的元素对数组进行一次划分并返回划分后的数组 去重（重复元素只保留一项）。 代码： 1np.unique(array5) 输出： 1array([0, 1, 2, 3]) 堆叠和拼接。 代码： 123array8 = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])array9 = np.array([[4, 4, 4], [5, 5, 5], [6, 6, 6]])np.hstack((array8, array9)) 输出： 123array([[1, 1, 1, 4, 4, 4], [2, 2, 2, 5, 5, 5], [3, 3, 3, 6, 6, 6]]) 代码： 1np.vstack((array8, array9)) 输出： 123456array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5], [6, 6, 6]]) 代码： 1np.concatenate((array8, array9)) 输出： 123456array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5], [6, 6, 6]]) 代码： 1np.concatenate((array8, array9), axis=1) 输出： 123array([[1, 1, 1, 4, 4, 4], [2, 2, 2, 5, 5, 5], [3, 3, 3, 6, 6, 6]]) 追加和插入元素。 代码： 1np.append(array1, [10, 100]) 输出： 1array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100]) 代码： 1np.insert(array1, 1, [98, 99, 100]) 输出： 1array([ 1, 98, 99, 100, 2, 3, 4, 5, 6, 7, 8, 9]) 抽取和处理元素。 代码： 1np.extract(array1 % 2 != 0, array1) 输出： 1array([1, 3, 5, 7, 9]) 说明：上面extract函数的操作相当于我们之前讲的布尔索引。 代码： 1np.select([array1 &lt;= 3, array1 &gt;= 7], [array1 * 10, array1 ** 2]) 输出： 1array([10, 20, 30, 0, 0, 0, 49, 64, 81]) 说明：上面select函数的第一个参数设置了两个条件，满足第一个条件的元素执行了乘以10的操作，满足第二个条件的元素执行了求平方的操作，两个条件都不能满足的数组元素会被处理为0。 代码： 1np.where(array1 &lt;= 5, array1 * 10, array1 ** 2) 输出： 1array([10, 20, 30, 40, 50, 36, 49, 64, 81]) 说明：上面where函数的第一个参数给出了条件，满足条件的元素执行了乘以10的操作，不能满足条件的元素执行了求平方的操作。 重复数组元素创建新数组。 代码： 1np.repeat(array1, 3) 输出： 1array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9]) 代码： 1np.tile(array1, 2) 输出： 1array([1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 调整数组大小。 代码： 1np.resize(array1, (5, 3)) 输出： 12345array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [1, 2, 3], [4, 5, 6]]) 提示：array1原本是一个有9个元素的一维数组，通过resize函数调整成为5行3列共15个元素的二维数组，缺少的元素通过复用原数组中的元素来补充。 代码： 1np.resize(array5, (2, 4)) 输出： 12array([[0, 0, 0, 1], [1, 1, 2, 2]]) 替换数组元素。 代码： 12np.put(array1, [0, 1, -1, 3, 5], [100, 200])array1 输出： 1array([100, 200, 3, 200, 5, 100, 7, 8, 100]) 说明：上面put函的第二个参数给出了要被替换的元素的索引，但是用来作为替换值的元素只有100和200，所以这两个值会被循环使用，因此索引为0、1、-1、3、5的元素被依次替换成了100、200、100、200、100。 代码： 12np.place(array1, array1 &gt; 5, [1, 2, 3])array1 输出： 1array([1, 2, 3, 3, 5, 1, 2, 3, 1]) 注意：put函数和place函数都没有返回新的数组对象，而是在原来的数组上直接进行替换。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/69.NumPy的应用-2","date":"2024-12-12T08:38:02.398Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/69.NumPy的应用-2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/69.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-2/","excerpt":"","text":"NumPy的应用-2数组对象的方法获取描述统计信息描述统计信息主要包括数据的集中趋势、离散程度和频数分析等，其中集中趋势主要看均值和中位数，离散程度可以看极值、方差、标准差等，详细的内容大家可以阅读《统计思维系列课程01：解读数据》。 12array1 = np.random.randint(1, 100, 10)array1 输出： 1array([46, 51, 15, 42, 53, 71, 20, 62, 6, 94]) 计算总和、均值和中位数。 代码： 123456print(array1.sum())print(np.sum(array1))print(array1.mean())print(np.mean(array1))print(np.median(array1))print(np.quantile(array1, 0.5)) 说明：上面代码中的mean、median和quantile分别是 NumPy 中计算算术平均值、中位数和分位数的函数，其中quantitle函数的第二个参数设置为0.5表示计算50%分位数，也就是中位数。 输出： 12345646046046.046.048.548.5 极值、全距和四分位距离。 代码： 12345678print(array1.max())print(np.amax(array1))print(array1.min())print(np.amin(array1))print(array1.ptp())print(np.ptp(array1))q1, q3 = np.quantile(array1, [0.25, 0.75])print(q3 - q1) 输出： 1234567949466888834.25 方差、标准差和变异系数。 代码： 12345print(array1.var())print(np.var(array1))print(array1.std())print(np.std(array1))print(array1.std() / array1.mean()) 输出： 12345651.2651.225.5186206523785325.518620652378530.5547526228777941 绘制箱线图。 箱线图又称为盒须图，是显示一组数据分散情况的统计图，因形状如箱子而得名。 它主要用于反映原始数据分布的特征，还可以进行多组数据分布特征的比较。 代码： 123plt.boxplot(array1, showmeans=True)plt.ylim([-20, 120])plt.show() 输出： 值得注意的是，对于二维或更高维的数组，在获取描述统计信息时，可以通过名为axis的参数指定均值、方差等运算是沿着哪一个轴来执行，axis参数不同，执行的结果可能是大相径庭的，如下所示。 代码： 12array2 = np.random.randint(60, 101, (5, 3))array2 输出： 12345array([[72, 64, 73], [61, 73, 61], [76, 85, 77], [97, 88, 90], [63, 93, 82]]) 代码： 1array2.mean() 输出： 177.0 代码： 1array2.mean(axis=0) 输出： 1array([73.8, 80.6, 76.6]) 代码： 1array2.mean(axis=1) 输出： 1array([69.66666667, 65. , 79.33333333, 91.66666667, 79.33333333]) 代码： 1array2.max(axis=0) 输出： 1array([97, 93, 90]) 代码： 1array2.max(axis=1) 输出： 1array([73, 73, 85, 97, 93]) 再看看绘制箱线图，对于二维数组每一列都会产生一个统计图形，如下所示。 代码： 123plt.boxplot(array2, showmeans=True)plt.ylim([-20, 120])plt.show() 输出： 说明：箱线图中的小圆圈用来表示离群点，也就是大于$\\small{Q_3 + 1.5 \\times IQR}$或小于$\\small{Q_1 - 1.5 \\times IQR}$的值。公式中的常量1.5可以通过绘制箱线图的boxplot函数的whis参数进行修改，常用的值是1.5和3，修改为3通常是为了标识出极度离群点。 需要说明的是，NumPy 的数组对象并没有提供计算几何平均值、调和平均值、去尾平均值等的方法，如果有这方面的需求，可以使用名为 scipy 的三方库，它的stats模块中提供了这些函数。此外，该模块还提供了计算众数、变异系数、偏态、峰度的函数，代码如下所示。 代码： 123456789from scipy import statsprint(np.mean(array1)) # 算术平均值print(stats.gmean(array1)) # 几何平均值print(stats.hmean(array1)) # 调和平均值print(stats.tmean(array1, [10, 90])) # 去尾平均值print(stats.variation(array1)) # 变异系数print(stats.skew(array1)) # 偏态系数print(stats.kurtosis(array1)) # 峰度系数 输出： 123456746.036.2234954882559924.49721953082549745.00.55475262287779410.11644192634527782-0.7106251396024126 其他相关方法概述 all() &#x2F; any()方法：判断数组是否所有元素都是True &#x2F; 判断数组是否有为True的元素。 astype()方法：拷贝数组，并将数组中的元素转换为指定的类型。 reshape()方法：调整数组对象的形状。 dump()方法：保存数组到二进制文件中，可以通过 NumPy 中的load()函数从保存的文件中加载数据创建数组。 代码： 123array.dump(&#x27;array1-data&#x27;)array3 = np.load(&#x27;array1-data&#x27;, allow_pickle=True)array3 输出： 1array([46, 51, 15, 42, 53, 71, 20, 62, 6, 94]) tofile()方法：将数组对象写入文件中。 1array1.tofile(&#x27;res/array.txt&#x27;, sep=&#x27;,&#x27;) fill()方法：向数组中填充指定的元素。 flatten()方法：将多维数组扁平化为一维数组。 代码： 1array2.flatten() 输出： 1array([1, 2, 3, 4, 5, 6, 7, 8, 9]) nonzero()方法：返回非0元素的索引。 round()方法：对数组中的元素做四舍五入操作。 sort()方法：对数组进行就地排序。 代码： 12array1.sort()array1 输出： 1array([ 6, 15, 20, 42, 46, 51, 53, 62, 71, 94]) swapaxes()和transpose()方法：交换数组指定的轴和转置。 代码： 1array2.swapaxes(0, 1) 输出： 123array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]) 代码： 1array2.transpose() 输出： 123array([[1, 4, 7], [2, 5, 8], [3, 6, 9]]) tolist()方法：将数组转成 Python 中的list。 代码： 12print(array2.tolist())print(type(array2.tolist())) 输出： 12[[1, 2, 3], [4, 5, 6], [7, 8, 9]]&lt;class &#x27;list&#x27;&gt;","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/68.NumPy的应用-1","date":"2024-12-12T08:38:02.396Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/68.NumPy的应用-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/68.NumPy%E7%9A%84%E5%BA%94%E7%94%A8-1/","excerpt":"","text":"NumPy的应用-1Numpy 是一个开源的 Python 科学计算库，用于快速处理任意维度的数组。Numpy 支持常见的数组和矩阵操作，对于同样的数值计算任务，使用 NumPy 不仅代码要简洁的多，而且 NumPy 在性能上也远远优于原生 Python，至少是一到两个数量级的差距，而且数据量越大，NumPy 的优势就越明显。 NumPy 最为核心的数据类型是ndarray，使用ndarray可以处理一维、二维和多维数组，该对象相当于是一个快速而灵活的大数据容器。NumPy 底层代码使用 C 语言编写，解决了 GIL 的限制，ndarray在存取数据的时候，数据与数据的地址都是连续的，这确保了可以进行高效率的批量操作，性能上远远优于 Python 中的list；另一方面ndarray对象提供了更多的方法来处理数据，尤其获取数据统计特征的方法，这些方法也是 Python 原生的list没有的。 准备工作 启动 JupyterLab 1jupyter lab 提示：在启动 JupyterLab 之前，建议先安装好数据分析相关依赖项，包括之前提到的三大神器以及相关依赖项。如果使用 Anaconda，则无需单独安装，可以通过 Anaconda 的 Navigator 来启动。 导入 123import numpy as npimport pandas as pdimport matplotlib.pyplot as plt 说明：如果已经启动了 JupyterLab 但尚未安装相关依赖库，例如尚未安装numpy，可以在单元格中输入%pip install numpy并运行该单元格来安装 NumPy。当然，我们也可以在单元格中输入%pip install numpy pandas matplotlib把 Python 数据分析三个核心的三方库都安装上。注意上面的代码，我们不仅导入了 NumPy，还将 pandas 和 matplotlib 库一并导入了。 创建数组对象创建ndarray对象有很多种方法，下面我们介绍一些常用的方法。 方法一：使用array函数，通过list创建数组对象 代码： 12array1 = np.array([1, 2, 3, 4, 5])array1 输出： 1array([1, 2, 3, 4, 5]) 代码： 12array2 = np.array([[1, 2, 3], [4, 5, 6]])array2 输出： 12array([[1, 2, 3], [4, 5, 6]]) 方法二：使用arange函数，指定取值范围和跨度创建数组对象 代码： 12array3 = np.arange(0, 20, 2)array3 输出： 1array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) 方法三：使用linspace函数，用指定范围和元素个数创建数组对象，生成等差数列 代码： 12array4 = np.linspace(-1, 1, 11)array4 输出： 1array([-1. , -0.8, -0.6, -0.4, -0.2, 0. , 0.2, 0.4, 0.6, 0.8, 1. ]) 方法四：使用logspace函数，生成等比数列 代码： 12array5 = np.logspace(1, 10, num=10, base=2)array5 注意：等比数列的起始值是$2^1$，等比数列的终止值是$2^{10}$，num是元素的个数，base就是底数。 输出： 1array([ 2., 4., 8., 16., 32., 64., 128., 256., 512., 1024.]) 方法五：通过fromstring函数从字符串提取数据创建数组对象 代码： 12array6 = np.fromstring(&#x27;1, 2, 3, 4, 5&#x27;, sep=&#x27;,&#x27;, dtype=&#x27;i8&#x27;)array6 输出： 1array([1, 2, 3, 4, 5]) 方法六：通过fromiter函数从生成器（迭代器）中获取数据创建数组对象 代码： 12345678910def fib(how_many): a, b = 0, 1 for _ in range(how_many): a, b = b, a + b yield agen = fib(20)array7 = np.fromiter(gen, dtype=&#x27;i8&#x27;)array7 输出： 12array([ 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765]) 方法七：使用numpy.random模块的函数生成随机数创建数组对象 产生10个$[0, 1)$范围的随机小数，代码： 12array8 = np.random.rand(10)array8 输出： 12array([0.45556132, 0.67871326, 0.4552213 , 0.96671509, 0.44086463, 0.72650875, 0.79877188, 0.12153022, 0.24762739, 0.6669852 ]) 产生10个$[1, 100)$范围的随机整数，代码： 12array9 = np.random.randint(1, 100, 10)array9 输出： 1array([29, 97, 87, 47, 39, 19, 71, 32, 79, 34]) 产生20个$\\small{\\mu&#x3D;50}$，$\\small{\\sigma&#x3D;10}$的正态分布随机数，代码： 12array10 = np.random.normal(50, 10, 20)array10 输出： 1234array([55.04155586, 46.43510797, 20.28371158, 62.67884053, 61.23185964, 38.22682148, 53.17126151, 43.54741592, 36.11268017, 40.94086676, 63.27911699, 46.92688903, 37.1593374 , 67.06525656, 67.47269463, 23.37925889, 31.45312239, 48.34532466, 55.09180924, 47.95702787]) 产生$[0, 1)$范围的随机小数构成的3行4列的二维数组，代码： 12array11 = np.random.rand(3, 4)array11 输出： 123array([[0.54017809, 0.46797771, 0.78291445, 0.79501326], [0.93973783, 0.21434806, 0.03592874, 0.88838892], [0.84130479, 0.3566601 , 0.99935473, 0.26353598]]) 产生$[1, 100)$范围的随机整数构成的三维数组，代码： 12array12 = np.random.randint(1, 100, (3, 4, 5))array12 输出： 1234567891011121314array([[[94, 26, 49, 24, 43], [27, 27, 33, 98, 33], [13, 73, 6, 1, 77], [54, 32, 51, 86, 59]], [[62, 75, 62, 29, 87], [90, 26, 6, 79, 41], [31, 15, 32, 56, 64], [37, 84, 61, 71, 71]], [[45, 24, 78, 77, 41], [75, 37, 4, 74, 93], [ 1, 36, 36, 60, 43], [23, 84, 44, 89, 79]]]) 方法八：创建全0、全1或指定元素的数组 使用zeros函数，代码： 12array13 = np.zeros((3, 4))array13 输出： 123array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) 使用ones函数，代码： 12array14 = np.ones((3, 4))array14 输出： 123array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) 使用full函数，代码： 12array15 = np.full((3, 4), 10)array15 输出： 123array([[10, 10, 10, 10], [10, 10, 10, 10], [10, 10, 10, 10]]) 方法九：使用eye函数创建单位矩阵 代码： 1np.eye(4) 输出： 1234array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]) 方法十：读取图片获得对应的三维数组 代码： 12array16 = plt.imread(&#x27;res/guido.jpg&#x27;)array16 输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849array([[[ 36, 33, 28], [ 36, 33, 28], [ 36, 33, 28], ..., [ 32, 31, 29], [ 32, 31, 27], [ 31, 32, 26]], [[ 37, 34, 29], [ 38, 35, 30], [ 38, 35, 30], ..., [ 31, 30, 28], [ 31, 30, 26], [ 30, 31, 25]], [[ 38, 35, 30], [ 38, 35, 30], [ 38, 35, 30], ..., [ 30, 29, 27], [ 30, 29, 25], [ 29, 30, 25]], ..., [[239, 178, 123], [237, 176, 121], [235, 174, 119], ..., [ 78, 68, 56], [ 75, 67, 54], [ 73, 65, 52]], [[238, 177, 120], [236, 175, 118], [234, 173, 116], ..., [ 82, 70, 58], [ 78, 68, 56], [ 75, 66, 51]], [[238, 176, 119], [236, 175, 118], [234, 173, 116], ..., [ 84, 70, 61], [ 81, 69, 57], [ 79, 67, 53]]], dtype=uint8) 说明：上面的代码读取了当前路径下res目录中名为guido.jpg 的图片文件，计算机系统中的图片通常由若干行若干列的像素点构成，而每个像素点又是由红绿蓝三原色构成的，刚好可以用三维数组来表示。读取图片用到了matplotlib库的imread函数。 数组对象的属性size属性：获取数组元素个数。 代码： 12345array17 = np.arange(1, 100, 2)array18 = np.random.rand(3, 4)print(array16.size)print(array17.size)print(array18.size) 输出： 12311250005012 shape属性：获取数组的形状。 代码： 123print(array16.shape)print(array17.shape)print(array18.shape) 输出： 123(750, 500, 3)(50,)(3, 4) dtype属性：获取数组元素的数据类型。 代码： 123print(array16.dtype)print(array17.dtype)print(array18.dtype) 输出： 123uint8int64float64 ndarray对象元素的数据类型可以参考如下所示的表格。 ndim属性：获取数组的维度。 代码： 123print(array16.ndim)print(array17.ndim)print(array18.ndim) 输出： 123312 itemsize属性：获取数组单个元素占用内存空间的字节数。 代码： 123print(array16.itemsize)print(array17.itemsize)print(array18.itemsize) 输出： 123188 nbytes属性：获取数组所有元素占用内存空间的字节数。 代码： 123print(array16.nbytes)print(array17.nbytes)print(array18.nbytes) 输出： 123112500040096 数组的索引运算和 Python 中的列表类似，NumPy 的ndarray对象可以进行索引和切片操作，通过索引可以获取或修改数组中的元素，通过切片操作可以取出数组的一部分，我们把切片操作也称为切片索引。 普通索引类似于 Python 中list类型的索引运算。 代码： 123array19 = np.arange(1, 10)print(array19[0], array19[array19.size - 1])print(array19[-array20.size], array19[-1]) 输出： 121 91 9 代码： 12array20 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])array20[2] 输出： 1array([7, 8, 9]) 代码： 12print(array20[0][0])print(array20[-1][-1]) 输出： 1219 代码： 12print(array20[1][1])print(array20[1, 1]) 输出： 1255 代码： 12array20[1][1] = 10array20 输出： 123array([[ 1, 2, 3], [ 4, 10, 6], [ 7, 8, 9]]) 代码： 12array20[1] = [10, 11, 12]array20 输出： 123array([[ 1, 2, 3], [10, 11, 12], [ 7, 8, 9]]) 切片索引切片索引是形如[开始索引:结束索引:跨度]的语法，通过指定开始索引（默认值无穷小）、结束索引（默认值无穷大）和跨度（默认值1），从数组中取出指定部分的元素并构成新的数组。因为开始索引、结束索引和步长都有默认值，所以它们都可以省略，如果不指定步长，第二个冒号也可以省略。一维数组的切片运算跟 Python 中的list类型的切片非常类似，此处不再赘述，二维数组的切片可以参考下面的代码，相信非常容易理解。 代码： 1array20[:2, 1:] 输出： 12array([[ 2, 3], [11, 12]]) 代码： 1array20[2, :] 输出： 1array([7, 8, 9]) 代码： 1array20[2:, :] 输出： 1array([[7, 8, 9]]) 代码： 1array20[:, :2] 输出： 123array([[ 1, 2], [10, 11], [ 7, 8]]) 代码： 1array20[::2, ::2] 输出： 12array([[1, 3], [7, 9]]) 代码： 1array20[::-2, ::-2] 输出： 12array([[9, 7], [3, 1]]) 关于数组的索引和切片运算，大家可以通过下面的两张图来增强印象，这两张图来自《利用Python进行数据分析》一书，它是 pandas 库的作者 Wes McKinney 撰写的 Python 数据分析领域的经典教科书，有兴趣的读者可以购买和阅读原书。 图1：二维数组的普通索引 图2：二维数组的切片索引 花式索引花式索引是用保存整数的数组充当一个数组的索引，这里所说的数组可以是 NumPy 的ndarray，也可以是 Python 中list、tuple等可迭代类型，可以使用正向或负向索引。 代码： 1array19[[0, 1, 1, -1, 4, -1]] 输出： 1array([1, 2, 2, 9, 5, 9]) 代码： 1array20[[0, 2]] 输出： 12array([[1, 2, 3], [7, 8, 9]]) 代码： 1array20[[0, 2], [1, 2]] 输出： 1array([2, 9]) 代码： 1array20[[0, 2], 1] 输出： 1array([2, 8]) 布尔索引布尔索引就是通过保存布尔值的数组充当一个数组的索引，布尔值为True的元素保留，布尔值为False的元素不会被选中。布尔值的数组可以手动构造，也可以通过关系运算来产生。 代码： 1array19[[True, True, False, False, True, False, False, True, True]] 输出： 1array([1, 2, 5, 8, 9]) 代码： 1array19 &gt; 5 输出： 1array([False, False, False, False, False, True, True, True, True]) 代码： 1~(array19 &gt; 5) 输出： 1array([ True, True, True, True, True, False, False, False, False]) 说明：~运算符可以对布尔数组中的布尔值进行逻辑取反，也就是原来的True会变成False，原来的False会变成True。 代码： 1array19[array20 &gt; 5] 输出： 1array([6, 7, 8, 9]) 代码： 1array19 % 2 == 0 输出： 1array([False, True, False, True, False, True, False, True, False]) 代码： 1array19[array20 % 2 == 0] 输出： 1array([2, 4, 6, 8]) 代码： 1(array19 &gt; 5) &amp; (array19 % 2 == 0) 输出： 1array([False, False, False, False, False, True, False, True, False]) 说明：&amp;运算符可以作用于两个布尔数组，如果两个数组对应元素都是True，那么运算的结果就是True，否则就是False，该运算符的运算规则类似于 Python 中的 and 运算符，只不过作用的对象是两个布尔数组。 代码： 1array19[(array19 &gt; 5) &amp; (array19 % 2 == 0)] 输出： 1array([6, 8]) 代码： 1array19[(array19 &gt; 5) | (array19 % 2 == 0)] 输出： 1array([2, 4, 6, 7, 8, 9]) 说明：|运算符可以作用于两个布尔数组，如果两个数组对应元素都是False，那么运算的结果就是False，否则就是True，该运算符的运算规则类似于 Python 中的 or 运算符，只不过作用的对象是两个布尔数组。 代码： 1array20[array21 % 2 != 0] 输出： 1array([1, 3, 5, 7, 9]) 关于索引运算需要说明的是，切片索引虽然创建了新的数组对象，但是新数组和原数组共享了数组中的数据，简单的说，无论你通过新数组对象或原数组对象修改数组中的数据，修改的其实是内存中的同一块数据。花式索引和布尔索引也会创建新的数组对象，而且新数组复制了原数组的元素，新数组和原数组并不是共享数据的关系，这一点可以查看数组对象的base属性，有兴趣的读者可以自行探索。 案例：通过数组切片处理图像学习基础知识总是比较枯燥且没有成就感的，所以我们还是来个案例为大家演示下上面学习的数组索引和切片操作到底有什么用。前面我们说到过，可以用三维数组来表示图像，那么通过图像对应的三维数组进行操作，就可以实现对图像的处理，如下所示。 读入图片创建三维数组对象。 12guido_image = plt.imread(&#x27;guido.jpg&#x27;)plt.imshow(guido_image) 对数组的0轴进行反向切片，实现图像的垂直翻转。 1plt.imshow(guido_image[::-1]) 对数组的1轴进行反向切片，实现图像的水平翻转。 1plt.imshow(guido_image[:,::-1]) 通过切片操作实现抠图，将吉多大叔的头抠出来。 1plt.imshow(guido_image[30:350, 90:300]) 通过切片操作实现降采样。 1plt.imshow(guido_image[::10, ::10])","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/67.环境准备","date":"2024-12-12T08:38:02.393Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/67.环境准备/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/67.%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/","excerpt":"","text":"环境准备如果希望快速开始使用 Python 处理数据科学相关的工作，建议大家直接安装 Anaconda，然后使用 Anaconda 中集成的 Notebook 或 JupyterLab 工具来编写代码。因为对于新手来说，先安装官方的 Python 解释器，再逐个安装工作中会使用到的三方库文件会比较麻烦，尤其是在 Windows 环境下，经常会因为构建工具或 DLL 文件的缺失导致安装失败，而一般新手也很难根据错误提示信息采取正确的解决措施，容易产生严重的挫败感。如果计算机上已经有 Python 解释器环境了，也可以直接使用 Python 的包管理工具 pip 安装 Jupyter，再根据实际工作的需要安装三方库，这种方式适合有一定经验的用户。 安装和使用 Anaconda对于个人用户来说，可以从 Anaconda 的官方网站下载它的“个人版（Individual Edition）”安装程序，安装完成后，你的计算机上不仅拥有了 Python 环境和 Spyder（类似于 PyCharm 的集成开发工具），还拥有了与数据科学工作相关的近200个工具包，包括我们上面提到 Python 数据分析三大神器。除此之外，Anaconda 还提供了一个名为 conda 的包管理工具，通过这个工具不仅可以管理 Python 的工具包，还可以用于创建运行 Python 程序的虚拟环境。 如上图所示，可以通过 Anaconda 官网提供的下载链接选择适合自己操作系统的安装程序，建议大家选择图形化的安装程序，下载完成后双击安装程序开始安装。安装过程基本使用默认设置即可，完成安装后，macOS 用户可以在“应用程序”或“启动台”中找到名为“Anaconda-Navigator”的应用程序，运行该程序可以看到如下所示的界面，我们可以在这里选择需要执行的操作。 对于 Windows 用户，建议按照安装向导的提示和推荐的选项来安装 Anaconda（除了安装路径，基本也没有什么需要选择的），安装完成后可以在“开始菜单”中找到“Anaconda3”。 提示：可以选择 Miniconda 作为 Anaconda 的替代品，Miniconda 只会安装 Python 解释器环境和一些必要的工具，其他的三方库由用户自行选择安装。其实我个人并不喜欢 Anaconda，因为它是给小白用户使用的，我们有了 Python 环境以后完全可以按照自己的意愿来安装需要的三方库。 conda命令对于非新手用户，如果希望使用 conda 工具来管理依赖项或者创建项目的虚拟环境，可以在终端或命令行提示符中使用 conda 命令。Windows 用户可以在“开始菜单”中找到“Anaconda3”，然后点击“Anaconda Prompt”或“Anaconda PowerShell”来启动支持 conda 的命令行提示符。新手用户如果想创建新的虚拟环境或管理三方库（依赖项），建议直接使用“Anaconda-Navigator”中的“Environments”，通过可视化的方式对虚拟环境和依赖项进行管理。 版本和帮助信息。 查看版本：conda -V或conda --version 获取帮助：conda -h或conda --help 相关信息：conda list 虚拟环境相关。 显示所有虚拟环境：conda env list 创建虚拟环境：conda create --name venv 指定 Python 版本创建虚拟环境：conda create --name venv python=3.7 指定 Python 版本创建虚拟环境并安装指定依赖项：conda create --name venv python=3.7 numpy pandas 通过克隆现有虚拟环境的方式创建虚拟环境：conda create --name venv2 --clone venv 分享虚拟环境并重定向到指定的文件中：conda env export &gt; environment.yml 通过分享的虚拟环境文件创建虚拟环境：conda env create -f environment.yml 激活虚拟环境：conda activate venv 退出虚拟环境：conda deactivate 删除虚拟环境：conda remove --name venv --all 说明：上面的命令中，venv和venv2是虚拟环境文件夹的名字，可以将其替换为自己喜欢的名字，但是强烈建议使用英文且不要有特殊字符。 包（三方库或工具）管理。 查看已经安装的包：conda list 搜索指定的包：conda search matplotlib 安装指定的包：conda install matplotlib 更新指定的包：conda update matplotlib 移除指定的包：conda remove matplotlib 说明：在搜索、安装和更新软件包时，默认会连接到官方网站进行操作，如果觉得速度不给力，可以将默认的官方网站替换为国内的镜像网站，推荐使用清华大学的开源镜像网站。将默认源更换为国内镜像的命令是：conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/和conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main。如果需要换回默认源，可以使用命令conda config --remove-key channels。 安装和使用JupyterLab安装和启动如果已经安装了 Anaconda，可以按照上面所说的方式在“Anaconda-Navigator”中直接启动 Notebook 或 JupyterLab。按照官方的说法，JupyterLab 是下一代的 Notebook，提供了更友好的界面和更强大的功能，我们也推荐大家使用 JupyterLab。Windows 用户也可以在开始菜单中打开“Anaconda Prompt”或“Anaconda PowerShell”，由于已经激活了 Anaconda 默认的虚拟环境，只需要输入jupyter lab命令来启动JupyterLab。macOS 系统在安装 Anaconda以后，每次打开终端时会自动激活 Anaconda 默认的虚拟环境，也是通过输入jupyter lab命令就可以启动JupyterLab。 对于安装了 Python 环境但是没有安装 Anaconda 的用户，可以用 Python 的包管理工具pip来安装 JupyterLab，安装成功后在终端或命令行提示符中执行jupyter lab命令来启动 JupyterLab，如下所示。 安装 JupyterLab： 1pip install jupyterlab 安装 Python 数据分析三大神器： 1pip install numpy pandas matplotlib 启动 JupyterLab： 1jupyter lab JupyterLab 是基于网页的用于交互计算的应用程序，可以用于代码开发、文档撰写、代码运行和结果展示。简单的说，你可以在网页中直接编写代码和运行代码，代码的运行结果也会直接在代码块下方进行展示。如在编写代码的过程中需要编写说明文档，可在同一个页面中使用 Markdown 格式进行编写，而且可以直接看到渲染后的效果。此外，Notebook 的设计初衷是提供一个能够支持多种编程语言的工作环境，目前它能够支持超过40种编程语言，包括 Python、R、Julia、Scala 等。 首先，我们可以创建一个用于书写 Python 代码的 Notebook，如下图所示。 接下来，我们就可以编写代码、撰写文档和运行程序啦，如下图所示。 使用技巧如果使用 Python 做工程化的项目开发，PyCharm 肯定是最好的选择，它提供了一个集成开发环境应该具有的所有功能，尤其是智能提示、代码补全、自动纠错这类功能会让开发人员感到非常舒服。如果使用 Python 做数据科学相关的工作，JupyterLab 并不比 PyCharm 逊色，在数据和图表展示方面 JupyterLab 更加优秀。为此，JetBrains 公司还专门开发了一个对标 JupyterLab 的新工具 DataSpell，有兴趣的读者可以自行了解。下面我们为大家介绍一些 JupyterLab 的使用技巧，希望能够帮助大家提升工作效率。 自动补全。在使用 JupyterLab 编写代码时，按Tab键会获得代码提示和补全功能。 获得帮助。如果希望了解一个对象（如变量、类、函数等）的相关信息或使用方式，可以在对象后面使用?并运行代码， 窗口下方会显示出对应的信息，帮助我们了解该对象，如下所示。 搜索命名。如果只记得一个类或一个函数名字的一部分，可以使用通配符*并配合?进行搜索，如下所示。 调用命令。可以在 JupyterLab 中使用!后面跟系统命令的方式来执行系统命令。 魔法指令。JupyterLab 中有很多非常有趣且有用的魔法指令，例如可以使用%timeit测试语句的执行时间，可以使用%pwd查看当前工作目录等。如果想查看所有的魔法指令，可以使用%lsmagic，如果了解魔法指令的用法，可以使用%magic来查看，如下图所示。 常用的魔法指令有： 魔法指令 功能说明 %pwd 查看当前工作目录 %ls 列出当前或指定文件夹下的内容 %cat 查看指定文件的内容 %hist 查看输入历史 %matplotlib inline 设置在页面中嵌入matplotlib输出的统计图表 %config Inlinebackend.figure_format=&#39;svg&#39; 设置统计图表使用SVG格式（矢量图） %run 运行指定的程序 %load 加载指定的文件到单元格中 %quickref 显示IPython的快速参考 %timeit 多次运行代码并统计代码执行时间 %prun 用cProfile.run运行代码并显示分析器的输出 %who &#x2F; %whos 显示命名空间中的变量 %xdel 删除一个对象并清理所有对它的引用 快捷键。JupyterLab 中的很多操作可以通过快捷键来实现，使用快捷键可以提升工作效率。JupyterLab 的快捷键可以分为命令模式下的快捷键和编辑模式下的快捷键，所谓编辑模式就是处于输入代码或撰写文档状态的模式，在编辑模式下按Esc可以回到命令模式，在命令模式下按Enter可以进入编辑模式。 命令模式下的快捷键： 快捷键 功能说明 Alt + Enter 运行当前单元格并在下面插入新的单元格 Shift + Enter 运行当前单元格并选中下方的单元格 Ctrl + Enter 运行当前单元格 j &#x2F; k、Shift + j &#x2F; Shift + k 选中下方&#x2F;上方单元格、连续选中下方&#x2F;上方单元格 a &#x2F; b 在下方&#x2F;上方插入新的单元格 c &#x2F; x 复制单元格 &#x2F; 剪切单元格 v &#x2F; Shift + v 在下方&#x2F;上方粘贴单元格 dd &#x2F; z 删除单元格 &#x2F; 恢复删除的单元格 Shift + l 显示或隐藏当前&#x2F;所有单元格行号 Space &#x2F; Shift + Space 向下&#x2F;向上滚动页面 编辑模式下的快捷键： 快捷键 功能说明 Shift + Tab 获得提示信息 Ctrl + ]&#x2F; Ctrl + [ 增加&#x2F;减少缩进 Alt + Enter 运行当前单元格并在下面插入新的单元格 Shift + Enter 运行当前单元格并选中下方的单元格 Ctrl + Enter 运行当前单元格 Ctrl + Left &#x2F; Right 光标移到行首&#x2F;行尾 Ctrl + Up &#x2F; Down 光标移动代码开头&#x2F;结尾处 Up &#x2F; Down 光标上移&#x2F;下移一行或移到上&#x2F;下一个单元格 说明：对于 macOS 系统可以将Alt键替换成Option键，将Ctrl键替换成Command键。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day66-80/66.数据分析概述","date":"2024-12-12T08:38:02.389Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day66-80/66.数据分析概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day66-80/66.%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A6%82%E8%BF%B0/","excerpt":"","text":"数据分析概述当今世界对信息技术的依赖程度在不断加深，每天都会有大量的数据产生，我们经常会感到数据越来越多，但是要从中发现有价值的信息却越来越难。这里所说的信息，可以理解为对数据集处理之后的结果，是从数据集中提炼出的可用于其他场合的结论性的东西，而从原始数据中抽取出有价值的信息的这个过程我们就称之为数据分析，它是数据科学工作的一部分。 定义：数据分析是有针对性的收集、加工、整理数据并采用统计、挖掘等技术对数据进行探索、分析、呈现和解释的科学。 数据分析师的职责和技能栈HR在发布招聘需求时，通常将数据工程、数据分析、数据挖掘等岗位都统称为数据分析岗位，但是根据工作性质的不同，又可以分为偏工程的数据治理方向、偏业务的数据分析方向、偏算法的数据挖掘方向、偏开发的数据开发方向、偏产品的数据产品经理。我们通常所说的数据分析师主要是指业务数据分析师，很多数据分析师的职业生涯都是从这个岗位开始的，而且这个岗位也是招聘数量最多的岗位。业务数据分析师在公司通常不属于研发部门而属于运营部门，所以这个岗位也称为数据运营或商业分析，这类人员通常也被称为“BI工程师”。通常招聘信息对这个岗位的描述（JD）是： 负责相关报表的输出。 建立和优化指标体系。 监控数据波动和异常，找出问题。 优化和驱动业务，推动数字化运营。 找出潜在的市场和产品的上升空间。 根据上面的描述，作为业务数据分析师，我们的工作不是给领导一个简单浅显的结论，而是结合公司的业务，完成监控数据、揪出异常、找到原因、探索趋势等工作。作为数据分析师，不管是用 Python 语言、Excel、SPSS或其他的商业智能工具，工具只是达成目标的手段，数据思维是核心技能，从实际业务问题出发到最终发现数据中的商业价值是终极目标。数据分析师在很多公司只是一个基础岗位，精于业务的数据分析师可以向数据分析经理或数据运营总监等管理岗位发展；对于熟悉机器学习算法的数据分析师来说，可以向数据挖掘工程师或算法专家方向发展，而这些岗位除了需要相应的数学和统计学知识，在编程能力方面也比数据分析师有更高的要求，可能还需要有大数据存储和处理的相关经验。数据治理岗位主要是帮助公司建设数据仓库或数据湖，实现数据从业务系统、埋点系统、日志系统到分析库的转移，为后续的数据分析和挖掘提供基础设施。数据治理岗位对 SQL 和 HiveSQL 有着较高的要求，需要熟练的使用 ETL 工具，此外还需要对 Hadoop 生态圈有一个较好的认知。作为数据产品经理，除了传统产品经理的技能栈之外，也需要较强的技术能力，例如要了解常用的推荐算法、机器学习模型，能够为算法的改进提供依据，能够制定相关埋点的规范和口径，虽然不需要精通各种算法，但是要站在产品的角度去考虑数据模型、指标、算法等的落地。 以下是我总结的数据分析师的技能栈，仅供参考。 计算机科学（数据分析工具、编程语言、数据库） 数学和统计学（数据思维、统计思维） 人工智能（机器学习中的数据挖掘算法） 业务理解能力（沟通、表达、经验） 总结和表述能力（商业PPT、文字总结） 数据分析的流程我们提到数分析这个词很多时候可能指的都是狭义的数据分析，这类数据分析主要目标就是生成可视化报表并通过这些报表来洞察业务中的问题，这类工作一般都是具有滞后性的。广义的数据分析还包含了数据挖掘的部分，不仅要通过数据实现对业务的监控和分析，还要利用机器学习算法，找出隐藏在数据背后的知识，并利用这些知识为将来的决策提供支撑，具备一定的前瞻性。 基本的数据分析工作一般包含以下几个方面的内容，当然因为行业和工作内容的不同会略有差异。 确定目标（输入）：理解业务，确定指标口径 获取数据：数据仓库（SQL提数）、电子表格、三方接口、网络爬虫、开放数据集等 清洗数据：包括对缺失值、重复值、异常值的处理以及相关的预处理（格式化、离散化、二值化等） 数据透视：排序、统计、分组聚合、交叉表、透视表等 数据呈现（输出）：数据可视化，发布工作成果（数据分析报告） 分析洞察（后续）：解释数据的变化，提出对应的方案 深入的数据挖掘工作通常包含以下几个方面的内容，当然因为行业和工作内容的不同会略有差异。 确定目标（输入）：理解业务，明确挖掘目标 数据准备：数据采集、数据描述、数据探索、质量判定等 数据加工：提取数据、清洗数据、数据变换、特殊编码、降维、特征选择等 数据建模：模型比较、模型选择、算法应用 模型评估：交叉检验、参数调优、结果评价 模型部署（输出）：模型落地、业务改进、运营监控、报告撰写 数据分析相关库使用 Python 从事数据科学相关的工作是一个非常棒的选择，因为 Python 整个生态圈中，有大量的成熟的用于数据科学的软件包（工具库）。而且不同于其他的用于数据科学的编程语言（如：Julia、R），Python 除了可以用于数据科学，还能做很多其他的事情，可以说 Python 语言几乎是无所不能的。 三大神器 NumPy：支持常见的数组和矩阵操作，通过ndarray类实现了对多维数组的封装，提供了操作这些数组的方法和函数集。由于 NumPy 内置了并行运算功能，当使用多核 CPU 时，Numpy会自动做并行计算。 Pandas：pandas 的核心是其特有的数据结构DataFrame和Series，这使得 pandas 可以处理包含不同类型数据的表格和时间序列，这一点是NumPy的ndarray做不到的。使用 pandas，可以轻松顺利的加载各种形式的数据，然后对数据进行切片、切块、处理缺失值、聚合、重塑和可视化等操作。 Matplotlib：matplotlib 是一个包含各种绘图模块的库，能够根据我们提供的数据创建高质量的图表。此外，matplotlib 还提供了 pylab 模块，这个模块包含了很多像 MATLAB 一样的绘图组件。 其他相关库 SciPy：完善了 NumPy 的功能，封装了大量科学计算的算法，包括线性代数、统计检验、稀疏矩阵、信号和图像处理、最优化问题、快速傅里叶变换等。 Seaborn：seaborn 是基于 matplotlib 的图形可视化工具，直接使用 matplotlib 虽然可以定制出漂亮的统计图表，但是总体来说还不够简单方便，seaborn 相当于是对 matplotlib 做了封装，让用户能够以更简洁有效的方式做出各种有吸引力的统计图表。 Scikit-learn：scikit-learn 最初是 SciPy 的一部分，提供了大量机器学习可能用到的工具，包括数据预处理、监督学习（分类、回归）、无监督学习（聚类）、模式选择、交叉检验等。 Statsmodels：包含了经典统计学和经济计量学算法的库。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/65.爬虫框架Scrapy简介","date":"2024-12-12T08:38:02.355Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/65.爬虫框架Scrapy简介/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/65.%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E7%AE%80%E4%BB%8B/","excerpt":"","text":"爬虫框架Scrapy简介当你写了很多个爬虫程序之后，你会发现每次写爬虫程序时，都需要将页面获取、页面解析、爬虫调度、异常处理、反爬应对这些代码从头至尾实现一遍，这里面有很多工作其实都是简单乏味的重复劳动。那么，有没有什么办法可以提升我们编写爬虫代码的效率呢？答案是肯定的，那就是利用爬虫框架，而在所有的爬虫框架中，Scrapy 应该是最流行、最强大的框架。 Scrapy 概述Scrapy 是基于 Python 的一个非常流行的网络爬虫框架，可以用来抓取 Web 站点并从页面中提取结构化的数据。下图展示了 Scrapy 的基本架构，其中包含了主要组件和系统的数据处理流程（图中带数字的红色箭头）。 Scrapy的组件我们先来说说 Scrapy 中的组件。 Scrapy 引擎（Engine）：用来控制整个系统的数据处理流程。 调度器（Scheduler）：调度器从引擎接受请求并排序列入队列，并在引擎发出请求后返还给它们。 下载器（Downloader）：下载器的主要职责是抓取网页并将网页内容返还给蜘蛛（Spiders）。 蜘蛛程序（Spiders）：蜘蛛是用户自定义的用来解析网页并抓取特定URL的类，每个蜘蛛都能处理一个域名或一组域名，简单的说就是用来定义特定网站的抓取和解析规则的模块。 数据管道（Item Pipeline）：管道的主要责任是负责处理有蜘蛛从网页中抽取的数据条目，它的主要任务是清理、验证和存储数据。当页面被蜘蛛解析后，将被发送到数据管道，并经过几个特定的次序处理数据。每个数据管道组件都是一个 Python 类，它们获取了数据条目并执行对数据条目进行处理的方法，同时还需要确定是否需要在数据管道中继续执行下一步或是直接丢弃掉不处理。数据管道通常执行的任务有：清理 HTML 数据、验证解析到的数据（检查条目是否包含必要的字段）、检查是不是重复数据（如果重复就丢弃）、将解析到的数据存储到数据库（关系型数据库或 NoSQL 数据库）中。 中间件（Middlewares）：中间件是介于引擎和其他组件之间的一个钩子框架，主要是为了提供自定义的代码来拓展 Scrapy 的功能，包括下载器中间件和蜘蛛中间件。 数据处理流程Scrapy 的整个数据处理流程由引擎进行控制，通常的运转流程包括以下的步骤： 引擎询问蜘蛛需要处理哪个网站，并让蜘蛛将第一个需要处理的 URL 交给它。 引擎让调度器将需要处理的 URL 放在队列中。 引擎从调度那获取接下来进行爬取的页面。 调度将下一个爬取的 URL 返回给引擎，引擎将它通过下载中间件发送到下载器。 当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎；如果下载失败了，引擎会通知调度器记录这个 URL，待会再重新下载。 引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。 蜘蛛处理响应并返回爬取到的数据条目，此外还要将需要跟进的新的 URL 发送给引擎。 引擎将抓取到的数据条目送入数据管道，把新的 URL 发送给调度器放入队列中。 上述操作中的第2步到第8步会一直重复直到调度器中没有需要请求的 URL，爬虫就停止工作。 安装和使用Scrapy可以使用 Python 的包管理工具pip来安装 Scrapy。 1pip install scrapy 在命令行中使用scrapy命令创建名为demo的项目。 1scrapy startproject demo 项目的目录结构如下图所示。 12345678910demo|____ demo|________ spiders|____________ __init__.py|________ __init__.py|________ items.py|________ middlewares.py|________ pipelines.py|________ settings.py|____ scrapy.cfg 切换到demo 目录，用下面的命令创建名为douban的蜘蛛程序。 1scrapy genspider douban movie.douban.com 一个简单的例子接下来，我们实现一个爬取豆瓣电影 Top250 电影标题、评分和金句的爬虫。 在items.py的Item类中定义字段，这些字段用来保存数据，方便后续的操作。 1234567import scrapyclass DoubanItem(scrapy.Item): title = scrapy.Field() score = scrapy.Field() motto = scrapy.Field() 修改spiders文件夹中名为douban.py 的文件，它是蜘蛛程序的核心，需要我们添加解析页面的代码。在这里，我们可以通过对Response对象的解析，获取电影的信息，代码如下所示。 123456789101112131415161718192021import scrapyfrom scrapy import Selector, Requestfrom scrapy.http import HtmlResponsefrom demo.items import MovieItemclass DoubanSpider(scrapy.Spider): name = &#x27;douban&#x27; allowed_domains = [&#x27;movie.douban.com&#x27;] start_urls = [&#x27;https://movie.douban.com/top250?start=0&amp;filter=&#x27;] def parse(self, response: HtmlResponse): sel = Selector(response) movie_items = sel.css(&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;) for movie_sel in movie_items: item = MovieItem() item[&#x27;title&#x27;] = movie_sel.css(&#x27;.title::text&#x27;).extract_first() item[&#x27;score&#x27;] = movie_sel.css(&#x27;.rating_num::text&#x27;).extract_first() item[&#x27;motto&#x27;] = movie_sel.css(&#x27;.inq::text&#x27;).extract_first() yield item 通过上面的代码不难看出，我们可以使用 CSS 选择器进行页面解析。当然，如果你愿意也可以使用 XPath 或正则表达式进行页面解析，对应的方法分别是xpath和re。 如果还要生成后续爬取的请求，我们可以用yield产出Request对象。Request对象有两个非常重要的属性，一个是url，它代表了要请求的地址；一个是callback，它代表了获得响应之后要执行的回调函数。我们可以将上面的代码稍作修改。 1234567891011121314151617181920212223242526import scrapyfrom scrapy import Selector, Requestfrom scrapy.http import HtmlResponsefrom demo.items import MovieItemclass DoubanSpider(scrapy.Spider): name = &#x27;douban&#x27; allowed_domains = [&#x27;movie.douban.com&#x27;] start_urls = [&#x27;https://movie.douban.com/top250?start=0&amp;filter=&#x27;] def parse(self, response: HtmlResponse): sel = Selector(response) movie_items = sel.css(&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;) for movie_sel in movie_items: item = MovieItem() item[&#x27;title&#x27;] = movie_sel.css(&#x27;.title::text&#x27;).extract_first() item[&#x27;score&#x27;] = movie_sel.css(&#x27;.rating_num::text&#x27;).extract_first() item[&#x27;motto&#x27;] = movie_sel.css(&#x27;.inq::text&#x27;).extract_first() yield item hrefs = sel.css(&#x27;#content &gt; div &gt; div.article &gt; div.paginator &gt; a::attr(&quot;href&quot;)&#x27;) for href in hrefs: full_url = response.urljoin(href.extract()) yield Request(url=full_url) 到这里，我们已经可以通过下面的命令让爬虫运转起来。 1scrapy crawl movie 可以在控制台看到爬取到的数据，如果想将这些数据保存到文件中，可以通过-o参数来指定文件名，Scrapy 支持我们将爬取到的数据导出成 JSON、CSV、XML 等格式。 1scrapy crawl moive -o result.json 不知大家是否注意到，通过运行爬虫获得的 JSON 文件中有275条数据，那是因为首页被重复爬取了。要解决这个问题，可以对上面的代码稍作调整，不在parse方法中解析获取新页面的 URL，而是通过start_requests方法提前准备好待爬取页面的 URL，调整后的代码如下所示。 123456789101112131415161718192021222324import scrapyfrom scrapy import Selector, Requestfrom scrapy.http import HtmlResponsefrom demo.items import MovieItemclass DoubanSpider(scrapy.Spider): name = &#x27;douban&#x27; allowed_domains = [&#x27;movie.douban.com&#x27;] def start_requests(self): for page in range(10): yield Request(url=f&#x27;https://movie.douban.com/top250?start=&#123;page * 25&#125;&#x27;) def parse(self, response: HtmlResponse): sel = Selector(response) movie_items = sel.css(&#x27;#content &gt; div &gt; div.article &gt; ol &gt; li&#x27;) for movie_sel in movie_items: item = MovieItem() item[&#x27;title&#x27;] = movie_sel.css(&#x27;.title::text&#x27;).extract_first() item[&#x27;score&#x27;] = movie_sel.css(&#x27;.rating_num::text&#x27;).extract_first() item[&#x27;motto&#x27;] = movie_sel.css(&#x27;.inq::text&#x27;).extract_first() yield item 如果希望完成爬虫数据的持久化，可以在数据管道中处理蜘蛛程序产生的Item对象。例如，我们可以通过前面讲到的openpyxl操作 Excel 文件，将数据写入 Excel 文件中，代码如下所示。 12345678910111213141516171819import openpyxlfrom demo.items import MovieItemclass MovieItemPipeline: def __init__(self): self.wb = openpyxl.Workbook() self.sheet = self.wb.active self.sheet.title = &#x27;Top250&#x27; self.sheet.append((&#x27;名称&#x27;, &#x27;评分&#x27;, &#x27;名言&#x27;)) def process_item(self, item: MovieItem, spider): self.sheet.append((item[&#x27;title&#x27;], item[&#x27;score&#x27;], item[&#x27;motto&#x27;])) return item def close_spider(self, spider): self.wb.save(&#x27;豆瓣电影数据.xlsx&#x27;) 上面的process_item和close_spider都是回调方法（钩子函数）， 简单的说就是 Scrapy 框架会自动去调用的方法。当蜘蛛程序产生一个Item对象交给引擎时，引擎会将该Item对象交给数据管道，这时我们配置好的数据管道的parse_item方法就会被执行，所以我们可以在该方法中获取数据并完成数据的持久化操作。另一个方法close_spider是在爬虫结束运行前会自动执行的方法，在上面的代码中，我们在这个地方进行了保存 Excel 文件的操作，相信这段代码大家是很容易读懂的。 总而言之，数据管道可以帮助我们完成以下操作： 清理 HTML 数据，验证爬取的数据。 丢弃重复的不必要的内容。 将爬取的结果进行持久化操作。 修改settings.py文件对项目进行配置，主要需要修改以下几个配置。 123456789101112131415161718# 用户浏览器USER_AGENT = &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;# 并发请求数量 CONCURRENT_REQUESTS = 4# 下载延迟DOWNLOAD_DELAY = 3# 随机化下载延迟RANDOMIZE_DOWNLOAD_DELAY = True# 是否遵守爬虫协议ROBOTSTXT_OBEY = True# 配置数据管道ITEM_PIPELINES = &#123; &#x27;demo.pipelines.MovieItemPipeline&#x27;: 300,&#125; 说明：上面配置文件中的ITEM_PIPELINES选项是一个字典，可以配置多个处理数据的管道，后面的数字代表了执行的优先级，数字小的先执行。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/64.使用Selenium抓取网页动态内容","date":"2024-12-12T08:38:02.353Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/64.使用Selenium抓取网页动态内容/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/64.%E4%BD%BF%E7%94%A8Selenium%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9/","excerpt":"","text":"使用Selenium抓取网页动态内容根据权威机构发布的全球互联网可访问性审计报告，全球约有四分之三的网站其内容或部分内容是通过JavaScript动态生成的，这就意味着在浏览器窗口中“查看网页源代码”时无法在HTML代码中找到这些内容，也就是说我们之前用的抓取数据的方式无法正常运转了。解决这样的问题基本上有两种方案，一是获取提供动态内容的数据接口，这种方式也适用于抓取手机 App 的数据；另一种是通过自动化测试工具 Selenium 运行浏览器获取渲染后的动态内容。对于第一种方案，我们可以使用浏览器的“开发者工具”或者更为专业的抓包工具（如：Charles、Fiddler、Wireshark等）来获取到数据接口，后续的操作跟上一个章节中讲解的获取“360图片”网站的数据是一样的，这里我们不再进行赘述。这一章我们重点讲解如何使用自动化测试工具 Selenium 来获取网站的动态内容。 Selenium 介绍Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的行为，最终帮助爬虫开发者获取到网页的动态内容。简单的说，只要我们在浏览器窗口中能够看到的内容，都可以使用 Selenium 获取到，对于那些使用了 JavaScript 动态渲染技术的网站，Selenium 会是一个重要的选择。下面，我们还是以 Chrome 浏览器为例，来讲解 Selenium 的用法，大家需要先安装 Chrome 浏览器并下载它的驱动。Chrome 浏览器的驱动程序可以在ChromeDriver官网进行下载，驱动的版本要跟浏览器的版本对应，如果没有完全对应的版本，就选择版本代号最为接近的版本。 使用Selenium我们可以先通过pip来安装 Selenium，命令如下所示。 1pip install selenium 加载页面接下来，我们通过下面的代码驱动 Chrome 浏览器打开百度。 123456from selenium import webdriver# 创建Chrome浏览器对象browser = webdriver.Chrome()# 加载指定的页面browser.get(&#x27;https://www.baidu.com/&#x27;) 如果不愿意使用 Chrome 浏览器，也可以修改上面的代码操控其他浏览器，只需创建对应的浏览器对象（如 Firefox、Safari 等）即可。运行上面的程序，如果看到如下所示的错误提示，那是说明我们还没有将 Chrome 浏览器的驱动添加到 PATH 环境变量中，也没有在程序中指定 Chrome 浏览器驱动所在的位置。 1selenium.common.exceptions.WebDriverException: Message: &#x27;chromedriver&#x27; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home 解决这个问题的办法有三种： 将下载的 ChromeDriver 放到已有的 PATH 环境变量下，建议直接跟 Python 解释器放在同一个目录，因为之前安装 Python 的时候我们已经将 Python 解释器的路径放到 PATH 环境变量中了。 将 ChromeDriver 放到项目虚拟环境下的 bin 文件夹中（Windows 系统对应的目录是 Scripts），这样 ChromeDriver 就跟虚拟环境下的 Python 解释器在同一个位置，肯定是能够找到的。 修改上面的代码，在创建 Chrome 对象时，通过service参数配置Service对象，并通过创建Service对象的executable_path参数指定 ChromeDriver 所在的位置，如下所示： 12345from selenium import webdriverfrom selenium.webdriver.chrome.service import Servicebrowser = webdriver.Chrome(service=Service(executable_path=&#x27;venv/bin/chromedriver&#x27;))browser.get(&#x27;https://www.baidu.com/&#x27;) 查找元素和模拟用户行为接下来，我们可以尝试模拟用户在百度首页的文本框输入搜索关键字并点击“百度一下”按钮。在完成页面加载后，可以通过Chrome对象的find_element和find_elements方法来获取页面元素，Selenium 支持多种获取元素的方式，包括：CSS 选择器、XPath、元素名字（标签名）、元素 ID、类名等，前者可以获取单个页面元素（WebElement对象），后者可以获取多个页面元素构成的列表。获取到WebElement对象以后，可以通过send_keys来模拟用户输入行为，可以通过click来模拟用户点击操作，代码如下所示。 12345678910111213from selenium import webdriverfrom selenium.webdriver.common.by import Bybrowser = webdriver.Chrome()browser.get(&#x27;https://www.baidu.com/&#x27;)# 通过元素ID获取元素kw_input = browser.find_element(By.ID, &#x27;kw&#x27;)# 模拟用户输入行为kw_input.send_keys(&#x27;Python&#x27;)# 通过CSS选择器获取元素su_button = browser.find_element(By.CSS_SELECTOR, &#x27;#su&#x27;)# 模拟用户点击行为su_button.click() 如果要执行一个系列动作，例如模拟拖拽操作，可以创建ActionChains对象，有兴趣的读者可以自行研究。 隐式等待和显式等待这里还有一个细节需要大家知道，网页上的元素可能是动态生成的，在我们使用find_element或find_elements方法获取的时候，可能还没有完成渲染，这时会引发NoSuchElementException错误。为了解决这个问题，我们可以使用隐式等待的方式，通过设置等待时间让浏览器完成对页面元素的渲染。除此之外，我们还可以使用显示等待，通过创建WebDriverWait对象，并设置等待时间和条件，当条件没有满足时，我们可以先等待再尝试进行后续的操作，具体的代码如下所示。 12345678910111213141516171819202122232425from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditionsfrom selenium.webdriver.support.wait import WebDriverWaitbrowser = webdriver.Chrome()# 设置浏览器窗口大小browser.set_window_size(1200, 800)browser.get(&#x27;https://www.baidu.com/&#x27;)# 设置隐式等待时间为10秒browser.implicitly_wait(10)kw_input = browser.find_element(By.ID, &#x27;kw&#x27;)kw_input.send_keys(&#x27;Python&#x27;)su_button = browser.find_element(By.CSS_SELECTOR, &#x27;#su&#x27;)su_button.click()# 创建显示等待对象wait_obj = WebDriverWait(browser, 10)# 设置等待条件（等搜索结果的div出现）wait_obj.until( expected_conditions.presence_of_element_located( (By.CSS_SELECTOR, &#x27;#content_left&#x27;) ))# 截屏browser.get_screenshot_as_file(&#x27;python_result.png&#x27;) 上面设置的等待条件presence_of_element_located表示等待指定元素出现，下面的表格列出了常用的等待条件及其含义。 等待条件 具体含义 title_is / title_contains 标题是指定的内容 &#x2F; 标题包含指定的内容 visibility_of 元素可见 presence_of_element_located 定位的元素加载完成 visibility_of_element_located 定位的元素变得可见 invisibility_of_element_located 定位的元素变得不可见 presence_of_all_elements_located 定位的所有元素加载完成 text_to_be_present_in_element 元素包含指定的内容 text_to_be_present_in_element_value 元素的value属性包含指定的内容 frame_to_be_available_and_switch_to_it 载入并切换到指定的内部窗口 element_to_be_clickable 元素可点击 element_to_be_selected 元素被选中 element_located_to_be_selected 定位的元素被选中 alert_is_present 出现 Alert 弹窗 执行JavaScript代码对于使用瀑布式加载的页面，如果希望在浏览器窗口中加载更多的内容，可以通过浏览器对象的execute_scripts方法执行 JavaScript 代码来实现。对于一些高级的爬取操作，也很有可能会用到类似的操作，如果你的爬虫代码需要 JavaScript 的支持，建议先对 JavaScript 进行适当的了解，尤其是 JavaScript 中的 BOM 和 DOM 操作。我们在上面的代码中截屏之前加入下面的代码，这样就可以利用 JavaScript 将网页滚到最下方。 12# 执行JavaScript代码browser.execute_script(&#x27;document.documentElement.scrollTop = document.documentElement.scrollHeight&#x27;) Selenium反爬的破解有一些网站专门针对 Selenium 设置了反爬措施，因为使用 Selenium 驱动的浏览器，在控制台中可以看到如下所示的webdriver属性值为true，如果要绕过这项检查，可以在加载页面之前，先通过执行 JavaScript 代码将其修改为undefined。 另一方面，我们还可以将浏览器窗口上的“Chrome正受到自动测试软件的控制”隐藏掉，完整的代码如下所示。 1234567891011121314# 创建Chrome参数对象options = webdriver.ChromeOptions()# 添加试验性参数options.add_experimental_option(&#x27;excludeSwitches&#x27;, [&#x27;enable-automation&#x27;])options.add_experimental_option(&#x27;useAutomationExtension&#x27;, False)# 创建Chrome浏览器对象并传入参数browser = webdriver.Chrome(options=options)# 执行Chrome开发者协议命令（在加载页面时执行指定的JavaScript代码）browser.execute_cdp_cmd( &#x27;Page.addScriptToEvaluateOnNewDocument&#x27;, &#123;&#x27;source&#x27;: &#x27;Object.defineProperty(navigator, &quot;webdriver&quot;, &#123;get: () =&gt; undefined&#125;)&#x27;&#125;)browser.set_window_size(1200, 800)browser.get(&#x27;https://www.baidu.com/&#x27;) 无头浏览器很多时候，我们在爬取数据时并不需要看到浏览器窗口，只要有 Chrome 浏览器以及对应的驱动程序，我们的爬虫就能够运转起来。如果不想看到浏览器窗口，我们可以通过下面的方式设置使用无头浏览器。 123options = webdriver.ChromeOptions()options.add_argument(&#x27;--headless&#x27;)browser = webdriver.Chrome(options=options) API参考Selenium 相关的知识还有很多，我们在此就不一一赘述了，下面为大家罗列一些浏览器对象和WebElement对象常用的属性和方法。具体的内容大家还可以参考 Selenium 官方文档的中文翻译。 浏览器对象表1. 常用属性 属性名 描述 current_url 当前页面的URL current_window_handle 当前窗口的句柄（引用） name 浏览器的名称 orientation 当前设备的方向（横屏、竖屏） page_source 当前页面的源代码（包括动态内容） title 当前页面的标题 window_handles 浏览器打开的所有窗口的句柄 表2. 常用方法 方法名 描述 back &#x2F; forward 在浏览历史记录中后退&#x2F;前进 close &#x2F; quit 关闭当前浏览器窗口 &#x2F; 退出浏览器实例 get 加载指定 URL 的页面到浏览器中 maximize_window 将浏览器窗口最大化 refresh 刷新当前页面 set_page_load_timeout 设置页面加载超时时间 set_script_timeout 设置 JavaScript 执行超时时间 implicit_wait 设置等待元素被找到或目标指令完成 get_cookie &#x2F; get_cookies 获取指定的Cookie &#x2F; 获取所有Cookie add_cookie 添加 Cookie 信息 delete_cookie &#x2F; delete_all_cookies 删除指定的 Cookie &#x2F; 删除所有 Cookie find_element &#x2F; find_elements 查找单个元素 &#x2F; 查找一系列元素 WebElement对象表1. WebElement常用属性 属性名 描述 location 元素的位置 size 元素的尺寸 text 元素的文本内容 id 元素的 ID tag_name 元素的标签名 表2. 常用方法 方法名 描述 clear 清空文本框或文本域中的内容 click 点击元素 get_attribute 获取元素的属性值 is_displayed 判断元素对于用户是否可见 is_enabled 判断元素是否处于可用状态 is_selected 判断元素（单选框和复选框）是否被选中 send_keys 模拟输入文本 submit 提交表单 value_of_css_property 获取指定的CSS属性值 find_element &#x2F; find_elements 获取单个子元素 &#x2F; 获取一系列子元素 screenshot 为元素生成快照 简单案例下面的例子演示了如何使用 Selenium 从“360图片”网站搜索和下载图片。 1234567891011121314151617181920212223242526272829303132333435363738394041import osimport timefrom concurrent.futures import ThreadPoolExecutorimport requestsfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import KeysDOWNLOAD_PATH = &#x27;images/&#x27;def download_picture(picture_url: str): &quot;&quot;&quot; 下载保存图片 :param picture_url: 图片的URL &quot;&quot;&quot; filename = picture_url[picture_url.rfind(&#x27;/&#x27;) + 1:] resp = requests.get(picture_url) with open(os.path.join(DOWNLOAD_PATH, filename), &#x27;wb&#x27;) as file: file.write(resp.content)if not os.path.exists(DOWNLOAD_PATH): os.makedirs(DOWNLOAD_PATH)browser = webdriver.Chrome()browser.get(&#x27;https://image.so.com/z?ch=beauty&#x27;)browser.implicitly_wait(10)kw_input = browser.find_element(By.CSS_SELECTOR, &#x27;input[name=q]&#x27;)kw_input.send_keys(&#x27;苍老师&#x27;)kw_input.send_keys(Keys.ENTER)for _ in range(10): browser.execute_script( &#x27;document.documentElement.scrollTop = document.documentElement.scrollHeight&#x27; ) time.sleep(1)imgs = browser.find_elements(By.CSS_SELECTOR, &#x27;div.waterfall img&#x27;)with ThreadPoolExecutor(max_workers=32) as pool: for img in imgs: pic_url = img.get_attribute(&#x27;src&#x27;) pool.submit(download_picture, pic_url) 运行上面的代码，检查指定的目录下是否下载了根据关键词搜索到的图片。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/63.并发编程在爬虫中的应用","date":"2024-12-12T08:38:02.350Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/63.并发编程在爬虫中的应用/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/63.%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%9C%A8%E7%88%AC%E8%99%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","excerpt":"","text":"并发编程在爬虫中的应用之前的课程，我们已经为大家介绍了 Python 中的多线程、多进程和异步编程，通过这三种手段，我们可以实现并发或并行编程，这一方面可以加速代码的执行，另一方面也可以带来更好的用户体验。爬虫程序是典型的 I&#x2F;O 密集型任务，对于 I&#x2F;O 密集型任务来说，多线程和异步 I&#x2F;O 都是很好的选择，因为当程序的某个部分因 I&#x2F;O 操作阻塞时，程序的其他部分仍然可以运转，这样我们不用在等待和阻塞中浪费大量的时间。下面我们以爬取“360图片”网站的图片并保存到本地为例，为大家分别展示使用单线程、多线程和异步 I&#x2F;O 编程的爬虫程序有什么区别，同时也对它们的执行效率进行简单的对比。 “360图片”网站的页面使用了 Ajax 技术，这是很多网站都会使用的一种异步加载数据和局部刷新页面的技术。简单的说，页面上的图片都是通过 JavaScript 代码异步获取 JSON 数据并动态渲染生成的，而且整个页面还使用了瀑布式加载（一边向下滚动，一边加载更多的图片）。我们在浏览器的“开发者工具”中可以找到提供动态内容的数据接口，如下图所示，我们需要的图片信息就在服务器返回的 JSON 数据中。 例如，要获取“美女”频道的图片，我们可以请求如下所示的URL，其中参数ch表示请求的频道，=后面的参数值beauty就代表了“美女”频道，参数sn相当于是页码，0表示第一页（共30张图片），30表示第二页，60表示第三页，以此类推。 1https://image.so.com/zjl?ch=beauty&amp;sn=0 单线程版本通过上面的 URL 下载“美女”频道共90张图片。 12345678910111213141516171819202122232425262728&quot;&quot;&quot;example04.py - 单线程版本爬虫&quot;&quot;&quot;import osimport requestsdef download_picture(url): filename = url[url.rfind(&#x27;/&#x27;) + 1:] resp = requests.get(url) if resp.status_code == 200: with open(f&#x27;images/beauty/&#123;filename&#125;&#x27;, &#x27;wb&#x27;) as file: file.write(resp.content)def main(): if not os.path.exists(&#x27;images/beauty&#x27;): os.makedirs(&#x27;images/beauty&#x27;) for page in range(3): resp = requests.get(f&#x27;https://image.so.com/zjl?ch=beauty&amp;sn=&#123;page * 30&#125;&#x27;) if resp.status_code == 200: pic_dict_list = resp.json()[&#x27;list&#x27;] for pic_dict in pic_dict_list: download_picture(pic_dict[&#x27;qhimg_url&#x27;])if __name__ == &#x27;__main__&#x27;: main() 在 macOS 或 Linux 系统上，我们可以使用time命令来了解上面代码的执行时间以及 CPU 的利用率，如下所示。 1time python3 example04.py 下面是单线程爬虫代码在我的电脑上执行的结果。 1python3 example04.py 2.36s user 0.39s system 12% cpu 21.578 total 这里我们只需要关注代码的总耗时为21.578秒，CPU 利用率为12%。 多线程版本我们使用之前讲到过的线程池技术，将上面的代码修改为多线程版本。 12345678910111213141516171819202122232425262728293031&quot;&quot;&quot;example05.py - 多线程版本爬虫&quot;&quot;&quot;import osfrom concurrent.futures import ThreadPoolExecutorimport requestsdef download_picture(url): filename = url[url.rfind(&#x27;/&#x27;) + 1:] resp = requests.get(url) if resp.status_code == 200: with open(f&#x27;images/beauty/&#123;filename&#125;&#x27;, &#x27;wb&#x27;) as file: file.write(resp.content)def main(): if not os.path.exists(&#x27;images/beauty&#x27;): os.makedirs(&#x27;images/beauty&#x27;) with ThreadPoolExecutor(max_workers=16) as pool: for page in range(3): resp = requests.get(f&#x27;https://image.so.com/zjl?ch=beauty&amp;sn=&#123;page * 30&#125;&#x27;) if resp.status_code == 200: pic_dict_list = resp.json()[&#x27;list&#x27;] for pic_dict in pic_dict_list: pool.submit(download_picture, pic_dict[&#x27;qhimg_url&#x27;])if __name__ == &#x27;__main__&#x27;: main() 执行如下所示的命令。 1time python3 example05.py 代码的执行结果如下所示： 1python3 example05.py 2.65s user 0.40s system 95% cpu 3.193 total 异步I&#x2F;O版本我们使用aiohttp将上面的代码修改为异步 I&#x2F;O 的版本。为了以异步 I&#x2F;O 的方式实现网络资源的获取和写文件操作，我们首先得安装三方库aiohttp和aiofile，命令如下所示。 1pip install aiohttp aiofile aiohttp 的用法在之前的课程中已经做过简要介绍，aiofile模块中的async_open函数跟 Python 内置函数open的用法大致相同，只不过它支持异步操作。下面是异步 I&#x2F;O 版本的爬虫代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344&quot;&quot;&quot;example06.py - 异步I/O版本爬虫&quot;&quot;&quot;import asyncioimport jsonimport osimport aiofileimport aiohttpasync def download_picture(session, url): filename = url[url.rfind(&#x27;/&#x27;) + 1:] async with session.get(url, ssl=False) as resp: if resp.status == 200: data = await resp.read() async with aiofile.async_open(f&#x27;images/beauty/&#123;filename&#125;&#x27;, &#x27;wb&#x27;) as file: await file.write(data)async def fetch_json(): async with aiohttp.ClientSession() as session: for page in range(3): async with session.get( url=f&#x27;https://image.so.com/zjl?ch=beauty&amp;sn=&#123;page * 30&#125;&#x27;, ssl=False ) as resp: if resp.status == 200: json_str = await resp.text() result = json.loads(json_str) for pic_dict in result[&#x27;list&#x27;]: await download_picture(session, pic_dict[&#x27;qhimg_url&#x27;])def main(): if not os.path.exists(&#x27;images/beauty&#x27;): os.makedirs(&#x27;images/beauty&#x27;) loop = asyncio.get_event_loop() loop.run_until_complete(fetch_json()) loop.close()if __name__ == &#x27;__main__&#x27;: main() 执行如下所示的命令。 1time python3 example06.py 代码的执行结果如下所示： 1python3 example06.py 0.82s user 0.21s system 27% cpu 3.782 total 总结通过上面三段代码执行结果的比较，我们可以得出一个结论，使用多线程和异步 I&#x2F;O 都可以改善爬虫程序的性能，因为我们不用将时间浪费在因 I&#x2F;O 操作造成的等待和阻塞上，而time命令的执行结果也告诉我们，单线程的代码 CPU 利用率仅仅只有12%，而多线程版本的 CPU 利用率则高达95%；单线程版本的爬虫执行时间约21秒，而多线程和异步 I&#x2F;O 的版本仅执行了3秒钟。另外，在运行时间差别不大的情况下，多线程的代码比异步 I&#x2F;O 的代码耗费了更多的 CPU 资源，这是因为多线程的调度和切换也需要花费 CPU 时间。至此，三种方式在 I&#x2F;O 密集型任务上的优劣已经一目了然，当然这只是在我的电脑上跑出来的结果。如果网络状况不是很理想或者目标网站响应很慢，那么使用多线程和异步 I&#x2F;O 的优势将更为明显，有兴趣的读者可以自行试验。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/63.Python中的并发编程-3","date":"2024-12-12T08:38:02.348Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/63.Python中的并发编程-3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-3/","excerpt":"","text":"Python中的并发编程-3爬虫是典型的 I&#x2F;O 密集型任务，I&#x2F;O 密集型任务的特点就是程序会经常性的因为 I&#x2F;O 操作而进入阻塞状态，比如我们之前使用requests获取页面代码或二进制内容，发出一个请求之后，程序必须要等待网站返回响应之后才能继续运行，如果目标网站不是很给力或者网络状况不是很理想，那么等待响应的时间可能会很久，而在这个过程中整个程序是一直阻塞在那里，没有做任何的事情。通过前面的课程，我们已经知道了可以通过多线程的方式为爬虫提速，使用多线程的本质就是，当一个线程阻塞的时候，程序还有其他的线程可以继续运转，因此整个程序就不会在阻塞和等待中浪费了大量的时间。 事实上，还有一种非常适合 I&#x2F;O 密集型任务的并发编程方式，我们称之为异步编程，你也可以将它称为异步 I&#x2F;O。这种方式并不需要启动多个线程或多个进程来实现并发，它是通过多个子程序相互协作的方式来提升 CPU 的利用率，解决了 I&#x2F;O 密集型任务 CPU 利用率很低的问题，我一般将这种方式称为“协作式并发”。这里，我不打算探讨操作系统的各种 I&#x2F;O 模式，因为这对很多读者来说都太过抽象；但是我们得先抛出两组概念给大家，一组叫做“阻塞”和“非阻塞”，一组叫做“同步”和“异步”。 基本概念阻塞阻塞状态指程序未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续处理其他的事情，则称该程序在该操作上是阻塞的。阻塞随时都可能发生，最典型的就是 I&#x2F;O 中断（包括网络 I&#x2F;O 、磁盘 I&#x2F;O 、用户输入等）、休眠操作、等待某个线程执行结束，甚至包括在 CPU 切换上下文时，程序都无法真正的执行，这就是所谓的阻塞。 非阻塞程序在等待某操作过程中，自身不被阻塞，可以继续处理其他的事情，则称该程序在该操作上是非阻塞的。非阻塞并不是在任何程序级别、任何情况下都可以存在的。仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态。显然，某个操作的阻塞可能会导程序耗时以及效率低下，所以我们会希望把它变成非阻塞的。 同步不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，我们称这些程序单元是同步执行的。例如前面讲过的给银行账户存钱的操作，我们在代码中使用了“锁”作为通信信号，让多个存钱操作强制排队顺序执行，这就是所谓的同步。 异步不同程序单元在执行过程中无需通信协调，也能够完成一个任务，这种方式我们就称之为异步。例如，使用爬虫下载页面时，调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为。不同网页的下载、保存等操作都是不相关的，也无需相互通知协调。很显然，异步操作的完成时刻和先后顺序并不能确定。 很多人都不太能准确的把握这几个概念，这里我们简单的总结一下，同步与异步的关注点是消息通信机制，最终表现出来的是“有序”和“无序”的区别；阻塞和非阻塞的关注点是程序在等待消息时状态，最终表现出来的是程序在等待时能不能做点别的。如果想深入理解这些内容，推荐大家阅读经典著作《UNIX网络编程》，这本书非常的赞。 生成器和协程前面我们说过，异步编程是一种“协作式并发”，即通过多个子程序相互协作的方式提升 CPU 的利用率，从而减少程序在阻塞和等待中浪费的时间，最终达到并发的效果。我们可以将多个相互协作的子程序称为“协程”，它是实现异步编程的关键。在介绍协程之前，我们先通过下面的代码，看看什么是生成器。 12345def fib(max_count): a, b = 0, 1 for _ in range(max_count): a, b = b, a + b yield a 上面我们编写了一个生成斐波那契数列的生成器，调用上面的fib函数并不是执行该函数获得返回值，因为fib函数中有一个特殊的关键字yield。这个关键字使得fib函数跟普通的函数有些区别，调用该函数会得到一个生成器对象，我们可以通过下面的代码来验证这一点。 12gen_obj = fib(20)print(gen_obj) 输出： 1&lt;generator object fib at 0x106daee40&gt; 我们可以使用内置函数next从生成器对象中获取斐波那契数列的值，也可以通过for-in循环对生成器能够提供的值进行遍历，代码如下所示。 12for value in gen_obj: print(value) 生成器经过预激活，就是一个协程，它可以跟其他子程序协作。 1234567891011121314151617181920def calc_average(): total, counter = 0, 0 avg_value = None while True: curr_value = yield avg_value total += curr_value counter += 1 avg_value = total / counterdef main(): obj = calc_average() # 生成器预激活 obj.send(None) for _ in range(5): print(obj.send(float(input())))if __name__ == &#x27;__main__&#x27;: main() 上面的main函数首先通过生成器对象的send方法发送一个None值来将其激活为协程，也可以通过next(obj)达到同样的效果。接下来，协程对象会接收main函数发送的数据并产出（yield）数据的平均值。通过上面的例子，不知道大家是否看出两段子程序是怎么“协作”的。 异步函数Python 3.5版本中，引入了两个非常有意思的元素，一个叫async，一个叫await，它们在Python 3.7版本中成为了正式的关键字。通过这两个关键字，可以简化协程代码的编写，可以用更为简单的方式让多个子程序很好的协作起来。我们通过一个例子来加以说明，请大家先看看下面的代码。 123456789101112131415161718import timedef display(num): time.sleep(1) print(num)def main(): start = time.time() for i in range(1, 10): display(i) end = time.time() print(f&#x27;&#123;end - start:.3f&#125;秒&#x27;)if __name__ == &#x27;__main__&#x27;: main() 上面的代码每次执行都会依次输出1到9的数字，每个间隔1秒钟，整个代码需要执行大概需要9秒多的时间，这一点我相信大家都能看懂。不知道大家是否意识到，这段代码就是以同步和阻塞的方式执行的，同步可以从代码的输出看出来，而阻塞是指在调用display函数发生休眠时，整个代码的其他部分都不能继续执行，必须等待休眠结束。 接下来，我们尝试用异步的方式改写上面的代码，让display函数以异步的方式运转。 123456789101112131415161718192021import asyncioimport timeasync def display(num): await asyncio.sleep(1) print(num)def main(): start = time.time() objs = [display(i) for i in range(1, 10)] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(objs)) loop.close() end = time.time() print(f&#x27;&#123;end - start:.3f&#125;秒&#x27;)if __name__ == &#x27;__main__&#x27;: main() Python 中的asyncio模块提供了对异步 I&#x2F;O 的支持。上面的代码中，我们首先在display函数前面加上了async关键字使其变成一个异步函数，调用异步函数不会执行函数体而是获得一个协程对象。我们将display函数中的time.sleep(1)修改为await asyncio.sleep(1)，二者的区别在于，后者不会让整个代码陷入阻塞，因为await操作会让其他协作的子程序有获得 CPU 资源而得以运转的机会。为了让这些子程序可以协作起来，我们需要将他们放到一个事件循环（实现消息分派传递的系统）上，因为当协程遭遇 I&#x2F;O 操作阻塞时，就会到事件循环中监听 I&#x2F;O 操作是否完成，并注册自身的上下文以及自身的唤醒函数（以便恢复执行），之后该协程就变为阻塞状态。上面的第12行代码创建了9个协程对象并放到一个列表中，第13行代码通过asyncio模块的get_event_loop函数获得了系统的事件循环，第14行通过asyncio模块的run_until_complete函数将协程对象挂载到事件循环上。执行上面的代码会发现，9个分别会阻塞1秒钟的协程总共只阻塞了约1秒种的时间，因为阻塞的协程对象会放弃对 CPU 的占有而不是让 CPU 处于闲置状态，这种方式大大的提升了 CPU 的利用率。而且我们还会注意到，数字并不是按照从1到9的顺序打印输出的，这正是我们想要的结果，说明它们是异步执行的。对于爬虫这样的 I&#x2F;O 密集型任务来说，这种协作式并发在很多场景下是比使用多线程更好的选择，因为这种做法减少了管理和维护多个线程以及多个线程切换所带来的开销。 aiohttp库我们之前使用的requests三方库并不支持异步 I&#x2F;O，如果希望使用异步 I&#x2F;O 的方式来加速爬虫代码的执行，我们可以安装和使用名为aiohttp的三方库。 安装aiohttp。 1pip install aiohttp 下面的代码使用aiohttp抓取了10个网站的首页并解析出它们的标题。 123456789101112131415161718192021222324252627282930313233343536373839404142import asyncioimport reimport aiohttpfrom aiohttp import ClientSessionTITLE_PATTERN = re.compile(r&#x27;&lt;title.*?&gt;(.*?)&lt;/title&gt;&#x27;, re.DOTALL)async def fetch_page_title(url): async with aiohttp.ClientSession(headers=&#123; &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36&#x27;, &#125;) as session: # type: ClientSession async with session.get(url, ssl=False) as resp: if resp.status == 200: html_code = await resp.text() matcher = TITLE_PATTERN.search(html_code) title = matcher.group(1).strip() print(title)def main(): urls = [ &#x27;https://www.python.org/&#x27;, &#x27;https://www.jd.com/&#x27;, &#x27;https://www.baidu.com/&#x27;, &#x27;https://www.taobao.com/&#x27;, &#x27;https://git-scm.com/&#x27;, &#x27;https://www.sohu.com/&#x27;, &#x27;https://gitee.com/&#x27;, &#x27;https://www.amazon.com/&#x27;, &#x27;https://www.usa.gov/&#x27;, &#x27;https://www.nasa.gov/&#x27; ] objs = [fetch_page_title(url) for url in urls] loop = asyncio.get_event_loop() loop.run_until_complete(asyncio.wait(objs)) loop.close()if __name__ == &#x27;__main__&#x27;: main() 输出： 12345678910京东(JD.COM)-正品低价、品质保障、配送及时、轻松购物！搜狐淘宝网 - 淘！我喜欢百度一下，你就知道Gitee - 基于 Git 的代码托管和研发协作平台GitNASAOfficial Guide to Government Information and Services &amp;#124; USAGovAmazon.com. Spend less. Smile more.Welcome to Python.org 从上面的输出可以看出，网站首页标题的输出顺序跟它们的 URL 在列表中的顺序没有关系。代码的第11行到第13行创建了ClientSession对象，通过它的get方法可以向指定的 URL 发起请求，如第14行所示，跟requests中的Session对象并没有本质区别，唯一的区别是这里使用了异步上下文。代码第16行的await会让因为 I&#x2F;O 操作阻塞的子程序放弃对 CPU 的占用，这使得其他的子程序可以运转起来去抓取页面。代码的第17行和第18行使用了正则表达式捕获组操作解析网页标题。fetch_page_title是一个被async关键字修饰的异步函数，调用该函数会获得协程对象，如代码第35行所示。后面的代码跟之前的例子没有什么区别，相信大家能够理解。 大家可以尝试将aiohttp换回到requests，看看不使用异步 I&#x2F;O 也不使用多线程，到底和上面的代码有什么区别，相信通过这样的对比，大家能够更深刻的理解我们之前强调的几个概念：同步和异步，阻塞和非阻塞。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/63.Python中的并发编程-2","date":"2024-12-12T08:38:02.346Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/63.Python中的并发编程-2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-2/","excerpt":"","text":"Python中的并发编程-2在上一课中我们说过，由于 GIL 的存在，CPython 中的多线程并不能发挥 CPU 的多核优势，如果希望突破 GIL 的限制，可以考虑使用多进程。对于多进程的程序，每个进程都有一个属于自己的 GIL，所以多进程不会受到 GIL 的影响。那么，我们应该如何在 Python 程序中创建和使用多进程呢？ ###创建进程 在 Python 中可以基于Process类来创建进程，虽然进程和线程有着本质的差别，但是Process类和Thread类的用法却非常类似。在使用Process类的构造器创建对象时，也是通过target参数传入一个函数来指定进程要执行的代码，而args和kwargs参数可以指定该函数使用的参数值。 12345678910111213141516171819202122232425262728293031from multiprocessing import Process, current_processfrom time import sleepdef sub_task(content, nums): # 通过current_process函数获取当前进程对象 # 通过进程对象的pid和name属性获取进程的ID号和名字 print(f&#x27;PID: &#123;current_process().pid&#125;&#x27;) print(f&#x27;Name: &#123;current_process().name&#125;&#x27;) # 通过下面的输出不难发现，每个进程都有自己的nums列表，进程之间本就不共享内存 # 在创建子进程时复制了父进程的数据结构，三个进程从列表中pop(0)得到的值都是20 counter, total = 0, nums.pop(0) print(f&#x27;Loop count: &#123;total&#125;&#x27;) sleep(0.5) while counter &lt; total: counter += 1 print(f&#x27;&#123;counter&#125;: &#123;content&#125;&#x27;) sleep(0.01)def main(): nums = [20, 30, 40] # 创建并启动进程来执行指定的函数 Process(target=sub_task, args=(&#x27;Ping&#x27;, nums)).start() Process(target=sub_task, args=(&#x27;Pong&#x27;, nums)).start() # 在主进程中执行sub_task函数 sub_task(&#x27;Good&#x27;, nums)if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码通过current_process函数获取当前进程对象，再通过进程对象的pid属性获取进程ID。在 Python 中，使用os模块的getpid函数也可以达到同样的效果。 如果愿意，也可以使用os模块的fork函数来创建进程，调用该函数时，操作系统自动把当前进程（父进程）复制一份（子进程），父进程的fork函数会返回子进程的ID，而子进程中的fork函数会返回0，也就是说这个函数调用一次会在父进程和子进程中得到两个不同的返回值。需要注意的是，Windows 系统并不支持fork函数，如果你使用的是 Linux 或 macOS 系统，可以试试下面的代码。 12345678910import osprint(f&#x27;PID: &#123;os.getpid()&#125;&#x27;)pid = os.fork()if pid == 0: print(f&#x27;子进程 - PID: &#123;os.getpid()&#125;&#x27;) print(&#x27;Todo: 在子进程中执行的代码&#x27;)else: print(f&#x27;父进程 - PID: &#123;os.getpid()&#125;&#x27;) print(&#x27;Todo: 在父进程中执行的代码&#x27;) 简而言之，我们还是推荐大家通过直接使用Process类、继承Process类和使用进程池（ProcessPoolExecutor）这三种方式来创建和使用多进程，这三种方式不同于上面的fork函数，能够保证代码的兼容性和可移植性。具体的做法跟之前讲过的创建和使用多线程的方式比较接近，此处不再进行赘述。 多进程和多线程的比较对于爬虫这类 I&#x2F;O 密集型任务来说，使用多进程并没有什么优势；但是对于计算密集型任务来说，多进程相比多线程，在效率上会有显著的提升，我们可以通过下面的代码来加以证明。下面的代码会通过多线程和多进程两种方式来判断一组大整数是不是质数，很显然这是一个计算密集型任务，我们将任务分别放到多个线程和多个进程中来加速代码的执行，让我们看看多线程和多进程的代码具体表现有何不同。 我们先实现一个多线程的版本，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536import concurrent.futuresPRIMES = [ 1116281, 1297337, 104395303, 472882027, 533000389, 817504243, 982451653, 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419] * 5def is_prime(n): &quot;&quot;&quot;判断素数&quot;&quot;&quot; for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return n != 1def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print(&#x27;%d is prime: %s&#x27; % (number, prime))if __name__ == &#x27;__main__&#x27;: main() 假设上面的代码保存在名为example.py的文件中，在 Linux 或 macOS 系统上，可以使用time python example.py命令执行程序并获得操作系统关于执行时间的统计，在我的 macOS 上，某次的运行结果的最后一行输出如下所示。 1python example09.py 38.69s user 1.01s system 101% cpu 39.213 total 从运行结果可以看出，多线程的代码只能让 CPU 利用率达到100%，这其实已经证明了多线程的代码无法利用 CPU 多核特性来加速代码的执行，我们再看看多进程的版本，我们将上面代码中的线程池（ThreadPoolExecutor）更换为进程池（ProcessPoolExecutor）。 多进程的版本。 123456789101112131415161718192021222324252627282930313233343536import concurrent.futuresPRIMES = [ 1116281, 1297337, 104395303, 472882027, 533000389, 817504243, 982451653, 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419] * 5def is_prime(n): &quot;&quot;&quot;判断素数&quot;&quot;&quot; for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return n != 1def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print(&#x27;%d is prime: %s&#x27; % (number, prime))if __name__ == &#x27;__main__&#x27;: main() 提示：运行上面的代码时，可以通过操作系统的任务管理器（资源监视器）来查看是否启动了多个 Python 解释器进程。 我们仍然通过time python example.py的方式来执行上述代码，运行结果的最后一行如下所示。 1python example09.py 106.63s user 0.57s system 389% cpu 27.497 total 可以看出，多进程的版本在我使用的这台电脑上，让 CPU 的利用率达到了将近400%，而运行代码时用户态耗费的 CPU 的时间（106.63秒）几乎是代码运行总时间（27.497秒）的4倍，从这两点都可以看出，我的电脑使用了一款4核的 CPU。当然，要知道自己的电脑有几个 CPU 或几个核，可以直接使用下面的代码。 123import osprint(os.cpu_count()) 综上所述，多进程可以突破 GIL 的限制，充分利用 CPU 多核特性，对于计算密集型任务，这一点是相当重要的。常见的计算密集型任务包括科学计算、图像处理、音视频编解码等，如果这些计算密集型任务本身是可以并行的，那么使用多进程应该是更好的选择。 进程间通信在讲解进程间通信之前，先给大家一个任务：启动两个进程，一个输出“Ping”，一个输出“Pong”，两个进程输出的“Ping”和“Pong”加起来一共有50个时，就结束程序。听起来是不是非常简单，但是实际编写代码时，由于多个进程之间不能够像多个线程之间直接通过共享内存的方式交换数据，所以下面的代码是达不到我们想要的结果的。 123456789101112131415161718192021from multiprocessing import Processfrom time import sleepcounter = 0def sub_task(string): global counter while counter &lt; 50: print(string, end=&#x27;&#x27;, flush=True) counter += 1 sleep(0.01) def main(): Process(target=sub_task, args=(&#x27;Ping&#x27;, )).start() Process(target=sub_task, args=(&#x27;Pong&#x27;, )).start()if __name__ == &#x27;__main__&#x27;: main() 上面的代码看起来没毛病，但是最后的结果是“Ping”和“Pong”各输出了50个。再次提醒大家，当我们在程序中创建进程的时候，子进程会复制父进程及其所有的数据结构，每个子进程有自己独立的内存空间，这也就意味着两个子进程中各有一个counter变量，它们都会从0加到50，所以结果就可想而知了。要解决这个问题比较简单的办法是使用multiprocessing模块中的Queue类，它是可以被多个进程共享的队列，底层是通过操作系统底层的管道和信号量（semaphore）机制来实现的，代码如下所示。 12345678910111213141516171819202122232425262728import timefrom multiprocessing import Process, Queuedef sub_task(content, queue): counter = queue.get() while counter &lt; 50: print(content, end=&#x27;&#x27;, flush=True) counter += 1 queue.put(counter) time.sleep(0.01) counter = queue.get()def main(): queue = Queue() queue.put(0) p1 = Process(target=sub_task, args=(&#x27;Ping&#x27;, queue)) p1.start() p2 = Process(target=sub_task, args=(&#x27;Pong&#x27;, queue)) p2.start() while p1.is_alive() and p2.is_alive(): pass queue.put(50)if __name__ == &#x27;__main__&#x27;: main() 提示：multiprocessing.Queue对象的get方法默认在队列为空时是会阻塞的，直到获取到数据才会返回。如果不希望该方法阻塞以及需要指定阻塞的超时时间，可以通过指定block和timeout参数进行设定。 上面的代码通过Queue类的get和put方法让三个进程（p1、p2和主进程）实现了数据的共享，这就是所谓的进程间的通信，通过这种方式，当Queue中取出的值已经大于等于50时，p1和p2就会跳出while循环，从而终止进程的执行。代码第22行的循环是为了等待p1和p2两个进程中的一个结束，这时候主进程还需要向Queue中放置一个大于等于50的值，这样另一个尚未结束的进程也会因为读到这个大于等于50的值而终止。 进程间通信的方式还有很多，比如使用套接字也可以实现两个进程的通信，甚至于这两个进程并不在同一台主机上，有兴趣的读者可以自行了解。 简单的总结在 Python 中，我们还可以通过subprocess模块的call函数执行其他的命令来创建子进程，相当于就是在我们的程序中调用其他程序，这里我们暂不探讨这些知识，有兴趣的读者可以自行研究。 对于Python开发者来说，以下情况需要考虑使用多线程： 程序需要维护许多共享的状态（尤其是可变状态），Python 中的列表、字典、集合都是线程安全的（多个线程同时操作同一个列表、字典或集合，不会引发错误和数据问题），所以使用线程而不是进程维护共享状态的代价相对较小。 程序会花费大量时间在 I&#x2F;O 操作上，没有太多并行计算的需求且不需占用太多的内存。 那么在遇到下列情况时，应该考虑使用多进程： 程序执行计算密集型任务（如：音视频编解码、数据压缩、科学计算等）。 程序的输入可以并行的分成块，并且可以将运算结果合并。 程序在内存使用方面没有任何限制且不强依赖于 I&#x2F;O 操作（如读写文件、套接字等）。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/63.Python中的并发编程-1","date":"2024-12-12T08:38:02.343Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/63.Python中的并发编程-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/63.Python%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-1/","excerpt":"","text":"Python中的并发编程-1现如今，我们使用的计算机早已是多 CPU 或多核的计算机，而我们使用的操作系统基本都支持“多任务”，这使得我们可以同时运行多个程序，也可以将一个程序分解为若干个相对独立的子任务，让多个子任务“并行”或“并发”的执行，从而缩短程序的执行时间，同时也让用户获得更好的体验。因此当下，不管用什么编程语言进行开发，实现“并行”或“并发”编程已经成为了程序员的标配技能。为了讲述如何在 Python 程序中实现“并行”或“并发”，我们需要先了解两个重要的概念：进程和线程。 线程和进程我们通过操作系统运行一个程序会创建出一个或多个进程，进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。简单的说，进程是操作系统分配存储空间的基本单位，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据；操作系统管理所有进程的执行，为它们合理的分配资源。一个进程可以通过 fork 或 spawn 的方式创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此两个进程如果要共享数据，必须通过进程间通信机制来实现，具体的方式包括管道、信号、套接字等。 一个进程还可以拥有多个执行线索，简单的说就是拥有多个可以获得 CPU 调度的执行单元，这就是所谓的线程。由于线程在同一个进程下，它们可以共享相同的上下文，因此相对于进程而言，线程间的信息共享和通信更加容易。当然在单核 CPU 系统中，多个线程不可能同时执行，因为在某个时刻只有一个线程能够获得 CPU，多个线程通过共享 CPU 执行时间的方式来达到并发的效果。 在程序中使用多线程技术通常都会带来不言而喻的好处，最主要的体现在提升程序的性能和改善用户体验，今天我们使用的软件几乎都用到了多线程技术，这一点可以利用系统自带的进程监控工具（如 macOS 中的“活动监视器”、Windows 中的“任务管理器”）来证实，如下图所示。 这里，我们还需要跟大家再次强调两个概念：并发（concurrency）和并行（parallel）。并发通常是指同一时刻只能有一条指令执行，但是多个线程对应的指令被快速轮换地执行。比如一个处理器，它先执行线程 A 的指令一段时间，再执行线程 B 的指令一段时间，再切回到线程 A 执行一段时间。由于处理器执行指令的速度和切换的速度极快，人们完全感知不到计算机在这个过程中有多个线程切换上下文执行的操作，这就使得宏观上看起来多个线程在同时运行，但微观上其实只有一个线程在执行。并行是指同一时刻，有多条指令在多个处理器上同时执行，并行必须要依赖于多个处理器，不论是从宏观上还是微观上，多个线程可以在同一时刻一起执行的。很多时候，我们并不用严格区分并发和并行两个词，所以我们有时候也把 Python 中的多线程、多进程以及异步 I&#x2F;O 都视为实现并发编程的手段，但实际上前面两者也可以实现并行编程，当然这里还有一个全局解释器锁（GIL）的问题，我们稍后讨论。 多线程编程Python 标准库中threading模块的Thread类可以帮助我们非常轻松的实现多线程编程。我们用一个联网下载文件的例子来对比使用多线程和不使用多线程到底有什么区别，代码如下所示。 不使用多线程的下载。 123456789101112131415161718192021222324import randomimport timedef download(*, filename): start = time.time() print(f&#x27;开始下载 &#123;filename&#125;.&#x27;) time.sleep(random.randint(3, 6)) print(f&#x27;&#123;filename&#125; 下载完成.&#x27;) end = time.time() print(f&#x27;下载耗时: &#123;end - start:.3f&#125;秒.&#x27;)def main(): start = time.time() download(filename=&#x27;Python从入门到住院.pdf&#x27;) download(filename=&#x27;MySQL从删库到跑路.avi&#x27;) download(filename=&#x27;Linux从精通到放弃.mp4&#x27;) end = time.time() print(f&#x27;总耗时: &#123;end - start:.3f&#125;秒.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码并没有真正实现联网下载的功能，而是通过time.sleep()休眠一段时间来模拟下载文件需要一些时间上的开销，跟实际下载的状况比较类似。 运行上面的代码，可以得到如下所示的运行结果。可以看出，当我们的程序只有一个工作线程时，每个下载任务都需要等待上一个下载任务执行结束才能开始，所以程序执行的总耗时是三个下载任务各自执行时间的总和。 12345678910开始下载Python从入门到住院.pdf.Python从入门到住院.pdf下载完成.下载耗时: 3.005秒.开始下载MySQL从删库到跑路.avi.MySQL从删库到跑路.avi下载完成.下载耗时: 5.006秒.开始下载Linux从精通到放弃.mp4.Linux从精通到放弃.mp3下载完成.下载耗时: 6.007秒.总耗时: 14.018秒. 事实上，上面的三个下载任务之间并没有逻辑上的因果关系，三者是可以“并发”的，下一个下载任务没有必要等待上一个下载任务结束，为此，我们可以使用多线程编程来改写上面的代码。 123456789101112131415161718192021222324252627282930313233import randomimport timefrom threading import Threaddef download(*, filename): start = time.time() print(f&#x27;开始下载 &#123;filename&#125;.&#x27;) time.sleep(random.randint(3, 6)) print(f&#x27;&#123;filename&#125; 下载完成.&#x27;) end = time.time() print(f&#x27;下载耗时: &#123;end - start:.3f&#125;秒.&#x27;)def main(): threads = [ Thread(target=download, kwargs=&#123;&#x27;filename&#x27;: &#x27;Python从入门到住院.pdf&#x27;&#125;), Thread(target=download, kwargs=&#123;&#x27;filename&#x27;: &#x27;MySQL从删库到跑路.avi&#x27;&#125;), Thread(target=download, kwargs=&#123;&#x27;filename&#x27;: &#x27;Linux从精通到放弃.mp4&#x27;&#125;) ] start = time.time() # 启动三个线程 for thread in threads: thread.start() # 等待线程结束 for thread in threads: thread.join() end = time.time() print(f&#x27;总耗时: &#123;end - start:.3f&#125;秒.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 某次的运行结果如下所示。 12345678910开始下载 Python从入门到住院.pdf.开始下载 MySQL从删库到跑路.avi.开始下载 Linux从精通到放弃.mp4.MySQL从删库到跑路.avi 下载完成.下载耗时: 3.005秒.Python从入门到住院.pdf 下载完成.下载耗时: 5.006秒.Linux从精通到放弃.mp4 下载完成.下载耗时: 6.003秒.总耗时: 6.004秒. 通过上面的运行结果可以发现，整个程序的执行时间几乎等于耗时最长的一个下载任务的执行时间，这也就意味着，三个下载任务是并发执行的，不存在一个等待另一个的情况，这样做很显然提高了程序的执行效率。简单的说，如果程序中有非常耗时的执行单元，而这些耗时的执行单元之间又没有逻辑上的因果关系，即 B 单元的执行不依赖于 A 单元的执行结果，那么 A 和 B 两个单元就可以放到两个不同的线程中，让他们并发的执行。这样做的好处除了减少程序执行的等待时间，还可以带来更好的用户体验，因为一个单元的阻塞不会造成程序的“假死”，因为程序中还有其他的单元是可以运转的。 使用 Thread 类创建线程对象通过上面的代码可以看出，直接使用Thread类的构造器就可以创建线程对象，而线程对象的start()方法可以启动一个线程。线程启动后会执行target参数指定的函数，当然前提是获得 CPU 的调度；如果target指定的线程要执行的目标函数有参数，需要通过args参数为其进行指定，对于关键字参数，可以通过kwargs参数进行传入。Thread类的构造器还有很多其他的参数，我们遇到的时候再为大家进行讲解，目前需要大家掌握的，就是target、args和kwargs。 继承 Thread 类自定义线程除了上面的代码展示的创建线程的方式外，还可以通过继承Thread类并重写run()方法的方式来自定义线程，具体的代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839import randomimport timefrom threading import Threadclass DownloadThread(Thread): def __init__(self, filename): self.filename = filename super().__init__() def run(self): start = time.time() print(f&#x27;开始下载 &#123;self.filename&#125;.&#x27;) time.sleep(random.randint(3, 6)) print(f&#x27;&#123;self.filename&#125; 下载完成.&#x27;) end = time.time() print(f&#x27;下载耗时: &#123;end - start:.3f&#125;秒.&#x27;)def main(): threads = [ DownloadThread(&#x27;Python从入门到住院.pdf&#x27;), DownloadThread(&#x27;MySQL从删库到跑路.avi&#x27;), DownloadThread(&#x27;Linux从精通到放弃.mp4&#x27;) ] start = time.time() # 启动三个线程 for thread in threads: thread.start() # 等待线程结束 for thread in threads: thread.join() end = time.time() print(f&#x27;总耗时: &#123;end - start:.3f&#125;秒.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 使用线程池我们还可以通过线程池的方式将任务放到多个线程中去执行，通过线程池来使用线程应该是多线程编程最理想的选择。事实上，线程的创建和释放都会带来较大的开销，频繁的创建和释放线程通常都不是很好的选择。利用线程池，可以提前准备好若干个线程，在使用的过程中不需要再通过自定义的代码创建和释放线程，而是直接复用线程池中的线程。Python 内置的concurrent.futures模块提供了对线程池的支持，代码如下所示。 123456789101112131415161718192021222324252627import randomimport timefrom concurrent.futures import ThreadPoolExecutorfrom threading import Threaddef download(*, filename): start = time.time() print(f&#x27;开始下载 &#123;filename&#125;.&#x27;) time.sleep(random.randint(3, 6)) print(f&#x27;&#123;filename&#125; 下载完成.&#x27;) end = time.time() print(f&#x27;下载耗时: &#123;end - start:.3f&#125;秒.&#x27;)def main(): with ThreadPoolExecutor(max_workers=4) as pool: filenames = [&#x27;Python从入门到住院.pdf&#x27;, &#x27;MySQL从删库到跑路.avi&#x27;, &#x27;Linux从精通到放弃.mp4&#x27;] start = time.time() for filename in filenames: pool.submit(download, filename=filename) end = time.time() print(f&#x27;总耗时: &#123;end - start:.3f&#125;秒.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 守护线程所谓“守护线程”就是在主线程结束的时候，不值得再保留的执行线程。这里的不值得保留指的是守护线程会在其他非守护线程全部运行结束之后被销毁，它守护的是当前进程内所有的非守护线程。简单的说，守护线程会跟随主线程一起挂掉，而主线程的生命周期就是一个进程的生命周期。如果不理解，我们可以看一段简单的代码。 1234567891011121314151617import timefrom threading import Threaddef display(content): while True: print(content, end=&#x27;&#x27;, flush=True) time.sleep(0.1)def main(): Thread(target=display, args=(&#x27;Ping&#x27;, )).start() Thread(target=display, args=(&#x27;Pong&#x27;, )).start()if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码中，我们将print函数的参数flush设置为True，这是因为flush参数的值如果为False，而print又没有做换行处理，就会导致每次print输出的内容被放到操作系统的输出缓冲区，直到缓冲区被输出的内容塞满，才会清空缓冲区产生一次输出。上述现象是操作系统为了减少 I&#x2F;O 中断，提升 CPU 利用率做出的设定，为了让代码产生直观交互，我们才将flush参数设置为True，强制每次输出都清空输出缓冲区。 上面的代码运行起来之后是不会停止的，因为两个子线程中都有死循环，除非你手动中断代码的执行。但是，如果在创建线程对象时，将名为daemon的参数设置为True，这两个线程就会变成守护线程，那么在其他线程结束时，即便有死循环，两个守护线程也会挂掉，不会再继续执行下去，代码如下所示。 123456789101112131415161718import timefrom threading import Threaddef display(content): while True: print(content, end=&#x27;&#x27;, flush=True) time.sleep(0.1)def main(): Thread(target=display, args=(&#x27;Ping&#x27;, ), daemon=True).start() Thread(target=display, args=(&#x27;Pong&#x27;, ), daemon=True).start() time.sleep(5)if __name__ == &#x27;__main__&#x27;: main() 上面的代码，我们在主线程中添加了一行time.sleep(5)让主线程休眠5秒，在这个过程中，输出Ping和Pong的守护线程会持续运转，直到主线程在5秒后结束，这两个守护线程也被销毁，不再继续运行。 思考：如果将上面代码第12行的daemon=True去掉，代码会怎样执行？有兴趣的读者可以尝试一下，并看看实际执行的结果跟你想象的是否一致。 资源竞争在编写多线程代码时，不可避免的会遇到多个线程竞争同一个资源（对象）的情况。在这种情况下，如果没有合理的机制来保护被竞争的资源，那么就有可能出现非预期的状况。下面的代码创建了100个线程向同一个银行账户（初始余额为0元）转账，每个线程转账金额为1元。在正常的情况下，我们的银行账户最终的余额应该是100元，但是运行下面的代码我们并不能得到100元这个结果。 1234567891011121314151617181920212223242526272829import timefrom concurrent.futures import ThreadPoolExecutorclass Account(object): &quot;&quot;&quot;银行账户&quot;&quot;&quot; def __init__(self): self.balance = 0.0 def deposit(self, money): &quot;&quot;&quot;存钱&quot;&quot;&quot; new_balance = self.balance + money time.sleep(0.01) self.balance = new_balancedef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; account = Account() with ThreadPoolExecutor(max_workers=16) as pool: for _ in range(100): pool.submit(account.deposit, 1) print(account.balance)if __name__ == &#x27;__main__&#x27;: main() 上面代码中的Account类代表了银行账户，它的deposit方法代表存款行为，参数money代表存入的金额，该方法通过time.sleep函数模拟受理存款需要一段时间。我们通过线程池的方式启动了100个线程向一个账户转账，但是上面的代码并不能运行出100这个我们期望的结果，这就是在多个线程竞争一个资源的时候，可能会遇到的数据不一致的问题。注意上面代码的第14行，当多个线程都执行到这行代码时，它们会在相同的余额上执行加上存入金额的操作，这就会造成“丢失更新”现象，即之前修改数据的成果被后续的修改给覆盖掉了，所以才得不到正确的结果。 要解决上面的问题，可以使用锁机制，通过锁对操作数据的关键代码加以保护。Python 标准库的threading模块提供了Lock和RLock类来支持锁机制，这里我们不去深究二者的区别，建议大家直接使用RLock。接下来，我们给银行账户添加一个锁对象，通过锁对象来解决刚才存款时发生“丢失更新”的问题，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536import timefrom concurrent.futures import ThreadPoolExecutorfrom threading import RLockclass Account(object): &quot;&quot;&quot;银行账户&quot;&quot;&quot; def __init__(self): self.balance = 0.0 self.lock = RLock() def deposit(self, money): # 获得锁 self.lock.acquire() try: new_balance = self.balance + money time.sleep(0.01) self.balance = new_balance finally: # 释放锁 self.lock.release()def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; account = Account() with ThreadPoolExecutor(max_workers=16) as pool: for _ in range(100): pool.submit(account.deposit, 1) print(account.balance)if __name__ == &#x27;__main__&#x27;: main() 上面代码中，获得锁和释放锁的操作也可以通过上下文语法来实现，使用上下文语法会让代码更加简单优雅，这也是我们推荐大家使用的方式。 1234567891011121314151617181920212223242526272829303132import timefrom concurrent.futures import ThreadPoolExecutorfrom threading import RLockclass Account(object): &quot;&quot;&quot;银行账户&quot;&quot;&quot; def __init__(self): self.balance = 0.0 self.lock = RLock() def deposit(self, money): # 通过上下文语法获得锁和释放锁 with self.lock: new_balance = self.balance + money time.sleep(0.01) self.balance = new_balancedef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; account = Account() with ThreadPoolExecutor(max_workers=16) as pool: for _ in range(100): pool.submit(account.deposit, 1) print(account.balance)if __name__ == &#x27;__main__&#x27;: main() 思考：将上面的代码修改为5个线程向银行账户存钱，5个线程从银行账户取钱，取钱的线程在银行账户余额不足时，需要停下来等待存钱的线程将钱存入后再尝试取钱。这里需要用到线程调度的知识，大家可以自行研究下threading模块中的Condition类，看看是否能够完成这个任务。 GIL问题如果使用官方的 Python 解释器（通常称之为 CPython）运行 Python 程序，我们并不能通过使用多线程的方式将 CPU 的利用率提升到逼近400%（对于4核 CPU）或逼近800%（对于8核 CPU）这样的水平，因为 CPython 在执行代码时，会受到 GIL（全局解释器锁）的限制。具体的说，CPython 在执行任何代码时，都需要对应的线程先获得 GIL，然后每执行100条（字节码）指令，CPython 就会让获得 GIL 的线程主动释放 GIL，这样别的线程才有机会执行。因为 GIL 的存在，无论你的 CPU 有多少个核，我们编写的 Python 代码也没有机会真正并行的执行。 GIL 是官方 Python 解释器在设计上的历史遗留问题，要解决这个问题，让多线程能够发挥 CPU 的多核优势，需要重新实现一个不带 GIL 的 Python 解释器。这个问题按照官方的说法，在 Python 发布4.0版本时会得到解决，就让我们拭目以待吧。当下，对于 CPython 而言，如果希望充分发挥 CPU 的多核优势，可以考虑使用多进程，因为每个进程都对应一个 Python 解释器，因此每个进程都有自己独立的 GIL，这样就可以突破 GIL 的限制。在下一个章节中，我们会为大家介绍关于多进程的相关知识，并对多线程和多进程的代码及其执行效果进行比较。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/62.用Python解析HTML页面-2","date":"2024-12-12T08:38:02.340Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/62.用Python解析HTML页面-2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/62.%E7%94%A8Python%E8%A7%A3%E6%9E%90HTML%E9%A1%B5%E9%9D%A2-2/","excerpt":"","text":"用Python解析HTML页面在前面的课程中，我们讲到了使用request三方库获取网络资源，还介绍了一些前端的基础知识。接下来，我们继续探索如何解析 HTML 代码，从页面中提取出有用的信息。之前，我们尝试过用正则表达式的捕获组操作提取页面内容，但是写出一个正确的正则表达式也是一件让人头疼的事情。为了解决这个问题，我们得先深入的了解一下 HTML 页面的结构，并在此基础上研究另外的解析页面的方法。 HTML 页面的结构我们在浏览器中打开任意一个网站，然后通过鼠标右键菜单，选择“显示网页源代码”菜单项，就可以看到网页对应的 HTML 代码。 代码的第1行是文档类型声明，第2行的&lt;html&gt;标签是整个页面根标签的开始标签，最后一行是根标签的结束标签&lt;/html&gt;。&lt;html&gt;标签下面有两个子标签&lt;head&gt;和&lt;body&gt;，放在&lt;body&gt;标签下的内容会显示在浏览器窗口中，这部分内容是网页的主体；放在&lt;head&gt;标签下的内容不会显示在浏览器窗口中，但是却包含了页面重要的元信息，通常称之为网页的头部。HTML 页面大致的代码结构如下所示。 123456789&lt;!doctype html&gt;&lt;html&gt; &lt;head&gt; &lt;!-- 页面的元信息，如字符编码、标题、关键字、媒体查询等 --&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 页面的主体，显示在浏览器窗口中的内容 --&gt; &lt;/body&gt;&lt;/html&gt; 标签、层叠样式表（CSS）、JavaScript 是构成 HTML 页面的三要素，其中标签用来承载页面要显示的内容，CSS 负责对页面的渲染，而 JavaScript 用来控制页面的交互式行为。要实现 HTML 页面的解析，可以使用 XPath 的语法，它原本是 XML 的一种查询语法，可以根据 HTML 标签的层次结构提取标签中的内容或标签属性；此外，也可以使用 CSS 选择器来定位页面元素，就跟用 CSS 渲染页面元素是同样的道理。 XPath 解析XPath 是在 XML（eXtensible Markup Language）文档中查找信息的一种语法，XML 跟 HTML 类似也是一种用标签承载数据的标签语言，不同之处在于 XML 的标签是可扩展的，可以自定义的，而且 XML 对语法有更严格的要求。XPath 使用路径表达式来选取 XML 文档中的节点或者节点集，这里所说的节点包括元素、属性、文本、命名空间、处理指令、注释、根节点等。下面我们通过一个例子来说明如何使用 XPath 对页面进行解析。 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;bookstore&gt; &lt;book&gt; &lt;title lang=&quot;eng&quot;&gt;Harry Potter&lt;/title&gt; &lt;price&gt;29.99&lt;/price&gt; &lt;/book&gt; &lt;book&gt; &lt;title lang=&quot;zh&quot;&gt;Learning XML&lt;/title&gt; &lt;price&gt;39.95&lt;/price&gt; &lt;/book&gt;&lt;/bookstore&gt; 对于上面的 XML 文件，我们可以用如下所示的 XPath 语法获取文档中的节点。 路径表达式 结果 /bookstore 选取根元素 bookstore。注意：假如路径起始于正斜杠( &#x2F; )，则此路径始终代表到某元素的绝对路径！ //book 选取所有 book 子元素，而不管它们在文档中的位置。 //@lang 选取名为 lang 的所有属性。 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。 /bookstore/book[position()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。 //title[@lang=&#39;eng&#39;] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 XPath还支持通配符用法，如下所示。 路径表达式 结果 /bookstore/* 选取 bookstore 元素的所有子元素。 //* 选取文档中的所有元素。 //title[@*] 选取所有带有属性的 title 元素。 如果要选取多个节点，可以使用如下所示的方法。 路径表达式 结果 //book/title | //book/price 选取 book 元素的所有 title 和 price 元素。 //title | //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title | //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 说明：上面的例子来自于“菜鸟教程”网站上的 XPath 教程，有兴趣的读者可以自行阅读原文。 当然，如果不理解或不熟悉 XPath 语法，可以在浏览器的开发者工具中按照如下所示的方法查看元素的 XPath 语法，下图是在 Chrome 浏览器的开发者工具中查看豆瓣网电影详情信息中影片标题的 XPath 语法。 实现 XPath 解析需要三方库lxml 的支持，可以使用下面的命令安装lxml。 1pip install lxml 下面我们用 XPath 解析方式改写之前获取豆瓣电影 Top250的代码，如下所示。 123456789101112131415from lxml import etreeimport requestsfor page in range(1, 11): resp = requests.get( url=f&#x27;https://movie.douban.com/top250?start=&#123;(page - 1) * 25&#125;&#x27;, headers=&#123;&#x27;User-Agent&#x27;: &#x27;BaiduSpider&#x27;&#125; ) tree = etree.HTML(resp.text) # 通过XPath语法从页面中提取电影标题 title_spans = tree.xpath(&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]&#x27;) # 通过XPath语法从页面中提取电影评分 rank_spans = tree.xpath(&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/div/span[2]&#x27;) for title_span, rank_span in zip(title_spans, rank_spans): print(title_span.text, rank_span.text) CSS 选择器解析对于熟悉 CSS 选择器和 JavaScript 的开发者来说，通过 CSS 选择器获取页面元素可能是更为简单的选择，因为浏览器中运行的 JavaScript 本身就可以document对象的querySelector()和querySelectorAll()方法基于 CSS 选择器获取页面元素。在 Python 中，我们可以利用三方库beautifulsoup4或pyquery来做同样的事情。Beautiful Soup 可以用来解析 HTML 和 XML 文档，修复含有未闭合标签等错误的文档，通过为待解析的页面在内存中创建一棵树结构，实现对从页面中提取数据操作的封装。可以用下面的命令来安装 Beautiful Soup。 1pip install beautifulsoup4 下面是使用bs4改写的获取豆瓣电影Top250电影名称的代码。 12345678910111213141516import bs4import requestsfor page in range(1, 11): resp = requests.get( url=f&#x27;https://movie.douban.com/top250?start=&#123;(page - 1) * 25&#125;&#x27;, headers=&#123;&#x27;User-Agent&#x27;: &#x27;BaiduSpider&#x27;&#125; ) # 创建BeautifulSoup对象 soup = bs4.BeautifulSoup(resp.text, &#x27;lxml&#x27;) # 通过CSS选择器从页面中提取包含电影标题的span标签 title_spans = soup.select(&#x27;div.info &gt; div.hd &gt; a &gt; span:nth-child(1)&#x27;) # 通过CSS选择器从页面中提取包含电影评分的span标签 rank_spans = soup.select(&#x27;div.info &gt; div.bd &gt; div &gt; span.rating_num&#x27;) for title_span, rank_span in zip(title_spans, rank_spans): print(title_span.text, rank_span.text) 关于 BeautifulSoup 更多的知识，可以参考它的官方文档。 简单的总结下面我们对三种解析方式做一个简单比较。 解析方式 对应的模块 速度 使用难度 正则表达式解析 re 快 困难 XPath 解析 lxml 快 一般 CSS 选择器解析 bs4或pyquery 不确定 简单","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/62.用Python获取网络资源-1","date":"2024-12-12T08:38:02.338Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/62.用Python获取网络资源-1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/62.%E7%94%A8Python%E8%8E%B7%E5%8F%96%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90-1/","excerpt":"","text":"用Python获取网络数据网络数据采集是 Python 语言非常擅长的领域，上节课我们讲到，实现网络数据采集的程序通常称之为网络爬虫或蜘蛛程序。即便是在大数据时代，数据对于中小企业来说仍然是硬伤和短板，有些数据需要通过开放或付费的数据接口来获得，其他的行业数据和竞对数据则必须要通过网络数据采集的方式来获得。不管使用哪种方式获取网络数据资源，Python 语言都是非常好的选择，因为 Python 的标准库和三方库都对网络数据采集提供了良好的支持。 requests库要使用 Python 获取网络数据，我们推荐大家使用名为requests 的三方库，这个库我们在之前的课程中其实已经使用过了。按照官方网站的解释，requests是基于 Python 标准库进行了封装，简化了通过 HTTP 或 HTTPS 访问网络资源的操作。上课我们提到过，HTTP 是一个请求响应式的协议，当我们在浏览器中输入正确的 URL（通常也称为网址）并按下 Enter 键时，我们就向网络上的 Web 服务器发送了一个 HTTP 请求，服务器在收到请求后会给我们一个 HTTP 响应。在 Chrome 浏览器中的菜单中打开“开发者工具”切换到“Network”选项卡就能够查看 HTTP 请求和响应到底是什么样子的，如下图所示。 通过requests库，我们可以让 Python 程序向浏览器一样向 Web 服务器发起请求，并接收服务器返回的响应，从响应中我们就可以提取出想要的数据。浏览器呈现给我们的网页是用 HTML 编写的，浏览器相当于是 HTML 的解释器环境，我们看到的网页中的内容都包含在 HTML 的标签中。在获取到 HTML 代码后，就可以从标签的属性或标签体中提取内容。下面例子演示了如何获取网页 HTML 代码，我们通过requests库的get函数，获取了搜狐首页的代码。 12345import requestsresp = requests.get(&#x27;https://www.sohu.com/&#x27;)if resp.status_code == 200: print(resp.text) 说明：上面代码中的变量resp是一个Response对象（requests库封装的类型），通过该对象的status_code属性可以获取响应状态码，而该对象的text属性可以帮我们获取到页面的 HTML 代码。 由于Response对象的text是一个字符串，所以我们可以利用之前讲过的正则表达式的知识，从页面的 HTML 代码中提取新闻的标题和链接，代码如下所示。 1234567891011import reimport requestspattern = re.compile(r&#x27;&lt;a.*?href=&quot;(.*?)&quot;.*?title=&quot;(.*?)&quot;.*?&gt;&#x27;)resp = requests.get(&#x27;https://www.sohu.com/&#x27;)if resp.status_code == 200: all_matches = pattern.findall(resp.text) for href, title in all_matches: print(href) print(title) 除了文本内容，我们也可以使用requests库通过 URL 获取二进制资源。下面的例子演示了如何获取百度 Logo 并保存到名为baidu.png的本地文件中。可以在百度的首页上右键点击百度Logo，并通过“复制图片地址”菜单项获取图片的 URL。 12345import requestsresp = requests.get(&#x27;https://www.baidu.com/img/PCtm_d9c8750bed0b3c7d089fa7d55720d6cf.png&#x27;)with open(&#x27;baidu.png&#x27;, &#x27;wb&#x27;) as file: file.write(resp.content) 说明：Response对象的content属性可以获得服务器响应的二进制数据。 requests库非常好用而且功能上也比较强大和完整，具体的内容我们在使用的过程中为大家一点点剖析。想解锁关于requests库更多的知识，可以阅读它的官方文档。 编写爬虫代码接下来，我们以“豆瓣电影”为例，为大家讲解如何编写爬虫代码。按照上面提供的方法，我们先使用requests获取到网页的HTML代码，然后将整个代码看成一个长字符串，这样我们就可以使用正则表达式的捕获组从字符串提取我们需要的内容。下面的代码演示了如何从豆瓣电影获取排前250名的电影的名称。豆瓣电影Top250的页面结构和对应代码如下图所示，可以看出，每页共展示了25部电影，如果要获取到 Top250 数据，我们共需要访问10个页面，对应的地址是https://movie.douban.com/top250?start=xxx，这里的xxx如果为0就是第一页，如果xxx的值是100，那么我们可以访问到第五页。为了代码简单易读，我们只获取电影的标题和评分。 12345678910111213141516171819202122232425import randomimport reimport timeimport requestsfor page in range(1, 11): resp = requests.get( url=f&#x27;https://movie.douban.com/top250?start=&#123;(page - 1) * 25&#125;&#x27;, # 如果不设置HTTP请求头中的User-Agent，豆瓣会检测出不是浏览器而阻止我们的请求。 # 通过get函数的headers参数设置User-Agent的值，具体的值可以在浏览器的开发者工具查看到。 # 用爬虫访问大部分网站时，将爬虫伪装成来自浏览器的请求都是非常重要的一步。 headers=&#123;&#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;&#125; ) # 通过正则表达式获取class属性为title且标签体不以&amp;开头的span标签并用捕获组提取标签内容 pattern1 = re.compile(r&#x27;&lt;span class=&quot;title&quot;&gt;([^&amp;]*?)&lt;/span&gt;&#x27;) titles = pattern1.findall(resp.text) # 通过正则表达式获取class属性为rating_num的span标签并用捕获组提取标签内容 pattern2 = re.compile(r&#x27;&lt;span class=&quot;rating_num&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;) ranks = pattern2.findall(resp.text) # 使用zip压缩两个列表，循环遍历所有的电影标题和评分 for title, rank in zip(titles, ranks): print(title, rank) # 随机休眠1-5秒，避免爬取页面过于频繁 time.sleep(random.random() * 4 + 1) 说明：通过分析豆瓣网的robots协议，我们发现豆瓣网并不拒绝百度爬虫获取它的数据，因此我们也可以将爬虫伪装成百度的爬虫，将get函数的headers参数修改为：headers=&#123;&#39;User-Agent&#39;: &#39;BaiduSpider&#39;&#125;。 使用 IP 代理让爬虫程序隐匿自己的身份对编写爬虫程序来说是比较重要的，很多网站对爬虫都比较反感的，因为爬虫会耗费掉它们很多的网络带宽并制造很多无效的流量。要隐匿身份通常需要使用商业 IP 代理（如蘑菇代理、芝麻代理、快代理等），让被爬取的网站无法获取爬虫程序来源的真实 IP 地址，也就无法简单的通过 IP 地址对爬虫程序进行封禁。 下面以蘑菇代理为例，为大家讲解商业 IP 代理的使用方法。首先需要在该网站注册一个账号，注册账号后就可以购买相应的套餐来获得商业 IP 代理。作为商业用途，建议大家购买不限量套餐，这样可以根据实际需要获取足够多的代理 IP 地址；作为学习用途，可以购买包时套餐或根据自己的需求来决定。蘑菇代理提供了两种接入代理的方式，分别是 API 私密代理和 HTTP 隧道代理，前者是通过请求蘑菇代理的 API 接口获取代理服务器地址，后者是直接使用统一的入口（蘑菇代理提供的域名）进行接入。 下面，我们以HTTP隧道代理为例，为大家讲解接入 IP 代理的方式，大家也可以直接参考蘑菇代理官网提供的代码来为爬虫设置代理。 123456789101112131415161718192021222324252627import requestsAPP_KEY = &#x27;Wnp******************************XFx&#x27;PROXY_HOST = &#x27;secondtransfer.moguproxy.com:9001&#x27;for page in range(1, 11): resp = requests.get( url=f&#x27;https://movie.douban.com/top250?start=&#123;(page - 1) * 25&#125;&#x27;, # 需要在HTTP请求头设置代理的身份认证方式 headers=&#123; &#x27;Proxy-Authorization&#x27;: f&#x27;Basic &#123;APP_KEY&#125;&#x27;, &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;, &#x27;Accept-Language&#x27;: &#x27;zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4&#x27; &#125;, # 设置代理服务器 proxies=&#123; &#x27;http&#x27;: f&#x27;http://&#123;PROXY_HOST&#125;&#x27;, &#x27;https&#x27;: f&#x27;https://&#123;PROXY_HOST&#125;&#x27; &#125;, verify=False ) pattern1 = re.compile(r&#x27;&lt;span class=&quot;title&quot;&gt;([^&amp;]*?)&lt;/span&gt;&#x27;) titles = pattern1.findall(resp.text) pattern2 = re.compile(r&#x27;&lt;span class=&quot;rating_num&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;) ranks = pattern2.findall(resp.text) for title, rank in zip(titles, ranks): print(title, rank) 说明：上面的代码需要修改APP_KEY为自己创建的订单对应的Appkey值，这个值可以在用户中心用户订单中查看到。蘑菇代理提供了免费的 API 代理和 HTTP 隧道代理试用，但是试用的代理接通率不能保证，建议大家还是直接购买一个在自己支付能力范围内的代理服务来体验。 简单的总结Python 语言能做的事情真的很多，就网络数据采集这一项而言，Python 几乎是一枝独秀的，大量的企业和个人都在使用 Python 从网络上获取自己需要的数据，这可能也是你将来日常工作的一部分。另外，用编写正则表达式的方式从网页中提取内容虽然可行，但是写出一个能够满足需求的正则表达式本身也不是件容易的事情，这一点对于新手来说尤为明显。在下一节课中，我们将会为大家介绍另外两种从页面中提取数据的方法，虽然从性能上来讲，它们可能不如正则表达式，但是却降低了编码的复杂性，相信大家会喜欢上它们的。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day61-65/61.网络数据采集概述","date":"2024-12-12T08:38:02.336Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day61-65/61.网络数据采集概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day61-65/61.%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E6%A6%82%E8%BF%B0/","excerpt":"","text":"网络数据采集概述爬虫（crawler）也经常被称为网络蜘蛛（spider），是按照一定的规则自动浏览网站并获取所需信息的机器人程序（自动化脚本代码），被广泛的应用于互联网搜索引擎和数据采集。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接，网络爬虫正是通过网页中的超链接信息，不断获得网络上其它页面的地址，然后持续的进行数据采集。正因如此，网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游，所以才被形象的称为爬虫或者网络蜘蛛。 爬虫的应用领域在理想的状态下，所有 ICP（Internet Content Provider）都应该为自己的网站提供 API 接口来共享它们允许其他程序获取的数据，在这种情况下就根本不需要爬虫程序。国内比较有名的电商平台（如淘宝、京东等）、社交平台（如微博、微信等）等都提供了自己的 API 接口，但是这类 API 接口通常会对可以抓取的数据以及抓取数据的频率进行限制。对于大多数的公司而言，及时的获取行业数据和竞对数据是企业生存的重要环节之一，然而对大部分企业来说，数据都是其与生俱来的短板。在这种情况下，合理的利用爬虫来获取数据并从中提取出有商业价值的信息对这些企业来说就显得至关重要的。 爬虫的应用领域其实非常广泛，下面我们列举了其中的一部分，有兴趣的读者可以自行探索相关内容。 搜索引擎 新闻聚合 社交应用 舆情监控 行业数据 爬虫合法性探讨经常听人说起“爬虫写得好，牢饭吃到饱”，那么编程爬虫程序是否违法呢？关于这个问题，我们可以从以下几个角度进行解读。 网络爬虫这个领域目前还属于拓荒阶段，虽然互联网世界已经通过自己的游戏规则建立起了一定的道德规范，即 Robots 协议（全称是“网络爬虫排除标准”），但法律部分还在建立和完善中，也就是说，现在这个领域暂时还是灰色地带。 “法不禁止即为许可”，如果爬虫就像浏览器一样获取的是前端显示的数据（网页上的公开信息）而不是网站后台的私密敏感信息，就不太担心法律法规的约束，因为目前大数据产业链的发展速度远远超过了法律的完善程度。 在爬取网站的时候，需要限制自己的爬虫遵守 Robots 协议，同时控制网络爬虫程序的抓取数据的速度；在使用数据的时候，必须要尊重网站的知识产权（从Web 2.0时代开始，虽然Web上的数据很多都是由用户提供的，但是网站平台是投入了运营成本的，当用户在注册和发布内容时，平台通常就已经获得了对数据的所有权、使用权和分发权）。如果违反了这些规定，在打官司的时候败诉几率相当高。 适当的隐匿自己的身份在编写爬虫程序时必要的，而且最好不要被对方举证你的爬虫有破坏别人动产（例如服务器）的行为。 不要在公网（如代码托管平台）上去开源或者展示你的爬虫代码，这些行为通常会给自己带来不必要的麻烦。 Robots协议大多数网站都会定义robots.txt文件，这是一个君子协议，并不是所有爬虫都必须遵守的游戏规则。下面以淘宝的robots.txt文件为例，看看淘宝网对爬虫有哪些限制。 12345User-agent: BaiduspiderDisallow: /User-agent: baiduspiderDisallow: / 通过上面的文件可以看出，淘宝禁止百度爬虫爬取它任何资源，因此当你在百度搜索“淘宝”的时候，搜索结果下方会出现：“由于该网站的robots.txt文件存在限制指令（限制搜索引擎抓取），系统无法提供该页面的内容描述”。百度作为一个搜索引擎，至少在表面上遵守了淘宝网的robots.txt协议，所以用户不能从百度上搜索到淘宝内部的产品信息。 图1. 百度搜索淘宝的结果 下面是豆瓣网的robots.txt文件，大家可以自行解读，看看它做出了什么样的限制。 12345678910111213141516171819202122232425262728293031323334User-agent: *Disallow: /subject_searchDisallow: /amazon_searchDisallow: /searchDisallow: /group/searchDisallow: /event/searchDisallow: /celebrities/searchDisallow: /location/drama/searchDisallow: /forum/Disallow: /new_subjectDisallow: /service/iframeDisallow: /j/Disallow: /link2/Disallow: /recommend/Disallow: /doubanapp/cardDisallow: /update/topic/Disallow: /share/Allow: /ads.txtSitemap: https://www.douban.com/sitemap_index.xmlSitemap: https://www.douban.com/sitemap_updated_index.xml# Crawl-delay: 5User-agent: Wandoujia SpiderDisallow: /User-agent: Mediapartners-GoogleDisallow: /subject_searchDisallow: /amazon_searchDisallow: /searchDisallow: /group/searchDisallow: /event/searchDisallow: /celebrities/searchDisallow: /location/drama/searchDisallow: /j/ 超文本传输协议（HTTP）在开始讲解爬虫之前，我们稍微对超文本传输协议（HTTP）做一些回顾，因为我们在网页上看到的内容通常是浏览器执行 HTML （超文本标记语言）得到的结果，而 HTTP 就是传输 HTML 数据的协议。HTTP 和其他很多应用级协议一样是构建在 TCP（传输控制协议）之上的，它利用了 TCP 提供的可靠的传输服务实现了 Web 应用中的数据交换。按照维基百科上的介绍，设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法，也就是说，这个协议是浏览器和 Web 服务器之间传输的数据的载体。关于 HTTP 的详细信息以及目前的发展状况，大家可以阅读《HTTP 协议入门》、《互联网协议入门》、《图解 HTTPS 协议》等文章进行了解。 下图是我在四川省网络通信技术重点实验室工作期间用开源协议分析工具 Ethereal（WireShark 的前身）截取的访问百度首页时的 HTTP 请求和响应的报文（协议数据），由于 Ethereal 截取的是经过网络适配器的数据，因此可以清晰的看到从物理链路层到应用层的协议数据。 图2. HTTP请求 HTTP 请求通常是由请求行、请求头、空行、消息体四个部分构成，如果没有数据发给服务器，消息体就不是必须的部分。请求行中包含了请求方法（GET、POST 等，如下表所示）、资源路径和协议版本；请求头由若干键值对构成，包含了浏览器、编码方式、首选语言、缓存策略等信息；请求头的后面是空行和消息体。 图3. HTTP响应 HTTP 响应通常是由响应行、响应头、空行、消息体四个部分构成，其中消息体是服务响应的数据，可能是 HTML 页面，也有可能是JSON或二进制数据等。响应行中包含了协议版本和响应状态码，响应状态码有很多种，常见的如下表所示。 相关工具下面我们先介绍一些开发爬虫程序的辅助工具，这些工具相信能帮助你事半功倍。 Chrome Developer Tools：谷歌浏览器内置的开发者工具。该工具最常用的几个功能模块是： 元素（ELements）：用于查看或修改 HTML 元素的属性、CSS 属性、监听事件等。CSS 可以即时修改，即时显示，大大方便了开发者调试页面。 控制台（Console）：用于执行一次性代码，查看 JavaScript 对象，查看调试日志信息或异常信息。控制台其实就是一个执行 JavaScript 代码的交互式环境。 源代码（Sources）：用于查看页面的 HTML 文件源代码、JavaScript 源代码、CSS 源代码，此外最重要的是可以调试 JavaScript 源代码，可以给代码添加断点和单步执行。 网络（Network）：用于 HTTP 请求、HTTP 响应以及与网络连接相关的信息。 应用（Application）：用于查看浏览器本地存储、后台任务等内容，本地存储主要包括Cookie、Local Storage、Session Storage等。 Postman：功能强大的网页调试与 RESTful 请求工具。Postman可以帮助我们模拟请求，非常方便的定制我们的请求以及查看服务器的响应。 HTTPie：命令行HTTP客户端。 安装。 1pip install httpie 使用。 1234567891011121314http --header http --header https://movie.douban.com/HTTP/1.1 200 OKConnection: keep-aliveContent-Encoding: gzipContent-Type: text/html; charset=utf-8Date: Tue, 24 Aug 2021 16:48:00 GMTKeep-Alive: timeout=30Server: daeSet-Cookie: bid=58h4BdKC9lM; Expires=Wed, 24-Aug-22 16:48:00 GMT; Domain=.douban.com; Path=/Strict-Transport-Security: max-age=15552000Transfer-Encoding: chunkedX-Content-Type-Options: nosniffX-DOUBAN-NEWBID: 58h4BdKC9lM builtwith库：识别网站所用技术的工具。 安装。 1pip install builtwith 使用。 123456import sslimport builtwithssl._create_default_https_context = ssl._create_unverified_contextprint(builtwith.parse(&#x27;http://www.bootcss.com/&#x27;)) python-whois库：查询网站所有者的工具。 安装。 1pip3 install python-whois 使用。 123import whoisprint(whois.whois(&#x27;https://www.bootcss.com&#x27;)) 爬虫的基本工作流程一个基本的爬虫通常分为数据采集（网页下载）、数据处理（网页解析）和数据存储（将有用的信息持久化）三个部分的内容，当然更为高级的爬虫在数据采集和处理时会使用并发编程或分布式技术，这就需要有调度器（安排线程或进程执行对应的任务）、后台管理程序（监控爬虫的工作状态以及检查数据抓取的结果）等的参与。 一般来说，爬虫的工作流程包括以下几个步骤： 设定抓取目标（种子页面&#x2F;起始页面）并获取网页。 当服务器无法访问时，按照指定的重试次数尝试重新下载页面。 在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。 对获取的页面进行必要的解码操作然后抓取出需要的信息。 在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。 对链接进行进一步的处理（获取页面并重复上面的动作）。 将有用的信息进行持久化以备后续的处理。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/code/hellodjango/templates/index","date":"2024-12-12T08:38:02.200Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/code/hellodjango/templates/index/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/code/hellodjango/templates/index/","excerpt":"","text":"首页 #fruits { font-size: 1.25em; } 今天推荐的水果是：","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/60.项目上线","date":"2024-12-12T08:38:02.133Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/60.项目上线/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/60.%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF/","excerpt":"","text":"项目上线请各位读者移步到《项目部署上线和性能调优》一文。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/59.单元测试","date":"2024-12-12T08:38:02.131Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/59.单元测试/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/59.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"单元测试请各位读者移步到《使用Django开发商业项目》一文。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/58.异步任务和定时任务","date":"2024-12-12T08:38:02.129Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/58.异步任务和定时任务/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/58.%E5%BC%82%E6%AD%A5%E4%BB%BB%E5%8A%A1%E5%92%8C%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"异步任务和定时任务在Web应用中，如果一个请求执行了耗时间的操作或者该请求的执行时间无法确定，而且对于用户来说只需要知道服务器接收了他的请求，并不需要马上得到请求的执行结果，这样的操作我们就应该对其进行异步化处理。如果说使用缓存是优化网站性能的第一要义，那么将耗时间或执行时间不确定的任务异步化则是网站性能优化的第二要义，简单的说就是能推迟做的事情都不要马上做。 上一章节中讲到的发短信和上传文件到云存储为例，这两个操作前者属于时间不确定的操作（因为作为调用者，我们不能确定三方平台响应的时间），后者属于耗时间的操作（如果文件较大或者三方平台不稳定，都可能导致上传的时间较长），很显然，这两个操作都可以做异步化处理。 在 Python 项目中，我们可以使用三方库Celery来完成异步任务和定时任务，关于Celery的内容，请移步到《使用Django开发商业项目》一文。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/57.接入三方平台","date":"2024-12-12T08:38:02.127Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/57.接入三方平台/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/57.%E6%8E%A5%E5%85%A5%E4%B8%89%E6%96%B9%E5%B9%B3%E5%8F%B0/","excerpt":"","text":"接入三方平台在Web应用的开发过程中，有一些任务并不是我们自己能够完成的。例如，我们的Web项目中需要做个人或企业的实名认证，很显然我们并没有能力判断用户提供的认证信息的真实性，这个时候我们就要借助三方平台提供的服务来完成该项操作。再比如说，我们的项目中需要提供在线支付功能，这类业务通常也是借助支付网关来完成而不是自己去实现，我们只需要接入像微信、支付宝、银联这样的三方平台即可。 在项目中接入三方平台基本上就两种方式：API接入和SDK接入。 API接入指的是通过访问三方提供的URL来完成操作或获取数据。国内有很多这样的平台提供了大量常用的服务，例如聚合数据上提供了生活服务类、金融科技类、交通地理类、充值缴费类等各种类型的API。我们可以通过Python程序发起网络请求，通过访问URL获取数据，这些API接口跟我们项目中提供的数据接口是一样的，只不过我们项目中的API是供自己使用的，而这类三方平台提供的API是开放的。当然开放并不代表免费，大多数能够提供有商业价值的数据的API都是需要付费才能使用的。 SDK接入指的是通过安装三方库并使用三方库封装的类、函数来使用三方平台提供的服务的方式。例如我们刚才说到的接入支付宝，就需要先安装支付宝的SDK，然后通过支付宝封装的类和方法完成对支付服务的调用。 下面我们通过具体的例子来讲解如何接入三方平台。 接入短信网关一个Web项目有很多地方都可以用到短信服务，例如：手机验证码登录、重要消息提醒、产品营销短信等。要实现发送短信的功能，可以通过接入短信网关来实现，国内比较有名的短信网关包括：云片短信、网易云信、螺丝帽、SendCloud等，这些短信网关一般都提供了免费试用功能。下面我们以螺丝帽平台为例，讲解如何在项目中接入短信网关，其他平台操作基本类似。 注册账号，新用户可以免费试用。 登录到管理后台，进入短信版块。 点击“触发发送”可以找到自己专属的API Key（身份标识）。 点击“签名管理”可以添加短信签名，短信都必须携带签名，免费试用的短信要在短信中添加“【铁壳测试】”这个签名，否则短信无法发送。 点击“IP白名单”将运行Django项目的服务器地址（公网IP地址，本地运行可以打开xxx网站查看自己本机的公网IP地址）填写到白名单中，否则短信无法发送。 如果没有剩余的短信条数，可以到“充值”页面选择“短信服务”进行充值。 接下来，我们可以通过调用螺丝帽短信网关实现发送短信验证码的功能，代码如下所示。 123456789101112def send_mobile_code(tel, code): &quot;&quot;&quot;发送短信验证码&quot;&quot;&quot; resp = requests.post( url=&#x27;http://sms-api.luosimao.com/v1/send.json&#x27;, auth=(&#x27;api&#x27;, &#x27;key-自己的APIKey&#x27;), data=&#123; &#x27;mobile&#x27;: tel, &#x27;message&#x27;: f&#x27;您的短信验证码是&#123;code&#125;，打死也不能告诉别人哟。【Python小课】&#x27; &#125;, verify=False ) return resp.json() 运行上面的代码需要先安装requests三方库，这个三方库封装了HTTP网络请求的相关功能，使用起来非常的简单，我们在之前的内容中也讲到过这个三方库。send_mobile_code函数有两个参数，第一个参数是手机号，第二个参数是短信验证码的内容，第5行代码需要提供自己的API Key，就是上面第2步中查看到的自己的API Key。请求螺丝帽的短信网关会返回JSON格式的数据，对于上面的代码如果返回&#123;&#39;err&#39;: 0, &#39;msg&#39;: &#39;ok&#39;&#125;，则表示短信发送成功，如果err字段的值不为0而是其他值，则表示短信发送失败，可以在螺丝帽官方的开发文档页面上查看到不同的数值代表的含义，例如：-20表示余额不足，-32表示缺少短信签名。 可以在视图函数中调用上面的函数来完成发送短信验证码的功能，稍后我们可以把这个功能跟用户注册结合起来。 生成随机验证码和验证手机号的函数。 1234567891011121314import randomimport reTEL_PATTERN = re.compile(r&#x27;1[3-9]\\d&#123;9&#125;&#x27;)def check_tel(tel): &quot;&quot;&quot;检查手机号&quot;&quot;&quot; return TEL_PATTERN.fullmatch(tel) is not Nonedef random_code(length=6): &quot;&quot;&quot;生成随机短信验证码&quot;&quot;&quot; return &#x27;&#x27;.join(random.choices(&#x27;0123456789&#x27;, k=length)) 发送短信验证码的视图函数。 123456789101112131415161718@api_view((&#x27;GET&#x27;, ))def get_mobilecode(request, tel): &quot;&quot;&quot;获取短信验证码&quot;&quot;&quot; if check_tel(tel): redis_cli = get_redis_connection() if redis_cli.exists(f&#x27;vote:block-mobile:&#123;tel&#125;&#x27;): data = &#123;&#x27;code&#x27;: 30001, &#x27;message&#x27;: &#x27;请不要在60秒内重复发送短信验证码&#x27;&#125; else: code = random_code() send_mobile_code(tel, code) # 通过Redis阻止60秒内容重复发送短信验证码 redis_cli.set(f&#x27;vote:block-mobile:&#123;tel&#125;&#x27;, &#x27;x&#x27;, ex=60) # 将验证码在Redis中保留10分钟（有效期10分钟） redis_cli.set(f&#x27;vote2:valid-mobile:&#123;tel&#125;&#x27;, code, ex=600) data = &#123;&#x27;code&#x27;: 30000, &#x27;message&#x27;: &#x27;短信验证码已发送，请注意查收&#x27;&#125; else: data = &#123;&#x27;code&#x27;: 30002, &#x27;message&#x27;: &#x27;请输入有效的手机号&#x27;&#125; return Response(data) 说明：上面的代码利用Redis实现了两个额外的功能，一个是阻止用户60秒内重复发送短信验证码，一个是将用户的短信验证码保留10分钟，也就是说这个短信验证码的有效期只有10分钟，我们可以要求用户在注册时提供该验证码来验证用户手机号的真实性。 接入云存储服务当我们提到云存储这个词的时候，通常是指把数据存放在由第三方提供的虚拟服务器环境下，简单的说就是将某些数据或资源通过第三平台托管。一般情况下，提供云存储服务的公司都运营着大型的数据中心，需要云存储服务的个人或组织通过向其购买或租赁存储空间来满足数据存储的需求。在开发Web应用时，可以将静态资源，尤其是用户上传的静态资源直接置于云存储服务中，云存储通常会提供对应的URL使得用户可以访问该静态资源。国内外比较有名的云存储服务（如：亚马逊的S3、阿里的OSS2等）一般都物美价廉，相比自己架设静态资源服务器，云存储的代价更小，而且一般的云存储平台都提供了CDN服务，用于加速对静态资源的访问，所以不管从哪个角度出发，使用云存储的方式管理Web应用的数据和静态资源都是非常好的选择，除非这些资源涉及到个人或商业隐私，否则就可以托管到云存储中。 下面我们以接入七牛云为例，讲解如何实现将用户上传的文件保存到七牛云存储。七牛云是国内知名的云计算及数据服务提供商，七牛云在海量文件存储、CDN、视频点播、互动直播以及大规模异构数据的智能分析与处理等领域都有自己的产品，而且非付费用户也可以免费接入，使用其提供的服务。下面是接入七牛云的流程： 注册账号，登录管理控制台。 选择左侧菜单中的对象存储。 在空间管理中选择新建空间（例如：myvote），如果提示空间名称已被占用，更换一个再尝试即可。注意，创建空间后会提示绑定自定义域名，如果暂时还没有自己的域名，可以使用七牛云提供的临时域名，但是临时域名会在30天后被回收，所以最好准备自己的域名（域名需要备案，不清楚如何操作的请自行查阅相关资料）。 在网页的右上角点击个人头像中的“密钥管理”，查看自己的密钥，稍后在代码中需要使用AK（AccessKey）和SK（SecretKey）两个密钥来认证用户身份。 点击网页上方菜单中的“文档”，进入到七牛开发者中心，选择导航菜单中的“SDK&amp;工具”并点击“官方SDK”子菜单，找到Python（服务端）并点击“文档”查看官方文档。 接下来，只要安装官方文档提供的示例，就可以接入七牛云，使用七牛云提供的云存储以及其他服务。首先可以通过下面的命令安装七牛云的三方库。 1pip install qiniu 接下来可以通过qiniu模块中的put_file和put_stream两个函数实现文件上传，前者可以上传指定路径的文件，后者可以将内存中的二进制数据上传至七牛云，具体的代码如下所示。 12345678910111213141516import qiniuAUTH = qiniu.Auth(&#x27;密钥管理中的AccessKey&#x27;, &#x27;密钥管理中的SecretKey&#x27;)BUCKET_NAME = &#x27;myvote&#x27;def upload_file_to_qiniu(key, file_path): &quot;&quot;&quot;上传指定路径的文件到七牛云&quot;&quot;&quot; token = AUTH.upload_token(BUCKET_NAME, key) return qiniu.put_file(token, key, file_path)def upload_stream_to_qiniu(key, stream, size): &quot;&quot;&quot;上传二进制数据流到七牛云&quot;&quot;&quot; token = AUTH.upload_token(BUCKET_NAME, key) return qiniu.put_stream(token, key, stream, None, size) 下面是一个文件上传的简单前端页。 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;上传文件&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=&quot;/upload/&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;div&gt; &lt;input type=&quot;file&quot; name=&quot;photo&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt; &lt;/div&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 说明：前端如果使用表单实现文件上传，表单的method属性必须设置为post，enctype属性需要设置为multipart&#x2F;form-data，表单中type属性为file的input标签，就是上传文件的文件选择器。 实现上传功能的视图函数如下所示。 123456789101112131415from django.views.decorators.csrf import csrf_exempt@csrf_exemptdef upload(request): # 如果上传的文件小于2.5M，则photo对象的类型为InMemoryUploadedFile，文件在内存中 # 如果上传的文件超过2.5M，则photo对象的类型为TemporaryUploadedFile，文件在临时路径下 photo = request.FILES.get(&#x27;photo&#x27;) _, ext = os.path.splitext(photo.name) # 通过UUID和原来文件的扩展名生成独一无二的新的文件名 filename = f&#x27;&#123;uuid.uuid1().hex&#125;&#123;ext&#125;&#x27; # 对于内存中的文件，可以使用上面封装好的函数upload_stream_to_qiniu上传文件到七牛云 # 如果文件保存在临时路径下，可以使用upload_file_to_qiniu实现文件上传 upload_stream_to_qiniu(filename, photo.file, photo.size) return redirect(&#x27;/static/html/upload.html&#x27;) 注意：上面的视图函数使用了csrf_exempt装饰器，该装饰器能够让表单免除必须提供CSRF令牌的要求。此外，代码第11行使用了uuid模块的uuid1函数来生成全局唯一标识符。 运行项目尝试文件上传的功能，文件上传成功后，可以在七牛云“空间管理”中点击自己空间并进入“文件管理”界面，在这里可以看到我们刚才上传成功的文件，而且可以通过七牛云提供的域名获取该文件。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/56.使用缓存","date":"2024-12-12T08:38:02.125Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/56.使用缓存/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/56.%E4%BD%BF%E7%94%A8%E7%BC%93%E5%AD%98/","excerpt":"","text":"使用缓存通常情况下，Web应用的性能瓶颈都会出现在关系型数据库上，当并发访问量较大时，如果所有的请求都需要通过关系型数据库完成数据持久化操作，那么数据库一定会不堪重负。优化Web应用性能最为重要的一点就是使用缓存，把那些数据体量不大但访问频率非常高的数据提前加载到缓存服务器中，这又是典型的空间换时间的方法。通常缓存服务器都是直接将数据置于内存中而且使用了非常高效的数据存取策略（哈希存储、键值对方式等），在读写性能上远远优于关系型数据库的，因此我们可以让Web应用接入缓存服务器来优化其性能，其中一个非常好的选择就是使用Redis。 Web应用的缓存架构大致如下图所示。 Django项目接入Redis在此前的课程中，我们介绍过Redis的安装和使用，此处不再进行赘述。如果需要在Django项目中接入Redis，可以使用三方库django-redis，这个三方库又依赖了一个名为redis 的三方库，它封装了对Redis的各种操作。 安装django-redis。 1pip install django-redis 修改Django配置文件中关于缓存的配置。 123456789101112131415161718192021CACHES = &#123; &#x27;default&#x27;: &#123; # 指定通过django-redis接入Redis服务 &#x27;BACKEND&#x27;: &#x27;django_redis.cache.RedisCache&#x27;, # Redis服务器的URL &#x27;LOCATION&#x27;: [&#x27;redis://1.2.3.4:6379/0&#x27;, ], # Redis中键的前缀（解决命名冲突） &#x27;KEY_PREFIX&#x27;: &#x27;vote&#x27;, # 其他的配置选项 &#x27;OPTIONS&#x27;: &#123; &#x27;CLIENT_CLASS&#x27;: &#x27;django_redis.client.DefaultClient&#x27;, # 连接池（预置若干备用的Redis连接）参数 &#x27;CONNECTION_POOL_KWARGS&#x27;: &#123; # 最大连接数 &#x27;max_connections&#x27;: 512, &#125;, # 连接Redis的用户口令 &#x27;PASSWORD&#x27;: &#x27;foobared&#x27;, &#125; &#125;,&#125; 至此，我们的Django项目已经可以接入Redis，接下来我们修改项目代码，用Redis为之写的获取学科数据的接口提供缓存服务。 为视图提供缓存服务声明式缓存所谓声明式缓存是指不修改原来的代码，通过Python中的装饰器（代理）为原有的代码增加缓存功能。对于FBV，代码如下所示。 12345678910from django.views.decorators.cache import cache_page@api_view((&#x27;GET&#x27;, ))@cache_page(timeout=86400, cache=&#x27;default&#x27;)def show_subjects(request): &quot;&quot;&quot;获取学科数据&quot;&quot;&quot; queryset = Subject.objects.all() data = SubjectSerializer(queryset, many=True).data return Response(&#123;&#x27;code&#x27;: 20000, &#x27;subjects&#x27;: data&#125;) 上面的代码通过Django封装的cache_page装饰器缓存了视图函数的返回值（响应对象），cache_page的本意是缓存视图函数渲染的页面，对于返回JSON数据的视图函数，相当于是缓存了JSON数据。在使用cache_page装饰器时，可以传入timeout参数来指定缓存过期时间，还可以使用cache参数来指定需要使用哪一组缓存服务来缓存数据。Django项目允许在配置文件中配置多组缓存服务，上面的cache=&#39;default&#39;指定了使用默认的缓存服务（因为之前的配置文件中我们也只配置了名为default的缓存服务）。视图函数的返回值会被序列化成字节串放到Redis中（Redis中的str类型可以接收字节串），缓存数据的序列化和反序列化也不需要我们自己处理，因为cache_page装饰器会调用django-redis库中的RedisCache来对接Redis，该类使用了DefaultClient来连接Redis并使用了pickle序列化，django_redis.serializers.pickle.PickleSerializer是默认的序列化类。 如果缓存中没有学科的数据，那么通过接口访问学科数据时，我们的视图函数会通过执行Subject.objects.all()向数据库发出SQL语句来获得数据，视图函数的返回值会被缓存，因此下次请求该视图函数如果缓存没有过期，可以直接从缓存中获取视图函数的返回值，无需再次查询数据库。如果想了解缓存的使用情况，可以配置数据库日志或者使用Django-Debug-Toolbar来查看，第一次访问学科数据接口时会看到查询学科数据的SQL语句，再次获取学科数据时，不会再向数据库发出SQL语句，因为可以直接从缓存中获取数据。 对于CBV，可以利用Django中名为method_decorator的装饰器将cache_page这个装饰函数的装饰器放到类中的方法上，效果跟上面的代码是一样的。需要提醒大家注意的是，cache_page装饰器不能直接放在类上，因为它是装饰函数的装饰器，所以Django框架才提供了method_decorator来解决这个问题，很显然，method_decorator是一个装饰类的装饰器。 123456789from django.utils.decorators import method_decoratorfrom django.views.decorators.cache import cache_page@method_decorator(decorator=cache_page(timeout=86400, cache=&#x27;default&#x27;), name=&#x27;get&#x27;)class SubjectView(ListAPIView): &quot;&quot;&quot;获取学科数据的视图类&quot;&quot;&quot; queryset = Subject.objects.all() serializer_class = SubjectSerializer 编程式缓存所谓编程式缓存是指通过自己编写的代码来使用缓存服务，这种方式虽然代码量会稍微大一些，但是相较于声明式缓存，它对缓存的操作和使用更加灵活，在实际开发中使用得更多。下面的代码去掉了之前使用的cache_page装饰器，通过django-redis提供的get_redis_connection函数直接获取Redis连接来操作Redis。 123456789101112131415def show_subjects(request): &quot;&quot;&quot;获取学科数据&quot;&quot;&quot; redis_cli = get_redis_connection() # 先尝试从缓存中获取学科数据 data = redis_cli.get(&#x27;vote:polls:subjects&#x27;) if data: # 如果获取到学科数据就进行反序列化操作 data = json.loads(data) else: # 如果缓存中没有获取到学科数据就查询数据库 queryset = Subject.objects.all() data = SubjectSerializer(queryset, many=True).data # 将查到的学科数据序列化后放到缓存中 redis_cli.set(&#x27;vote:polls:subjects&#x27;, json.dumps(data), ex=86400) return Response(&#123;&#x27;code&#x27;: 20000, &#x27;subjects&#x27;: data&#125;) 需要说明的是，Django框架提供了cache和caches两个现成的变量来支持缓存操作，前者访问的是默认的缓存（名为default的缓存），后者可以通过索引运算获取指定的缓存服务（例如：caches[&#39;default&#39;]）。向cache对象发送get和set消息就可以实现对缓存的读和写操作，但是这种方式能做的操作有限，不如上面代码中使用的方式灵活。还有一个值得注意的地方，由于可以通过get_redis_connection函数获得的Redis连接对象向Redis发起各种操作，包括FLUSHDB、SHUTDOWN等危险的操作，所以在实际商业项目开发中，一般都会对django-redis再做一次封装，例如封装一个工具类，其中只提供了项目需要用到的缓存操作的方法，从而避免了直接使用get_redis_connection的潜在风险。当然，自己封装对缓存的操作还可以使用“Read Through”和“Write Through”的方式实现对缓存的更新，这个在下面会介绍到。 缓存相关问题缓存数据的更新在使用缓存时，一个必须搞清楚的问题就是，当数据改变时，如何更新缓存中的数据。通常更新缓存有如下几种套路，分别是： Cache Aside Pattern Read&#x2F;Write Through Pattern Write Behind Caching Pattern 第1种方式的具体做法就是，当数据更新时，先更新数据库，再删除缓存。注意，不能够使用先更新数据库再更新缓存的方式，也不能够使用先删除缓存再更新数据库的方式，大家可以自己想一想为什么（考虑一下有并发的读操作和写操作的场景）。当然，先更新数据库再删除缓存的做法在理论上也存在风险，但是发生问题的概率是极低的，所以不少的项目都使用了这种方式。 第1种方式相当于编写业务代码的开发者要自己负责对两套存储系统（缓存和关系型数据库）的操作，代码写起来非常的繁琐。第2种方式的主旨是将后端的存储系统变成一套代码，对缓存的维护封装在这套代码中。其中，Read Through指在查询操作中更新缓存，也就是说，当缓存失效的时候，由缓存服务自己负责对数据的加载，从而对应用方是透明的；而Write Through是指在更新数据时，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由缓存服务自己更新数据库（同步更新）。刚才我们说过，如果自己对项目中的Redis操作再做一次封装，就可以实现“Read Through”和“Write Through”模式，这样做虽然会增加工作量，但无疑是一件“一劳永逸”且“功在千秋”的事情。 第3种方式是在更新数据的时候，只更新缓存，不更新数据库，而缓存服务这边会异步的批量更新数据库。这种做法会大幅度提升性能，但代价是牺牲数据的强一致性。第3种方式的实现逻辑比较复杂，因为他需要追踪有哪数据是被更新了的，然后再批量的刷新到持久层上。 缓存穿透缓存是为了缓解数据库压力而添加的一个中间层，如果恶意的访问者频繁的访问缓存中没有的数据，那么缓存就失去了存在的意义，瞬间所有请求的压力都落在了数据库上，这样会导致数据库承载着巨大的压力甚至连接异常，类似于分布式拒绝服务攻击（DDoS）的做法。解决缓存穿透的一个办法是约定如果查询返回为空值，把这个空值也缓存起来，但是需要为这个空值的缓存设置一个较短的超时时间，毕竟缓存这样的值就是对缓存空间的浪费。另一个解决缓存穿透的办法是使用布隆过滤器，具体的做法大家可以自行了解。 缓存击穿在实际的项目中，可能存在某个缓存的key某个时间点过期，但恰好在这个时间点对有对该key的大量的并发请求过来，这些请求没有从缓存中找到key对应的数据，就会直接从数据库中获取数据并写回到缓存，这个时候大并发的请求可能会瞬间把数据库压垮，这种现象称为缓存击穿。比较常见的解决缓存击穿的办法是使用互斥锁，简单的说就是在缓存失效的时候，不是立即去数据库加载数据，而是先设置互斥锁（例如：Redis中的setnx），只有设置互斥锁的操作成功的请求，才能执行查询从数据库中加载数据并写入缓存，其他设置互斥锁失败的请求，可以先执行一个短暂的休眠，然后尝试重新从缓存中获取数据，如果缓存还没有数据，则重复刚才的设置互斥锁的操作，大致的参考代码如下所示。 12345678910data = redis_cli.get(key)while not data: if redis_cli.setnx(&#x27;mutex&#x27;, &#x27;x&#x27;): redis.expire(&#x27;mutex&#x27;, timeout) data = db.query(...) redis.set(key, data) redis.delete(&#x27;mutex&#x27;) else: time.sleep(0.1) data = redis_cli.get(key) 缓存雪崩缓存雪崩是指在将数据放入缓存时采用了相同的过期时间，这样就导致缓存在某一时刻同时失效，请求全部转发到数据库，导致数据库瞬时压力过大而崩溃。解决缓存雪崩问题的方法也比较简单，可以在既定的缓存过期时间上加一个随机时间，这样可以从一定程度上避免不同的key在同一时间集体失效。还有一种办法就是使用多级缓存，每一级缓存的过期时间都不一样，这样的话即便某个级别的缓存集体失效，但是其他级别的缓存还能够提供数据，避免所有的请求都落到数据库上。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/55.RESTful架构和DRF进阶","date":"2024-12-12T08:38:02.122Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/55.RESTful架构和DRF进阶/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/55.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E8%BF%9B%E9%98%B6/","excerpt":"","text":"RESTful架构和DRF进阶除了上一节讲到的方法，使用DRF创建REST风格的数据接口也可以通过CBV（基于类的视图）的方式。使用CBV创建数据接口的特点是代码简单，开发效率高，但是没有FBV（基于函数的视图）灵活，因为使用FBV的方式，数据接口对应的视图函数执行什么样的代码以及返回什么的数据是高度可定制的。下面我们以定制学科的数据接口为例，讲解通过CBV方式定制数据接口的具体做法。 使用CBV继承APIView的子类修改之前项目中的polls/views.py，去掉show_subjects视图函数，添加一个名为SubjectView的类，该类继承自ListAPIView，ListAPIView能接收GET请求，它封装了获取数据列表并返回JSON数据的get方法。ListAPIView是APIView 的子类，APIView还有很多的子类，例如CreateAPIView可以支持POST请求，UpdateAPIView可以支持PUT和PATCH请求，DestoryAPIView可以支持DELETE请求。SubjectView 的代码如下所示。 12345678from rest_framework.generics import ListAPIViewclass SubjectView(ListAPIView): # 通过queryset指定如何获取学科数据 queryset = Subject.objects.all() # 通过serializer_class指定如何序列化学科数据 serializer_class = SubjectSerializer 刚才说过，由于SubjectView的父类ListAPIView已经实现了get方法来处理获取学科列表的GET请求，所以我们只需要声明如何获取学科数据以及如何序列化学科数据，前者用queryset属性指定，后者用serializer_class属性指定。要使用上面的SubjectView，需要修改urls.py文件，如下所示。 123urlpatterns = [ path(&#x27;api/subjects/&#x27;, SubjectView.as_view()), ] 很显然，上面的做法较之之前讲到的FBV要简单很多。 继承ModelViewSet如果学科对应的数据接口需要支持GET、POST、PUT、PATCH、DELETE请求来支持对学科资源的获取、新增、更新、删除操作，更为简单的做法是继承ModelViewSet来编写学科视图类。再次修改polls/views.py文件，去掉SubjectView类，添加一个名为SubjectViewSet的类，代码如下所示。 123456from rest_framework.viewsets import ModelViewSetclass SubjectViewSet(ModelViewSet): queryset = Subject.objects.all() serializer_class = SubjectSerializer 通过查看ModelViewSet类的源代码可以发现，该类共有6个父类，其中前5个父类分别实现对POST（新增学科）、GET（获取指定学科）、PUT&#x2F;PATCH（更新学科）、DELETE（删除学科）和GET（获取学科列表）操作的支持，对应的方法分别是create、retrieve、update、destroy和list。由于ModelViewSet的父类中已经实现了这些方法，所以我们几乎没有编写任何代码就完成了学科数据全套接口的开发，我们要做的仅仅是指出如何获取到数据（通过queryset属性指定）以及如何序列化数据（通过serializer_class属性指定），这一点跟上面继承APIView的子类做法是一致的。 1234567891011class ModelViewSet(mixins.CreateModelMixin, mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, mixins.ListModelMixin, GenericViewSet): &quot;&quot;&quot; A viewset that provides default `create()`, `retrieve()`, `update()`, `partial_update()`, `destroy()` and `list()` actions. &quot;&quot;&quot; pass 要使用上面的SubjectViewSet，需要在urls.py文件中进行URL映射。由于ModelViewSet相当于是多个视图函数的汇总，所以不同于之前映射URL的方式，我们需要先创建一个路由器并通过它注册SubjectViewSet，然后将注册成功后生成的URL一并添加到urlspattern列表中，代码如下所示。 12345from rest_framework.routers import DefaultRouterrouter = DefaultRouter()router.register(&#x27;api/subjects&#x27;, SubjectViewSet)urlpatterns += router.urls 除了ModelViewSet类外，DRF还提供了一个名为ReadOnlyModelViewSet 的类，从名字上就可以看出，该类是只读视图的集合，也就意味着，继承该类定制的数据接口只能支持GET请求，也就是获取单个资源和资源列表的请求。 数据分页在使用GET请求获取资源列表时，我们通常不会一次性的加载所有的数据，除非数据量真的很小。大多数获取资源列表的操作都支持数据分页展示，也就说我们可以通过指定页码（或类似于页码的标识）和页面大小（一次加载多少条数据）来获取不同的数据。我们可以通过对QuerySet对象的切片操作来实现分页，也可以利用Django框架封装的Paginator和Page对象来实现分页。使用DRF时，可以在Django配置文件中修改REST_FRAMEWORK并配置默认的分页类和页面大小来实现分页，如下所示。 1234REST_FRAMEWORK = &#123; &#x27;PAGE_SIZE&#x27;: 10, &#x27;DEFAULT_PAGINATION_CLASS&#x27;: &#x27;rest_framework.pagination.PageNumberPagination&#x27;&#125; 除了上面配置的PageNumberPagination分页器之外，DRF还提供了LimitOffsetPagination和CursorPagination分页器，值得一提的是CursorPagination，它可以避免使用页码分页时暴露网站的数据体量，有兴趣的读者可以自行了解。如果不希望使用配置文件中默认的分页设定，可以在视图类中添加一个pagination_class属性来重新指定分页器，通常可以将该属性指定为自定义的分页器，如下所示。 12345678910from rest_framework.pagination import PageNumberPaginationclass CustomizedPagination(PageNumberPagination): # 默认页面大小 page_size = 5 # 页面大小对应的查询参数 page_size_query_param = &#x27;size&#x27; # 页面大小的最大值 max_page_size = 50 1234567class SubjectView(ListAPIView): # 指定如何获取数据 queryset = Subject.objects.all() # 指定如何序列化数据 serializer_class = SubjectSerializer # 指定如何分页 pagination_class = CustomizedPagination 如果不希望数据分页，可以将pagination_class属性设置为None来取消默认的分页器。 数据筛选如果希望使用CBV定制获取老师信息的数据接口，也可以通过继承ListAPIView来实现。但是因为要通过指定的学科来获取对应的老师信息，因此需要对老师数据进行筛选而不是直接获取所有老师的数据。如果想从请求中获取学科编号并通过学科编号对老师进行筛选，可以通过重写get_queryset方法来做到，代码如下所示。 1234567891011class TeacherView(ListAPIView): serializer_class = TeacherSerializer def get_queryset(self): queryset = Teacher.objects.defer(&#x27;subject&#x27;) try: sno = self.request.GET.get(&#x27;sno&#x27;, &#x27;&#x27;) queryset = queryset.filter(subject__no=sno) return queryset except ValueError: raise Http404(&#x27;No teachers found.&#x27;) 除了上述方式之外，还可以使用三方库django-filter来配合DRF实现对数据的筛选，使用django-filter后，可以通过为视图类配置filter-backends属性并指定使用DjangoFilterBackend来支持数据筛选。在完成上述配置后，可以使用filter_fields 属性或filterset_class属性来指定如何筛选数据，有兴趣的读者可以自行研究。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/54.RESTful架构和DRF入门","date":"2024-12-12T08:38:02.120Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/54.RESTful架构和DRF入门/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/54.RESTful%E6%9E%B6%E6%9E%84%E5%92%8CDRF%E5%85%A5%E9%97%A8/","excerpt":"","text":"RESTful架构和DRF入门把软件（Software）、平台（Platform）、基础设施（Infrastructure）做成服务（Service）是很多IT企业都一直在做的事情，这就是大家经常听到的SasS（软件即服务）、PasS（平台即服务）和IasS（基础设置即服务）。实现面向服务的架构（SOA）有诸多的方式，包括RPC（远程过程调用）、Web Service、REST等，在技术层面上，SOA是一种抽象的、松散耦合的粗粒度软件架构；在业务层面上，SOA的核心概念是“重用”和“互操作”，它将系统资源整合成可操作的、标准的服务，使得这些资源能够被重新组合和应用。在实现SOA的诸多方案中，REST被认为是最适合互联网应用的架构，符合REST规范的架构也经常被称作RESTful架构。 REST概述REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的，Roy是HTTP协议（1.0和1.1版）的主要设计者、Apache服务器软件主要作者、Apache基金会第一任主席。在他的博士论文中，Roy把他对互联网软件的架构原则定名为REST，即REpresentational State Transfer的缩写，中文通常翻译为“表现层状态转移”或“表述状态转移”。 这里的“表现层”其实指的是“资源”的“表现层”。所谓资源，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲或一种服务。我们可以用一个URI（统一资源定位符）指向资源，要获取到这个资源，访问它的URI即可，URI就是资源在互联网上的唯一标识。资源可以有多种外在表现形式。我们把资源具体呈现出来的形式，叫做它的“表现层”。比如，文本可以用text/plain格式表现，也可以用text/html格式、text/xml格式、application/json格式表现，甚至可以采用二进制格式；图片可以用image/jpeg格式表现，也可以用image/png格式表现。URI只代表资源的实体，不代表它的表现形式。严格地说，有些网址最后的.html后缀名是不必要的，因为这个后缀名表示格式，属于“表现层”范畴，而URI应该只代表“资源”的位置，它的具体表现形式，应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对“表现层”的描述。 访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。Web应用通常使用HTTP作为其通信协议，客户端想要操作服务器，必须通过HTTP请求，让服务器端发生“状态转移”，而这种转移是建立在表现层之上的，所以就是“表现层状态转移”。客户端通过HTTP的动词GET、POST、PUT（或PATCH）、DELETE，分别对应对资源的四种基本操作，其中GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT（或PATCH）用来更新资源，DELETE用来删除资源。 简单的说RESTful架构就是：“每一个URI代表一种资源，客户端通过四个HTTP动词，对服务器端资源进行操作，实现资源的表现层状态转移”。 我们在设计Web应用时，如果需要向客户端提供资源，就可以使用REST风格的URI，这是实现RESTful架构的第一步。当然，真正的RESTful架构并不只是URI符合REST风格，更为重要的是“无状态”和“幂等性”两个词，我们在后面的课程中会为大家阐述这两点。下面的例子给出了一些符合REST风格的URI，供大家在设计URI时参考。 请求方法（HTTP动词） URI 解释 GET /students/ 获取所有学生 POST /students/ 新建一个学生 GET /students/ID/ 获取指定ID的学生信息 PUT /students/ID/ 更新指定ID的学生信息（提供该学生的全部信息） PATCH /students/ID/ 更新指定ID的学生信息（提供该学生的部分信息） DELETE /students/ID/ 删除指定ID的学生信息 GET /students/ID/friends/ 列出指定ID的学生的所有朋友 DELETE /students/ID/friends/ID/ 删除指定ID的学生的指定ID的朋友 DRF使用入门在Django项目中，如果要实现REST架构，即将网站的资源发布成REST风格的API接口，可以使用著名的三方库djangorestframework ，我们通常将其简称为DRF。 安装和配置DRF安装DRF。 1pip install djangorestframework 配置DRF。 123456789101112131415161718192021222324252627282930313233INSTALLED_APPS = [ &#x27;rest_framework&#x27;, ]# 下面的配置根据项目需要进行设置REST_FRAMEWORK = &#123; # 配置默认页面大小 # &#x27;PAGE_SIZE&#x27;: 10, # 配置默认的分页类 # &#x27;DEFAULT_PAGINATION_CLASS&#x27;: &#x27;...&#x27;, # 配置异常处理器 # &#x27;EXCEPTION_HANDLER&#x27;: &#x27;...&#x27;, # 配置默认解析器 # &#x27;DEFAULT_PARSER_CLASSES&#x27;: ( # &#x27;rest_framework.parsers.JSONParser&#x27;, # &#x27;rest_framework.parsers.FormParser&#x27;, # &#x27;rest_framework.parsers.MultiPartParser&#x27;, # ), # 配置默认限流类 # &#x27;DEFAULT_THROTTLE_CLASSES&#x27;: ( # &#x27;...&#x27; # ), # 配置默认授权类 # &#x27;DEFAULT_PERMISSION_CLASSES&#x27;: ( # &#x27;...&#x27;, # ), # 配置默认认证类 # &#x27;DEFAULT_AUTHENTICATION_CLASSES&#x27;: ( # &#x27;...&#x27;, # ),&#125; 编写序列化器前后端分离的开发需要后端为前端、移动端提供API数据接口，而API接口通常情况下都是返回JSON格式的数据，这就需要对模型对象进行序列化处理。DRF中封装了Serializer类和ModelSerializer类用于实现序列化操作，通过继承Serializer类或ModelSerializer类，我们可以自定义序列化器，用于将对象处理成字典，代码如下所示。 12345678from rest_framework import serializers class SubjectSerializer(serializers.ModelSerializer): class Meta: model = Subject fields = &#x27;__all__&#x27; 上面的代码直接继承了ModelSerializer，通过Meta类的model属性指定要序列化的模型以及fields属性指定需要序列化的模型字段，稍后我们就可以在视图函数中使用该类来实现对Subject模型的序列化。 编写视图函数DRF框架支持两种实现数据接口的方式，一种是FBV（基于函数的视图），另一种是CBV（基于类的视图）。我们先看看FBV的方式如何实现数据接口，代码如下所示。 1234567891011from rest_framework.decorators import api_viewfrom rest_framework.response import Response@api_view((&#x27;GET&#x27;, ))def show_subjects(request: HttpRequest) -&gt; HttpResponse: subjects = Subject.objects.all().order_by(&#x27;no&#x27;) # 创建序列化器对象并指定要序列化的模型 serializer = SubjectSerializer(subjects, many=True) # 通过序列化器的data属性获得模型对应的字典并通过创建Response对象返回JSON格式的数据 return Response(serializer.data) 对比上一个章节的使用bpmapper实现模型序列化的代码，使用DRF的代码更加简单明了，而且DRF本身自带了一套页面，可以方便我们查看我们使用DRF定制的数据接口，如下图所示。 直接使用上一节写好的页面，就可以通过Vue.js把上面接口提供的学科数据渲染并展示出来，此处不再进行赘述。 实现老师信息数据接口编写序列化器。 123456789101112class SubjectSimpleSerializer(serializers.ModelSerializer): class Meta: model = Subject fields = (&#x27;no&#x27;, &#x27;name&#x27;)class TeacherSerializer(serializers.ModelSerializer): class Meta: model = Teacher exclude = (&#x27;subject&#x27;, ) 编写视图函数。 1234567891011@api_view((&#x27;GET&#x27;, ))def show_teachers(request: HttpRequest) -&gt; HttpResponse: try: sno = int(request.GET.get(&#x27;sno&#x27;)) subject = Subject.objects.only(&#x27;name&#x27;).get(no=sno) teachers = Teacher.objects.filter(subject=subject).defer(&#x27;subject&#x27;).order_by(&#x27;no&#x27;) subject_seri = SubjectSimpleSerializer(subject) teacher_seri = TeacherSerializer(teachers, many=True) return Response(&#123;&#x27;subject&#x27;: subject_seri.data, &#x27;teachers&#x27;: teacher_seri.data&#125;) except (TypeError, ValueError, Subject.DoesNotExist): return Response(status=404) 配置URL映射。 12345urlpatterns = [ path(&#x27;api/teachers/&#x27;, show_teachers), ] 通过Vue.js渲染页面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;老师信息&lt;/title&gt; &lt;style&gt; /* 此处省略掉层叠样式表 */ &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;h1&gt;&#123;&#123; subject.name &#125;&#125;学科的老师信息&lt;/h1&gt; &lt;hr&gt; &lt;h2 v-if=&quot;loaded &amp;&amp; teachers.length == 0&quot;&gt;暂无该学科老师信息&lt;/h2&gt; &lt;div class=&quot;teacher&quot; v-for=&quot;teacher in teachers&quot;&gt; &lt;div class=&quot;photo&quot;&gt; &lt;img :src=&quot;&#x27;/static/images/&#x27; + teacher.photo&quot; height=&quot;140&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;info&quot;&gt; &lt;div&gt; &lt;span&gt;&lt;strong&gt;姓名：&#123;&#123; teacher.name &#125;&#125;&lt;/strong&gt;&lt;/span&gt; &lt;span&gt;性别：&#123;&#123; teacher.sex | maleOrFemale &#125;&#125;&lt;/span&gt; &lt;span&gt;出生日期：&#123;&#123; teacher.birth &#125;&#125;&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;intro&quot;&gt;&#123;&#123; teacher.intro &#125;&#125;&lt;/div&gt; &lt;div class=&quot;comment&quot;&gt; &lt;a href=&quot;&quot; @click.prevent=&quot;vote(teacher, true)&quot;&gt;好评&lt;/a&gt;&amp;nbsp;&amp;nbsp; (&lt;strong&gt;&#123;&#123; teacher.good_count &#125;&#125;&lt;/strong&gt;) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;&quot; @click.prevent=&quot;vote(teacher, false)&quot;&gt;差评&lt;/a&gt;&amp;nbsp;&amp;nbsp; (&lt;strong&gt;&#123;&#123; teacher.bad_count &#125;&#125;&lt;/strong&gt;) &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;a href=&quot;/static/html/subjects.html&quot;&gt;返回首页&lt;/a&gt; &lt;/div&gt; &lt;script src=&quot;https://cdn.bootcdn.net/ajax/libs/vue/2.6.11/vue.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; let app = new Vue(&#123; el: &#x27;#container&#x27;, data: &#123; subject: &#123;&#125;, teachers: [], loaded: false &#125;, created() &#123; fetch(&#x27;/api/teachers/&#x27; + location.search) .then(resp =&gt; resp.json()) .then(json =&gt; &#123; this.subject = json.subject this.teachers = json.teachers &#125;) &#125;, filters: &#123; maleOrFemale(sex) &#123; return sex? &#x27;男&#x27;: &#x27;女&#x27; &#125; &#125;, methods: &#123; vote(teacher, flag) &#123; let url = flag? &#x27;/praise/&#x27; : &#x27;/criticize/&#x27; url += &#x27;?tno=&#x27; + teacher.no fetch(url).then(resp =&gt; resp.json()).then(json =&gt; &#123; if (json.code === 10000) &#123; if (flag) &#123; teacher.good_count = json.count &#125; else &#123; teacher.bad_count = json.count &#125; &#125; &#125;) &#125; &#125; &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 前后端分离下的用户登录之前我们提到过， HTTP是无状态的，一次请求结束连接断开，下次服务器再收到请求，它就不知道这个请求是哪个用户发过来的。但是对于一个Web应用而言，它是需要有状态管理的，这样才能让服务器知道HTTP请求来自哪个用户，从而判断是否允许该用户请求以及为用户提供更好的服务，这个过程就是常说的会话管理。 之前我们做会话管理（用户跟踪）的方法是：用户登录成功后，在服务器端通过一个session对象保存用户相关数据，然后把session对象的ID写入浏览器的cookie中；下一次请求时，HTTP请求头中携带cookie的数据，服务器从HTTP请求头读取cookie中的sessionid，根据这个标识符找到对应的session对象，这样就能够获取到之前保存在session中的用户数据。我们刚才说过，REST架构是最适合互联网应用的架构，它强调了HTTP的无状态性，这样才能保证应用的水平扩展能力（当并发访问量增加时，可以通过增加新的服务器节点来为系统扩容）。显然，基于session实现用户跟踪的方式需要服务器保存session对象，在做水平扩展增加新的服务器节点时，需要复制和同步session对象，这显然是非常麻烦的。解决这个问题有两种方案，一种是架设缓存服务器（如Redis），让多个服务器节点共享缓存服务并将session对象直接置于缓存服务器中；另一种方式放弃基于session的用户跟踪，使用基于token的用户跟踪。 基于token的用户跟踪是在用户登录成功后，为用户生成身份标识并保存在浏览器本地存储（localStorage、sessionStorage、cookie等）中，这样的话服务器不需要保存用户状态，从而可以很容易的做到水平扩展。基于token的用户跟踪具体流程如下： 用户登录时，如果登录成功就按照某种方式为用户生成一个令牌（token），该令牌中通常包含了用户标识、过期时间等信息而且需要加密并生成指纹（避免伪造或篡改令牌），服务器将令牌返回给前端； 前端获取到服务器返回的token，保存在浏览器本地存储中（可以保存在localStorage或sessionStorage中，对于使用Vue.js的前端项目来说，还可以通过Vuex进行状态管理）； 对于使用了前端路由的项目来说，前端每次路由跳转，可以先判断localStroage中有无token，如果没有则跳转到登录页； 每次请求后端数据接口，在HTTP请求头里携带token；后端接口判断请求头有无token，如果没有token以及token是无效的或过期的，服务器统一返回401； 如果前端收到HTTP响应状态码401，则重定向到登录页面。 通过上面的描述，相信大家已经发现了，基于token的用户跟踪最为关键是在用户登录成功时，要为用户生成一个token作为用户的身份标识。生成token的方法很多，其中一种比较成熟的解决方案是使用JSON Web Token。 JWT概述JSON Web Token通常简称为JWT，它是一种开放标准（RFC 7519）。随着RESTful架构的流行，越来越多的项目使用JWT作为用户身份认证的方式。JWT相当于是三个JSON对象经过编码后，用.分隔并组合到一起，这三个JSON对象分别是头部（header）、载荷（payload）和签名（signature），如下图所示。 头部 1234&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; 其中，alg属性表示签名的算法，默认是HMAC SHA256（简写成HS256）；typ属性表示这个令牌的类型，JWT中都统一书写为JWT。 载荷 载荷部分用来存放实际需要传递的数据。JWT官方文档中规定了7个可选的字段： iss ：签发人 exp：过期时间 sub：主题 aud：受众 nbf：生效时间 iat：签发时间 jti：编号 除了官方定义的字典，我们可以根据应用的需要添加自定义的字段，如下所示。 12345&#123; &quot;sub&quot;: &quot;1234567890&quot;, &quot;nickname&quot;: &quot;jackfrued&quot;, &quot;role&quot;: &quot;admin&quot;&#125; 签名 签名部分是对前面两部分生成一个指纹，防止数据伪造和篡改。实现签名首先需要指定一个密钥。这个密钥只有服务器才知道，不能泄露给用户。然后，使用头部指定的签名算法（默认是HS256），按照下面的公式产生签名。 1HS256(base64Encode(header) + &#x27;.&#x27; + base64Encode(payload), secret) 算出签名以后，把头部、载荷、签名三个部分拼接成一个字符串，每个部分用.进行分隔，这样一个JWT就生成好了。 JWT的优缺点使用JWT的优点非常明显，包括： 更容易实现水平扩展，因为令牌保存在浏览器中，服务器不需要做状态管理。 更容易防范CSRF攻击，因为在请求头中添加localStorage或sessionStorage中的token必须靠JavaScript代码完成，而不是自动添加到请求头中的。 可以防伪造和篡改，因为JWT有签名，伪造和篡改的令牌无法通过签名验证，会被认定是无效的令牌。 当然，任何技术不可能只有优点没有缺点，JWT也有诸多缺点，大家需要在使用的时候引起注意，具体包括： 可能会遭受到XSS攻击（跨站脚本攻击），通过注入恶意脚本执行JavaScript代码获取到用户令牌。 在令牌过期之前，无法作废已经颁发的令牌，要解决这个问题，还需要额外的中间层和代码来辅助。 JWT是用户的身份令牌，一旦泄露，任何人都可以获得该用户的所有权限。为了降低令牌被盗用后产生的风险，JWT的有效期应该设置得比较短。对于一些比较重要的权限，使用时应通过其他方式再次对用户进行认证，例如短信验证码等。 使用PyJWT在Python代码中，可以使用三方库PyJWT生成和验证JWT，下面是安装PyJWT的命令。 1pip install pyjwt 生成令牌。 12345payload = &#123; &#x27;exp&#x27;: datetime.datetime.utcnow() + datetime.timedelta(days=1), &#x27;userid&#x27;: 10001&#125;token = jwt.encode(payload, settings.SECRET_KEY).decode() 验证令牌。 12345try: token = &#x27;eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE1OTQ4NzIzOTEsInVzZXJpZCI6MTAwMDF9.FM-bNxemWLqQQBIsRVvc4gq71y42I9m2zt5nlFxNHUo&#x27; payload = jwt.decode(token, settings.SECRET_KEY)except InvalidTokenError: raise AuthenticationFailed(&#x27;无效的令牌或令牌已经过期&#x27;) 如果不清楚JWT具体的使用方式，可以先看看第55天的内容，里面提供了完整的投票项目代码的地址。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/53.前后端分离开发入门","date":"2024-12-12T08:38:02.118Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/53.前后端分离开发入门/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/53.%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%BC%80%E5%8F%91%E5%85%A5%E9%97%A8/","excerpt":"","text":"前后端分离开发入门在传统的Web应用开发中，大多数的程序员会将浏览器作为前后端的分界线。将浏览器中为用户进行页面展示的部分称之为前端，而将运行在服务器为前端提供业务逻辑和数据准备的所有代码统称为后端。所谓前后端分离的开发，就是前后端工程师约定好数据交互接口，并行的进行开发和测试，后端只提供数据，不负责将数据渲染到页面上，前端通过HTTP请求获取数据并负责将数据渲染到页面上，这个工作是交给浏览器中的JavaScript代码来完成。 使用前后端分离开发有诸多的好处，下面我们简要的说下这些好处： 提升开发效率。前后端分离以后，可以实现前后端代码的解耦，只要前后端沟通约定好应用所需接口以及接口参数，便可以开始并行开发，无需等待对方的开发工作结束。在这种情况下，前后端工程师都可以只专注于自己的开发工作，有助于打造出更好的团队。除此之外，在前后端分离的开发模式下，即使需求发生变更，只要接口与数据格式不变，后端开发人员就不需要修改代码，只要前端进行变动即可。 增强代码的可维护性。前后端分离后，应用的代码不再是前后端混合，只有在运行期才会有调用依赖关系，这样的话维护代码的工作将变得轻松愉快很多，再不会牵一发而动全身。当你的代码变得简明且整洁时，代码的可读性和可维护性都会有质的提升。 支持多终端和服务化架构。前后端分离后，同一套数据接口可以为不同的终端提供服务，更有助于打造多终端应用；此外，由于后端提供的接口之间可以通过HTTP(S)进行调用，有助于打造服务化架构（包括微服务）。 接下来我们就用前后端分离的方式来改写之前的投票应用。 返回JSON格式的数据刚才说过，在前后端分离的开发模式下，后端需要为前端提供数据接口，这些接口通常返回JSON格式的数据。在Django项目中，我们可以先将对象处理成字典，然后就可以利用Django封装的JsonResponse向浏览器返回JSON格式的数据，具体的做法如下所示。 1234567891011def show_subjects(request): queryset = Subject.objects.all() subjects = [] for subject in queryset: subjects.append(&#123; &#x27;no&#x27;: subject.no, &#x27;name&#x27;: subject.name, &#x27;intro&#x27;: subject.intro, &#x27;isHot&#x27;: subject.is_hot &#125;) return JsonResponse(subjects, safe=False) 上面的代码中，我们通过循环遍历查询学科得到的QuerySet对象，将每个学科的数据处理成一个字典，在将字典保存在名为subjects的列表容器中，最后利用JsonResponse完成对列表的序列化，向浏览器返回JSON格式的数据。由于JsonResponse序列化的是一个列表而不是字典，所以需要指定safe参数的值为False才能完成对subjects的序列化，否则会产生TypeError异常。 可能大家已经发现了，自己写代码将一个对象转成字典是比较麻烦的，如果对象的属性很多而且某些属性又关联到一个比较复杂的对象时，情况会变得更加糟糕。为此我们可以使用一个名为bpmappers的三方库来简化将对象转成字典的操作，这个三方库本身也提供了对Django框架的支持。 安装三方库bpmappers。 1pip install bpmappers 编写映射器（实现对象到字典转换）。 123456789from bpmappers.djangomodel import ModelMapperfrom poll2.models import Subjectclass SubjectMapper(ModelMapper): class Meta: model = Subject 修改视图函数。 123456def show_subjects(request): queryset = Subject.objects.all() subjects = [] for subject in queryset: subjects.append(SubjectMapper(subject).as_dict()) return JsonResponse(subjects, safe=False) 配置URL映射。 12345urlpatterns = [ path(&#x27;api/subjects/&#x27;, show_subjects), ] 然后访问该接口，可以得到如下所示的JSON格式数据。 123456789[ &#123; &quot;no&quot;: 1, &quot;name&quot;: &quot;Python全栈+人工智能&quot;, &quot;intro&quot;: &quot;Python是一种计算机程序设计语言。是一种面向对象的动态类型语言，最初被设计用于编写自动化脚本(shell)，随着版本的不断更新和语言新功能的添加，越来越多被用于独立的、大型项目的开发。&quot;, &quot;is_hot&quot;: true &#125;, // 此处省略下面的内容] 如果不希望在JSON数据中显示学科的成立时间，我们可以在映射器中排除create_date属性；如果希望将是否为热门学科对应的键取名为isHot（默认的名字是is_hot），也可以通过修改映射器来做到。具体的做法如下所示： 123456789101112from bpmappers import RawFieldfrom bpmappers.djangomodel import ModelMapperfrom poll2.models import Subjectclass SubjectMapper(ModelMapper): isHot = RawField(&#x27;is_hot&#x27;) class Meta: model = Subject exclude = (&#x27;is_hot&#x27;, ) 再次查看学科接口返回的JSON数据。 123456789[ &#123; &quot;no&quot;: 101, &quot;name&quot;: &quot;Python全栈+人工智能&quot;, &quot;intro&quot;: &quot;Python是一种计算机程序设计语言。是一种面向对象的动态类型语言，最初被设计用于编写自动化脚本(shell)，随着版本的不断更新和语言新功能的添加，越来越多被用于独立的、大型项目的开发。&quot;, &quot;isHot&quot;: true &#125;, // 此处省略下面的内容] 关于bpmappers详细的使用指南，请参考它的官方文档，这个官方文档是用日语书写的，可以使用浏览器的翻译功能将它翻译成你熟悉的语言即可。 使用Vue.js渲染页面接下来我们通过前端框架Vue.js来实现页面的渲染。如果希望全面的了解和学习Vue.js，建议阅读它的官方教程或者在YouTube上搜索Vue.js的新手教程（Vue.js Crash Course）进行学习。 重新改写subjects.html页面，使用Vue.js来渲染页面。 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;学科信息&lt;/title&gt; &lt;style&gt; /* 此处省略层叠样式表 */ &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;h1&gt;扣丁学堂所有学科&lt;/h1&gt; &lt;hr&gt; &lt;div id=&quot;main&quot;&gt; &lt;dl v-for=&quot;subject in subjects&quot;&gt; &lt;dt&gt; &lt;a :href=&quot;&#x27;/static/html/teachers.html?sno=&#x27; + subject.no&quot;&gt; &#123;&#123; subject.name &#125;&#125; &lt;/a&gt; &lt;img v-if=&quot;subject.is_hot&quot; src=&quot;/static/images/hot-icon-small.png&quot;&gt; &lt;/dt&gt; &lt;dd&gt;&#123;&#123; subject.intro &#125;&#125;&lt;/dd&gt; &lt;/dl&gt; &lt;/div&gt; &lt;/div&gt; &lt;script src=&quot;https://cdn.bootcdn.net/ajax/libs/vue/2.6.11/vue.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; let app = new Vue(&#123; el: &#x27;#main&#x27;, data: &#123; subjects: [] &#125;, created() &#123; fetch(&#x27;/api/subjects/&#x27;) .then(resp =&gt; resp.json()) .then(json =&gt; &#123; this.subjects = json &#125;) &#125; &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 前后端分离的开发需要将前端页面作为静态资源进行部署，项目实际上线的时候，我们会对整个Web应用进行动静分离，静态资源通过Nginx或Apache服务器进行部署，生成动态内容的Python程序部署在uWSGI或者Gunicorn服务器上，对动态内容的请求由Nginx或Apache路由到uWSGI或Gunicorn服务器上。 在开发阶段，我们通常会使用Django自带的测试服务器，如果要尝试前后端分离，可以先将静态页面放在之前创建的放静态资源的目录下，具体的做法可以参考项目完整代码。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/52.中间件的应用","date":"2024-12-12T08:38:02.116Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/52.中间件的应用/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/52.%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E5%BA%94%E7%94%A8/","excerpt":"","text":"中间件的应用之前我们已经实现了用户必须登录才能投票的限制，但是一个新的问题来了。如果我们的应用中有很多功能都需要用户先登录才能执行，例如将前面导出Excel报表和查看统计图表的功能都做了必须登录才能访问的限制，那么我们是不是需要在每个视图函数中添加代码来检查session中是否包含userid的代码呢？答案是否定的，如果这样做了，我们的视图函数中必然会充斥着大量的重复代码。编程大师Martin Fowler曾经说过：代码有很多种坏味道，重复是最坏的一种。在Python程序中，我们可以通过装饰器来为函数提供额外的能力；在Django项目中，我们可以把类似于验证用户是否登录这样的重复性代码放到中间件中。 Django中间件概述中间件是安插在Web应用请求和响应过程之间的组件，它在整个Web应用中扮演了拦截过滤器的角色，通过中间件可以拦截请求和响应，并对请求和响应进行过滤（简单的说就是执行额外的处理）。通常，一个中间件组件只专注于完成一件特定的事，例如：Django框架通过SessionMiddleware中间件实现了对session的支持，又通过AuthenticationMiddleware中间件实现了基于session的请求认证。通过把多个中间件组合在一起，我们可以完成更为复杂的任务，Django框架就是这么做的。 Django项目的配置文件中就包含了对中间件的配置，代码如下所示。 123456789MIDDLEWARE = [ &#x27;django.middleware.security.SecurityMiddleware&#x27;, &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;, &#x27;django.middleware.common.CommonMiddleware&#x27;, &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;, &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;, &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;, &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;,] 我们稍微为大家解释一下这些中间件的作用： CommonMiddleware - 基础设置中间件，可以处理以下一些配置参数。 DISALLOWED_USER_AGENTS - 不被允许的用户代理（浏览器） APPEND_SLASH - 是否追加/ USE_ETAG - 浏览器缓存相关 SecurityMiddleware - 安全相关中间件，可以处理和安全相关的配置项。 SECURE_HSTS_SECONDS - 强制使用HTTPS的时间 SECURE_HSTS_INCLUDE_SUBDOMAINS - HTTPS是否覆盖子域名 SECURE_CONTENT_TYPE_NOSNIFF - 是否允许浏览器推断内容类型 SECURE_BROWSER_XSS_FILTER - 是否启用跨站脚本攻击过滤器 SECURE_SSL_REDIRECT - 是否重定向到HTTPS连接 SECURE_REDIRECT_EXEMPT - 免除重定向到HTTPS SessionMiddleware - 会话中间件。 CsrfViewMiddleware - 通过生成令牌，防范跨请求份伪的造中间件。 XFrameOptionsMiddleware - 通过设置请求头参数，防范点击劫持攻击的中间件。 在请求的过程中，上面的中间件会按照书写的顺序从上到下执行，然后是URL解析，最后请求才会来到视图函数；在响应的过程中，上面的中间件会按照书写的顺序从下到上执行，与请求时中间件执行的顺序正好相反。 自定义中间件Django中的中间件有两种实现方式：基于类的实现方式和基于函数的实现方式，后者更接近于装饰器的写法。装饰器实际上是代理模式的应用，将横切关注功能（与正常业务逻辑没有必然联系的功能，例如：身份认证、日志记录、编码转换之类的功能）置于代理中，由代理对象来完成被代理对象的行为并添加额外的功能。中间件对用户请求和响应进行拦截过滤并增加额外的处理，在这一点上它跟装饰器是完全一致的，所以基于函数的写法来实现中间件就跟装饰器的写法几乎一模一样。下面我们用自定义的中间件来实现用户登录验证的功能。 12345678910111213141516171819202122232425262728&quot;&quot;&quot;middlewares.py&quot;&quot;&quot;from django.http import JsonResponsefrom django.shortcuts import redirect# 需要登录才能访问的资源路径LOGIN_REQUIRED_URLS = &#123;&#x27;/praise/&#x27;, &#x27;/criticize/&#x27;, &#x27;/excel/&#x27;, &#x27;/teachers_data/&#x27;&#125;def check_login_middleware(get_resp): def wrapper(request, *args, **kwargs): # 请求的资源路径在上面的集合中 if request.path in LOGIN_REQUIRED_URLS: # 会话中包含userid则视为已经登录 if &#x27;userid&#x27; not in request.session: # 判断是不是Ajax请求 if request.is_ajax(): # Ajax请求返回JSON数据提示用户登录 return JsonResponse(&#123;&#x27;code&#x27;: 10003, &#x27;hint&#x27;: &#x27;请先登录&#x27;&#125;) else: backurl = request.get_full_path() # 非Ajax请求直接重定向到登录页 return redirect(f&#x27;/login/?backurl=&#123;backurl&#125;&#x27;) return get_resp(request, *args, **kwargs) return wrapper 当然，我们也可以定义一个类来充当装饰器，如果类中有__call__魔术方法，这个类的对象就像函数一样可调用，所以下面是另一种实现中间件的方式，道理跟上面的代码完全一样。 还有一种基于类实现中间件的方式，这种方式在较新版本的Django中已经不推荐使用了，但是大家接触到的代码中，仍然有可能遇到这种写法，大致的代码如下所示。 12345678910111213141516171819from django.utils.deprecation import MiddlewareMixinclass MyMiddleware(MiddlewareMixin): def process_request(self, request): pass def process_view(self, request, view_func, view_args, view_kwargs): pass def process_template_response(self, request, response): pass def process_response(self, request, response): pass def process_exception(self, request, exception): pass 上面类中的五个方法都是中间件的钩子函数，分别在收到用户请求、进入视图函数之前、渲染模板、返回响应和出现异常的时候被回调。当然，写不写这些方法是根据中间件的需求来确定的，并不是所有的场景都需要重写五个方法，下面的图相信能够帮助大家理解这种写法。 写好中间件代码后，需要修改配置文件来激活中间件使其生效。 1234567891011MIDDLEWARE = [ &#x27;django.middleware.security.SecurityMiddleware&#x27;, &#x27;django.contrib.sessions.middleware.SessionMiddleware&#x27;, &#x27;django.middleware.common.CommonMiddleware&#x27;, &#x27;django.middleware.csrf.CsrfViewMiddleware&#x27;, &#x27;django.contrib.auth.middleware.AuthenticationMiddleware&#x27;, &#x27;django.contrib.messages.middleware.MessageMiddleware&#x27;, &#x27;django.middleware.clickjacking.XFrameOptionsMiddleware&#x27;, &#x27;debug_toolbar.middleware.DebugToolbarMiddleware&#x27;, &#x27;vote.middlewares.check_login_middleware&#x27;,] 注意上面这个中间件列表中元素的顺序，当收到来自用户的请求时，中间件按照从上到下的顺序依次执行，这行完这些中间件以后，请求才会最终到达视图函数。当然，在这个过程中，用户的请求可以被拦截，就像上面我们自定义的中间件那样，如果用户在没有登录的情况下访问了受保护的资源，中间件会将请求直接重定向到登录页，后面的中间件和视图函数将不再执行。在响应用户请求的过程中，上面的中间件会按照从下到上的顺序依次执行，这样的话我们还可以对响应做进一步的处理。 中间件执行的顺序是非常重要的，对于有依赖关系的中间件必须保证被依赖的中间件要置于依赖它的中间件的前面，就好比我们刚才自定义的中间件要放到SessionMiddleware的后面，因为我们要依赖这个中间件为请求绑定的session对象才能判定用户是否登录。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/51.日志和调试工具栏","date":"2024-12-12T08:38:02.115Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/51.日志和调试工具栏/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/51.%E6%97%A5%E5%BF%97%E5%92%8C%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E6%A0%8F/","excerpt":"","text":"日志和调试工具栏配置日志项目开发阶段，显示足够的调试信息以辅助开发人员调试代码还是非常必要的；项目上线以后，将系统运行时出现的警告、错误等信息记录下来以备相关人员了解系统运行状况并维护代码也是很有必要的。与此同时，采集日志数据也是为网站做数字化运营奠定一个基础，通过对系统运行日志的分析，我们可以监测网站的流量以及流量分布，同时还可以挖掘出用户的使用习惯和行为模式。 接下来，我们先看看如何通过Django的配置文件来配置日志。Django的日志配置基本可以参照官方文档再结合项目实际需求来进行，这些内容基本上可以从官方文档上复制下来，然后进行局部的调整即可，下面给出一些参考配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263LOGGING = &#123; &#x27;version&#x27;: 1, # 是否禁用已经存在的日志器 &#x27;disable_existing_loggers&#x27;: False, # 日志格式化器 &#x27;formatters&#x27;: &#123; &#x27;simple&#x27;: &#123; &#x27;format&#x27;: &#x27;%(asctime)s %(module)s.%(funcName)s: %(message)s&#x27;, &#x27;datefmt&#x27;: &#x27;%Y-%m-%d %H:%M:%S&#x27;, &#125;, &#x27;verbose&#x27;: &#123; &#x27;format&#x27;: &#x27;%(asctime)s %(levelname)s [%(process)d-%(threadName)s] &#x27; &#x27;%(module)s.%(funcName)s line %(lineno)d: %(message)s&#x27;, &#x27;datefmt&#x27;: &#x27;%Y-%m-%d %H:%M:%S&#x27;, &#125; &#125;, # 日志过滤器 &#x27;filters&#x27;: &#123; # 只有在Django配置文件中DEBUG值为True时才起作用 &#x27;require_debug_true&#x27;: &#123; &#x27;()&#x27;: &#x27;django.utils.log.RequireDebugTrue&#x27;, &#125;, &#125;, # 日志处理器 &#x27;handlers&#x27;: &#123; # 输出到控制台 &#x27;console&#x27;: &#123; &#x27;class&#x27;: &#x27;logging.StreamHandler&#x27;, &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#x27;filters&#x27;: [&#x27;require_debug_true&#x27;], &#x27;formatter&#x27;: &#x27;simple&#x27;, &#125;, # 输出到文件(每周切割一次) &#x27;file1&#x27;: &#123; &#x27;class&#x27;: &#x27;logging.handlers.TimedRotatingFileHandler&#x27;, &#x27;filename&#x27;: &#x27;access.log&#x27;, &#x27;when&#x27;: &#x27;W0&#x27;, &#x27;backupCount&#x27;: 12, &#x27;formatter&#x27;: &#x27;simple&#x27;, &#x27;level&#x27;: &#x27;INFO&#x27;, &#125;, # 输出到文件(每天切割一次) &#x27;file2&#x27;: &#123; &#x27;class&#x27;: &#x27;logging.handlers.TimedRotatingFileHandler&#x27;, &#x27;filename&#x27;: &#x27;error.log&#x27;, &#x27;when&#x27;: &#x27;D&#x27;, &#x27;backupCount&#x27;: 31, &#x27;formatter&#x27;: &#x27;verbose&#x27;, &#x27;level&#x27;: &#x27;WARNING&#x27;, &#125;, &#125;, # 日志器记录器 &#x27;loggers&#x27;: &#123; &#x27;django&#x27;: &#123; # 需要使用的日志处理器 &#x27;handlers&#x27;: [&#x27;console&#x27;, &#x27;file1&#x27;, &#x27;file2&#x27;], # 是否向上传播日志信息 &#x27;propagate&#x27;: True, # 日志级别(不一定是最终的日志级别) &#x27;level&#x27;: &#x27;DEBUG&#x27;, &#125;, &#125;&#125; 大家可能已经注意到了，上面日志配置中的formatters是日志格式化器，它代表了如何格式化输出日志，其中格式占位符分别表示： %(name)s - 记录器的名称 %(levelno)s - 数字形式的日志记录级别 %(levelname)s - 日志记录级别的文本名称 %(filename)s - 执行日志记录调用的源文件的文件名称 %(pathname)s - 执行日志记录调用的源文件的路径名称 %(funcName)s - 执行日志记录调用的函数名称 %(module)s - 执行日志记录调用的模块名称 %(lineno)s - 执行日志记录调用的行号 %(created)s - 执行日志记录的时间 %(asctime)s - 日期和时间 %(msecs)s - 毫秒部分 %(thread)d - 线程ID（整数） %(threadName)s - 线程名称 %(process)d - 进程ID （整数） 日志配置中的handlers用来指定日志处理器，简单的说就是指定将日志输出到控制台还是文件又或者是网络上的服务器，可用的处理器包括： logging.StreamHandler(stream=None) - 可以向类似与sys.stdout或者sys.stderr的任何文件对象输出信息 logging.FileHandler(filename, mode=&#39;a&#39;, encoding=None, delay=False) - 将日志消息写入文件 logging.handlers.DatagramHandler(host, port) - 使用UDP协议，将日志信息发送到指定主机和端口的网络主机上 logging.handlers.HTTPHandler(host, url) - 使用HTTP的GET或POST方法将日志消息上传到一台HTTP 服务器 logging.handlers.RotatingFileHandler(filename, mode=&#39;a&#39;, maxBytes=0, backupCount=0, encoding=None, delay=False) - 将日志消息写入文件，如果文件的大小超出maxBytes指定的值，那么将重新生成一个文件来记录日志 logging.handlers.SocketHandler(host, port) - 使用TCP协议，将日志信息发送到指定主机和端口的网络主机上 logging.handlers.SMTPHandler(mailhost, fromaddr, toaddrs, subject, credentials=None, secure=None, timeout=1.0) - 将日志输出到指定的邮件地址 logging.MemoryHandler(capacity, flushLevel=ERROR, target=None, flushOnClose=True) - 将日志输出到内存指定的缓冲区中 上面每个日志处理器都指定了一个名为level的属性，它代表了日志的级别，不同的日志级别反映出日志中记录信息的严重性。Python中定义了六个级别的日志，按照从低到高的顺序依次是：NOTSET、DEBUG、INFO、WARNING、ERROR、CRITICAL。 最后配置的日志记录器是用来真正输出日志的，Django框架提供了如下所示的内置记录器： django - 在Django层次结构中的所有消息记录器 django.request - 与请求处理相关的日志消息。5xx响应被视为错误消息；4xx响应被视为为警告消息 django.server - 与通过runserver调用的服务器所接收的请求相关的日志消息。5xx响应被视为错误消息；4xx响应被记录为警告消息；其他一切都被记录为INFO django.template - 与模板渲染相关的日志消息 django.db.backends - 有与数据库交互产生的日志消息，如果希望显示ORM框架执行的SQL语句，就可以使用该日志记录器。 日志记录器中配置的日志级别有可能不是最终的日志级别，因为还要参考日志处理器中配置的日志级别，取二者中级别较高者作为最终的日志级别。 配置Django-Debug-Toolbar如果想调试你的Django项目，你一定不能不过名为Django-Debug-Toolbar的神器，它是项目开发阶段辅助调试和优化的必备工具，只要配置了它，就可以很方便的查看到如下表所示的项目运行信息，这些信息对调试项目和优化Web应用性能都是至关重要的。 项目 说明 Versions Django的版本 Time 显示视图耗费的时间 Settings 配置文件中设置的值 Headers HTTP请求头和响应头的信息 Request 和请求相关的各种变量及其信息 StaticFiles 静态文件加载情况 Templates 模板的相关信息 Cache 缓存的使用情况 Signals Django内置的信号信息 Logging 被记录的日志信息 SQL 向数据库发送的SQL语句及其执行时间 安装Django-Debug-Toolbar。 1pip install django-debug-toolbar 配置 - 修改settings.py。 12345678910111213141516INSTALLED_APPS = [ &#x27;debug_toolbar&#x27;,]MIDDLEWARE = [ &#x27;debug_toolbar.middleware.DebugToolbarMiddleware&#x27;,]DEBUG_TOOLBAR_CONFIG = &#123; # 引入jQuery库 &#x27;JQUERY_URL&#x27;: &#x27;https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js&#x27;, # 工具栏是否折叠 &#x27;SHOW_COLLAPSED&#x27;: True, # 是否显示工具栏 &#x27;SHOW_TOOLBAR_CALLBACK&#x27;: lambda x: True,&#125; 配置 - 修改urls.py。 12345if settings.DEBUG: import debug_toolbar urlpatterns.insert(0, path(&#x27;__debug__/&#x27;, include(debug_toolbar.urls))) 在配置好Django-Debug-Toolbar之后，页面右侧会看到一个调试工具栏，如下图所示，上面包括了如前所述的各种调试信息，包括执行时间、项目设置、请求头、SQL、静态资源、模板、缓存、信号等，查看起来非常的方便。 优化ORM代码在配置了日志或Django-Debug-Toolbar之后，我们可以查看一下之前将老师数据导出成Excel报表的视图函数执行情况，这里我们关注的是ORM框架生成的SQL查询到底是什么样子的，相信这里的结果会让你感到有一些意外。执行Teacher.objects.all()之后我们可以注意到，在控制台看到的或者通过Django-Debug-Toolbar输出的SQL是下面这样的： 1234567SELECT `tb_teacher`.`no`, `tb_teacher`.`name`, `tb_teacher`.`detail`, `tb_teacher`.`photo`, `tb_teacher`.`good_count`, `tb_teacher`.`bad_count`, `tb_teacher`.`sno` FROM `tb_teacher`; args=()SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 101; args=(101,)SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 101; args=(101,)SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 101; args=(101,)SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 101; args=(101,)SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 103; args=(103,)SELECT `tb_subject`.`no`, `tb_subject`.`name`, `tb_subject`.`intro`, `tb_subject`.`create_date`, `tb_subject`.`is_hot` FROM `tb_subject` WHERE `tb_subject`.`no` = 103; args=(103,) 这里的问题通常被称为“1+N查询”（有的地方也将其称之为“N+1查询”），原本获取老师的数据只需要一条SQL，但是由于老师关联了学科，当我们查询到N条老师的数据时，Django的ORM框架又向数据库发出了N条SQL去查询老师所属学科的信息。每条SQL执行都会有较大的开销而且会给数据库服务器带来压力，如果能够在一条SQL中完成老师和学科的查询肯定是更好的做法，这一点也很容易做到，相信大家已经想到怎么做了。是的，我们可以使用连接查询，但是在使用Django的ORM框架时如何做到这一点呢？对于多对一关联（如投票应用中的老师和学科），我们可以使用QuerySet的用select_related()方法来加载关联对象；而对于多对多关联（如电商网站中的订单和商品），我们可以使用prefetch_related()方法来加载关联对象。 在导出老师Excel报表的视图函数中，我们可以按照下面的方式优化代码。 1queryset = Teacher.objects.all().select_related(&#x27;subject&#x27;) 事实上，用ECharts生成前端报表的视图函数中，查询老师好评和差评数据的操作也能够优化，因为在这个例子中，我们只需要获取老师的姓名、好评数和差评数这三项数据，但是在默认的情况生成的SQL会查询老师表的所有字段。可以用QuerySet的only()方法来指定需要查询的属性，也可以用QuerySet的defer()方法来指定暂时不需要查询的属性，这样生成的SQL会通过投影操作来指定需要查询的列，从而改善查询性能，代码如下所示： 1queryset = Teacher.objects.all().only(&#x27;name&#x27;, &#x27;good_count&#x27;, &#x27;bad_count&#x27;) 当然，如果要统计出每个学科的老师好评和差评的平均数，利用Django的ORM框架也能够做到，代码如下所示： 1queryset = Teacher.objects.values(&#x27;subject&#x27;).annotate(good=Avg(&#x27;good_count&#x27;), bad=Avg(&#x27;bad_count&#x27;)) 这里获得的QuerySet中的元素是字典对象，每个字典中有三组键值对，分别是代表学科编号的subject、代表好评数的good和代表差评数的bad。如果想要获得学科的名称而不是编号，可以按照如下所示的方式调整代码： 1queryset = Teacher.objects.values(&#x27;subject__name&#x27;).annotate(good=Avg(&#x27;good_count&#x27;), bad=Avg(&#x27;bad_count&#x27;)) 可见，Django的ORM框架允许我们用面向对象的方式完成关系数据库中的分组和聚合查询。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/50.制作报表","date":"2024-12-12T08:38:02.112Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/50.制作报表/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/50.%E5%88%B6%E4%BD%9C%E6%8A%A5%E8%A1%A8/","excerpt":"","text":"制作报表导出Excel报表报表就是用表格、图表等格式来动态显示数据，所以有人用这样的公式来描述报表： 1报表 = 多样的格式 + 动态的数据 有很多的三方库支持在Python程序中写Excel文件，包括xlwt、xlwings、openpyxl、xlswriter等，其中的xlwt虽然只支持写xls格式的Excel文件，但在性能方面的表现还是不错的。下面我们就以xlwt为例，来演示如何在Django项目中导出Excel报表。 安装xlwt。 1pip install xlwt 导出包含所有老师信息的Excel表格的视图函数。 1234567891011121314151617181920212223242526272829def export_teachers_excel(request): # 创建工作簿 wb = xlwt.Workbook() # 添加工作表 sheet = wb.add_sheet(&#x27;老师信息表&#x27;) # 查询所有老师的信息 queryset = Teacher.objects.all() # 向Excel表单中写入表头 colnames = (&#x27;姓名&#x27;, &#x27;介绍&#x27;, &#x27;好评数&#x27;, &#x27;差评数&#x27;, &#x27;学科&#x27;) for index, name in enumerate(colnames): sheet.write(0, index, name) # 向单元格中写入老师的数据 props = (&#x27;name&#x27;, &#x27;detail&#x27;, &#x27;good_count&#x27;, &#x27;bad_count&#x27;, &#x27;subject&#x27;) for row, teacher in enumerate(queryset): for col, prop in enumerate(props): value = getattr(teacher, prop, &#x27;&#x27;) if isinstance(value, Subject): value = value.name sheet.write(row + 1, col, value) # 保存Excel buffer = BytesIO() wb.save(buffer) # 将二进制数据写入响应的消息体中并设置MIME类型 resp = HttpResponse(buffer.getvalue(), content_type=&#x27;application/vnd.ms-excel&#x27;) # 中文文件名需要处理成百分号编码 filename = quote(&#x27;老师.xls&#x27;) # 通过响应头告知浏览器下载该文件以及对应的文件名 resp[&#x27;content-disposition&#x27;] = f&#x27;attachment; filename*=utf-8\\&#x27;\\&#x27;&#123;filename&#125;&#x27; return resp 映射URL。 12345urlpatterns = [ path(&#x27;excel/&#x27;, views.export_teachers_excel), ] 导出PDF报表在Django项目中，如果需要导出PDF报表，可以借助三方库reportlab来生成PDF文件的内容，再将文件的二进制数据输出给浏览器并指定MIME类型为application/pdf，具体的代码如下所示。 1234567891011def export_pdf(request: HttpRequest) -&gt; HttpResponse: buffer = io.BytesIO() pdf = canvas.Canvas(buffer) pdf.setFont(&quot;Helvetica&quot;, 80) pdf.setFillColorRGB(0.2, 0.5, 0.3) pdf.drawString(100, 550, &#x27;hello, world!&#x27;) pdf.showPage() pdf.save() resp = HttpResponse(buffer.getvalue(), content_type=&#x27;application/pdf&#x27;) resp[&#x27;content-disposition&#x27;] = &#x27;inline; filename=&quot;demo.pdf&quot;&#x27; return resp 关于如何用reportlab定制PDF报表的内容，可以参考reportlab的官方文档。 生成前端统计图表如果项目中需要生成前端统计图表，可以使用百度的ECharts。具体的做法是后端通过提供数据接口返回统计图表所需的数据，前端使用ECharts来渲染出柱状图、折线图、饼图、散点图等图表。例如我们要生成一个统计所有老师好评数和差评数的报表，可以按照下面的方式来做。 123456def get_teachers_data(request): queryset = Teacher.objects.all() names = [teacher.name for teacher in queryset] good_counts = [teacher.good_count for teacher in queryset] bad_counts = [teacher.bad_count for teacher in queryset] return JsonResponse(&#123;&#x27;names&#x27;: names, &#x27;good&#x27;: good_counts, &#x27;bad&#x27;: bad_counts&#125;) 映射URL。 123urlpatterns = [ path(&#x27;teachers_data/&#x27;, views.get_teachers_data),] 使用ECharts生成柱状图。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;老师评价统计&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px; height: 400px&quot;&gt;&lt;/div&gt; &lt;p&gt; &lt;a href=&quot;/&quot;&gt;返回首页&lt;/a&gt; &lt;/p&gt; &lt;script src=&quot;https://cdn.bootcss.com/echarts/4.2.1-rc1/echarts.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var myChart = echarts.init(document.querySelector(&#x27;#main&#x27;)) fetch(&#x27;/teachers_data/&#x27;) .then(resp =&gt; resp.json()) .then(json =&gt; &#123; var option = &#123; color: [&#x27;#f00&#x27;, &#x27;#00f&#x27;], title: &#123; text: &#x27;老师评价统计图&#x27; &#125;, tooltip: &#123;&#125;, legend: &#123; data:[&#x27;好评&#x27;, &#x27;差评&#x27;] &#125;, xAxis: &#123; data: json.names &#125;, yAxis: &#123;&#125;, series: [ &#123; name: &#x27;好评&#x27;, type: &#x27;bar&#x27;, data: json.good &#125;, &#123; name: &#x27;差评&#x27;, type: &#x27;bar&#x27;, data: json.bad &#125; ] &#125; myChart.setOption(option) &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 运行效果如下图所示。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/49.Cookie和Session","date":"2024-12-12T08:38:02.110Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/49.Cookie和Session/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/49.Cookie%E5%92%8CSession/","excerpt":"","text":"Cookie和Session我们继续来完成上一章节中的项目，实现“用户登录”的功能，并限制只有登录的用户才能投票。 用户登录的准备工作我们先为实现用户登录做一些准备工作。 创建用户模型。之前我们讲解过如果通过Django的ORM实现从二维表到模型的转换（反向工程），这次我们尝试把模型变成二维表（正向工程）。 12345678910111213class User(models.Model): &quot;&quot;&quot;用户&quot;&quot;&quot; no = models.AutoField(primary_key=True, verbose_name=&#x27;编号&#x27;) username = models.CharField(max_length=20, unique=True, verbose_name=&#x27;用户名&#x27;) password = models.CharField(max_length=32, verbose_name=&#x27;密码&#x27;) tel = models.CharField(max_length=20, verbose_name=&#x27;手机号&#x27;) reg_date = models.DateTimeField(auto_now_add=True, verbose_name=&#x27;注册时间&#x27;) last_visit = models.DateTimeField(null=True, verbose_name=&#x27;最后登录时间&#x27;) class Meta: db_table = &#x27;tb_user&#x27; verbose_name = &#x27;用户&#x27; verbose_name_plural = &#x27;用户&#x27; 使用下面的命令生成迁移文件并执行迁移，将User模型直接变成关系型数据库中的二维表tb_user。 12python manage.py makemigrations pollspython manage.py migrate polls 用下面的SQL语句直接插入两条测试数据，通常不能将用户的密码直接保存在数据库中，因此我们将用户密码处理成对应的MD5摘要。MD5消息摘要算法是一种被广泛使用的密码哈希函数（散列函数），可以产生出一个128位（比特）的哈希值（散列值），用于确保信息传输完整一致。在使用哈希值时，通常会将哈希值表示为16进制字符串，因此128位的MD5摘要通常表示为32个十六进制符号。 12345insert into `tb_user` (`username`, `password`, `tel`, `reg_date`)values (&#x27;wangdachui&#x27;, &#x27;1c63129ae9db9c60c3e8aa94d3e00495&#x27;, &#x27;13122334455&#x27;, now()), (&#x27;hellokitty&#x27;, &#x27;c6f8cf68e5f68b0aa4680e089ee4742c&#x27;, &#x27;13890006789&#x27;, now()); 说明：上面创建的两个用户wangdachui和hellokitty密码分别是1qaz2wsx和Abc123!!。 我们在应用下增加一个名为utils.py的模块用来保存需要使用的工具函数。Python标准库中的hashlib模块封装了常用的哈希算法，包括：MD5、SHA1、SHA256等。下面是使用hashlib中的md5类将字符串处理成MD5摘要的函数如下所示。 12345import hashlibdef gen_md5_digest(content): return hashlib.md5(content.encode()).hexdigest() 编写用户登录的视图函数和模板页。 添加渲染登录页面的视图函数： 123def login(request: HttpRequest) -&gt; HttpResponse: hint = &#x27;&#x27; return render(request, &#x27;login.html&#x27;, &#123;&#x27;hint&#x27;: hint&#125;) 增加login.html模板页： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;用户登录&lt;/title&gt; &lt;style&gt; #container &#123; width: 520px; margin: 10px auto; &#125; .input &#123; margin: 20px 0; width: 460px; height: 40px; &#125; .input&gt;label &#123; display: inline-block; width: 140px; text-align: right; &#125; .input&gt;img &#123; width: 150px; vertical-align: middle; &#125; input[name=captcha] &#123; vertical-align: middle; &#125; form+div &#123; margin-top: 20px; &#125; form+div&gt;a &#123; text-decoration: none; color: darkcyan; font-size: 1.2em; &#125; .button &#123; width: 500px; text-align: center; margin-top: 20px; &#125; .hint &#123; color: red; font-size: 12px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;h1&gt;用户登录&lt;/h1&gt; &lt;hr&gt; &lt;p class=&quot;hint&quot;&gt;&#123;&#123; hint &#125;&#125;&lt;/p&gt; &lt;form action=&quot;/login/&quot; method=&quot;post&quot;&gt; &#123;% csrf_token %&#125; &lt;fieldset&gt; &lt;legend&gt;用户信息&lt;/legend&gt; &lt;div class=&quot;input&quot;&gt; &lt;label&gt;用户名：&lt;/label&gt; &lt;input type=&quot;text&quot; name=&quot;username&quot;&gt; &lt;/div&gt; &lt;div class=&quot;input&quot;&gt; &lt;label&gt;密码：&lt;/label&gt; &lt;input type=&quot;password&quot; name=&quot;password&quot;&gt; &lt;/div&gt; &lt;div class=&quot;input&quot;&gt; &lt;label&gt;验证码：&lt;/label&gt; &lt;input type=&quot;text&quot; name=&quot;captcha&quot;&gt; &lt;img id=&quot;code&quot; src=&quot;/captcha/&quot; alt=&quot;&quot; width=&quot;150&quot; height=&quot;40&quot;&gt; &lt;/div&gt; &lt;/fieldset&gt; &lt;div class=&quot;button&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;登录&quot;&gt; &lt;input type=&quot;reset&quot; value=&quot;重置&quot;&gt; &lt;/div&gt; &lt;/form&gt; &lt;div&gt; &lt;a href=&quot;/&quot;&gt;返回首页&lt;/a&gt; &lt;a href=&quot;/register/&quot;&gt;注册新用户&lt;/a&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意，在上面的表单中，我们使用了模板指令&#123;% csrf_token %&#125;为表单添加一个隐藏域（大家可以在浏览器中显示网页源代码就可以看到这个指令生成的type属性为hidden的input标签），它的作用是在表单中生成一个随机令牌（token）来防范跨站请求伪造（简称为CSRF），这也是Django在提交表单时的硬性要求。如果我们的表单中没有这样的令牌，那么提交表单时，Django框架会产生一个响应状态码为403的响应（禁止访问），除非我们设置了免除CSRF令牌。下图是一个关于CSRF简单生动的例子。 接下来，我们可以编写提供验证码和实现用户登录的视图函数，在此之前，我们先说说一个Web应用实现用户跟踪的方式以及Django框架对实现用户跟踪所提供的支持。对一个Web应用来说，用户登录成功后必然要让服务器能够记住该用户已经登录，这样服务器才能为这个用户提供更好的服务，而且上面说到的CSRF也是通过钓鱼网站来套取用户登录信息进行恶意操作的攻击手段，这些都是以用户跟踪技术为基础的。在理解了这些背景知识后，我们就清楚用户登录时到底需要执行哪些操作。 实现用户跟踪如今，一个网站如果不通过某种方式记住你是谁以及你之前在网站的活动情况，失去的就是网站的可用性和便利性，继而很有可能导致网站用户的流式，所以记住一个用户（更专业的说法叫用户跟踪）对绝大多数Web应用来说都是必需的功能。 在服务器端，我们想记住一个用户最简单的办法就是创建一个对象，通过这个对象就可以把用户相关的信息都保存起来，这个对象就是我们常说的session（用户会话对象）。那么问题来了，HTTP本身是一个无连接（每次请求和响应的过程中，服务器一旦完成对客户端请求的响应之后就断开连接）、无状态（客户端再次发起对服务器的请求时，服务器无法得知这个客户端之前的任何信息）的协议，即便服务器通过session对象保留了用户数据，还得通过某种方式来确定当前的请求与之前保存过的哪一个session是有关联的。相信很多人都能想到，我们可以给每个session对象分配一个全局唯一的标识符来识别session对象，我们姑且称之为sessionid，每次客户端发起请求时，只要携带上这个sessionid，就有办法找到与之对应的session对象，从而实现在两次请求之间记住该用户的信息，也就是我们之前说的用户跟踪。 要让客户端记住并在每次请求时带上sessionid又有以下几种做法： URL重写。所谓URL重写就是在URL中携带sessionid，例如：http://www.example.com/index.html?sessionid=123456，服务器通过获取sessionid参数的值来取到与之对应的session对象。 隐藏域（隐式表单域）。在提交表单的时候，可以通过在表单中设置隐藏域向服务器发送额外的数据。例如：&lt;input type=&quot;hidden&quot; name=&quot;sessionid&quot; value=&quot;123456&quot;&gt;。 本地存储。现在的浏览器都支持多种本地存储方案，包括：cookie、localStorage、sessionStorage、IndexedDB等。在这些方案中，cookie是历史最为悠久也是被诟病得最多的一种方案，也是我们接下来首先为大家讲解的一种方案。简单的说，cookie是一种以键值对方式保存在浏览器临时文件中的数据，每次请求时，请求头中会携带本站点的cookie到服务器，那么只要将sessionid写入cookie，下次请求时服务器只要读取请求头中的cookie就能够获得这个sessionid，如下图所示。 在HTML5时代要，除了cookie，还可以使用新的本地存储API来保存数据，就是刚才提到的localStorage、sessionStorage、IndexedDB等技术，如下图所示。 总结一下，要实现用户跟踪，服务器端可以为每个用户会话创建一个session对象并将session对象的ID写入到浏览器的cookie中；用户下次请求服务器时，浏览器会在HTTP请求头中携带该网站保存的cookie信息，这样服务器就可以从cookie中找到session对象的ID并根据此ID获取到之前创建的session对象；由于session对象可以用键值对的方式保存用户数据，这样之前保存在session对象中的信息可以悉数取出，服务器也可以根据这些信息判定用户身份和了解用户偏好，为用户提供更好的个性化服务。 Django框架对session的支持在创建Django项目时，默认的配置文件settings.py文件中已经激活了一个名为SessionMiddleware的中间件（关于中间件的知识我们在后面的章节做详细讲解，这里只需要知道它的存在即可），因为这个中间件的存在，我们可以直接通过请求对象的session属性来操作会话对象。前面我们说过，session属性是一个像字典一样可以读写数据的容器对象，因此我们可以使用“键值对”的方式来保留用户数据。与此同时，SessionMiddleware中间件还封装了对cookie的操作，在cookie中保存了sessionid，这一点我们在上面已经提到过了。 在默认情况下，Django将session的数据序列化后保存在关系型数据库中，在Django 1.6以后的版本中，默认的序列化数据的方式是JSON序列化，而在此之前一直使用Pickle序列化。JSON序列化和Pickle序列化的差别在于前者将对象序列化为字符串（字符形式），而后者将对象序列化为字节串（二进制形式），因为安全方面的原因，JSON序列化成为了目前Django框架默认序列化数据的方式，这就要求在我们保存在session中的数据必须是能够JSON序列化的，否则就会引发异常。还有一点需要说明的是，使用关系型数据库保存session中的数据在大多数时候并不是最好的选择，因为数据库可能会承受巨大的压力而成为系统性能的瓶颈，在后面的章节中我们会告诉大家如何将session保存到缓存服务中以提升系统的性能。 实现用户登录验证首先，我们在刚才的polls/utils.py文件中编写生成随机验证码的函数gen_random_code，内容如下所示。 1234567import randomALL_CHARS = &#x27;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&#x27;def gen_random_code(length=4): return &#x27;&#x27;.join(random.choices(ALL_CHARS, k=length)) 编写生成验证码图片的类Captcha。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199&quot;&quot;&quot;图片验证码&quot;&quot;&quot;import osimport randomfrom io import BytesIOfrom PIL import Imagefrom PIL import ImageFilterfrom PIL.ImageDraw import Drawfrom PIL.ImageFont import truetypeclass Bezier: &quot;&quot;&quot;贝塞尔曲线&quot;&quot;&quot; def __init__(self): self.tsequence = tuple([t / 20.0 for t in range(21)]) self.beziers = &#123;&#125; def make_bezier(self, n): &quot;&quot;&quot;绘制贝塞尔曲线&quot;&quot;&quot; try: return self.beziers[n] except KeyError: combinations = pascal_row(n - 1) result = [] for t in self.tsequence: tpowers = (t ** i for i in range(n)) upowers = ((1 - t) ** i for i in range(n - 1, -1, -1)) coefs = [c * a * b for c, a, b in zip(combinations, tpowers, upowers)] result.append(coefs) self.beziers[n] = result return resultclass Captcha: &quot;&quot;&quot;验证码&quot;&quot;&quot; def __init__(self, width, height, fonts=None, color=None): self._image = None self._fonts = fonts if fonts else \\ [os.path.join(os.path.dirname(__file__), &#x27;fonts&#x27;, font) for font in [&#x27;Arial.ttf&#x27;, &#x27;Georgia.ttf&#x27;, &#x27;Action.ttf&#x27;]] self._color = color if color else random_color(0, 200, random.randint(220, 255)) self._width, self._height = width, height @classmethod def instance(cls, width=200, height=75): &quot;&quot;&quot;用于获取Captcha对象的类方法&quot;&quot;&quot; prop_name = f&#x27;_instance_&#123;width&#125;_&#123;height&#125;&#x27; if not hasattr(cls, prop_name): setattr(cls, prop_name, cls(width, height)) return getattr(cls, prop_name) def _background(self): &quot;&quot;&quot;绘制背景&quot;&quot;&quot; Draw(self._image).rectangle([(0, 0), self._image.size], fill=random_color(230, 255)) def _smooth(self): &quot;&quot;&quot;平滑图像&quot;&quot;&quot; return self._image.filter(ImageFilter.SMOOTH) def _curve(self, width=4, number=6, color=None): &quot;&quot;&quot;绘制曲线&quot;&quot;&quot; dx, height = self._image.size dx /= number path = [(dx * i, random.randint(0, height)) for i in range(1, number)] bcoefs = Bezier().make_bezier(number - 1) points = [] for coefs in bcoefs: points.append(tuple(sum([coef * p for coef, p in zip(coefs, ps)]) for ps in zip(*path))) Draw(self._image).line(points, fill=color if color else self._color, width=width) def _noise(self, number=50, level=2, color=None): &quot;&quot;&quot;绘制扰码&quot;&quot;&quot; width, height = self._image.size dx, dy = width / 10, height / 10 width, height = width - dx, height - dy draw = Draw(self._image) for i in range(number): x = int(random.uniform(dx, width)) y = int(random.uniform(dy, height)) draw.line(((x, y), (x + level, y)), fill=color if color else self._color, width=level) def _text(self, captcha_text, fonts, font_sizes=None, drawings=None, squeeze_factor=0.75, color=None): &quot;&quot;&quot;绘制文本&quot;&quot;&quot; color = color if color else self._color fonts = tuple([truetype(name, size) for name in fonts for size in font_sizes or (65, 70, 75)]) draw = Draw(self._image) char_images = [] for c in captcha_text: font = random.choice(fonts) c_width, c_height = draw.textsize(c, font=font) char_image = Image.new(&#x27;RGB&#x27;, (c_width, c_height), (0, 0, 0)) char_draw = Draw(char_image) char_draw.text((0, 0), c, font=font, fill=color) char_image = char_image.crop(char_image.getbbox()) for drawing in drawings: d = getattr(self, drawing) char_image = d(char_image) char_images.append(char_image) width, height = self._image.size offset = int((width - sum(int(i.size[0] * squeeze_factor) for i in char_images[:-1]) - char_images[-1].size[0]) / 2) for char_image in char_images: c_width, c_height = char_image.size mask = char_image.convert(&#x27;L&#x27;).point(lambda i: i * 1.97) self._image.paste(char_image, (offset, int((height - c_height) / 2)), mask) offset += int(c_width * squeeze_factor) @staticmethod def _warp(image, dx_factor=0.3, dy_factor=0.3): &quot;&quot;&quot;图像扭曲&quot;&quot;&quot; width, height = image.size dx = width * dx_factor dy = height * dy_factor x1 = int(random.uniform(-dx, dx)) y1 = int(random.uniform(-dy, dy)) x2 = int(random.uniform(-dx, dx)) y2 = int(random.uniform(-dy, dy)) warp_image = Image.new( &#x27;RGB&#x27;, (width + abs(x1) + abs(x2), height + abs(y1) + abs(y2))) warp_image.paste(image, (abs(x1), abs(y1))) width2, height2 = warp_image.size return warp_image.transform( (width, height), Image.QUAD, (x1, y1, -x1, height2 - y2, width2 + x2, height2 + y2, width2 - x2, -y1)) @staticmethod def _offset(image, dx_factor=0.1, dy_factor=0.2): &quot;&quot;&quot;图像偏移&quot;&quot;&quot; width, height = image.size dx = int(random.random() * width * dx_factor) dy = int(random.random() * height * dy_factor) offset_image = Image.new(&#x27;RGB&#x27;, (width + dx, height + dy)) offset_image.paste(image, (dx, dy)) return offset_image @staticmethod def _rotate(image, angle=25): &quot;&quot;&quot;图像旋转&quot;&quot;&quot; return image.rotate(random.uniform(-angle, angle), Image.BILINEAR, expand=1) def generate(self, captcha_text=&#x27;&#x27;, fmt=&#x27;PNG&#x27;): &quot;&quot;&quot;生成验证码(文字和图片) :param captcha_text: 验证码文字 :param fmt: 生成的验证码图片格式 :return: 验证码图片的二进制数据 &quot;&quot;&quot; self._image = Image.new(&#x27;RGB&#x27;, (self._width, self._height), (255, 255, 255)) self._background() self._text(captcha_text, self._fonts, drawings=[&#x27;_warp&#x27;, &#x27;_rotate&#x27;, &#x27;_offset&#x27;]) self._curve() self._noise() self._smooth() image_bytes = BytesIO() self._image.save(image_bytes, format=fmt) return image_bytes.getvalue()def pascal_row(n=0): &quot;&quot;&quot;生成毕达哥拉斯三角形（杨辉三角）&quot;&quot;&quot; result = [1] x, numerator = 1, n for denominator in range(1, n // 2 + 1): x *= numerator x /= denominator result.append(x) numerator -= 1 if n &amp; 1 == 0: result.extend(reversed(result[:-1])) else: result.extend(reversed(result)) return resultdef random_color(start=0, end=255, opacity=255): &quot;&quot;&quot;获得随机颜色&quot;&quot;&quot; red = random.randint(start, end) green = random.randint(start, end) blue = random.randint(start, end) if opacity is None: return red, green, blue return red, green, blue, opacity 说明：上面的代码中用到了三个字体文件，字体文件位于polls/fonts目录下，大家可以自行添加字体文件，但是需要注意字体文件的文件名跟上面代码的第45行保持一致。 接下来，我们先完成提供验证码的视图函数。 123456def get_captcha(request: HttpRequest) -&gt; HttpResponse: &quot;&quot;&quot;验证码&quot;&quot;&quot; captcha_text = gen_random_code() request.session[&#x27;captcha&#x27;] = captcha_text image_data = Captcha.instance().generate(captcha_text) return HttpResponse(image_data, content_type=&#x27;image/png&#x27;) 注意上面代码中的第4行，我们将随机生成的验证码字符串保存到session中，稍后用户登录时，我们要将保存在session中的验证码字符串和用户输入的验证码字符串进行比对，如果用户输入了正确的验证码才能够执行后续的登录流程，代码如下所示。 1234567891011121314151617def login(request: HttpRequest) -&gt; HttpResponse: hint = &#x27;&#x27; if request.method == &#x27;POST&#x27;: username = request.POST.get(&#x27;username&#x27;) password = request.POST.get(&#x27;password&#x27;) if username and password: password = gen_md5_digest(password) user = User.objects.filter(username=username, password=password).first() if user: request.session[&#x27;userid&#x27;] = user.no request.session[&#x27;username&#x27;] = user.username return redirect(&#x27;/&#x27;) else: hint = &#x27;用户名或密码错误&#x27; else: hint = &#x27;请输入有效的用户名和密码&#x27; return render(request, &#x27;login.html&#x27;, &#123;&#x27;hint&#x27;: hint&#125;) 说明：上面的代码没有对用户名和密码没有进行验证，实际项目中建议使用正则表达式验证用户输入信息，否则有可能将无效的数据交给数据库进行处理或者造成其他安全方面的隐患。 上面的代码中，我们设定了登录成功后会在session中保存用户的编号（userid）和用户名（username），页面会重定向到首页。接下来我们可以稍微对首页的代码进行调整，在页面的右上角显示出登录用户的用户名。我们将这段代码单独写成了一个名为header.html的HTML文件，首页中可以通过在&lt;body&gt;标签中添加&#123;% include 'header.html' %&#125;来包含这个页面，代码如下所示。 123456789&lt;div class=&quot;user&quot;&gt; &#123;% if request.session.userid %&#125; &lt;span&gt;&#123;&#123; request.session.username &#125;&#125;&lt;/span&gt; &lt;a href=&quot;/logout&quot;&gt;注销&lt;/a&gt; &#123;% else %&#125; &lt;a href=&quot;/login&quot;&gt;登录&lt;/a&gt;&amp;nbsp;&amp;nbsp; &#123;% endif %&#125; &lt;a href=&quot;/register&quot;&gt;注册&lt;/a&gt;&lt;/div&gt; 如果用户没有登录，页面会显示登录和注册的超链接；而用户登录成功后，页面上会显示用户名和注销的链接，注销链接对应的视图函数如下所示，URL的映射与之前讲过的类似，不再赘述。 1234def logout(request): &quot;&quot;&quot;注销&quot;&quot;&quot; request.session.flush() return redirect(&#x27;/&#x27;) 上面的代码通过session对象flush方法来销毁session，一方面清除了服务器上session对象保存的用户数据，一方面将保存在浏览器cookie中的sessionid删除掉，稍后我们会对如何读写cookie的操作加以说明。 我们可以通过项目使用的数据库中名为django_session 的表来找到所有的session，该表的结构如下所示： session_key session_data expire_date c9g2gt5cxo0k2evykgpejhic5ae7bfpl MmI4YzViYjJhOGMyMDJkY2M5Yzg3… 2019-05-25 23:16:13.898522 其中，第1列就是浏览器cookie中保存的sessionid；第2列是经过BASE64编码后的session中的数据，如果使用Python的base64对其进行解码，解码的过程和结果如下所示。 123import base64base64.b64decode(&#x27;MmI4YzViYjJhOGMyMDJkY2M5Yzg3ZWIyZGViZmUzYmYxNzdlNDdmZjp7ImNhcHRjaGEiOiJzS3d0Iiwibm8iOjEsInVzZXJuYW1lIjoiamFja2ZydWVkIn0=&#x27;) 第3列是session的过期时间，session过期后浏览器保存的cookie中的sessionid就会失效，但是数据库中的这条对应的记录仍然会存在，如果想清除过期的数据，可以使用下面的命令。 1python manage.py clearsessions Django框架默认的session过期时间为两周（1209600秒），如果想修改这个时间，可以在项目的配置文件中添加如下所示的代码。 12# 配置会话的超时时间为1天（86400秒）SESSION_COOKIE_AGE = 86400 有很多对安全性要求较高的应用都必须在关闭浏览器窗口时让会话过期，不再保留用户的任何信息，如果希望在关闭浏览器窗口时就让会话过期（cookie中的sessionid失效），可以加入如下所示的配置。 12# 设置为True在关闭浏览器窗口时session就过期SESSION_EXPIRE_AT_BROWSER_CLOSE = True 如果不希望将session的数据保存在数据库中，可以将其放入缓存中，对应的配置如下所示，缓存的配置和使用我们在后面讲解。 1234# 配置将会话对象放到缓存中存储SESSION_ENGINE = &#x27;django.contrib.sessions.backends.cache&#x27;# 配置使用哪一组缓存来保存会话SESSION_CACHE_ALIAS = &#x27;default&#x27; 如果要修改session数据默认的序列化方式，可以将默认的JSONSerializer修改为PickleSerializer。 1SESSION_SERIALIZER = &#x27;django.contrib.sessions.serializers.PickleSerializer&#x27; 接下来，我们就可以限制只有登录用户才能为老师投票，修改后的praise_or_criticize函数如下所示，我们通过从request.session中获取userid来判定用户是否登录。 123456789101112131415161718def praise_or_criticize(request: HttpRequest) -&gt; HttpResponse: if request.session.get(&#x27;userid&#x27;): try: tno = int(request.GET.get(&#x27;tno&#x27;)) teacher = Teacher.objects.get(no=tno) if request.path.startswith(&#x27;/praise/&#x27;): teacher.good_count += 1 count = teacher.good_count else: teacher.bad_count += 1 count = teacher.bad_count teacher.save() data = &#123;&#x27;code&#x27;: 20000, &#x27;mesg&#x27;: &#x27;投票成功&#x27;, &#x27;count&#x27;: count&#125; except (ValueError, Teacher.DoesNotExist): data = &#123;&#x27;code&#x27;: 20001, &#x27;mesg&#x27;: &#x27;投票失败&#x27;&#125; else: data = &#123;&#x27;code&#x27;: 20002, &#x27;mesg&#x27;: &#x27;请先登录&#x27;&#125; return JsonResponse(data) 当然，在修改了视图函数后，teachers.html也需要进行调整，用户如果没有登录，就将用户引导至登录页，登录成功再返回到投票页，此处不再赘述。 在视图函数中读写cookie下面我们对如何使用cookie做一个更为细致的说明以便帮助大家在Web项目中更好的使用这项技术。Django封装的HttpRequest和HttpResponse对象分别提供了读写cookie的操作。 HttpRequest封装的属性和方法： COOKIES属性 - 该属性包含了HTTP请求携带的所有cookie。 get_signed_cookie方法 - 获取带签名的cookie，如果签名验证失败，会产生BadSignature异常。 HttpResponse封装的方法： set_cookie方法 - 该方法可以设置一组键值对并将其最终将写入浏览器。 set_signed_cookie方法 - 跟上面的方法作用相似，但是会对cookie进行签名来达到防篡改的作用。因为如果篡改了cookie中的数据，在不知道密钥和盐的情况下是无法生成有效的签名，这样服务器在读取cookie时会发现数据与签名不一致从而产生BadSignature异常。需要说明的是，这里所说的密钥就是我们在Django项目配置文件中指定的SECRET_KEY，而盐是程序中设定的一个字符串，你愿意设定为什么都可以，只要是一个有效的字符串。 上面提到的方法，如果不清楚它们的具体用法，可以自己查阅一下Django的官方文档，没有什么资料比官方文档能够更清楚的告诉你这些方法到底如何使用。 刚才我们说过了，激活SessionMiddleware之后，每个HttpRequest对象都会绑定一个session属性，它是一个类似字典的对象，除了保存用户数据之外还提供了检测浏览器是否支持cookie的方法，包括： set_test_cookie方法 - 设置用于测试的cookie。 test_cookie_worked方法 - 检测测试cookie是否工作。 delete_test_cookie方法 - 删除用于测试的cookie。 set_expiry方法 - 设置会话的过期时间。 get_expire_age&#x2F;get_expire_date方法 - 获取会话的过期时间。 clear_expired方法 - 清理过期的会话。 下面是在执行登录之前检查浏览器是否支持cookie的代码。通常情况下，浏览器默认开启了对cookie的支持，但是可能因为某种原因，用户禁用了浏览器的cookie功能，遇到这种情况我们可以在视图函数中提供一个检查功能，如果检查到用户浏览器不支持cookie，可以给出相应的提示。 123456789def login(request): if request.method == &#x27;POST&#x27;: if request.session.test_cookie_worked(): request.session.delete_test_cookie() # Add your code to perform login process here else: return HttpResponse(&quot;Please enable cookies and try again.&quot;) request.session.set_test_cookie() return render_to_response(&#x27;login.html&#x27;) Cookie的替代品之前我们说过了，cookie的名声一直都不怎么好，当然我们在实际开发中是不会在cookie中保存用户的敏感信息（如用户的密码、信用卡的账号等）的，而且保存在cookie中的数据一般也会做好编码和签名的工作。对于支持HTML5的浏览器来说，可以使用localStorage和sessionStorage做为cookie的替代方案，相信从名字上你就能听出二者的差别，存储在localStorage的数据可以长期保留；而存储在sessionStorage的数据会在浏览器关闭时会被清除 。关于这些cookie替代品的用法，建议大家查阅MDN来进行了解。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/48.静态资源和Ajax请求","date":"2024-12-12T08:38:02.107Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/48.静态资源和Ajax请求/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/48.%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%92%8CAjax%E8%AF%B7%E6%B1%82/","excerpt":"","text":"静态资源和Ajax请求加载静态资源如果要在Django项目中使用静态资源，可以先创建一个用于保存静态资源的目录。在vote项目中，我们将静态资源置于名为static的文件夹中，在该文件夹包含了三个子文件夹：css、js和images，分别用来保存外部CSS文件、外部JavaScript文件和图片资源，如下图所示。 为了能够找到保存静态资源的文件夹，我们还需要修改Django项目的配置文件settings.py，如下所示： 12STATICFILES_DIRS = [os.path.join(BASE_DIR, &#x27;static&#x27;), ]STATIC_URL = &#x27;/static/&#x27; 配置好静态资源之后，大家可以运行项目，然后看看之前我们写的页面上的图片是否能够正常加载出来。需要说明的是，在项目正式部署到线上环境后，我们通常会把静态资源交给专门的静态资源服务器（如Nginx、Apache）来处理，而不是有运行Python代码的服务器来管理静态资源，所以上面的配置并不适用于生产环境，仅供项目开发阶段测试使用。使用静态资源的正确姿势我们会在后续的章节为大家讲解。 Ajax概述接下来就可以实现“好评”和“差评”的功能了，很明显如果能够在不刷新页面的情况下实现这两个功能会带来更好的用户体验，因此我们考虑使用Ajax技术来实现“好评”和“差评”。Ajax是Asynchronous Javascript And XML的缩写 , 简单的说，使用Ajax技术可以在不重新加载整个页面的情况下对页面进行局部刷新。 对于传统的Web应用，每次页面上需要加载新的内容都需要重新请求服务器并刷新整个页面，如果服务器短时间内无法给予响应或者网络状况并不理想，那么可能会造成浏览器长时间的空白并使得用户处于等待状态，在这个期间用户什么都做不了，如下图所示。很显然，这样的Web应用并不能带来很好的用户体验。 对于使用Ajax技术的Web应用，浏览器可以向服务器发起异步请求来获取数据。异步请求不会中断用户体验，当服务器返回了新的数据，我们可以通过JavaScript代码进行DOM操作来实现对页面的局部刷新，这样就相当于在不刷新整个页面的情况下更新了页面的内容，如下图所示。 在使用Ajax技术时，浏览器跟服务器通常会交换XML或JSON格式的数据，XML是以前使用得非常多的一种数据格式，近年来几乎已经完全被JSON取代，下面是两种数据格式的对比。 XML格式： 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;message&gt; &lt;from&gt;Alice&lt;/from&gt; &lt;to&gt;Bob&lt;/to&gt; &lt;content&gt;Dinner is on me!&lt;/content&gt;&lt;/message&gt; JSON格式： 12345&#123; &quot;from&quot;: &quot;Alice&quot;, &quot;to&quot;: &quot;Bob&quot;, &quot;content&quot;: &quot;Dinner is on me!&quot;&#125; 通过上面的对比，明显JSON格式的数据要紧凑得多，所以传输效率更高，而且JSON本身也是JavaScript中的一种对象表达式语法，在JavaScript代码中处理JSON格式的数据更加方便。 用Ajax实现投票功能下面，我们使用Ajax技术来实现投票的功能，首先修改项目的urls.py文件，为“好评”和“差评”功能映射对应的URL。 123456789101112from django.contrib import adminfrom django.urls import pathfrom vote import viewsurlpatterns = [ path(&#x27;&#x27;, views.show_subjects), path(&#x27;teachers/&#x27;, views.show_teachers), path(&#x27;praise/&#x27;, views.praise_or_criticize), path(&#x27;criticize/&#x27;, views.praise_or_criticize), path(&#x27;admin/&#x27;, admin.site.urls),] 设计视图函数praise_or_criticize来支持“好评”和“差评”功能，该视图函数通过Django封装的JsonResponse类将字典序列化成JSON字符串作为返回给浏览器的响应内容。 12345678910111213141516def praise_or_criticize(request): &quot;&quot;&quot;好评&quot;&quot;&quot; try: tno = int(request.GET.get(&#x27;tno&#x27;)) teacher = Teacher.objects.get(no=tno) if request.path.startswith(&#x27;/praise&#x27;): teacher.good_count += 1 count = teacher.good_count else: teacher.bad_count += 1 count = teacher.bad_count teacher.save() data = &#123;&#x27;code&#x27;: 20000, &#x27;mesg&#x27;: &#x27;操作成功&#x27;, &#x27;count&#x27;: count&#125; except (ValueError, Teacher.DoseNotExist): data = &#123;&#x27;code&#x27;: 20001, &#x27;mesg&#x27;: &#x27;操作失败&#x27;&#125; return JsonResponse(data) 修改显示老师信息的模板页，引入jQuery库来实现事件处理、Ajax请求和DOM操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;老师信息&lt;/title&gt; &lt;style&gt; #container &#123; width: 80%; margin: 10px auto; &#125; .teacher &#123; width: 100%; margin: 0 auto; padding: 10px 0; border-bottom: 1px dashed gray; overflow: auto; &#125; .teacher&gt;div &#123; float: left; &#125; .photo &#123; height: 140px; border-radius: 75px; overflow: hidden; margin-left: 20px; &#125; .info &#123; width: 75%; margin-left: 30px; &#125; .info div &#123; clear: both; margin: 5px 10px; &#125; .info span &#123; margin-right: 25px; &#125; .info a &#123; text-decoration: none; color: darkcyan; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;h1&gt;&#123;&#123; subject.name &#125;&#125;学科的老师信息&lt;/h1&gt; &lt;hr&gt; &#123;% if not teachers %&#125; &lt;h2&gt;暂无该学科老师信息&lt;/h2&gt; &#123;% endif %&#125; &#123;% for teacher in teachers %&#125; &lt;div class=&quot;teacher&quot;&gt; &lt;div class=&quot;photo&quot;&gt; &lt;img src=&quot;/static/images/&#123;&#123; teacher.photo &#125;&#125;&quot; height=&quot;140&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;info&quot;&gt; &lt;div&gt; &lt;span&gt;&lt;strong&gt;姓名：&#123;&#123; teacher.name &#125;&#125;&lt;/strong&gt;&lt;/span&gt; &lt;span&gt;性别：&#123;&#123; teacher.sex | yesno:&#x27;男,女&#x27; &#125;&#125;&lt;/span&gt; &lt;span&gt;出生日期：&#123;&#123; teacher.birth &#125;&#125;&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;intro&quot;&gt;&#123;&#123; teacher.intro &#125;&#125;&lt;/div&gt; &lt;div class=&quot;comment&quot;&gt; &lt;a href=&quot;/praise/?tno=&#123;&#123; teacher.no &#125;&#125;&quot;&gt;好评&lt;/a&gt;&amp;nbsp;&amp;nbsp; (&lt;strong&gt;&#123;&#123; teacher.good_count &#125;&#125;&lt;/strong&gt;) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;/criticize/?tno=&#123;&#123; teacher.no &#125;&#125;&quot;&gt;差评&lt;/a&gt;&amp;nbsp;&amp;nbsp; (&lt;strong&gt;&#123;&#123; teacher.bad_count &#125;&#125;&lt;/strong&gt;) &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &#123;% endfor %&#125; &lt;a href=&quot;/&quot;&gt;返回首页&lt;/a&gt; &lt;/div&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $(() =&gt; &#123; $(&#x27;.comment&gt;a&#x27;).on(&#x27;click&#x27;, (evt) =&gt; &#123; evt.preventDefault() let url = $(evt.target).attr(&#x27;href&#x27;) $.getJSON(url, (json) =&gt; &#123; if (json.code == 20000) &#123; $(evt.target).next().text(json.count) &#125; else &#123; alert(json.mesg) &#125; &#125;) &#125;) &#125;) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 上面的前端代码中，使用了jQuery库封装的getJSON方法向服务器发送异步请求，如果不熟悉前端的jQuery库，可以参考《jQuery API手册》。 小结到此为止，这个投票项目的核心功能已然完成，在下面的章节中我们会要求用户必须登录才能投票，没有账号的用户可以通过注册功能注册一个账号。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/47.深入模型","date":"2024-12-12T08:38:02.105Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/47.深入模型/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/47.%E6%B7%B1%E5%85%A5%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"深入模型在上一个章节中，我们提到了Django是基于MVC架构的Web框架，MVC架构追求的是“模型”和“视图”的解耦合。所谓“模型”说得更直白一些就是数据（的表示），所以通常也被称作“数据模型”。在实际的项目中，数据模型通常通过数据库实现持久化操作，而关系型数据库在过去和当下都是持久化的首选方案，下面我们通过完成一个投票项目来讲解和模型相关的知识点。投票项目的首页会展示某在线教育平台所有的学科；点击学科可以查看到该学科的老师及其信息；用户登录后在查看老师的页面为老师投票，可以投赞成票和反对票；未登录的用户可以通过登录页进行登录；尚未注册的用户可以通过注册页输入个人信息进行注册。在这个项目中，我们使用MySQL数据库来实现数据持久化操作。 创建项目和应用我们首先创建Django项目vote并为其添加虚拟环境和依赖项。接下来，在项目下创建名为polls的应用和保存模板页的文件夹tempaltes，项目文件夹的结构如下所示。 根据上面描述的项目需求，我们准备了四个静态页面，分别是展示学科的页面subjects.html，显示学科老师的页面teachers.html，登录页面login.html，注册页面register.html，稍后我们会将静态页修改为Django项目所需的模板页。 配置关系型数据库MySQL 在MySQL中创建数据库，创建用户，授权用户访问该数据库。 1234create database vote default charset utf8;create user &#x27;hellokitty&#x27;@&#x27;%&#x27; identified by &#x27;Hellokitty.618&#x27;;grant all privileges on vote.* to &#x27;hellokitty&#x27;@&#x27;%&#x27;;flush privileges; 在MySQL中创建保存学科和老师信息的二维表（保存用户信息的表稍后处理）。 1234567891011121314151617181920212223242526use vote;-- 创建学科表create table `tb_subject`( `no` integer auto_increment comment &#x27;学科编号&#x27;, `name` varchar(50) not null comment &#x27;学科名称&#x27;, `intro` varchar(1000) not null default &#x27;&#x27; comment &#x27;学科介绍&#x27;, `is_hot` boolean not null default 0 comment &#x27;是不是热门学科&#x27;, primary key (`no`));-- 创建老师表create table `tb_teacher`( `no` integer auto_increment comment &#x27;老师编号&#x27;, `name` varchar(20) not null comment &#x27;老师姓名&#x27;, `sex` boolean not null default 1 comment &#x27;老师性别&#x27;, `birth` date not null comment &#x27;出生日期&#x27;, `intro` varchar(1000) not null default &#x27;&#x27; comment &#x27;老师介绍&#x27;, `photo` varchar(255) not null default &#x27;&#x27; comment &#x27;老师照片&#x27;, `gcount` integer not null default 0 comment &#x27;好评数&#x27;, `bcount` integer not null default 0 comment &#x27;差评数&#x27;, `sno` integer not null comment &#x27;所属学科&#x27;, primary key (`no`), foreign key (`sno`) references `tb_subject` (`no`)); 在虚拟环境中安装连接MySQL数据库所需的依赖项。 1pip install mysqlclient 说明：如果因为某些原因无法安装mysqlclient三方库，可以使用它的替代品pymysql，pymysql是用纯Python开发的连接MySQL的Python库，安装更容易成功，但是需要在Django项目文件夹的__init__.py中添加如下所示的代码。 123import pymysqlpymysql.install_as_MySQLdb() 如果使用Django 2.2及以上版本，还会遇到PyMySQL跟Django框架的兼容性问题，兼容性问题会导致项目无法运行，需要按照GitHub上PyMySQL仓库Issues中提供的方法进行处理。总体来说，使用pymysql会比较麻烦，强烈建议大家首选安装mysqlclient。 修改项目的settings.py文件，首先将我们创建的应用polls添加已安装的项目（INSTALLED_APPS）中，然后配置MySQL作为持久化方案。 1234567891011121314151617181920212223242526272829INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;, &#x27;django.contrib.auth&#x27;, &#x27;django.contrib.contenttypes&#x27;, &#x27;django.contrib.sessions&#x27;, &#x27;django.contrib.messages&#x27;, &#x27;django.contrib.staticfiles&#x27;, &#x27;polls&#x27;,]DATABASES = &#123; &#x27;default&#x27;: &#123; # 数据库引擎配置 &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;, # 数据库的名字 &#x27;NAME&#x27;: &#x27;vote&#x27;, # 数据库服务器的IP地址（本机可以写localhost或127.0.0.1） &#x27;HOST&#x27;: &#x27;localhost&#x27;, # 启动MySQL服务的端口号 &#x27;PORT&#x27;: 3306, # 数据库用户名和口令 &#x27;USER&#x27;: &#x27;hellokitty&#x27;, &#x27;PASSWORD&#x27;: &#x27;Hellokitty.618&#x27;, # 数据库使用的字符集 &#x27;CHARSET&#x27;: &#x27;utf8&#x27;, # 数据库时间日期的时区设定 &#x27;TIME_ZONE&#x27;: &#x27;Asia/Chongqing&#x27;, &#125;&#125; 在配置ENGINE属性时，常用的可选值包括： &#39;django.db.backends.sqlite3&#39;：SQLite嵌入式数据库。 &#39;django.db.backends.postgresql&#39;：BSD许可证下发行的开源关系型数据库产品。 &#39;django.db.backends.mysql&#39;：甲骨文公司经济高效的数据库产品。 &#39;django.db.backends.oracle&#39;：甲骨文公司关系型数据库旗舰产品。 其他的配置可以参考官方文档中数据库配置的部分。 Django框架提供了ORM来解决数据持久化问题，ORM翻译成中文叫“对象关系映射”。因为Python是面向对象的编程语言，我们在Python程序中使用对象模型来保存数据，而关系型数据库使用关系模型，用二维表来保存数据，这两种模型并不匹配。使用ORM是为了实现对象模型到关系模型的双向转换，这样就不用在Python代码中书写SQL语句和游标操作，因为这些都会由ORM自动完成。利用Django的ORM，我们可以直接将刚才创建的学科表和老师表变成Django中的模型类。 1python manage.py inspectdb &gt; polls/models.py 我们可以对自动生成的模型类稍作调整，代码如下所示。 12345678910111213141516171819202122232425262728from django.db import modelsclass Subject(models.Model): no = models.AutoField(primary_key=True, verbose_name=&#x27;编号&#x27;) name = models.CharField(max_length=50, verbose_name=&#x27;名称&#x27;) intro = models.CharField(max_length=1000, verbose_name=&#x27;介绍&#x27;) is_hot = models.BooleanField(verbose_name=&#x27;是否热门&#x27;) class Meta: managed = False db_table = &#x27;tb_subject&#x27;class Teacher(models.Model): no = models.AutoField(primary_key=True, verbose_name=&#x27;编号&#x27;) name = models.CharField(max_length=20, verbose_name=&#x27;姓名&#x27;) sex = models.BooleanField(default=True, verbose_name=&#x27;性别&#x27;) birth = models.DateField(verbose_name=&#x27;出生日期&#x27;) intro = models.CharField(max_length=1000, verbose_name=&#x27;个人介绍&#x27;) photo = models.ImageField(max_length=255, verbose_name=&#x27;照片&#x27;) good_count = models.IntegerField(default=0, db_column=&#x27;gcount&#x27;, verbose_name=&#x27;好评数&#x27;) bad_count = models.IntegerField(default=0, db_column=&#x27;bcount&#x27;, verbose_name=&#x27;差评数&#x27;) subject = models.ForeignKey(Subject, models.DO_NOTHING, db_column=&#x27;sno&#x27;) class Meta: managed = False db_table = &#x27;tb_teacher&#x27; 说明：模型类都直接或间接继承自Model类，模型类跟关系型数据库的二维表对应，模型对象跟表中的记录对应，模型对象的属性跟表中的字段对应。如果对上面模型类的属性定义不是特别理解，可以看看本文后面提供的“模型定义参考”部分的内容。 使用ORM完成模型的CRUD操作有了Django框架的ORM，我们可以直接使用面向对象的方式来实现对数据的CRUD（增删改查）操作。我们可以在PyCharm的终端中输入下面的命令进入到Django项目的交互式环境，然后尝试对模型的操作。 1python manage.py shell 新增1234567from polls.models import Subjectsubject1 = Subject(name=&#x27;Python全栈开发&#x27;, intro=&#x27;当下最热门的学科&#x27;, is_hot=True)subject1.save()subject2 = Subject(name=&#x27;全栈软件测试&#x27;, intro=&#x27;学习自动化测试的学科&#x27;, is_hot=False)subject2.save()subject3 = Subject(name=&#x27;JavaEE分布式开发&#x27;, intro=&#x27;基于Java语言的服务器应用开发&#x27;, is_hot=True) 删除12subject = Subject.objects.get(no=2)subject.delete() 更新123subject = Subject.objects.get(no=1)subject.name = &#x27;Python全栈+人工智能&#x27;subject.save() 查询 查询所有对象。 1Subjects.objects.all() 过滤数据。 123456789101112131415161718# 查询名称为“Python全栈+人工智能”的学科Subject.objects.filter(name=&#x27;Python全栈+人工智能&#x27;)# 查询名称包含“全栈”的学科（模糊查询）Subject.objects.filter(name__contains=&#x27;全栈&#x27;)Subject.objects.filter(name__startswith=&#x27;全栈&#x27;)Subject.objects.filter(name__endswith=&#x27;全栈&#x27;)# 查询所有热门学科Subject.objects.filter(is_hot=True)# 查询编号大于3小于10的学科Subject.objects.filter(no__gt=3).filter(no__lt=10)Subject.objects.filter(no__gt=3, no__lt=10)# 查询编号在3到7之间的学科Subject.objects.filter(no__ge=3, no__le=7)Subject.objects.filter(no__range=(3, 7)) 查询单个对象。 12345# 查询主键为1的学科Subject.objects.get(pk=1)Subject.objects.get(no=1)Subject.objects.filter(no=1).first()Subject.objects.filter(no=1).last() 排序。 1234# 查询所有学科按编号升序排列Subject.objects.order_by(&#x27;no&#x27;)# 查询所有部门按部门编号降序排列Subject.objects.order_by(&#x27;-no&#x27;) 切片（分页查询）。 12# 按编号从小到大查询前3个学科Subject.objects.order_by(&#x27;no&#x27;)[:3] 计数。 12# 查询一共有多少个学科Subject.objects.count() 高级查询。 123456# 查询编号为1的学科的老师Teacher.objects.filter(subject__no=1)Subject.objects.get(pk=1).teacher_set.all() # 查询学科名称有“全栈”二字的学科的老师Teacher.objects.filter(subject__name__contains=&#x27;全栈&#x27;) 说明1：由于老师与学科之间存在多对一外键关联，所以能通过学科反向查询到该学科的老师（从一对多关系中“一”的一方查询“多”的一方），反向查询属性默认的名字是类名小写_set（如上面例子中的teacher_set），当然也可以在创建模型时通过ForeingKey的related_name属性指定反向查询属性的名字。如果不希望执行反向查询可以将related_name属性设置为&#39;+&#39;或者以&#39;+&#39;开头的字符串。 说明2：ORM查询多个对象时会返回QuerySet对象，QuerySet使用了惰性查询，即在创建QuerySet对象的过程中不涉及任何数据库活动，等真正用到对象时（对QuerySet求值）才向数据库发送SQL语句并获取对应的结果，这一点在实际开发中需要引起注意！ 说明3：如果希望更新多条数据，不用先逐一获取模型对象再修改对象属性，可以直接使用QuerySet对象的update()方法一次性更新多条数据。 利用Django后台管理模型在创建好模型类之后，可以通过Django框架自带的后台管理应用（admin应用）实现对模型的管理。虽然实际应用中，这个后台可能并不能满足我们的需求，但是在学习Django框架时，我们可以利用admin应用来管理我们的模型，同时也通过它来了解一个项目的后台管理系统需要哪些功能。使用Django自带的admin应用步骤如下所示。 将admin应用所需的表迁移到数据库中。admin应用本身也需要数据库的支持，而且在admin应用中已经定义好了相关的数据模型类，我们只需要通过模型迁移操作就能自动在数据库中生成所需的二维表。 1python manage.py migrate 创建访问admin应用的超级用户账号，这里需要输入用户名、邮箱和口令。 1python manage.py createsuperuser 说明：输入口令时没有回显也不能退格，需要一气呵成完成输入。 运行项目，在浏览器中访问http://127.0.0.1:8000/admin，输入刚才创建的超级用户账号和密码进行登录。 登录后进入管理员操作平台。 注意，我们暂时还没能在admin应用中看到之前创建的模型类，为此需要在polls应用的admin.py文件中对需要管理的模型进行注册。 注册模型类。 123456from django.contrib import adminfrom polls.models import Subject, Teacheradmin.site.register(Subject)admin.site.register(Teacher) 注册模型类后，就可以在后台管理系统中看到它们。 对模型进行CRUD操作。 可以在管理员平台对模型进行C（新增）、R（查看）、U（更新）、D（删除）操作，如下图所示。 添加学科。 查看所有学科。 删除和更新学科。 注册模型管理类。 可能大家已经注意到了，刚才在后台查看部门信息的时候，显示的部门信息并不直观，为此我们再修改admin.py文件，通过注册模型管理类，可以在后台管理系统中更好的管理模型。 12345678910111213141516171819from django.contrib import adminfrom polls.models import Subject, Teacherclass SubjectModelAdmin(admin.ModelAdmin): list_display = (&#x27;no&#x27;, &#x27;name&#x27;, &#x27;intro&#x27;, &#x27;is_hot&#x27;) search_fields = (&#x27;name&#x27;, ) ordering = (&#x27;no&#x27;, )class TeacherModelAdmin(admin.ModelAdmin): list_display = (&#x27;no&#x27;, &#x27;name&#x27;, &#x27;sex&#x27;, &#x27;birth&#x27;, &#x27;good_count&#x27;, &#x27;bad_count&#x27;, &#x27;subject&#x27;) search_fields = (&#x27;name&#x27;, ) ordering = (&#x27;no&#x27;, )admin.site.register(Subject, SubjectModelAdmin)admin.site.register(Teacher, TeacherModelAdmin) 为了更好的查看模型，我们为Subject类添加__str__魔法方法，并在该方法中返回学科名字。这样在如上图所示的查看老师的页面上显示老师所属学科时，就不再是Subject object(1)这样晦涩的信息，而是学科的名称。 实现学科页和老师页效果 修改polls/views.py文件，编写视图函数实现对学科页和老师页的渲染。 1234567891011121314151617181920212223from django.shortcuts import render, redirectfrom polls.models import Subject, Teacherdef show_subjects(request): subjects = Subject.objects.all().order_by(&#x27;no&#x27;) return render(request, &#x27;subjects.html&#x27;, &#123;&#x27;subjects&#x27;: subjects&#125;)def show_teachers(request): try: sno = int(request.GET.get(&#x27;sno&#x27;)) teachers = [] if sno: subject = Subject.objects.only(&#x27;name&#x27;).get(no=sno) teachers = Teacher.objects.filter(subject=subject).order_by(&#x27;no&#x27;) return render(request, &#x27;teachers.html&#x27;, &#123; &#x27;subject&#x27;: subject, &#x27;teachers&#x27;: teachers &#125;) except (ValueError, Subject.DoesNotExist): return redirect(&#x27;/&#x27;) 修改templates/subjects.html和templates/teachers.html模板页。 subjects.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;学科信息&lt;/title&gt; &lt;style&gt; #container &#123; width: 80%; margin: 10px auto; &#125; .user &#123; float: right; margin-right: 10px; &#125; .user&gt;a &#123; margin-right: 10px; &#125; #main&gt;dl&gt;dt &#123; font-size: 1.5em; font-weight: bold; &#125; #main&gt;dl&gt;dd &#123; font-size: 1.2em; &#125; a &#123; text-decoration: none; color: darkcyan; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;div class=&quot;user&quot;&gt; &lt;a href=&quot;login.html&quot;&gt;用户登录&lt;/a&gt; &lt;a href=&quot;register.html&quot;&gt;快速注册&lt;/a&gt; &lt;/div&gt; &lt;h1&gt;扣丁学堂所有学科&lt;/h1&gt; &lt;hr&gt; &lt;div id=&quot;main&quot;&gt; &#123;% for subject in subjects %&#125; &lt;dl&gt; &lt;dt&gt; &lt;a href=&quot;/teachers/?sno=&#123;&#123; subject.no &#125;&#125;&quot;&gt;&#123;&#123; subject.name &#125;&#125;&lt;/a&gt; &#123;% if subject.is_hot %&#125; &lt;img src=&quot;/static/images/hot-icon-small.png&quot;&gt; &#123;% endif %&#125; &lt;/dt&gt; &lt;dd&gt;&#123;&#123; subject.intro &#125;&#125;&lt;/dd&gt; &lt;/dl&gt; &#123;% endfor %&#125; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; teachers.html 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;老师信息&lt;/title&gt; &lt;style&gt; #container &#123; width: 80%; margin: 10px auto; &#125; .teacher &#123; width: 100%; margin: 0 auto; padding: 10px 0; border-bottom: 1px dashed gray; overflow: auto; &#125; .teacher&gt;div &#123; float: left; &#125; .photo &#123; height: 140px; border-radius: 75px; overflow: hidden; margin-left: 20px; &#125; .info &#123; width: 75%; margin-left: 30px; &#125; .info div &#123; clear: both; margin: 5px 10px; &#125; .info span &#123; margin-right: 25px; &#125; .info a &#123; text-decoration: none; color: darkcyan; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;container&quot;&gt; &lt;h1&gt;&#123;&#123; subject.name &#125;&#125;学科的老师信息&lt;/h1&gt; &lt;hr&gt; &#123;% if not teachers %&#125; &lt;h2&gt;暂无该学科老师信息&lt;/h2&gt; &#123;% endif %&#125; &#123;% for teacher in teachers %&#125; &lt;div class=&quot;teacher&quot;&gt; &lt;div class=&quot;photo&quot;&gt; &lt;img src=&quot;/static/images/&#123;&#123; teacher.photo &#125;&#125;&quot; height=&quot;140&quot; alt=&quot;&quot;&gt; &lt;/div&gt; &lt;div class=&quot;info&quot;&gt; &lt;div&gt; &lt;span&gt;&lt;strong&gt;姓名：&#123;&#123; teacher.name &#125;&#125;&lt;/strong&gt;&lt;/span&gt; &lt;span&gt;性别：&#123;&#123; teacher.sex | yesno:&#x27;男,女&#x27; &#125;&#125;&lt;/span&gt; &lt;span&gt;出生日期：&#123;&#123; teacher.birth | date:&#x27;Y年n月j日&#x27;&#125;&#125;&lt;/span&gt; &lt;/div&gt; &lt;div class=&quot;intro&quot;&gt;&#123;&#123; teacher.intro &#125;&#125;&lt;/div&gt; &lt;div class=&quot;comment&quot;&gt; &lt;a href=&quot;&quot;&gt;好评&lt;/a&gt;&amp;nbsp;(&lt;strong&gt;&#123;&#123; teacher.good_count &#125;&#125;&lt;/strong&gt;) &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&quot;&quot;&gt;差评&lt;/a&gt;&amp;nbsp;&lt;strong&gt;&#123;&#123; teacher.bad_count &#125;&#125;&lt;/strong&gt;) &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &#123;% endfor %&#125; &lt;a href=&quot;/&quot;&gt;返回首页&lt;/a&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 修改vote/urls.py文件，实现映射URL。 12345678910from django.contrib import adminfrom django.urls import pathfrom polls.views import show_subjects, show_teachersurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;&#x27;, show_subjects), path(&#x27;teachers/&#x27;, show_teachers),] 到此为止，页面上需要的图片（静态资源）还没有能够正常展示，我们在下一章节中为大家介绍如何处理模板页上的需要的静态资源。 补充内容Django模型最佳实践 正确的为模型和关系字段命名。 设置适当的related_name属性。 用OneToOneField代替ForeignKeyField(unique=True)。 通过“迁移操作”（migrate）来添加模型。 用NoSQL来应对需要降低范式级别的场景。 如果布尔类型可以为空要使用NullBooleanField。 在模型中放置业务逻辑。 用&lt;ModelName&gt;.DoesNotExists取代ObjectDoesNotExists。 在数据库中不要出现无效数据。 不要对QuerySet调用len()函数。 将QuerySet的exists()方法的返回值用于if条件。 用DecimalField来存储货币相关数据而不是FloatField。 定义__str__方法。 不要将数据文件放在同一个目录中。 说明：以上内容来自于STEELKIWI网站的Best Practice working with Django models in Python，有兴趣的小伙伴可以阅读原文。 模型定义参考字段对字段名称的限制 字段名不能是Python的保留字，否则会导致语法错误 字段名不能有多个连续下划线，否则影响ORM查询操作 Django模型字段类 字段类 说明 AutoField 自增ID字段 BigIntegerField 64位有符号整数 BinaryField 存储二进制数据的字段，对应Python的bytes类型 BooleanField 存储True或False CharField 长度较小的字符串 DateField 存储日期，有auto_now和auto_now_add属性 DateTimeField 存储日期和日期，两个附加属性同上 DecimalField 存储固定精度小数，有max_digits（有效位数）和decimal_places（小数点后面）两个必要的参数 DurationField 存储时间跨度 EmailField 与CharField相同，可以用EmailValidator验证 FileField 文件上传字段 FloatField 存储浮点数 ImageField 其他同FileFiled，要验证上传的是不是有效图像 IntegerField 存储32位有符号整数。 GenericIPAddressField 存储IPv4或IPv6地址 NullBooleanField 存储True、False或null值 PositiveIntegerField 存储无符号整数（只能存储正数） SlugField 存储slug（简短标注） SmallIntegerField 存储16位有符号整数 TextField 存储数据量较大的文本 TimeField 存储时间 URLField 存储URL的CharField UUIDField 存储全局唯一标识符 字段属性通用字段属性 选项 说明 null 数据库中对应的字段是否允许为NULL，默认为False blank 后台模型管理验证数据时，是否允许为NULL，默认为False choices 设定字段的选项，各元组中的第一个值是设置在模型上的值，第二值是人类可读的值 db_column 字段对应到数据库表中的列名，未指定时直接使用字段的名称 db_index 设置为True时将在该字段创建索引 db_tablespace 为有索引的字段设置使用的表空间，默认为DEFAULT_INDEX_TABLESPACE default 字段的默认值 editable 字段在后台模型管理或ModelForm中是否显示，默认为True error_messages 设定字段抛出异常时的默认消息的字典，其中的键包括null、blank、invalid、invalid_choice、unique和unique_for_date help_text 表单小组件旁边显示的额外的帮助文本。 primary_key 将字段指定为模型的主键，未指定时会自动添加AutoField用于主键，只读。 unique 设置为True时，表中字段的值必须是唯一的 verbose_name 字段在后台模型管理显示的名称，未指定时使用字段的名称 ForeignKey属性 limit_choices_to：值是一个Q对象或返回一个Q对象，用于限制后台显示哪些对象。 related_name：用于获取关联对象的关联管理器对象（反向查询），如果不允许反向，该属性应该被设置为&#39;+&#39;，或者以&#39;+&#39;结尾。 to_field：指定关联的字段，默认关联对象的主键字段。 db_constraint：是否为外键创建约束，默认值为True。 on_delete：外键关联的对象被删除时对应的动作，可取的值包括django.db.models中定义的： CASCADE：级联删除。 PROTECT：抛出ProtectedError异常，阻止删除引用的对象。 SET_NULL：把外键设置为null，当null属性被设置为True时才能这么做。 SET_DEFAULT：把外键设置为默认值，提供了默认值才能这么做。 ManyToManyField属性 symmetrical：是否建立对称的多对多关系。 through：指定维持多对多关系的中间表的Django模型。 throughfields：定义了中间模型时可以指定建立多对多关系的字段。 db_table：指定维持多对多关系的中间表的表名。 模型元数据选项 选项 说明 abstract 设置为True时模型是抽象父类 app_label 如果定义模型的应用不在INSTALLED_APPS中可以用该属性指定 db_table 模型使用的数据表名称 db_tablespace 模型使用的数据表空间 default_related_name 关联对象回指这个模型时默认使用的名称，默认为_set get_latest_by 模型中可排序字段的名称。 managed 设置为True时，Django在迁移中创建数据表并在执行flush管理命令时把表移除 order_with_respect_to 标记对象为可排序的 ordering 对象的默认排序 permissions 创建对象时写入权限表的额外权限 default_permissions 默认为(&#39;add&#39;, &#39;change&#39;, &#39;delete&#39;) unique_together 设定组合在一起时必须独一无二的字段名 index_together 设定一起建立索引的多个字段名 verbose_name 为对象设定人类可读的名称 verbose_name_plural 设定对象的复数名称 查询参考按字段查找可以用的条件 exact &#x2F; iexact：精确匹配&#x2F;忽略大小写的精确匹配查询 contains &#x2F; icontains &#x2F; startswith &#x2F; istartswith &#x2F; endswith &#x2F; iendswith：基于like的模糊查询 in ：集合运算 gt &#x2F; gte &#x2F; lt &#x2F; lte：大于&#x2F;大于等于&#x2F;小于&#x2F;小于等于关系运算 range：指定范围查询（SQL中的between…and…） year &#x2F; month &#x2F; day &#x2F; week_day &#x2F; hour &#x2F; minute &#x2F; second：查询时间日期 isnull：查询空值（True）或非空值（False） search：基于全文索引的全文检索（一般很少使用） regex &#x2F; iregex：基于正则表达式的模糊匹配查询","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day46-60/46.Django快速上手","date":"2024-12-12T08:38:02.103Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day46-60/46.Django快速上手/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day46-60/46.Django%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/","excerpt":"","text":"Django快速上手Web开发的早期阶段，开发者需要手动编写每个页面，例如一个新闻门户网站，每天都要修改它的HTML页面，随着网站规模和体量的增大，这种做法一定是非常糟糕的。为了解决这个问题，开发人员想到了用程序来为Web服务器生成动态内容，也就是说网页中的动态内容不再通过手动编写而是通过程序自动生成。最早的时候，这项技术被称为CGI（公共网关接口），当然随着时间的推移，CGI暴露出的问题也越来越多，例如大量重复的样板代码，总体性能较为低下等。在时代呼唤新英雄的背景下，PHP、ASP、JSP这类Web应用开发技术在上世纪90年代中后期如雨后春笋般涌现。通常我们说的Web应用是指通过浏览器来访问网络资源的应用程序，因为浏览器的普及性以及易用性，Web应用使用起来方便简单，免除了安装和更新应用程序带来的麻烦；站在开发者的角度，也不用关心用户使用什么样的操作系统，甚至不用区分是PC端还是移动端。 Web应用机制和术语下图向我们展示了Web应用的工作流程，其中涉及到的术语如下表所示。 说明：相信有经验的读者会发现，这张图中其实还少了很多东西，例如反向代理服务器、数据库服务器、防火墙等，而且图中的每个节点在实际项目部署时可能是一组节点组成的集群。当然，如果你对这些没有什么概念也不要紧，继续下去就行了，后面会给大家一一讲解的。 术语 解释 URL&#x2F;URI 统一资源定位符&#x2F;统一资源标识符，网络资源的唯一标识 域名 与Web服务器地址对应的一个易于记忆的字符串名字 DNS 域名解析服务，可以将域名转换成对应的IP地址 IP地址 网络上的主机的身份标识，通过IP地址可以区分不同的主机 HTTP 超文本传输协议，构建在TCP之上的应用级协议，万维网数据通信的基础 反向代理 代理客户端向服务器发出请求，然后将服务器返回的资源返回给客户端 Web服务器 接受HTTP请求，然后返回HTML文件、纯文本文件、图像等资源给请求者 Nginx 高性能的Web服务器，也可以用作反向代理，负载均衡 和 HTTP缓存 HTTP协议这里我们先费一些笔墨来说说HTTP这个协议。HTTP（超文本传输协议）是构建于TCP（传输控制协议）之上应用级协议，它利用了TCP提供的可靠的传输服务实现了Web应用中的数据交换。按照维基百科上的介绍，设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法，也就是说这个协议是浏览器和Web服务器之间传输的数据的载体。关于这个协议的详细信息以及目前的发展状况，大家可以阅读《HTTP 协议入门》、《互联网协议入门》系列以及《图解HTTPS协议》这几篇文章进行了解。下图是我在四川省网络通信技术重点实验室学习和工作期间使用开源协议分析工具Ethereal（抓包工具WireShark的前身）截取的访问百度首页时的HTTP请求和响应的报文（协议数据），由于Ethereal截取的是经过网络适配器的数据，因此可以清晰的看到从物理链路层到应用层的协议数据。 HTTP请求（请求行+请求头+空行+[消息体]）： HTTP响应（响应行+响应头+空行+消息体）： 说明：这两张图是在2009年9月10日凌晨获得的，但愿这两张如同泛黄的照片般的截图能帮助你了解HTTP到底是什么样子的。当然，如果没有专业的抓包工具，也可以通过浏览器提供的“开发者工具”来查看HTTP请求和响应的数据格式。 Django概述Python的Web框架有上百个，比它的关键字还要多。所谓Web框架，就是用于开发Web服务器端应用的基础设施，说得通俗一点就是一系列封装好的模块和工具。事实上，即便没有Web框架，我们仍然可以通过socket或CGI来开发Web服务器端应用，但是这样做的成本和代价在商业项目中通常是不能接受的。通过Web框架，我们可以化繁为简，降低创建、更新、扩展应用程序的工作量。刚才我们说到Python有上百个Web框架，这些框架包括Django、Flask、Tornado、Sanic、Pyramid、Bottle、Web2py、web.py等。 在上述Python的Web框架中，Django无疑是最有代表性的重量级选手，开发者可以基于Django快速的开发可靠的Web应用程序，因为它减少了Web开发中不必要的开销，对常用的设计和开发模式进行了封装，并对MVC架构提供了支持（Django中称之为MTV架构）。MVC是软件系统开发领域中一种放之四海而皆准的架构，它将系统中的组件分为模型（Model）、视图（View）和控制器（Controller）三个部分并借此实现模型（数据）和视图（显示）的解耦合。由于模型和视图进行了分离，所以需要一个中间人将解耦合的模型和视图联系起来，扮演这个角色的就是控制器。稍具规模的软件系统都会使用MVC架构（或者是从MVC演进出的其他架构），Django项目中我们称之为MTV，MTV中的M跟MVC中的M没有区别，就是代表数据的模型，T代表了网页模板（显示数据的视图），而V代表了视图函数，在Django框架中，视图函数和Django框架本身一起扮演了MVC中C的角色。 Django框架诞生于2003年，它是一个在真正的应用中成长起来的项目，由劳伦斯出版集团旗下在线新闻网站的内容管理系统（CMS）研发团队（主要是Adrian Holovaty和Simon Willison）开发，以比利时的吉普赛爵士吉他手Django Reinhardt来命名。Django框架在2005年夏天作为开源框架发布，使用Django框架能用很短的时间构建出功能完备的网站，因为它代替程序员完成了那些重复乏味的劳动，剩下真正有意义的核心业务给程序员来开发，这一点就是对DRY（Don’t Repeat Yourself）理念的最好践行。许多成功的网站和应用都是基于Python语言进行开发的，国内比较有代表性的网站包括：知乎、豆瓣网、果壳网、搜狐闪电邮箱、101围棋网、海报时尚网、背书吧、堆糖、手机搜狐网、咕咚、爱福窝、果库等，其中不乏使用了Django框架的产品。 快速上手第一个Django项目 检查Python环境：Django 1.11需要Python 2.7或Python 3.4以上的版本；Django 2.0需要Python 3.4以上的版本；Django 2.1和2.2需要Python 3.5以上的版本；Django 3.0需要Python 3.6以上版本。 说明：Django框架不同版本所需的Python解释器环境，可以在Django官方文档的FAQ中找到。 可以在macOS的终端中输入下面的命令检查Python解释器版本，Windows系统可以在命令行提示符中输入python --version。 1python3 --version 也可以在Python的交互式环境中执行下面的代码来查看Python解释器的版本。 123import syssys.versionsys.version_info 更新包管理工具并安装Django环境（用于创建Django项目）。 说明：在更新这个文档时，Django最新的正式版本是3.0.7，Django 3.0提供了对ASGI的支持，可以实现全双工的异步通信，但是目前的使用体验一般，所以暂时不推荐大家使用Django 3.0，下面我们安装的是Django 2.2.13版本。使用pip安装三方库和工具时，可以通过==来指定安装的版本。 12pip3 install -U pippip3 install django==2.2.13 检查Django环境并使用django-admin命令创建Django项目（项目名称为hellodjango）。 12django-admin --versiondjango-admin startproject hellodjango 用PyCharm打开创建好的Djang项目，并为其添加虚拟环境。 如上图所示，PyCharm的项目浏览器中，最顶层的文件夹hellodjango是Python项目文件夹，这个文件夹的名字并不重要，Django项目也不关心这个文件夹叫什么名字。该文件夹下有一个同名的文件夹，它是Django项目文件夹，其中包含了__init__.py、settings.py、urls.py、wsgi.py四个文件，与名为hellodjango的Django项目文件夹同级的还有一个名为manage.py 的文件，这些文件的作用如下所示： hellodjango/__init__.py：空文件，告诉Python解释器这个目录应该被视为一个Python的包。 hellodjango/settings.py：Django项目的配置文件。 hellodjango/urls.py：Django项目的URL映射声明，就像是网站的“目录”。 hellodjango/wsgi.py：项目运行在WSGI兼容Web服务器上的入口文件。 manage.py： 管理Django项目的脚本程序。 说明：WSGI全称是Web服务器网关接口，维基百科上给出的解释是“为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口”。 创建虚拟环境的界面如下图所示。 安装项目依赖项。 方法一：打开PyCharm的终端，在终端中通过pip命令安装Django项目的依赖项。 说明：由于已经基于Python 3解释器环境为项目创建了虚拟环境，所以虚拟环境中的python命令对应的是Python 3的解释器，而pip命令对应的是Python 3的包管理工具。 1pip install django==2.2.13 方法二：在PyCharm的偏好设置中，可以找到项目的解释器环境和已经安装的三方库，可以通过点击添加按钮来安装新的依赖项，需要提醒大家的是在安装Django依赖项时，需要指定版本号，否则将默认安装更新本文时最新的3.0.7版本。 下图展示了Django版本和Python版本的对应关系，请大家自行对号入座。 Django版本 Python版本 1.8 2.7、3.2、3.3、3.4、3.5 1.9、1.10 2.7、3.4、3.5 1.11 2.7、3.4、3.5、3.6、3.7（Django 1.11.17） 2.0 3.4、3.5、3.6、3.7 2.1 3.5、3.6、3.7 2.2 3.5、3.6、3.7、3.8（Django 2.2.8） 3.0 3.6、3.7、3.8 启动Django自带的服务器运行项目。 方法一：在“Run”菜单选择“Edit Configuration”，配置“Django server”运行项目（适用于专业版PyCharm）。 方法二：在“Run”菜单选择“Edit Configuration”，配置运行“Python”程序运行项目（适用于专业版和社区版PyCharm）。 方法三：在PyCharm的终端（Terminal）中通过命令运行项目（适用于专业版和社区版PyCharm）。 1python manage.py runserver 查看运行效果。 在浏览器中输入http://127.0.0.1:8000访问我们的服务器，效果如下图所示。 说明： 刚刚启动的Django自带的服务器只能用于开发和测试环境，因为这个服务器是纯Python编写的轻量级Web服务器，不适合在生产环境中使用。 如果修改了代码，不需要为了让修改的代码生效而重新启动Django自带的服务器。但是，在添加新的项目文件时，该服务器不会自动重新加载，这个时候就得手动重启服务器。 可以在终端中通过python manage.py help命令查看Django管理脚本程序可用的命令参数。 使用python manage.py runserver启动服务器时，可以在后面添加参数来指定IP地址和端口号，默认情况下启动的服务器将运行在本机的8000端口。 在终端中运行的服务器，可以通过Ctrl+C来停止它 。通过PyCharm的“运行配置”运行的服务器直接点击窗口上的关闭按钮就可以终止服务器的运行。 不能在同一个端口上启动多个服务器，因为会导致地址的冲突（端口是对IP地址的扩展，也是计算机网络地址的一部分）。 修改项目的配置文件settings.py。 Django是一个支持国际化和本地化的框架，因此刚才我们看到的Django项目的默认首页也是支持国际化的，我们可以通过修改配置文件将默认语言修改为中文，时区设置为东八区。 找到修改前的配置（在settings.py文件第100行以后）。 12LANGUAGE_CODE = &#x27;en-us&#x27;TIME_ZONE = &#x27;UTC&#x27; 修改为以下内容。 12LANGUAGE_CODE = &#x27;zh-hans&#x27;TIME_ZONE = &#x27;Asia/Chongqing&#x27; 刷新刚才的页面，可以看到修改语言代码和时区之后的结果。 创建自己的应用如果要开发自己的Web应用，需要先在Django项目中创建“应用”，一个Django项目可以包含一个或多个应用。 在PyCharm的终端中执行下面的命令，创建名为first的应用。 1python manage.py startapp first 执行上面的命令会在当前路径下创建first目录，其目录结构如下所示： __init__.py：一个空文件，告诉Python解释器这个目录应该被视为一个Python的包。 admin.py：可以用来注册模型，用于在Django框架自带的管理后台中管理模型。 apps.py：当前应用的配置文件。 migrations：存放与模型有关的数据库迁移信息。 __init__.py：一个空文件，告诉Python解释器这个目录应该被视为一个Python的包。 models.py：存放应用的数据模型（MTV中的M）。 tests.py：包含测试应用各项功能的测试类和测试函数。 views.py：处理用户HTTP请求并返回HTTP响应的函数或类（MTV中的V）。 修改应用目录下的视图文件views.py。 12345from django.http import HttpResponsedef show_index(request): return HttpResponse(&#x27;&lt;h1&gt;Hello, Django!&lt;/h1&gt;&#x27;) 修改Django项目目录下的urls.py文件，将视图函数和用户在浏览器中请求的路径对应。 123456789from django.contrib import adminfrom django.urls import path, includefrom first.views import show_indexurlpatterns = [ path(&#x27;admin/&#x27;, admin.site.urls), path(&#x27;hello/&#x27;, show_index),] 重新运行项目，并打开浏览器中访问http://127.0.0.1:8000/hello/。 上面我们通过代码为浏览器生成了内容，但仍然是静态内容，如果要生成动态内容，可以修改views.py文件并添加如下所示的代码。 123456789101112131415161718from random import samplefrom django.http import HttpResponsedef show_index(request): fruits = [ &#x27;Apple&#x27;, &#x27;Orange&#x27;, &#x27;Pitaya&#x27;, &#x27;Durian&#x27;, &#x27;Waxberry&#x27;, &#x27;Blueberry&#x27;, &#x27;Grape&#x27;, &#x27;Peach&#x27;, &#x27;Pear&#x27;, &#x27;Banana&#x27;, &#x27;Watermelon&#x27;, &#x27;Mango&#x27; ] selected_fruits = sample(fruits, 3) content = &#x27;&lt;h3&gt;今天推荐的水果是：&lt;/h3&gt;&#x27; content += &#x27;&lt;hr&gt;&#x27; content += &#x27;&lt;ul&gt;&#x27; for fruit in selected_fruits: content += f&#x27;&lt;li&gt;&#123;fruit&#125;&lt;/li&gt;&#x27; content += &#x27;&lt;/ul&gt;&#x27; return HttpResponse(content) 刷新页面查看程序的运行结果，看看每次刷新的网页的时候，是不是可以看到不一样的内容。 使用模板上面通过拼接HTML代码的方式为浏览器生成动态内容的做法在实际开发中是无能接受的，因为实际项目中的前端页面可能非常复杂，无法用这种拼接动态内容的方式来完成，这一点大家一定能够想到。为了解决这个问题，我们可以提前准备一个模板页（MTV中的T），所谓模板页就是一个带占位符和模板指令的HTML页面。 Django框架中有一个名为render的便捷函数可以来完成渲染模板的操作。所谓的渲染就是用数据替换掉模板页中的模板指令和占位符，当然这里的渲染称为后端渲染，即在服务器端完成页面的渲染再输出到浏览器中。后端渲染的做法在Web应用的访问量较大时，会让服务器承受较大的负担，所以越来越多的Web应用会选择前端渲染的方式，即服务器只提供页面所需的数据（通常是JSON格式），在浏览器中通过JavaScript代码获取这些数据并渲染页面上。关于前端渲染的内容，我们会在后续的课程中为大家讲解，目前我们使用的是通过模板页进行后端渲染的做法，具体步骤如下所示。 使用模板页的步骤如下所示。 在项目目录下创建名为templates文件夹。 添加模板页index.html。 说明：实际项目开发中，静态页由前端开发者提供，后端开发者需要将静态页修改为模板页，以便通过Python程序对其进行渲染，这种做法就是上面提到的后端渲染。 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;首页&lt;/title&gt; &lt;style&gt; #fruits &#123; font-size: 1.25em; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;今天推荐的水果是：&lt;/h1&gt; &lt;hr&gt; &lt;ul id=&quot;fruits&quot;&gt; &#123;% for fruit in fruits %&#125; &lt;li&gt;&#123;&#123; fruit &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt; 在上面的模板页中我们使用了&#123;&#123; fruit &#125;&#125;这样的模板占位符语法，也使用了&#123;% for %&#125;这样的模板指令，这些都是Django模板语言（DTL）的一部分。关于模板语法和指令，大家可以看看官方文档，相信这些内容还是很容易理解的，并不需要过多的赘述，大家也可以参考官方文档了解模板指令和语法。 修改views.py文件，调用render函数渲染模板页。 123456789101112from random import samplefrom django.shortcuts import renderdef show_index(request): fruits = [ &#x27;Apple&#x27;, &#x27;Orange&#x27;, &#x27;Pitaya&#x27;, &#x27;Durian&#x27;, &#x27;Waxberry&#x27;, &#x27;Blueberry&#x27;, &#x27;Grape&#x27;, &#x27;Peach&#x27;, &#x27;Pear&#x27;, &#x27;Banana&#x27;, &#x27;Watermelon&#x27;, &#x27;Mango&#x27; ] selected_fruits = sample(fruits, 3) return render(request, &#x27;index.html&#x27;, &#123;&#x27;fruits&#x27;: selected_fruits&#125;) render函数的第一个参数是请求对象request，第二个参数是我们要渲染的模板页的名字，第三个参数是要渲染到页面上的数据，我们通过一个字典将数据交给模板页，字典中的键就是模板页中使用的模板指令或占位符中的变量名。 到此为止，视图函数中的render还无法找到模板文件index.html，需要修改settings.py文件，配置模板文件所在的路径。修改settings.py文件，找到TEMPLATES配置，修改其中的DIRS配置。 123456789101112131415TEMPLATES = [ &#123; &#x27;BACKEND&#x27;: &#x27;django.template.backends.django.DjangoTemplates&#x27;, &#x27;DIRS&#x27;: [os.path.join(BASE_DIR, &#x27;templates&#x27;), ], &#x27;APP_DIRS&#x27;: True, &#x27;OPTIONS&#x27;: &#123; &#x27;context_processors&#x27;: [ &#x27;django.template.context_processors.debug&#x27;, &#x27;django.template.context_processors.request&#x27;, &#x27;django.contrib.auth.context_processors.auth&#x27;, &#x27;django.contrib.messages.context_processors.messages&#x27;, ], &#125;, &#125;,] 重新运行项目或直接刷新页面查看结果。 总结至此，我们已经利用Django框架完成了一个非常小的Web应用，虽然它并没有任何的实际价值，但是可以通过这个项目对Django框架有一个感性的认识。学习Django最好的资料肯定是它的官方文档，官方文档提供了对多国语言的支持，而且有新手教程引导初学者学习使用Django框架，建议大家通过阅读Django的官方文档来学习和使用这个框架。当然图灵社区出版的《Django基础教程》也是非常适合初学者的入门级读物，有兴趣的读者可以点击链接进行购买。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/45.大数据平台和HiveSQL","date":"2024-12-12T08:38:01.930Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/45.大数据平台和HiveSQL/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/45.%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0%E5%92%8CHiveSQL/","excerpt":"","text":"Hive简介Hive 是 Facebook 开源的一款基于 Hadoop 的数据仓库工具，目前由 Apache 软件基金会维护，它是应用最广泛的大数据处理解决方案，它能将 SQL 查询转变为 MapReduce（Google提出的一个软件架构，用于大规模数据集的并行运算）任务，对 SQL 提供了完美的支持，能够非常方便的实现大数据统计。 说明：可以通过https://www.edureka.co/blog/hadoop-ecosystem来了解 Hadoop 生态圈。 如果要简单的介绍 Hive，那么以下两点是其核心： 把 HDFS 中结构化的数据映射成表。 通过把 HQL 进行解析和转换，最终生成一系列基于 Hadoop 的 MapReduce 任务或 Spark 任务，通过执行这些任务完成对数据的处理。也就是说，即便不学习 Java、Scala 这样的编程语言，一样可以实现对数据的处理。 Hive的应用场景。 Hive和传统关系型数据库的对比如下图和下表所示。 Hive RDBMS 查询语言 HQL SQL 存储数据 HDFS 本地文件系统 执行方式 MapReduce &#x2F; Spark Executor 执行延迟 高 低 数据规模 大 小 准备工作 搭建如下图所示的大数据平台。 通过Client节点（跳板机）访问大数据平台。 创建文件Hadoop的文件系统。 1hdfs dfs -mkdir /user/root 将准备好的数据文件拷贝到Hadoop文件系统中。 1hdfs dfs -put /home/ubuntu/data/* /user/root 进入 hive 命令行。 1hive 建库建表 创建。 1create database eshop; 删除。 1drop database eshop cascade; 切换。 1use eshop; 数据类型Hive的数据类型如下所示。 基本数据类型： 数据类型 占用空间 支持版本 tinyint 1-Byte smallint 2-Byte int 4-Byte bigint 8-Byte boolean float 4-Byte double 8-Byte string binary 0.8版本 timestamp 0.8版本 decimal 0.11版本 char 0.13版本 varchar 0.12版本 date 0.12版本 复合数据类型： 数据类型 描述 例子 struct 和C语言中的结构体类似 struct&lt;first_name:string, last_name:string&gt; map 由键值对构成的元素的集合 map&lt;string,int&gt; array 具有相同类型的变量的容器 array&lt;string&gt; 创建内部表。 1234567891011121314151617create table if not exists dim_user_info (user_id string,user_name string, sex string,age int,city string,firstactivetime string,level int,extra1 string,extra2 map&lt;string,string&gt;)row format delimited fields terminated by &#x27;\\t&#x27;collection items terminated by &#x27;,&#x27;map keys terminated by &#x27;:&#x27;lines terminated by &#x27;\\n&#x27;stored as textfile; 加载数据。 1load data local inpath &#x27;/home/ubuntu/data/user_info/user_info.txt&#x27; overwrite into table dim_user_info; 或 1load data inpath &#x27;/user/root/user_info.txt&#x27; overwrite into table dim_user_info; 创建分区表。 1234567891011create table if not exists fact_user_trade (user_name string,piece int,price double,pay_amount double,goods_category string,pay_time bigint) partitioned by (dt string)row format delimited fields terminated by &#x27;\\t&#x27;; 提供分区数据。 1hdfs dfs -put /home/ubuntu/data/user_trade/* /user/hive/warehouse/eshop.db/fact_user_trade 设置动态分区。 1234set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.max.dynamic.partitions=10000;set hive.exec.max.dynamic.partitions.pernode=10000; 修复分区。 1msck repair table fact_user_trade; 查询基本语法12345678-- 查询北京女用户的姓名取前10个select user_name from dim_user_info where city=&#x27;beijing&#x27; and sex=&#x27;female&#x27; limit 10;-- 查询2019年3月24日购买了food类商品的用户名、购买数量和支付金额（不聚合）select user_name, piece, pay_amount from fact_user_trade where dt=&#x27;2019-03-24&#x27; and goods_category=&#x27;food&#x27;;-- 统计用户 ELLA 在2018年的总支付金额和最近最远两次消费间隔天数select sum(pay_amount) as total, datediff(max(from_unixtime(pay_time, &#x27;yyyy-MM-dd&#x27;)), min(from_unixtime(pay_time, &#x27;yyyy-MM-dd&#x27;))) from fact_user_trade where year(dt)=&#x27;2018&#x27; and user_name=&#x27;ELLA&#x27;; group by12-- 查询2019年1月到4月，每个品类有多少人购买，累计金额是多少select goods_category, count(distinct user_name) as total_user, sum(pay_amount) as total_pay from fact_user_trade where dt between &#x27;2019-01-01&#x27; and &#x27;2019-04-30&#x27; group by goods_category; 12-- 查询2019年4月支付金额超过5万元的用户select user_name, sum(pay_amount) as total from fact_user_trade where dt between &#x27;2019-04-01&#x27; and &#x27;2019-04-30&#x27; group by user_name having sum(pay_amount) &gt; 50000; 12-- 查询2018年购买的商品品类在两个以上的用户数select count(tmp.user_name) from (select user_name, count(distinct goods_category) as total from fact_user_trade where year(dt)=&#x27;2018&#x27; group by user_name having count(distinct goods_category)&gt;2) tmp; order by12-- 查询2019年4月支付金额最多的用户前5名select user_name, sum(pay_amount) as total from fact_user_trade where dt between &#x27;2019-04-01&#x27; and &#x27;2019-04-30&#x27; group by user_name order by total desc limit 5; 常用函数 from_unixtime：将时间戳转换成日期 1select from_unixtime(pay_time, &#x27;yyyy-MM-dd hh:mm:ss&#x27;) from fact_user_trade limit 10; unix_timestamp：将日期转换成时间戳 datediff：计算两个日期的时间差 12-- 用户首次激活时间与设定参照时间的间隔select user_name, datediff(&#x27;2019-4-1&#x27;, to_date(firstactivetime)) from dim_user_info limit 10; if：根据条件返回不同的值 12-- 统计不同年龄段的用户数select case when age &lt; 20 then &#x27;20岁以下&#x27; when age &lt; 30 then &#x27;30岁以下&#x27; when age &lt; 40 then &#x27;40岁以下&#x27; else &#x27;40岁以上&#x27; end as age_seg, count(distinct user_id) as total from dim_user_info group by case when age &lt; 20 then &#x27;20岁以下&#x27; when age &lt; 30 then &#x27;30岁以下&#x27; when age &lt; 40 then &#x27;40岁以下&#x27; else &#x27;40岁以上&#x27; end; 12-- 不同性别高级等用户数量select sex, if(level &gt; 5, &#x27;高&#x27;, &#x27;低&#x27;) as level_type, count(distinct user_id) as total from dim_user_info group by sex, if(level &gt; 5, &#x27;高&#x27;, &#x27;低&#x27;); substr：字符串取子串 12-- 统计每个月激活的新用户数select substr(firstactivetime, 1, 7) as month, count(distinct user_id) as total from dim_user_info group by substr(firstactivetime, 1, 7); get_json_object：从JSON字符串中取出指定的key对应的value，如：get_json_object(info, &#39;$.first_name&#39;)。 1234-- 统计不同手机品牌的用户数select get_json_object(extra1, &#x27;$.phonebrand&#x27;) as phone, count(distinct user_id) as total from user_info group by get_json_object(extra1, &#x27;$.phonebrand&#x27;);select extra2[&#x27;phonebrand&#x27;] as phone, count(distinct user_id) as total from user_info group by extra2[&#x27;phonebrand&#x27;]; 说明：MySQL对应的函数名字叫json_extract。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/44.Python接入MySQL数据库","date":"2024-12-12T08:38:01.929Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/44.Python接入MySQL数据库/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/44.Python%E6%8E%A5%E5%85%A5MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"Python接入MySQL数据库在 Python3 中，我们可以使用mysqlclient或者pymysql三方库来接入 MySQL 数据库并实现数据持久化操作。二者的用法完全相同，只是导入的模块名不一样。我们推荐大家使用纯 Python 的三方库pymysql，因为它更容易安装成功。下面我们仍然以之前创建的名为hrs的数据库为例，为大家演示如何通过 Python 程序操作 MySQL 数据库实现数据持久化操作。 接入MySQL首先，我们可以在命令行或者 PyCharm 的终端中通过下面的命令安装pymysql，如果需要接入 MySQL 8，还需要安装一个名为cryptography的三方库来支持 MySQL 8 的密码认证方式。 1pip install pymysql cryptography 使用pymysql操作 MySQL 的步骤如下所示： 创建连接。MySQL 服务器启动后，提供了基于 TCP （传输控制协议）的网络服务。我们可以通过pymysql模块的connect函数连接 MySQL 服务器。在调用connect函数时，需要指定主机（host）、端口（port）、用户名（user）、口令（password）、数据库（database）、字符集（charset）等参数，该函数会返回一个Connection对象。 获取游标。连接 MySQL 服务器成功后，接下来要做的就是向数据库服务器发送 SQL 语句，MySQL 会执行接收到的 SQL 并将执行结果通过网络返回。要实现这项操作，需要先通过连接对象的cursor方法获取游标（Cursor）对象。 发出 SQL。通过游标对象的execute方法，我们可以向数据库发出 SQL 语句。 如果执行insert、delete或update操作，需要根据实际情况提交或回滚事务。因为创建连接时，默认开启了事务环境，在操作完成后，需要使用连接对象的commit或rollback方法，实现事务的提交或回滚，rollback方法通常会放在异常捕获代码块except中。如果执行select操作，需要通过游标对象抓取查询的结果，对应的方法有三个，分别是：fetchone、fetchmany和fetchall。其中fetchone方法会抓取到一条记录，并以元组或字典的方式返回；fetchmany和fetchall方法会抓取到多条记录，以嵌套元组或列表装字典的方式返回。 关闭连接。在完成持久化操作后，请不要忘记关闭连接，释放外部资源。我们通常会在finally代码块中使用连接对象的close方法来关闭连接。 代码实操下面，我们通过代码实操的方式为大家演示上面说的五个步骤。 插入数据1234567891011121314151617181920212223242526272829import pymysqlno = int(input(&#x27;部门编号: &#x27;))name = input(&#x27;部门名称: &#x27;)location = input(&#x27;部门所在地: &#x27;)# 1. 创建连接（Connection）conn = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8mb4&#x27;)try: # 2. 获取游标对象（Cursor） with conn.cursor() as cursor: # 3. 通过游标对象向数据库服务器发出SQL语句 affected_rows = cursor.execute( &#x27;insert into `tb_dept` values (%s, %s, %s)&#x27;, (no, name, location) ) if affected_rows == 1: print(&#x27;新增部门成功!!!&#x27;) # 4. 提交事务（transaction） conn.commit()except pymysql.MySQLError as err: # 4. 回滚事务 conn.rollback() print(type(err), err)finally: # 5. 关闭连接释放资源 conn.close() 说明：上面的127.0.0.1称为回环地址，它代表的是本机。下面的guest是我提前创建好的用户，该用户拥有对hrs数据库的insert、delete、update和select权限。我们不建议大家在项目中直接使用root超级管理员账号访问数据库，这样做实在是太危险了。我们可以使用下面的命令创建名为guest的用户并为其授权。 12create user &#x27;guest&#x27;@&#x27;%&#x27; identified by &#x27;Guest.618&#x27;;grant insert, delete, update, select on `hrs`.* to &#x27;guest&#x27;@&#x27;%&#x27;; 如果要插入大量数据，建议使用游标对象的executemany方法做批处理（一个insert操作后面跟上多组数据），大家可以尝试向一张表插入10000条记录，然后看看不使用批处理一条条的插入和使用批处理有什么差别。游标对象的executemany方法第一个参数仍然是 SQL 语句，第二个参数可以是包含多组数据的列表或元组。 删除数据12345678910111213141516171819202122import pymysqlno = int(input(&#x27;部门编号: &#x27;))# 1. 创建连接（Connection）conn = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8mb4&#x27;, autocommit=True)try: # 2. 获取游标对象（Cursor） with conn.cursor() as cursor: # 3. 通过游标对象向数据库服务器发出SQL语句 affected_rows = cursor.execute( &#x27;delete from `tb_dept` where `dno`=%s&#x27;, (no, ) ) if affected_rows == 1: print(&#x27;删除部门成功!!!&#x27;)finally: # 5. 关闭连接释放资源 conn.close() 说明：如果不希望每次 SQL 操作之后手动提交或回滚事务，可以connect函数中加一个名为autocommit的参数并将它的值设置为True，表示每次执行 SQL 成功后自动提交。但是我们建议大家手动提交或回滚，这样可以根据实际业务需要来构造事务环境。如果不愿意捕获异常并进行处理，可以在try代码块后直接跟finally块，省略except意味着发生异常时，代码会直接崩溃并将异常栈显示在终端中。 更新数据1234567891011121314151617181920212223242526272829import pymysqlno = int(input(&#x27;部门编号: &#x27;))name = input(&#x27;部门名称: &#x27;)location = input(&#x27;部门所在地: &#x27;)# 1. 创建连接（Connection）conn = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8mb4&#x27;)try: # 2. 获取游标对象（Cursor） with conn.cursor() as cursor: # 3. 通过游标对象向数据库服务器发出SQL语句 affected_rows = cursor.execute( &#x27;update `tb_dept` set `dname`=%s, `dloc`=%s where `dno`=%s&#x27;, (name, location, no) ) if affected_rows == 1: print(&#x27;更新部门信息成功!!!&#x27;) # 4. 提交事务 conn.commit()except pymysql.MySQLError as err: # 4. 回滚事务 conn.rollback() print(type(err), err)finally: # 5. 关闭连接释放资源 conn.close() 查询数据 查询部门表的数据。 123456789101112131415161718192021import pymysql# 1. 创建连接（Connection）conn = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8mb4&#x27;)try: # 2. 获取游标对象（Cursor） with conn.cursor() as cursor: # 3. 通过游标对象向数据库服务器发出SQL语句 cursor.execute(&#x27;select `dno`, `dname`, `dloc` from `tb_dept`&#x27;) # 4. 通过游标对象抓取数据 row = cursor.fetchone() while row: print(row) row = cursor.fetchone()except pymysql.MySQLError as err: print(type(err), err)finally: # 5. 关闭连接释放资源 conn.close() 说明：上面的代码中，我们通过构造一个while循环实现了逐行抓取查询结果的操作。这种方式特别适合查询结果有非常多行的场景。因为如果使用fetchall一次性将所有记录抓取到一个嵌套元组中，会造成非常大的内存开销，这在很多场景下并不是一个好主意。如果不愿意使用while循环，还可以考虑使用iter函数构造一个迭代器来逐行抓取数据，有兴趣的读者可以自行研究。 分页查询员工表的数据。 1234567891011121314151617181920212223import pymysqlpage = int(input(&#x27;页码: &#x27;))size = int(input(&#x27;大小: &#x27;))# 1. 创建连接（Connection）con = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8&#x27;)try: # 2. 获取游标对象（Cursor） with con.cursor(pymysql.cursors.DictCursor) as cursor: # 3. 通过游标对象向数据库服务器发出SQL语句 cursor.execute( &#x27;select `eno`, `ename`, `job`, `sal` from `tb_emp` order by `sal` desc limit %s,%s&#x27;, ((page - 1) * size, size) ) # 4. 通过游标对象抓取数据 for emp_dict in cursor.fetchall(): print(emp_dict)finally: # 5. 关闭连接释放资源 con.close() 案例讲解下面我们为大家讲解一个将数据库表数据导出到 Excel 文件的例子，我们需要先安装openpyxl三方库，命令如下所示。 1pip install openpyxl 接下来，我们通过下面的代码实现了将数据库hrs中所有员工的编号、姓名、职位、月薪、补贴和部门名称导出到一个 Excel 文件中。 123456789101112131415161718192021222324252627282930313233343536import openpyxlimport pymysql# 创建工作簿对象workbook = openpyxl.Workbook()# 获得默认的工作表sheet = workbook.active# 修改工作表的标题sheet.title = &#x27;员工基本信息&#x27;# 给工作表添加表头sheet.append((&#x27;工号&#x27;, &#x27;姓名&#x27;, &#x27;职位&#x27;, &#x27;月薪&#x27;, &#x27;补贴&#x27;, &#x27;部门&#x27;))# 创建连接（Connection）conn = pymysql.connect(host=&#x27;127.0.0.1&#x27;, port=3306, user=&#x27;guest&#x27;, password=&#x27;Guest.618&#x27;, database=&#x27;hrs&#x27;, charset=&#x27;utf8mb4&#x27;)try: # 获取游标对象（Cursor） with conn.cursor() as cursor: # 通过游标对象执行SQL语句 cursor.execute( &#x27;select `eno`, `ename`, `job`, `sal`, coalesce(`comm`, 0), `dname` &#x27; &#x27;from `tb_emp` natural join `tb_dept`&#x27; ) # 通过游标抓取数据 row = cursor.fetchone() while row: # 将数据逐行写入工作表中 sheet.append(row) row = cursor.fetchone() # 保存工作簿 workbook.save(&#x27;hrs.xlsx&#x27;)except pymysql.MySQLError as err: print(err)finally: # 关闭连接释放资源 conn.close() 大家可以参考上面的例子，试一试把 Excel 文件的数据导入到指定数据库的指定表中，看看是否可以成功。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/43.索引","date":"2024-12-12T08:38:01.927Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/43.索引/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/43.%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引索引是关系型数据库中用来提升查询性能最为重要的手段。关系型数据库中的索引就像一本书的目录，我们可以想象一下，如果要从一本书中找出某个知识点，但是这本书没有目录，这将是一件多么可怕的事情！我们估计得一篇一篇的翻下去，才能确定这个知识点到底在什么位置。创建索引虽然会带来存储空间上的开销，就像一本书的目录会占用一部分篇幅一样，但是在牺牲空间后换来的查询时间的减少也是非常显著的。 MySQL 数据库中所有数据类型的列都可以被索引。对于MySQL 8.0 版本的 InnoDB 存储引擎来说，它支持三种类型的索引，分别是 B+ 树索引、全文索引和 R 树索引。这里，我们只介绍使用得最为广泛的 B+ 树索引。使用 B+ 树的原因非常简单，因为它是目前在基于磁盘进行海量数据存储和排序上最有效率的数据结构。B+ 树是一棵平衡树，树的高度通常为3或4，但是却可以保存从百万级到十亿级的数据，而从这些数据里面查询一条数据，只需要3次或4次 I&#x2F;O 操作。 B+ 树由根节点、中间节点和叶子节点构成，其中叶子节点用来保存排序后的数据。由于记录在索引上是排序过的，因此在一个叶子节点内查找数据时可以使用二分查找，这种查找方式效率非常的高。当数据很少的时候，B+ 树只有一个根节点，数据也就保存在根节点上。随着记录越来越多，B+ 树会发生分裂，根节点不再保存数据，而是提供了访问下一层节点的指针，帮助快速确定数据在哪个叶子节点上。 在创建二维表时，我们通常都会为表指定主键列，主键列上默认会创建索引，而对于 MySQL InnoDB 存储引擎来说，因为它使用的是索引组织表这种数据存储结构，所以主键上的索引就是整张表的数据，而这种索引我们也将其称之为聚集索引（clustered index）。很显然，一张表只能有一个聚集索引，否则表的数据岂不是要保存多次。我们自己创建的索引都是二级索引（secondary index），更常见的叫法是非聚集索引（non-clustered index）。通过我们自定义的非聚集索引只能定位记录的主键，在获取数据时可能需要再通过主键上的聚集索引进行查询，这种现象称为“回表”，因此通过非聚集索引检索数据通常比使用聚集索引检索数据要慢。 接下来我们通过一个简单的例子来说明索引的意义，比如我们要根据学生的姓名来查找学生，这个场景在实际开发中应该经常遇到，就跟通过商品名称查找商品是一个道理。我们可以使用 MySQL 的explain关键字来查看 SQL 的执行计划（数据库执行 SQL 语句的具体步骤）。 1explain select * from tb_student where stuname=&#x27;林震南&#x27;\\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 11 filtered: 10.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 在上面的 SQL 执行计划中，有几项值得我们关注： select_type：查询的类型。 SIMPLE：简单 SELECT，不需要使用 UNION 操作或子查询。 PRIMARY：如果查询包含子查询，最外层的 SELECT 被标记为 PRIMARY。 UNION：UNION 操作中第二个或后面的 SELECT 语句。 SUBQUERY：子查询中的第一个 SELECT。 DERIVED：派生表的 SELECT 子查询。 table：查询对应的表。 type：MySQL 在表中找到满足条件的行的方式，也称为访问类型，包括：ALL（全表扫描）、index（索引全扫描，只遍历索引树）、range（索引范围扫描）、ref（非唯一索引扫描）、eq_ref（唯一索引扫描）、const &#x2F; system（常量级查询）、NULL（不需要访问表或索引）。在所有的访问类型中，很显然 ALL 是性能最差的，它代表的全表扫描是指要扫描表中的每一行才能找到匹配的行。 possible_keys：MySQL 可以选择的索引，但是有可能不会使用。 key：MySQL 真正使用的索引，如果为NULL就表示没有使用索引。 key_len：使用的索引的长度，在不影响查询的情况下肯定是长度越短越好。 rows：执行查询需要扫描的行数，这是一个预估值。 extra：关于查询额外的信息。 Using filesort：MySQL 无法利用索引完成排序操作。 Using index：只使用索引的信息而不需要进一步查表来获取更多的信息。 Using temporary：MySQL 需要使用临时表来存储结果集，常用于分组和排序。 Impossible where：where子句会导致没有符合条件的行。 Distinct：MySQL 发现第一个匹配行后，停止为当前的行组合搜索更多的行。 Using where：查询的列未被索引覆盖，筛选条件并不是索引的前导列。 从上面的执行计划可以看出，当我们通过学生名字查询学生时实际上是进行了全表扫描，不言而喻这个查询性能肯定是非常糟糕的，尤其是在表中的行很多的时候。如果我们需要经常通过学生姓名来查询学生，那么就应该在学生姓名对应的列上创建索引，通过索引来加速查询。 1create index idx_student_name on tb_student(stuname); 再次查看刚才的 SQL 对应的执行计划。 1explain select * from tb_student where stuname=&#x27;林震南&#x27;\\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: refpossible_keys: idx_student_name key: idx_student_name key_len: 62 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 可以注意到，在对学生姓名创建索引后，刚才的查询已经不是全表扫描而是基于索引的查询，而且扫描的行只有唯一的一行，这显然大大的提升了查询的性能。MySQL 中还允许创建前缀索引，即对索引字段的前N个字符创建索引，这样的话可以减少索引占用的空间（但节省了空间很有可能会浪费时间，时间和空间是不可调和的矛盾），如下所示。 1create index idx_student_name_1 on tb_student(stuname(1)); 上面的索引相当于是根据学生姓名的第一个字来创建的索引，我们再看看 SQL 执行计划。 1explain select * from tb_student where stuname=&#x27;林震南&#x27;\\G 1234567891011121314*************************** 1. row *************************** id: 1 select_type: SIMPLE table: tb_student partitions: NULL type: refpossible_keys: idx_student_name key: idx_student_name key_len: 5 ref: const rows: 2 filtered: 100.00 Extra: Using where1 row in set, 1 warning (0.00 sec) 不知道大家是否注意到，这一次扫描的行变成了2行，因为学生表中有两个姓“林”的学生，我们只用姓名的第一个字作为索引的话，在查询时通过索引就会找到这两行。 如果要删除索引，可以使用下面的SQL。 1alter table tb_student drop index idx_student_name; 或者 1drop index idx_student_name on tb_student; 在创建索引时，我们还可以使用复合索引、函数索引（MySQL 5.7 开始支持），用好复合索引实现索引覆盖可以减少不必要的排序和回表操作，这样就会让查询的性能成倍的提升，有兴趣的读者可以自行研究。 我们简单的为大家总结一下索引的设计原则： 最适合索引的列是出现在WHERE子句和连接子句中的列。 索引列的基数越大（取值多、重复值少），索引的效果就越好。 使用前缀索引可以减少索引占用的空间，内存中可以缓存更多的索引。 索引不是越多越好，虽然索引加速了读操作（查询），但是写操作（增、删、改）都会变得更慢，因为数据的变化会导致索引的更新，就如同书籍章节的增删需要更新目录一样。 使用 InnoDB 存储引擎时，表的普通索引都会保存主键的值，所以主键要尽可能选择较短的数据类型，这样可以有效的减少索引占用的空间，提升索引的缓存效果。 最后，还有一点需要说明，InnoDB 使用的 B-tree 索引，数值类型的列除了等值判断时索引会生效之外，使用&gt;、&lt;、&gt;=、&lt;=、BETWEEN...AND... 、&lt;&gt;时，索引仍然生效；对于字符串类型的列，如果使用不以通配符开头的模糊查询，索引也是起作用的，但是其他的情况会导致索引失效，这就意味着很有可能会做全表查询。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/42.视图、函数和过程","date":"2024-12-12T08:38:01.925Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/42.视图、函数和过程/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/42.%E8%A7%86%E5%9B%BE%E3%80%81%E5%87%BD%E6%95%B0%E5%92%8C%E8%BF%87%E7%A8%8B/","excerpt":"","text":"视图、函数和过程为了讲解视图、函数和过程，我们首先用下面的 DDL 和 DML 创建名为 hrs 的数据库并为其二维表添加如下所示的数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253-- 创建名为hrs的数据库并指定默认的字符集create database `hrs` default charset utf8mb4;-- 切换到hrs数据库use `hrs`;-- 创建部门表create table `tb_dept`(`dno` int not null comment &#x27;编号&#x27;,`dname` varchar(10) not null comment &#x27;名称&#x27;,`dloc` varchar(20) not null comment &#x27;所在地&#x27;,primary key (`dno`));-- 插入4个部门insert into `tb_dept` values (10, &#x27;会计部&#x27;, &#x27;北京&#x27;), (20, &#x27;研发部&#x27;, &#x27;成都&#x27;), (30, &#x27;销售部&#x27;, &#x27;重庆&#x27;), (40, &#x27;运维部&#x27;, &#x27;深圳&#x27;);-- 创建员工表create table `tb_emp`(`eno` int not null comment &#x27;员工编号&#x27;,`ename` varchar(20) not null comment &#x27;员工姓名&#x27;,`job` varchar(20) not null comment &#x27;员工职位&#x27;,`mgr` int comment &#x27;主管编号&#x27;,`sal` int not null comment &#x27;员工月薪&#x27;,`comm` int comment &#x27;每月补贴&#x27;,`dno` int not null comment &#x27;所在部门编号&#x27;,primary key (`eno`),constraint `fk_emp_mgr` foreign key (`mgr`) references tb_emp (`eno`),constraint `fk_emp_dno` foreign key (`dno`) references tb_dept (`dno`));-- 插入14个员工insert into `tb_emp` values (7800, &#x27;张三丰&#x27;, &#x27;总裁&#x27;, null, 9000, 1200, 20), (2056, &#x27;乔峰&#x27;, &#x27;分析师&#x27;, 7800, 5000, 1500, 20), (3088, &#x27;李莫愁&#x27;, &#x27;设计师&#x27;, 2056, 3500, 800, 20), (3211, &#x27;张无忌&#x27;, &#x27;程序员&#x27;, 2056, 3200, null, 20), (3233, &#x27;丘处机&#x27;, &#x27;程序员&#x27;, 2056, 3400, null, 20), (3251, &#x27;张翠山&#x27;, &#x27;程序员&#x27;, 2056, 4000, null, 20), (5566, &#x27;宋远桥&#x27;, &#x27;会计师&#x27;, 7800, 4000, 1000, 10), (5234, &#x27;郭靖&#x27;, &#x27;出纳&#x27;, 5566, 2000, null, 10), (3344, &#x27;黄蓉&#x27;, &#x27;销售主管&#x27;, 7800, 3000, 800, 30), (1359, &#x27;胡一刀&#x27;, &#x27;销售员&#x27;, 3344, 1800, 200, 30), (4466, &#x27;苗人凤&#x27;, &#x27;销售员&#x27;, 3344, 2500, null, 30), (3244, &#x27;欧阳锋&#x27;, &#x27;程序员&#x27;, 3088, 3200, null, 20), (3577, &#x27;杨过&#x27;, &#x27;会计&#x27;, 5566, 2200, null, 10), (3588, &#x27;朱九真&#x27;, &#x27;会计&#x27;, 5566, 2500, null, 10); 视图视图是关系型数据库中将一组查询指令构成的结果集组合成可查询的数据表的对象。简单的说，视图就是虚拟的表，但与数据表不同的是，数据表是一种实体结构，而视图是一种虚拟结构，你也可以将视图理解为保存在数据库中被赋予名字的 SQL 语句。 使用视图可以获得以下好处： 可以将实体数据表隐藏起来，让外部程序无法得知实际的数据结构，让访问者可以使用表的组成部分而不是整个表，降低数据库被攻击的风险。 在大多数的情况下视图是只读的（更新视图的操作通常都有诸多的限制），外部程序无法直接透过视图修改数据。 重用 SQL 语句，将高度复杂的查询包装在视图表中，直接访问该视图即可取出需要的数据；也可以将视图视为数据表进行连接查询。 视图可以返回与实体数据表不同格式的数据，在创建视图的时候可以对数据进行格式化处理。 创建视图。 1234567create view `vw_emp_simple`asselect `eno`, `ename`, `job`, `dno` from `tb_emp`; 提示：因为视图不包含数据，所以每次使用视图时，都必须执行查询以获得数据，如果你使用了连接查询、嵌套查询创建了较为复杂的视图，你可能会发现查询性能下降得很厉害。因此，在使用复杂的视图前，应该进行测试以确保其性能能够满足应用的需求。 有了上面的视图，我们就可以使用之前讲过的 DCL， 限制某些用户只能从视图中获取员工信息，这样员工表中的工资（sal）、补贴（comm）等敏感字段便不会暴露给用户。下面的代码演示了如何从视图中获取数据。 1select * from `vw_emp_simple`; 查询结果： 123456789101112131415161718+------+-----------+--------------+-----+| eno | ename | job | dno |+------+-----------+--------------+-----+| 1359 | 胡二刀 | 销售员 | 30 || 2056 | 乔峰 | 分析师 | 20 || 3088 | 李莫愁 | 设计师 | 20 || 3211 | 张无忌 | 程序员 | 20 || 3233 | 丘处机 | 程序员 | 20 || 3244 | 欧阳锋 | 程序员 | 20 || 3251 | 张翠山 | 程序员 | 20 || 3344 | 黄蓉 | 销售主管 | 30 || 3577 | 杨过 | 会计 | 10 || 3588 | 朱九真 | 会计 | 10 || 4466 | 苗人凤 | 销售员 | 30 || 5234 | 郭靖 | 出纳 | 10 || 5566 | 宋远桥 | 会计师 | 10 || 7800 | 张三丰 | 总裁 | 20 |+------+-----------+--------------+-----+ 既然视图是一张虚拟的表，那么视图的中的数据可以更新吗？视图的可更新性要视具体情况而定，以下类型的视图是不能更新的： 使用了聚合函数（SUM、MIN、MAX、AVG、COUNT等）、DISTINCT、GROUP BY、HAVING、UNION或者UNION ALL的视图。 SELECT中包含了子查询的视图。 FROM子句中包含了一个不能更新的视图的视图。 WHERE子句的子查询引用了FROM子句中的表的视图。 删除视图。 1drop view if exists `vw_emp_simple`; 说明：如果希望更新视图，可以先用上面的命令删除视图，也可以通过create or replace view来更新视图。 视图的规则和限制。 视图可以嵌套，可以利用从其他视图中检索的数据来构造一个新的视图。视图也可以和表一起使用。 创建视图时可以使用order by子句，但如果从视图中检索数据时也使用了order by，那么该视图中原先的order by会被覆盖。 视图无法使用索引，也不会激发触发器（实际开发中因为性能等各方面的考虑，通常不建议使用触发器，所以我们也不对这个概念进行介绍）的执行。 函数MySQL 中的函数跟 Python 中的函数大同小异，因为函数都是用来封装功能上相对独立且会被重复使用的代码的。如果非要找出一些差别来，那么 MySQL 中的函数是可以执行 SQL 语句的。下面的例子，我们通过自定义函数实现了截断超长字符串的功能。 12345678910111213141516delimiter $$create function fn_truncate_string( content varchar(10000), max_length int unsigned) returns varchar(10000) no sqlbegin declare result varchar(10000) default content; if char_length(content) &gt; max_length then set result = left(content, max_length); set result = concat(result, &#x27;……&#x27;); end if; return result;end $$delimiter ; 说明1：函数声明后面的no sql是声明函数体并没有使用 SQL 语句；如果函数体中需要通过 SQL 读取数据，需要声明为reads sql data。 说明2：定义函数前后的delimiter命令是为了修改终止符（定界符），因为函数体中的语句都是用;表示结束，如果不重新定义定界符，那么遇到的;的时候代码就会被截断执行，显然这不是我们想要的效果。 在查询中调用自定义函数。 1select fn_truncate_string(&#x27;和我在成都的街头走一走，直到所有的灯都熄灭了也不停留&#x27;, 10) as short_string; 12345+--------------------------------------+| short_string |+--------------------------------------+| 和我在成都的街头走一…… |+--------------------------------------+ 过程过程（又称存储过程）是事先编译好存储在数据库中的一组 SQL 的集合，调用过程可以简化应用程序开发人员的工作，减少与数据库服务器之间的通信，对于提升数据操作的性能也是有帮助的。其实迄今为止，我们使用的 SQL 语句都是针对一个或多个表的单条语句，但在实际开发中经常会遇到某个操作需要多条 SQL 语句才能完成的情况。例如，电商网站在受理用户订单时，需要做以下一系列的处理。 通过查询来核对库存中是否有对应的物品以及库存是否充足。 如果库存有物品，需要锁定库存以确保这些物品不再卖给别人， 并且要减少可用的物品数量以反映正确的库存量。 如果库存不足，可能需要进一步与供应商进行交互或者至少产生一条系统提示消息。 不管受理订单是否成功，都需要产生流水记录，而且需要给对应的用户产生一条通知信息。 我们可以通过过程将复杂的操作封装起来，这样不仅有助于保证数据的一致性，而且将来如果业务发生了变动，只需要调整和修改过程即可。对于调用过程的用户来说，过程并没有暴露数据表的细节，而且执行过程比一条条的执行一组 SQL 要快得多。 下面的过程实现 hrs 数据库中员工工资的普调，具体的规则是：10部门的员工薪资上浮300， 20部门的员工薪资上浮800，30部门的员工薪资上浮500。 123456789101112131415161718192021222324delimiter $$create procedure sp_upgrade_salary()begin declare flag boolean default 1; -- 定义一个异常处理器 declare continue handler for sqlexception set flag=0; -- 开启事务环境 start transaction; update tb_emp set sal=sal+300 where dno=10; update tb_emp set sal=sal+800 where dno=20; update tb_emp set sal=sal+500 where dno=30; -- 提交或回滚事务 if flag then commit; else rollback; end if;end $$delimiter ; 说明：上面的过程代码中使用了start transaction来开启事务环境，关于事务，在本课的最后有一个简单的介绍。为了确定代码中是否发生异常，从而提交或回滚事务，上面的过程中定义了一个名为flag的变量和一个异常处理器，如果发生了异常，flag将会被赋值为0，后面的分支结构会根据flag的值来决定是执行commit，还是执行rollback。 调用过程。 1call sp_upgrade_salary(); 删除过程。 1drop procedure if exists sp_upgrade_salary; 在过程中，我们可以定义变量、条件，可以使用分支和循环语句，可以通过游标操作查询结果，还可以使用事件调度器，这些内容我们暂时不在此处进行介绍。虽然我们说了很多过程的好处，但是在实际开发中，如果频繁的使用过程并将大量复杂的运算放到过程中，会给据库服务器造成巨大的压力，而数据库往往都是性能瓶颈所在，使用过程无疑是雪上加霜的操作。所以，对于互联网产品开发，我们一般建议让数据库只做好存储，复杂的运算和处理交给应用服务器上的程序去完成，如果应用服务器变得不堪重负了，我们可以比较容易的部署多台应用服务器来分摊这些压力。 如果大家对上面讲到的视图、函数、过程包括我们没有讲到的触发器这些知识有兴趣，建议大家阅读 MySQL 的入门读物《MySQL必知必会》进行一般性了解即可，因为这些知识点在大家将来的工作中未必用得上，学了也可能仅仅是为了应付面试而已。 其他内容范式理论范式理论是设计关系型数据库中二维表的指导思想。 第一范式：数据表的每个列的值域都是由原子值组成的，不能够再分割。 第二范式：数据表里的所有数据都要和该数据表的键（主键与候选键）有完全依赖关系。 第三范式：所有非键属性都只和候选键有相关性，也就是说非键属性之间应该是独立无关的。 说明：实际工作中，出于效率的考虑，我们在设计表时很有可能做出反范式设计，即故意降低方式级别，增加冗余数据来获得更好的操作性能。 数据完整性 实体完整性 - 每个实体都是独一无二的 主键（primary key） &#x2F; 唯一约束（unique） 引用完整性（参照完整性）- 关系中不允许引用不存在的实体 外键（foreign key） 域（domain）完整性 - 数据是有效的 数据类型及长度 非空约束（not null） 默认值约束（default） 检查约束（check） 说明：在 MySQL 8.x 以前，检查约束并不起作用。 数据一致性 事务：一系列对数据库进行读&#x2F;写的操作，这些操作要么全都成功，要么全都失败。 事务的 ACID 特性 原子性：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行 一致性：事务应确保数据库的状态从一个一致状态转变为另一个一致状态 隔离性：多个事务并发执行时，一个事务的执行不应影响其他事务的执行 持久性：已被提交的事务对数据库的修改应该永久保存在数据库中 MySQL 中的事务操作 开启事务环境 1start transaction 提交事务 1commit 回滚事务 1rollback 查看事务隔离级别 1show variables like &#x27;transaction_isolation&#x27;; 12345+-----------------------+-----------------+| Variable_name | Value |+-----------------------+-----------------+| transaction_isolation | REPEATABLE-READ |+-----------------------+-----------------+ 可以看出，MySQL 默认的事务隔离级别是REPEATABLE-READ。 修改（当前会话）事务隔离级别 1set session transaction isolation level read committed; 重新查看事务隔离级别，结果如下所示。 12345+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 关系型数据库的事务是一个很大的话题，因为当存在多个并发事务访问数据时，就有可能出现三类读数据的问题（脏读、不可重复读、幻读）和两类更新数据的问题（第一类丢失更新、第二类丢失更新）。想了解这五类问题的，可以阅读我发布在 CSDN 网站上的《Java面试题全集（上）》一文的第80题。为了避免这些问题，关系型数据库底层是有对应的锁机制的，按锁定对象不同可以分为表级锁和行级锁，按并发事务锁定关系可以分为共享锁和独占锁。然而直接使用锁是非常麻烦的，为此数据库为用户提供了自动锁机制，只要用户指定适当的事务隔离级别，数据库就会通过分析 SQL 语句，然后为事务访问的资源加上合适的锁。此外，数据库还会维护这些锁通过各种手段提高系统的性能，这些对用户来说都是透明的。想了解 MySQL 事务和锁的细节知识，推荐大家阅读进阶读物《高性能MySQL》，这也是数据库方面的经典书籍。 ANSI&#x2F;ISO SQL 92标准定义了4个等级的事务隔离级别，如下表所示。需要说明的是，事务隔离级别和数据访问的并发性是对立的，事务隔离级别越高并发性就越差。所以要根据具体的应用来确定到底使用哪种事务隔离级别，这个地方没有万能的原则。 总结关于 MySQL 的知识肯定远远不止上面列出的这些，比如 MySQL 性能调优、MySQL 运维相关工具、MySQL 数据的备份和恢复、监控 MySQL 服务、部署高可用架构等，这一系列的问题在这里都没有办法逐一展开来讨论，那就留到有需要的时候再进行讲解吧，各位读者也可以自行探索。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/41.MySQL新特性","date":"2024-12-12T08:38:01.923Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/41.MySQL新特性/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/41.MySQL%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"MySQL新特性JSON类型很多开发者在使用关系型数据库做数据持久化的时候，常常感到结构化的存储缺乏灵活性，因为必须事先设计好所有的列以及对应的数据类型。在业务发展和变化的过程中，如果需要修改表结构，这绝对是比较麻烦和难受的事情。从 MySQL 5.7 版本开始，MySQL引入了对 JSON 数据类型的支持（MySQL 8.0 解决了 JSON 的日志性能瓶颈问题），用好 JSON 类型，其实就是打破了关系型数据库和非关系型数据库之间的界限，为数据持久化操作带来了更多的便捷。 JSON 类型主要分为 JSON 对象和 JSON数组两种，如下所示。 JSON 对象 1&#123;&quot;name&quot;: &quot;骆昊&quot;, &quot;tel&quot;: &quot;13122335566&quot;, &quot;QQ&quot;: &quot;957658&quot;&#125; JSON 数组 1[1, 2, 3] 1[&#123;&quot;name&quot;: &quot;骆昊&quot;, &quot;tel&quot;: &quot;13122335566&quot;&#125;, &#123;&quot;name&quot;: &quot;王大锤&quot;, &quot;QQ&quot;: &quot;123456&quot;&#125;] 哪些地方需要用到JSON类型呢？举一个简单的例子，现在很多产品的用户登录都支持多种方式，例如手机号、微信、QQ、新浪微博等，但是一般情况下我们又不会要求用户提供所有的这些信息，那么用传统的设计方式，就需要设计多个列来对应多种登录方式，可能还需要允许这些列存在空值，这显然不是很好的选择；另一方面，如果产品又增加了一种登录方式，那么就必然要修改之前的表结构，这就更让人痛苦了。但是，有了 JSON 类型，刚才的问题就迎刃而解了，我们可以做出如下所示的设计。 12345678910create table `tb_test`(`user_id` bigint unsigned,`login_info` json,primary key (`user_id`)) engine=innodb;insert into `tb_test` values (1, &#x27;&#123;&quot;tel&quot;: &quot;13122335566&quot;, &quot;QQ&quot;: &quot;654321&quot;, &quot;wechat&quot;: &quot;jackfrued&quot;&#125;&#x27;), (2, &#x27;&#123;&quot;tel&quot;: &quot;13599876543&quot;, &quot;weibo&quot;: &quot;wangdachui123&quot;&#125;&#x27;); 如果要查询用户的手机和微信号，可以用如下所示的 SQL 语句。 12345select `user_id`, json_unquote(json_extract(`login_info`, &#x27;$.tel&#x27;)) as 手机号, json_unquote(json_extract(`login_info`, &#x27;$.wechat&#x27;)) as 微信 from `tb_test`; 123456+---------+-------------+-----------+| user_id | 手机号 | 微信 |+---------+-------------+-----------+| 1 | 13122335566 | jackfrued || 2 | 13599876543 | NULL |+---------+-------------+-----------+ 因为支持 JSON 类型，MySQL 也提供了配套的处理 JSON 数据的函数，就像上面用到的json_extract和json_unquote。当然，上面的 SQL 还有更为便捷的写法，如下所示。 12345select `user_id`, `login_info` -&gt;&gt; &#x27;$.tel&#x27; as 手机号, `login_info` -&gt;&gt; &#x27;$.wechat&#x27; as 微信from `tb_test`; 再举个例子，如果我们的产品要实现用户画像功能（给用户打标签），然后基于用户画像给用户推荐平台的服务或消费品之类的东西，我们也可以使用 JSON 类型来保存用户画像数据，示意代码如下所示。 创建画像标签表。 123456789101112131415161718192021create table `tb_tags`(`tag_id` int unsigned not null comment &#x27;标签ID&#x27;,`tag_name` varchar(20) not null comment &#x27;标签名&#x27;,primary key (`tag_id`)) engine=innodb;insert into `tb_tags` (`tag_id`, `tag_name`) values (1, &#x27;70后&#x27;), (2, &#x27;80后&#x27;), (3, &#x27;90后&#x27;), (4, &#x27;00后&#x27;), (5, &#x27;爱运动&#x27;), (6, &#x27;高学历&#x27;), (7, &#x27;小资&#x27;), (8, &#x27;有房&#x27;), (9, &#x27;有车&#x27;), (10, &#x27;爱看电影&#x27;), (11, &#x27;爱网购&#x27;), (12, &#x27;常点外卖&#x27;); 为用户打标签。 12345678910create table `tb_users_tags`(`user_id` bigint unsigned not null comment &#x27;用户ID&#x27;,`user_tags` json not null comment &#x27;用户标签&#x27;) engine=innodb;insert into `tb_users_tags` values (1, &#x27;[2, 6, 8, 10]&#x27;), (2, &#x27;[3, 10, 12]&#x27;), (3, &#x27;[3, 8, 9, 11]&#x27;); 接下来，我们通过一组查询来了解 JSON 类型的巧妙之处。 查询爱看电影（有10这个标签）的用户ID。 1select `user_id` from `tb_users_tags` where 10 member of (`user_tags`-&gt;&#x27;$&#x27;); 查询爱看电影（有10这个标签）的80后（有2这个标签）用户ID。 1select `user_id` from `tb_users_tags` where json_contains(`user_tags`-&gt;&#x27;$&#x27;, &#x27;[2, 10]&#x27;); 查询爱看电影或80后或90后的用户ID。 1select `user_id` from `tb_users_tags` where json_overlaps(user_tags-&gt;&#x27;$&#x27;, &#x27;[2, 3, 10]&#x27;); 说明：上面的查询用到了member of谓词和两个 JSON 函数，json_contains可以检查 JSON 数组是否包含了指定的元素，而json_overlaps可以检查 JSON 数组是否与指定的数组有重叠部分。 窗口函数MySQL 从8.0开始支持窗口函数，大多数商业数据库和一些开源数据库早已提供了对窗口函数的支持，有的也将其称之为 OLAP（联机分析和处理）函数，听名字就知道跟统计和分析相关。为了帮助大家理解窗口函数，我们先说说窗口的概念。 窗口可以理解为记录的集合，窗口函数也就是在满足某种条件的记录集合上执行的特殊函数，对于每条记录都要在此窗口内执行函数。窗口函数和我们上面讲到的聚合函数比较容易混淆，二者的区别主要在于聚合函数是将多条记录聚合为一条记录，窗口函数是每条记录都会执行，执行后记录条数不会变。窗口函数不仅仅是几个函数，它是一套完整的语法，函数只是该语法的一部分，基本语法如下所示： 12&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt; order by &lt;用于排序的列名&gt; rows between ... and ...)&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt; order by &lt;用于排序的列名&gt; range between ... and ...) 上面语法中，窗口函数的位置可以放以下两种函数： 专用窗口函数，包括：lead、lag、first_value、last_value、rank、dense_rank和row_number等。 聚合函数，包括：sum、avg、max、min和count等。 下面为大家举几个使用窗口函数的简单例子，我们直接使用上一课创建的 hrs 数据库。 例子1：查询按月薪从高到低排在第4到第6名的员工的姓名和月薪。 123456select * from ( select `ename`, `sal`, row_number() over (order by `sal` desc) as `rank` from `tb_emp`) `temp` where `rank` between 4 and 6; 说明：上面使用的函数row_number()可以为每条记录生成一个行号，在实际工作中可以根据需要将其替换为rank()或dense_rank()函数，三者的区别可以参考官方文档或阅读《通俗易懂的学会：SQL窗口函数》进行了解。在MySQL 8以前的版本，我们可以通过下面的方式来完成类似的操作。 1234select `rank`, `ename`, `sal` from ( select @a:=@a+1 as `rank`, `ename`, `sal` from `tb_emp`, (select @a:=0) as t1 order by `sal` desc) as `temp` where `rank` between 4 and 6; 例子2：查询每个部门月薪最高的两名的员工的姓名和部门名称。 1234567select `ename`, `sal`, `dname` from ( select `ename`, `sal`, `dno`, rank() over (partition by `dno` order by `sal` desc) as `rank` from `tb_emp`) as `temp` natural join `tb_dept` where `rank`&lt;=2; 说明：在MySQL 8以前的版本，我们可以通过下面的方式来完成类似的操作。 123456select `ename`, `sal`, `dname` from `tb_emp` as `t1` natural join `tb_dept` where ( select count(*) from `tb_emp` as `t2` where `t1`.`dno`=`t2`.`dno` and `t2`.`sal`&gt;`t1`.`sal` )&lt;2 order by `dno` asc, `sal` desc; 公用表表达式（CTE）","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/40.SQL详解之DCL","date":"2024-12-12T08:38:01.919Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/40.SQL详解之DCL/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/40.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDCL/","excerpt":"","text":"SQL详解之DCL数据库服务器通常包含了非常重要的数据，可以通过访问控制来确保这些数据的安全，而 DCL 就是解决这一问题的，它可以为指定的用户授予访问权限或者从指定用户处召回指定的权限。DCL 对数据库管理员来说非常重要，因为用户权限的管理关系到数据库的安全。简单的说，我们可以通过 DCL 允许受信任的用户访问数据库，阻止不受信任的用户访问数据库，同时还可以通过 DCL 将每个访问者的的权限最小化（让访问者的权限刚刚够用）。 创建用户我们可以使用下面的 SQL 来创建一个用户并为其指定访问口令。 1create user &#x27;wangdachui&#x27;@&#x27;%&#x27; identified by &#x27;Wang.618&#x27;; 上面的 SQL 创建了名为 wangdachui 的用户，它的访问口令是 Wang.618，该用户可以从任意主机访问数据库服务器，因为 @ 后面使用了可以表示任意多个字符的通配符 %。如果要限制 wangdachui 这个用户只能从 192.168.0.x 这个网段的主机访问数据库服务器，可以按照下面的方式来修改 SQL 语句。 123drop user if exists &#x27;wangdachui&#x27;@&#x27;%&#x27;;create user &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27; identified by &#x27;Wang.618&#x27;; 此时，如果我们使用 wangdachui 这个账号访问数据库服务器，我们几乎不能做任何操作，因为该账号没有任何操作权限。 授予权限我们用下面的语句为 wangdachui 授予查询 school 数据库学院表（tb_college）的权限。 1grant select on `school`.`tb_college` to &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 我们也可以让 wangdachui 对 school 数据库的所有对象都具有查询权限，代码如下所示。 1grant select on `school`.* to &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 如果我们希望 wangdachui 还有 insert、delete 和 update 权限，可以使用下面的方式进行操作。 1grant insert, delete, update on `school`.* to &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 如果我们还想授予 wangdachui 执行 DDL 的权限，可以使用如下所示的 SQL。 1grant create, drop, alter on `school`.* to &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 如果我们希望 wangdachui 账号对所有数据库的所有对象都具备所有的操作权限，可以执行如下所示的操作，但是一般情况下，我们不会这样做，因为我们之前说过，权限刚刚够用就行，一个普通的账号不应该拥有这么大的权限。 1grant all privileges on *.* to &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 召回权限如果要召回 wangdachui 对 school 数据库的 insert、delete 和 update 权限，可以使用下面的操作。 1revoke insert, delete, update on `school`.* from &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 如果要召回所有的权限，可以按照如下所示的方式进行操作。 1revoke all privileges on *.* from &#x27;wangdachui&#x27;@&#x27;192.168.0.%&#x27;; 需要说明的是，由于数据库可能会缓存用户的权限，可以在授予或召回权限后执行下面的语句使新的权限即时生效。 1flush privileges;","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/39.SQL详解之DQL","date":"2024-12-12T08:38:01.916Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/39.SQL详解之DQL/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/39.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDQL/","excerpt":"","text":"SQL详解之DQL接下来，我们利用之前创建的学校选课系统数据库，为大家讲解 DML 中的查询操作。无论对于开发人员还是数据分析师，查询都是非常重要的，它关系着我们能否从关系数据库中获取我们需要的数据。建议大家把上上一节课中建库建表的 DDL 以及 上一节课中插入数据的 DML 重新执行一次，确保表和数据跟没有问题再执行下面的操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365USE school;-- 查询所有学生的所有信息SELECT stu_id, stu_name, stu_sex, stu_birth, stu_addr, col_id FROM tb_student;-- 查询学生的学号、姓名和籍贯(投影和别名)SELECT stu_id AS 学号, stu_name AS 姓名, stu_addr AS 籍贯 FROM tb_student;-- 查询所有课程的名称及学分(投影和别名)SELECT cou_name AS 课程名称, cou_credit AS 学分 FROM tb_course;-- 查询所有女学生的姓名和出生日期(数据筛选)SELECT stu_name, stu_birth FROM tb_student WHERE stu_sex = 0;-- 查询籍贯为“四川成都”的女学生的姓名和出生日期(数据筛选)SELECT stu_name, stu_birth FROM tb_student WHERE stu_sex = 0 AND stu_addr = &#x27;四川成都&#x27;;-- 查询籍贯为“四川成都”或者性别是女的学生(数据筛选)SELECT stu_name, stu_birth FROM tb_student WHERE stu_sex = 0 OR stu_addr = &#x27;四川成都&#x27;;-- 查询所有80后学生的姓名、性别和出生日期(数据筛选)SELECT stu_name, stu_sex, stu_birth FROM tb_student WHERE &#x27;1980-1-1&#x27; &lt;= stu_birth AND stu_birth &lt;= &#x27;1989-12-31&#x27;; SELECT stu_name, stu_sex, stu_birth FROM tb_student WHERE stu_birth BETWEEN &#x27;1980-1-1&#x27; AND &#x27;1989-12-31&#x27;;-- 查询学分大于2的课程的名称和学分(数据筛选)SELECT cou_name, cou_credit FROM tb_course WHERE cou_credit &gt; 2;-- 查询学分是奇数的课程的名称和学分(数据筛选)SELECT cou_name, cou_credit FROM tb_course WHERE cou_credit MOD 2 &lt;&gt; 0;-- 查询选择选了1111的课程考试成绩在90分以上的学生学号(数据筛选)SELECT stu_id FROM tb_record WHERE cou_id = 1111 AND score &gt; 90;-- 查询名字叫“杨过”的学生的姓名和性别(数据筛选)SELECT stu_name AS 姓名, CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别 FROM tb_student WHERE stu_name = &#x27;杨过&#x27;; SELECT stu_name AS 姓名, IF(stu_sex, &#x27;男&#x27;, &#x27;女&#x27;) AS 性别 FROM tb_student WHERE stu_name = &#x27;杨过&#x27;; -- 查询姓“杨”的学生姓名和性别(模糊匹配)-- 通配符 % 匹配零个或任意多个字符SELECT stu_name AS 姓名, CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别 FROM tb_student WHERE stu_name LIKE &#x27;杨%&#x27;;-- 查询姓“杨”名字两个字的学生姓名和性别(模糊匹配)-- 通过符 _ 匹配一个字符SELECT stu_name AS 姓名, CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别 FROM tb_student WHERE stu_name LIKE &#x27;杨_&#x27;;-- 查询姓“杨”名字三个字的学生姓名和性别(模糊匹配)SELECT stu_name AS 姓名, CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别 FROM tb_student WHERE stu_name LIKE &#x27;杨__&#x27;; -- 查询学号最后一位是3的学生的学号和姓名(模糊匹配)SELECT stu_id, stu_name FROM tb_student WHERE stu_id LIKE &#x27;%3&#x27;;-- 查询名字中有“不”字或“嫣”字的学生的学号和姓名(模糊匹配和并集运算)SELECT stu_id, stu_name FROM tb_student WHERE stu_name LIKE &#x27;%不%&#x27; OR stu_name LIKE &#x27;%嫣%&#x27;; SELECT stu_id, stu_name FROM tb_student WHERE stu_name LIKE &#x27;%不%&#x27; UNIONSELECT stu_id, stu_name FROM tb_student WHERE stu_name LIKE &#x27;%嫣%&#x27;;-- 查询姓“杨”或姓“林”名字三个字的学生的学号和姓名(正则表达式模糊匹配)SELECT stu_id, stu_name FROM tb_student WHERE stu_name REGEXP &#x27;[林杨][\\\\u4e00-\\\\u9fa5]&#123;2&#125;&#x27;;-- 查询没有录入籍贯的学生姓名(空值处理)SELECT stu_name FROM tb_student WHERE TRIM(stu_addr) = &#x27;&#x27; OR stu_addr is null; -- 查询录入了籍贯的学生姓名(空值处理)SELECT stu_name FROM tb_student WHERE TRIM(stu_addr) &lt;&gt; &#x27;&#x27; AND stu_addr is not null;-- 查询学生选课的所有日期(去重)SELECT DISTINCT sel_date FROM tb_record;-- 查询学生的籍贯(去重)SELECT DISTINCT stu_addr FROM tb_student WHERE TRIM(stu_addr) &lt;&gt; &#x27;&#x27; AND stu_addr is not null;-- 查询男学生的姓名和生日按年龄从大到小排列(排序)SELECT stu_name, stu_birth FROM tb_student WHERE stu_sex = 1 ORDER BY stu_birth ASC; -- 补充：将上面的生日换算成年龄(日期函数、数值函数)SELECT stu_name AS 姓名, FLOOR(DATEDIFF(CURDATE(), stu_birth) / 365) AS 年龄 FROM tb_student WHERE stu_sex = 1 ORDER BY 年龄 DESC;-- 查询年龄最大的学生的出生日期(聚合函数)SELECT MIN(stu_birth) FROM tb_student;-- 查询年龄最小的学生的出生日期(聚合函数)SELECT MAX(stu_birth) FROM tb_student;-- 查询编号为1111的课程考试成绩的最高分(聚合函数)SELECT MAX(score) FROM tb_record WHERE cou_id = 1111;-- 查询学号为1001的学生考试成绩的最低分、最高分、平均分、标准差、方差(聚合函数)SELECT MIN(score) AS 最低分, MAX(score) AS 最高分, ROUND(AVG(score), 1) AS 平均分, STDDEV(score) AS 标准差, VARIANCE(score) AS 方差 FROM tb_record WHERE stu_id = 1001;-- 查询学号为1001的学生考试成绩的平均分，如果有null值，null值算0分(聚合函数)SELECT ROUND(SUM(score) / COUNT(*), 1) AS 平均分 FROM tb_record WHERE stu_id = 1001;-- 查询男女学生的人数(分组和聚合函数)SELECT CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别, COUNT(*) AS 人数 FROM tb_student GROUP BY stu_sex;-- 查询每个学院学生人数(分组和聚合函数)SELECT col_id AS 学院编号, COUNT(*) AS 人数 FROM tb_student GROUP BY col_id WITH ROLLUP;-- 查询每个学院男女学生人数(分组和聚合函数)SELECT col_id AS 学院编号, CASE stu_sex WHEN 1 THEN &#x27;男&#x27; ELSE &#x27;女&#x27; END AS 性别, COUNT(*) AS 人数 FROM tb_student GROUP BY col_id, stu_sex;-- 查询每个学生的学号和平均成绩(分组和聚合函数)SELECT stu_id AS 学号, ROUND(AVG(score), 1) AS 平均分 FROM tb_record GROUP BY stu_id;-- 查询平均成绩大于等于90分的学生的学号和平均成绩(分组后的数据筛选)SELECT stu_id AS 学号, ROUND(AVG(score), 1) AS 平均分 FROM tb_record GROUP BY stu_idHAVING 平均分 &gt;= 90;-- 查询1111、2222、3333三门课程平均成绩大于等于90分的学生的学号和平均成绩(分组前后的数据筛选)SELECT stu_id AS 学号, ROUND(AVG(score), 1) AS 平均分 FROM tb_record WHERE cou_id in (1111, 2222, 3333) GROUP BY stu_idHAVING 平均分 &gt;= 90 ORDER BY 平均分 ASC;-- 查询年龄最大的学生的姓名(子查询)SELECT stu_name FROM tb_student WHERE stu_birth = (SELECT MIN(stu_birth) FROM tb_student);-- 查询选了两门以上的课程的学生姓名(子查询和集合运算)SELECT stu_name FROM tb_student WHERE stu_id in (SELECT stu_id FROM tb_record GROUP BY stu_id HAVING COUNT(*) &gt; 2);-- 查询学生的姓名、生日和所在学院名称(表连接)SELECT stu_name, stu_birth, col_name FROM tb_student AS t1, tb_college AS t2 WHERE t1.col_id = t2.col_id; SELECT stu_name, stu_birth, col_name FROM tb_student INNER JOIN tb_college ON tb_student.col_id = tb_college.col_id;SELECT stu_name, stu_birth, col_name FROM tb_student NATURAL JOIN tb_college; SELECT stu_name, stu_birth, col_name FROM tb_student CROSS JOIN tb_college;-- 查询学生姓名、课程名称以及成绩(表连接)SELECT stu_name, cou_name, score FROM tb_student, tb_course, tb_record WHERE tb_student.stu_id = tb_record.stu_id AND tb_course.cou_id = tb_record.cou_id AND score is not null;SELECT stu_name, cou_name, score FROM tb_student INNER JOIN tb_record ON tb_student.stu_id = tb_record.stu_id INNER JOIN tb_course ON tb_course.cou_id = tb_record.cou_id WHERE score is not null; SELECT stu_name, cou_name, score FROM tb_student NATURAL JOIN tb_record NATURAL JOIN tb_course WHERE score is not null;-- 补充：上面的查询结果取前5条数据(分页查询)SELECT stu_name, cou_name, score FROM tb_student NATURAL JOIN tb_record NATURAL JOIN tb_course WHERE score is not null ORDER BY cou_id ASC, score DESC LIMIT 5;-- 补充：上面的查询结果取第6-10条数据(分页查询)SELECT stu_name, cou_name, score FROM tb_student NATURAL JOIN tb_record NATURAL JOIN tb_course WHERE score is not null ORDER BY cou_id ASC, score DESC LIMIT 5OFFSET 5;-- 补充：上面的查询结果取第11-15条数据(分页查询)SELECT stu_name, cou_name, score FROM tb_student NATURAL JOIN tb_record NATURAL JOIN tb_course WHERE score is not null ORDER BY cou_id ASC, score DESC LIMIT 10, 5;-- 查询选课学生的姓名和平均成绩(子查询和表连接)-- Error Code: 1248. Every derived table must have its own aliasSELECT stu_name, avg_score FROM tb_student NATURAL JOIN (SELECT stu_id, ROUND(AVG(score), 1) AS avg_score FROM tb_record GROUP BY stu_id) as tmp;-- 查询学生的姓名和选课的数量(子查询和表连接)SELECT stu_name, total FROM tb_student NATURAL JOIN (SELECT stu_id, COUNT(*) AS total FROM tb_record GROUP BY stu_id) as tmp;-- 查询每个学生的姓名和选课数量(子查询和左外连接)SELECT stu_name AS 姓名, COALESCE(total, 0) AS 选课数量 FROM tb_student AS t1 LEFT JOIN (SELECT stu_id, COUNT(*) AS total FROM tb_record GROUP BY stu_id) AS t2 ON t1.stu_id = t2.stu_id; 有几个地方需要加以说明： MySQL目前的版本不支持全外连接，上面我们通过union操作，将左外连接和右外连接的结果求并集实现全外连接的效果。大家可以通过下面的图来加深对连表操作的认识。 MySQL 中支持多种类型的运算符，包括：算术运算符（+、-、*、/、%）、比较运算符（=、&lt;&gt;、&lt;=&gt;、&lt;、&lt;=、&gt;、&gt;=、BETWEEN...AND...、IN、IS NULL、IS NOT NULL、LIKE、RLIKE、REGEXP）、逻辑运算符（NOT、AND、OR、XOR）和位运算符（&amp;、|、^、~、&gt;&gt;、&lt;&lt;），我们可以在 DML 中使用这些运算符处理数据。 在查询数据时，可以在SELECT语句及其子句（如WHERE子句、ORDER BY子句、HAVING子句等）中使用函数，这些函数包括字符串函数、数值函数、时间日期函数、流程函数等，如下面的表格所示。 常用字符串函数。 函数 功能 CONCAT 将多个字符串连接成一个字符串 FORMAT 将数值格式化成字符串并指定保留几位小数 FROM_BASE64 &#x2F; TO_BASE64 BASE64解码&#x2F;编码 BIN &#x2F; OCT &#x2F; HEX 将数值转换成二进制&#x2F;八进制&#x2F;十六进制字符串 LOCATE 在字符串中查找一个子串的位置 LEFT &#x2F; RIGHT 返回一个字符串左边&#x2F;右边指定长度的字符 LENGTH &#x2F; CHAR_LENGTH 返回字符串的长度以字节&#x2F;字符为单位 LOWER &#x2F; UPPER 返回字符串的小写&#x2F;大写形式 LPAD &#x2F; RPAD 如果字符串的长度不足，在字符串左边&#x2F;右边填充指定的字符 LTRIM &#x2F; RTRIM 去掉字符串前面&#x2F;后面的空格 ORD &#x2F; CHAR 返回字符对应的编码&#x2F;返回编码对应的字符 STRCMP 比较字符串，返回-1、0、1分别表示小于、等于、大于 SUBSTRING 返回字符串指定范围的子串 常用数值函数。 函数 功能 ABS 返回一个数的绝度值 CEILING &#x2F; FLOOR 返回一个数上取整&#x2F;下取整的结果 CONV 将一个数从一种进制转换成另一种进制 CRC32 计算循环冗余校验码 EXP &#x2F; LOG &#x2F; LOG2 &#x2F; LOG10 计算指数&#x2F;对数 POW 求幂 RAND 返回[0,1)范围的随机数 ROUND 返回一个数四舍五入后的结果 SQRT 返回一个数的平方根 TRUNCATE 截断一个数到指定的精度 SIN &#x2F; COS &#x2F; TAN &#x2F; COT &#x2F; ASIN &#x2F; ACOS &#x2F; ATAN 三角函数 常用时间日期函数。 函数 功能 CURDATE &#x2F; CURTIME &#x2F; NOW 获取当前日期&#x2F;时间&#x2F;日期和时间 ADDDATE &#x2F; SUBDATE 将两个日期表达式相加&#x2F;相减并返回结果 DATE &#x2F; TIME 从字符串中获取日期&#x2F;时间 YEAR &#x2F; MONTH &#x2F; DAY 从日期中获取年&#x2F;月&#x2F;日 HOUR &#x2F; MINUTE &#x2F; SECOND 从时间中获取时&#x2F;分&#x2F;秒 DATEDIFF &#x2F; TIMEDIFF &#x2F; TIMESTAMPDIFF 返回两个时间日期表达式相差多少天&#x2F;小时 MAKEDATE &#x2F; MAKETIME 制造一个日期&#x2F;时间 常用流程控制函数。 函数 功能 IF 根据条件是否成立返回不同的值 IFNULL 如果为NULL则返回指定的值否则就返回本身 NULLIF 两个表达式相等就返回NULL否则返回第一个表达式的值 其他常用函数。 函数 功能 MD5 &#x2F; SHA1 &#x2F; SHA2 返回字符串对应的哈希摘要 CHARSET &#x2F; COLLATION 返回字符集&#x2F;校对规则 USER &#x2F; CURRENT_USER 返回当前用户 DATABASE 返回当前数据库名 VERSION 返回当前数据库版本 FOUND_ROWS &#x2F; ROW_COUNT 返回查询到的行数&#x2F;受影响的行数 LAST_INSERT_ID 返回最后一个自增主键的值 UUID &#x2F; UUID_SHORT 返回全局唯一标识符","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/38.SQL详解之DML","date":"2024-12-12T08:38:01.914Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/38.SQL详解之DML/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/38.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDML/","excerpt":"","text":"SQL详解之DML我们接着上一课中创建的学校选课系统数据库，为大家讲解 DML 的使用。DML 可以帮助将数据插入到二维表（insert操作）、从二维表删除数据（delete操作）以及更新二维表的数据（update操作）。在执行 DML 之前，我们先通过下面的use命令切换到school数据库。 1use `school`; insert操作顾名思义，insert是用来插入行到二维表中的，插入的方式包括：插入完整的行、插入行的一部分、插入多行、插入查询的结果。我们通过如下所示的 SQL 向学院表中添加一个学院。 1insert into `tb_college` values (default, &#x27;计算机学院&#x27;, &#x27;学习计算机科学与技术的地方&#x27;); 其中，由于学院表的主键是一个自增字段，因此上面的 SQL 中用default表示该列使用默认值，我们也可以使用下面的方式完成同样的操作。 1insert into `tb_college` (`col_name`, `col_intro`) values (&#x27;计算机学院&#x27;, &#x27;学习计算机科学与技术的地方&#x27;); 我们推荐大家使用下面这种做法，指定为哪些字段赋值，这样做可以不按照建表时设定的字段顺序赋值，可以按照values前面的元组中给定的字段顺序为字段赋值，但是需要注意，除了允许为null和有默认值的字段外，其他的字段都必须要一一列出并在values后面的元组中为其赋值。如果希望一次性插入多条记录，我们可以在values后面跟上多个元组来实现批量插入，代码如下所示。 123456insert into `tb_college` (`col_name`, `col_intro`) values (&#x27;外国语学院&#x27;, &#x27;学习歪果仁的语言的学院&#x27;), (&#x27;经济管理学院&#x27;, &#x27;经世济民，治理国家；管理科学，兴国之道&#x27;), (&#x27;体育学院&#x27;, &#x27;发展体育运动，增强人民体质&#x27;); 在插入数据时，要注意主键是不能重复的，如果插入的数据与表中已有记录主键相同，那么insert操作将会产生 Duplicated Entry 的报错信息。再次提醒大家，如果insert操作省略了某些列，那么这些列要么有默认值，要么允许为null，否则也将产生错误。在业务系统中，为了让insert操作不影响其他操作（主要是后面要讲的select操作）的性能，可以在insert和into之间加一个low_priority来降低insert操作的优先级，这个做法也适用于下面要讲的delete和update操作。 假如有一张名为tb_temp的表中有a和b两个列，分别保存了学院的名称和学院的介绍，我们也可以通过查询操作获得tb_temp表的数据并插入到学院表中，如下所示，其中的select就是我们之前提到的 DQL，在下一课中会详细讲解。 123insert into `tb_college` (`col_name`, `col_intro`)select `a`, `b` from `tb_temp`; delete 操作如果需要从表中删除数据，可以使用delete操作，它可以帮助我们删除指定行或所有行，例如我们要删除编号为1的学院，就可以使用如下所示的 SQL。 1delete from `tb_college` where col_id=1; 注意，上面的delete操作中的where子句是用来指定条件的，只有满足条件的行会被删除。如果我们不小心写出了下面的 SQL，就会删除学院表中所有的记录，这是相当危险的，在实际工作中通常也不会这么做。 1delete from `tb_college`; 需要说明的是，即便删除了所有的数据，delete操作不会删除表本身，也不会让 AUTO_INCREMENT 字段的值回到初始值。如果需要删除所有的数据而且让 AUTO_INCREMENT 字段回到初始值，可以使用truncate table执行截断表操作，truncate的本质是删除原来的表并重新创建一个表，它的速度其实更快，因为不需要逐行删除数据。但是请大家记住一点，用truncate table删除数据是非常危险的，因为它会删除所有的数据，而且由于原来的表已经被删除了，要想恢复误删除的数据也会变得极为困难。 update 操作如果要修改表中的数据，可以使用update操作，它可以用来删除指定的行或所有的行。例如，我们将学生表中的“杨过”修改为“杨逍”，这里我们假设“杨过”的学号为1001，代码如下所示。 1update `tb_student` set `stu_name`=&#x27;杨逍&#x27; where `stu_id`=1001; 注意上面 SQL 中的where子句，我们使用学号作为条件筛选出对应的学生，然后通过前面的赋值操作将其姓名修改为“杨逍”。这里为什么不直接使用姓名作为筛选条件，那是因为学生表中可能有多个名为“杨过”的学生，如果使用 stu_name 作为筛选条件，那么我们的update操作有可能会一次更新多条数据，这显然不是我们想要看到的。还有一个需要注意的地方是update操作中的set关键字，因为 SQL 中的=并不表示赋值，而是判断相等的运算符，只有出现在set 关键字后面的=，才具备赋值的能力。 如果要同时修改学生的姓名和生日，我们可以对上面的update语句稍作修改，如下所示。 1update `tb_student` set `stu_name`=&#x27;杨逍&#x27;, `stu_birth`=&#x27;1975-12-29&#x27; where `stu_id`=1001; update语句中也可以使用查询的方式获得数据并以此来更新指定的表数据，有兴趣的读者可以自行研究。在书写update语句时，通常都会有where子句，因为实际工作中几乎不太会用到更新全表的操作，这一点大家一定要注意。 完整的数据下面我们给出完整的向 school 数据库的五张表中插入数据的 SQL。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071use `school`;-- 插入学院数据insert into `tb_college` (`col_name`, `col_intro`) values (&#x27;计算机学院&#x27;, &#x27;计算机学院1958年设立计算机专业，1981年建立计算机科学系，1998年设立计算机学院，2005年5月，为了进一步整合教学和科研资源，学校决定，计算机学院和软件学院行政班子合并统一运作、实行教学和学生管理独立运行的模式。 学院下设三个系：计算机科学与技术系、物联网工程系、计算金融系；两个研究所：图象图形研究所、网络空间安全研究院（2015年成立）；三个教学实验中心：计算机基础教学实验中心、IBM技术中心和计算机专业实验中心。&#x27;), (&#x27;外国语学院&#x27;, &#x27;外国语学院设有7个教学单位，6个文理兼收的本科专业；拥有1个一级学科博士授予点，3个二级学科博士授予点，5个一级学科硕士学位授权点，5个二级学科硕士学位授权点，5个硕士专业授权领域，同时还有2个硕士专业学位（MTI）专业；有教职员工210余人，其中教授、副教授80余人，教师中获得中国国内外名校博士学位和正在职攻读博士学位的教师比例占专任教师的60%以上。&#x27;), (&#x27;经济管理学院&#x27;, &#x27;经济学院前身是创办于1905年的经济科；已故经济学家彭迪先、张与九、蒋学模、胡寄窗、陶大镛、胡代光，以及当代学者刘诗白等曾先后在此任教或学习。&#x27;);-- 插入学生数据insert into `tb_student` (`stu_id`, `stu_name`, `stu_sex`, `stu_birth`, `stu_addr`, `col_id`) values (1001, &#x27;杨过&#x27;, 1, &#x27;1990-3-4&#x27;, &#x27;湖南长沙&#x27;, 1), (1002, &#x27;任我行&#x27;, 1, &#x27;1992-2-2&#x27;, &#x27;湖南长沙&#x27;, 1), (1033, &#x27;王语嫣&#x27;, 0, &#x27;1989-12-3&#x27;, &#x27;四川成都&#x27;, 1), (1572, &#x27;岳不群&#x27;, 1, &#x27;1993-7-19&#x27;, &#x27;陕西咸阳&#x27;, 1), (1378, &#x27;纪嫣然&#x27;, 0, &#x27;1995-8-12&#x27;, &#x27;四川绵阳&#x27;, 1), (1954, &#x27;林平之&#x27;, 1, &#x27;1994-9-20&#x27;, &#x27;福建莆田&#x27;, 1), (2035, &#x27;东方不败&#x27;, 1, &#x27;1988-6-30&#x27;, null, 2), (3011, &#x27;林震南&#x27;, 1, &#x27;1985-12-12&#x27;, &#x27;福建莆田&#x27;, 3), (3755, &#x27;项少龙&#x27;, 1, &#x27;1993-1-25&#x27;, &#x27;四川成都&#x27;, 3), (3923, &#x27;杨不悔&#x27;, 0, &#x27;1985-4-17&#x27;, &#x27;四川成都&#x27;, 3);-- 插入老师数据insert into `tb_teacher` (`tea_id`, `tea_name`, `tea_title`, `col_id`) values (1122, &#x27;张三丰&#x27;, &#x27;教授&#x27;, 1), (1133, &#x27;宋远桥&#x27;, &#x27;副教授&#x27;, 1), (1144, &#x27;杨逍&#x27;, &#x27;副教授&#x27;, 1), (2255, &#x27;范遥&#x27;, &#x27;副教授&#x27;, 2), (3366, &#x27;韦一笑&#x27;, default, 3);-- 插入课程数据insert into `tb_course` (`cou_id`, `cou_name`, `cou_credit`, `tea_id`) values (1111, &#x27;Python程序设计&#x27;, 3, 1122), (2222, &#x27;Web前端开发&#x27;, 2, 1122), (3333, &#x27;操作系统&#x27;, 4, 1122), (4444, &#x27;计算机网络&#x27;, 2, 1133), (5555, &#x27;编译原理&#x27;, 4, 1144), (6666, &#x27;算法和数据结构&#x27;, 3, 1144), (7777, &#x27;经贸法语&#x27;, 3, 2255), (8888, &#x27;成本会计&#x27;, 2, 3366), (9999, &#x27;审计学&#x27;, 3, 3366);-- 插入选课数据insert into `tb_record` (`stu_id`, `cou_id`, `sel_date`, `score`) values (1001, 1111, &#x27;2017-09-01&#x27;, 95), (1001, 2222, &#x27;2017-09-01&#x27;, 87.5), (1001, 3333, &#x27;2017-09-01&#x27;, 100), (1001, 4444, &#x27;2018-09-03&#x27;, null), (1001, 6666, &#x27;2017-09-02&#x27;, 100), (1002, 1111, &#x27;2017-09-03&#x27;, 65), (1002, 5555, &#x27;2017-09-01&#x27;, 42), (1033, 1111, &#x27;2017-09-03&#x27;, 92.5), (1033, 4444, &#x27;2017-09-01&#x27;, 78), (1033, 5555, &#x27;2017-09-01&#x27;, 82.5), (1572, 1111, &#x27;2017-09-02&#x27;, 78), (1378, 1111, &#x27;2017-09-05&#x27;, 82), (1378, 7777, &#x27;2017-09-02&#x27;, 65.5), (2035, 7777, &#x27;2018-09-03&#x27;, 88), (2035, 9999, &#x27;2019-09-02&#x27;, null), (3755, 1111, &#x27;2019-09-02&#x27;, null), (3755, 8888, &#x27;2019-09-02&#x27;, null), (3755, 9999, &#x27;2017-09-01&#x27;, 92); 注意：上面的insert语句使用了批处理的方式来插入数据，这种做法插入数据的效率比较高。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/37.SQL详解之DDL","date":"2024-12-12T08:38:01.912Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/37.SQL详解之DDL/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/37.SQL%E8%AF%A6%E8%A7%A3%E4%B9%8BDDL/","excerpt":"","text":"SQL详解之DDL我们通常可以将 SQL 分为四类，分别是 DDL（数据定义语言）、DML（数据操作语言）、 DCL（数据控制语言）和 TCL（事务控制语言）。DDL 主要用于创建、删除、修改数据库中的对象，比如创建、删除和修改二维表，核心的关键字包括create、drop和alter；DML 主要负责数据的插入、删除、更新和查询，关键词包括insert、delete、update和select；DCL 用于授予和召回权限，核心关键词是grant和revoke；TCL 通常用于事务控制。 说明：SQL 是不区分大小写的语言，有人会建议将关键字大写，其他部分小写。为了书写和识别方便，下面的 SQL 我都是使用小写字母进行书写的。 如果公司的 SQL 编程规范有强制规定，那么就按照公司的要求来，个人的喜好不应该凌驾于公司的编程规范之上，这一点对职业人来说应该是常识。 建库建表下面我们来实现一个非常简单的学校选课系统的数据库。我们将数据库命名为school，四个关键的实体分别是学院、老师、学生和课程，其中，学生跟学院是从属关系，这个关系从数量上来讲是多对一关系，因为一个学院可以有多名学生，而一个学生通常只属于一个学院；同理，老师跟学院的从属关系也是多对一关系。一名老师可以讲授多门课程，一门课程如果只有一个授课老师的话，那么课程跟老师也是多对一关系；如果允许多个老师合作讲授一门课程，那么课程和老师就是多对多关系。简单起见，我们将课程和老师设计为多对一关系。学生和课程是典型的多对多关系，因为一个学生可以选择多门课程，一门课程也可以被多个学生选择，而关系型数据库需要借助中间表才能维持维持两个实体的多对多关系。最终，我们的学校选课系统一共有五张表，分别是学院表（tb_college）、学生表（tb_student）、教师表（tb_teacher）、课程表（tb_course）和选课记录表（tb_record），其中选课记录表就是维持学生跟课程多对多关系的中间表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566-- 如果存在名为school的数据库就删除它drop database if exists `school`;-- 创建名为school的数据库并设置默认的字符集和排序方式create database `school` default character set utf8mb4 collate utf8mb4_general_ci;-- 切换到school数据库上下文环境use `school`;-- 创建学院表create table `tb_college`(`col_id` int unsigned auto_increment comment &#x27;编号&#x27;,`col_name` varchar(50) not null comment &#x27;名称&#x27;,`col_intro` varchar(500) default &#x27;&#x27; comment &#x27;介绍&#x27;,primary key (`col_id`)) engine=innodb auto_increment=1 comment &#x27;学院表&#x27;;-- 创建学生表create table `tb_student`(`stu_id` int unsigned not null comment &#x27;学号&#x27;,`stu_name` varchar(20) not null comment &#x27;姓名&#x27;,`stu_sex` boolean default 1 not null comment &#x27;性别&#x27;,`stu_birth` date not null comment &#x27;出生日期&#x27;,`stu_addr` varchar(255) default &#x27;&#x27; comment &#x27;籍贯&#x27;,`col_id` int unsigned not null comment &#x27;所属学院&#x27;,primary key (`stu_id`),constraint `fk_student_col_id` foreign key (`col_id`) references `tb_college` (`col_id`)) engine=innodb comment &#x27;学生表&#x27;;-- 创建教师表create table `tb_teacher`(`tea_id` int unsigned not null comment &#x27;工号&#x27;,`tea_name` varchar(20) not null comment &#x27;姓名&#x27;,`tea_title` varchar(10) default &#x27;助教&#x27; comment &#x27;职称&#x27;,`col_id` int unsigned not null comment &#x27;所属学院&#x27;,primary key (`tea_id`),constraint `fk_teacher_col_id` foreign key (`col_id`) references `tb_college` (`col_id`)) engine=innodb comment &#x27;老师表&#x27;;-- 创建课程表create table `tb_course`(`cou_id` int unsigned not null comment &#x27;编号&#x27;,`cou_name` varchar(50) not null comment &#x27;名称&#x27;,`cou_credit` int not null comment &#x27;学分&#x27;,`tea_id` int unsigned not null comment &#x27;授课老师&#x27;,primary key (`cou_id`),constraint `fk_course_tea_id` foreign key (`tea_id`) references `tb_teacher` (`tea_id`)) engine=innodb comment &#x27;课程表&#x27;;-- 创建选课记录表create table `tb_record`(`rec_id` bigint unsigned auto_increment comment &#x27;选课记录号&#x27;,`stu_id` int unsigned not null comment &#x27;学号&#x27;,`cou_id` int unsigned not null comment &#x27;课程编号&#x27;,`sel_date` date not null comment &#x27;选课日期&#x27;,`score` decimal(4,1) comment &#x27;考试成绩&#x27;,primary key (`rec_id`),constraint `fk_record_stu_id` foreign key (`stu_id`) references `tb_student` (`stu_id`),constraint `fk_record_cou_id` foreign key (`cou_id`) references `tb_course` (`cou_id`),constraint `uk_record_stu_cou` unique (`stu_id`, `cou_id`)) engine=innodb comment &#x27;选课记录表&#x27;; 上面的DDL有几个地方需要强调一下： 首先，上面 SQL 中的数据库名、表名、字段名都被反引号（&#96;）包裹起来，反引号并不是必须的，但是却可以解决表名、字段名等跟 SQL 关键字（SQL 中有特殊含义的单词）冲突的问题。 创建数据库时，我们通过default character set utf8mb4指定了数据库默认使用的字符集为utf8mb4（最大4字节的utf-8编码），我们推荐使用该字符集，它也是 MySQL 8.x 默认使用的字符集，因为它能够支持国际化编码，还可以存储 Emoji 字符。可以通过下面的命令查看 MySQL 支持的字符集以及默认的排序规则。 1show character set; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546+----------+---------------------------------+---------------------+--------+| Charset | Description | Default collation | Maxlen |+----------+---------------------------------+---------------------+--------+| big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 || dec8 | DEC West European | dec8_swedish_ci | 1 || cp850 | DOS West European | cp850_general_ci | 1 || hp8 | HP West European | hp8_english_ci | 1 || koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 || latin1 | cp1252 West European | latin1_swedish_ci | 1 || latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 || swe7 | 7bit Swedish | swe7_swedish_ci | 1 || ascii | US ASCII | ascii_general_ci | 1 || ujis | EUC-JP Japanese | ujis_japanese_ci | 3 || sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 || hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 || tis620 | TIS620 Thai | tis620_thai_ci | 1 || euckr | EUC-KR Korean | euckr_korean_ci | 2 || koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 || gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 || greek | ISO 8859-7 Greek | greek_general_ci | 1 || cp1250 | Windows Central European | cp1250_general_ci | 1 || gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 || latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 || armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 || utf8 | UTF-8 Unicode | utf8_general_ci | 3 || ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 || cp866 | DOS Russian | cp866_general_ci | 1 || keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 || macce | Mac Central European | macce_general_ci | 1 || macroman | Mac West European | macroman_general_ci | 1 || cp852 | DOS Central European | cp852_general_ci | 1 || latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 || utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 || cp1251 | Windows Cyrillic | cp1251_general_ci | 1 || utf16 | UTF-16 Unicode | utf16_general_ci | 4 || utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 || cp1256 | Windows Arabic | cp1256_general_ci | 1 || cp1257 | Windows Baltic | cp1257_general_ci | 1 || utf32 | UTF-32 Unicode | utf32_general_ci | 4 || binary | Binary pseudo charset | binary | 1 || geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 || cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 || eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 || gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 |+----------+---------------------------------+---------------------+--------+41 rows in set (0.00 sec) 如果要设置 MySQL 服务启动时默认使用的字符集，可以修改MySQL的配置并添加以下内容。 12[mysqld]character-set-server=utf8 提示：如果不清楚如何修改 MySQL 的配置文件就先不要管它。 创建和删除数据库时，关键字database也可以替换为schema，二者作用相同。 建表语句中的not null是非空约束，它限定了字段不能为空；default用于为字段指定默认值，我们称之为默认值约束；primary key是主键约束，它设定了能够唯一确定一条记录的列，也确保了每条记录都是独一无二的，因为主键不允许重复；foreign key是外键约束，它维持了两张表的参照完整性，举个例子，由于学生表中为 col_id 字段添加了外键约束，限定其必须引用（references）学院表中的 col_id，因此学生表中的学院编号必须来自于学院表中的学院编号，不能够随意为该字段赋值。如果需要给主键约束、外键约束等起名字，可以使用constriant关键字并在后面跟上约束的名字。 建表语句中的comment 关键字用来给列和表添加注释，增强代码的可读性和可维护性。 在创建表的时候，可以自行选择底层的存储引擎。MySQL 支持多种存储引擎，可以通过show engines命令进行查看。MySQL 5.5 以后的版本默认使用的存储引擎是 InnoDB，它是我们推荐大家使用的存储引擎（因为更适合当下互联网应用对高并发、性能以及事务支持等方面的需求），为了 SQL 语句的向下兼容性，我们可以在建表语句结束处右圆括号的后面通过engine=innodb来指定使用 InnoDB 存储引擎。 1show engines\\G 说明：上面的 \\G 是为了换一种输出方式，在命令行客户端中，如果表的字段很多一行显示不完，就会导致输出的内容看起来非常不舒服，使用 \\G 可以将记录的每个列以独占整行的的方式输出，这种输出方式在命令行客户端中看起来会舒服很多。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364*************************** 1. row *************************** Engine: InnoDB Support: DEFAULT Comment: Supports transactions, row-level locking, and foreign keysTransactions: YES XA: YES Savepoints: YES*************************** 2. row *************************** Engine: MRG_MYISAM Support: YES Comment: Collection of identical MyISAM tablesTransactions: NO XA: NO Savepoints: NO*************************** 3. row *************************** Engine: MEMORY Support: YES Comment: Hash based, stored in memory, useful for temporary tablesTransactions: NO XA: NO Savepoints: NO*************************** 4. row *************************** Engine: BLACKHOLE Support: YES Comment: /dev/null storage engine (anything you write to it disappears)Transactions: NO XA: NO Savepoints: NO*************************** 5. row *************************** Engine: MyISAM Support: YES Comment: MyISAM storage engineTransactions: NO XA: NO Savepoints: NO*************************** 6. row *************************** Engine: CSV Support: YES Comment: CSV storage engineTransactions: NO XA: NO Savepoints: NO*************************** 7. row *************************** Engine: ARCHIVE Support: YES Comment: Archive storage engineTransactions: NO XA: NO Savepoints: NO*************************** 8. row *************************** Engine: PERFORMANCE_SCHEMA Support: YES Comment: Performance SchemaTransactions: NO XA: NO Savepoints: NO*************************** 9. row *************************** Engine: FEDERATED Support: NO Comment: Federated MySQL storage engineTransactions: NULL XA: NULL Savepoints: NULL9 rows in set (0.00 sec) 下面的表格对MySQL几种常用的数据引擎进行了简单的对比。 特性 InnoDB MRG_MYISAM MEMORY MyISAM 存储限制 有 没有 有 有 事务 支持 锁机制 行锁 表锁 表锁 表锁 B树索引 支持 支持 支持 支持 哈希索引 支持 全文检索 支持（5.6+） 支持 集群索引 支持 数据缓存 支持 支持 索引缓存 支持 支持 支持 支持 数据可压缩 支持 内存使用 高 低 中 低 存储空间使用 高 低 低 批量插入性能 低 高 高 高 是否支持外键 支持 通过上面的比较我们可以了解到，InnoDB 是唯一能够支持外键、事务以及行锁的存储引擎，所以我们之前说它更适合互联网应用，而且在较新版本的 MySQL 中，它也是默认使用的存储引擎。 在定义表结构为每个字段选择数据类型时，如果不清楚哪个数据类型更合适，可以通过 MySQL 的帮助系统来了解每种数据类型的特性、数据的长度和精度等相关信息。 1? data types 说明：在 MySQLWorkbench 中，不能使用?获取帮助，要使用对应的命令help。 1234567891011121314151617181920212223242526272829303132333435363738You asked for help about help category: &quot;Data Types&quot;For more information, type &#x27;help &lt;item&gt;&#x27;, where &lt;item&gt; is one of the followingtopics: AUTO_INCREMENT BIGINT BINARY BIT BLOB BLOB DATA TYPE BOOLEAN CHAR CHAR BYTE DATE DATETIME DEC DECIMAL DOUBLE DOUBLE PRECISION ENUM FLOAT INT INTEGER LONGBLOB LONGTEXT MEDIUMBLOB MEDIUMINT MEDIUMTEXT SET DATA TYPE SMALLINT TEXT TIME TIMESTAMP TINYBLOB TINYINT TINYTEXT VARBINARY VARCHAR YEAR DATA TYPE 获取 varchar 类型的帮助： 1? varchar 执行结果： 12345678910111213141516171819202122232425262728293031Name: &#x27;VARCHAR&#x27;Description:[NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATEcollation_name]A variable-length string. M represents the maximum column length incharacters. The range of M is 0 to 65,535. The effective maximum lengthof a VARCHAR is subject to the maximum row size (65,535 bytes, which isshared among all columns) and the character set used. For example, utf8characters can require up to three bytes per character, so a VARCHARcolumn that uses the utf8 character set can be declared to be a maximumof 21,844 characters. Seehttp://dev.mysql.com/doc/refman/5.7/en/column-count-limit.html.MySQL stores VARCHAR values as a 1-byte or 2-byte length prefix plusdata. The length prefix indicates the number of bytes in the value. AVARCHAR column uses one length byte if values require no more than 255bytes, two length bytes if values may require more than 255 bytes.*Note*:MySQL follows the standard SQL specification, and does not removetrailing spaces from VARCHAR values.VARCHAR is shorthand for CHARACTER VARYING. NATIONAL VARCHAR is thestandard SQL way to define that a VARCHAR column should use somepredefined character set. MySQL uses utf8 as this predefined characterset. http://dev.mysql.com/doc/refman/5.7/en/charset-national.html.NVARCHAR is shorthand for NATIONAL VARCHAR.URL: http://dev.mysql.com/doc/refman/5.7/en/string-type-overview.html 在数据类型的选择上，保存字符串数据通常都使用 VARCHAR 和 CHAR 两种类型，前者通常称为变长字符串，而后者通常称为定长字符串；对于 InnoDB 存储引擎，行存储格式没有区分固定长度和可变长度列，因此 VARCHAR 类型和 CHAR 类型没有本质区别，后者不一定比前者性能更好。如果要保存的很大字符串，可以使用 TEXT 类型；如果要保存很大的字节串，可以使用 BLOB（二进制大对象）类型。在 MySQL 中，TEXT 和 BLOB又分别包括 TEXT、MEDIUMTEXT、LONGTEXT 和 BLOB、MEDIUMBLOB、LONGBLOB 三种不同的类型，它们主要的区别在于存储数据的最大大小不同。保存浮点数可以用 FLOAT 或 DOUBLE 类型，FLOAT 已经不推荐使用了，而且在 MySQL 后续的版本中可能会被移除掉。而保存定点数应该使用 DECIMAL 类型，它可以指定小数点前后有效数字的位数。如果要保存时间日期，DATETIME 类型优于 TIMESTAMP 类型，因为前者能表示的时间日期范围更大，后者底层其实就是一个整数，记录了指定的日期时间和 1970-01-01 00:00:00 相差多少个毫秒，该类型在 2038-01-19 03:14:07 之后就会溢出。 对于自增字段 AUTO_INCREMENT，如果使用 MySQL 5.x 版本要注意自增字段的回溯问题，当然这个问题在 MySQL 8.x 中已经得到了很好的解决，当然，MySQL 8.x 还有很多其他的好处，不管是功能还是性能上都有很多的优化和调整，因此强烈推荐大家使用 MySQL 8.x 版本。对于高并发访问数据库的场景，AUTO_INCREMENT 不仅存在性能上的问题，还可能在多机结构上产生重复的 ID 值，在这种场景下，使用分布式 ID 生成算法（SnowFlake、TinyID等）才是最好的选择，有兴趣的读者可以自行研究。 删除表和修改表下面以学生表为例，为大家说明如何删除表和修改表。删除表可以使用drop table，代码如下所示。 1drop table `tb_student`; 或 1drop table if exists `tb_student`; 需要注意的是，如果学生表已经录入了数据而且该数据被其他表引用了，那么就不能删除学生表，否则上面的操作会报错。在下一课中，我们会讲解如何向表中插入数据，到时候大家可以试一试，能否顺利删除学生表。 如果要修改学生表，可以使用alter table，具体可以分为以下几种情况： 修改表，添加一个新列，例如给学生表添加一个联系电话的列。 1alter table `tb_student` add column `stu_tel` varchar(20) not null comment &#x27;联系电话&#x27;; 注意：如果新增列的时候指定了非空约束（not null），那么学生表不能够有数据，否则原来的数据增加了 stu_tel 列之后是没有数据的，这就违反了非空约束的要求；当然，我们在添加列的时候也可以使用默认值约束来解决这个问题。 修改表，删除指定的列，例如将上面添加的联系电话列删除掉。 1alter table `tb_student` drop column `stu_tel`; 修改表，修改列的数据类型，例如将学生表的 stu_sex 修改为字符。 1alter table `tb_student` modify column `stu_sex` char(1) not null default &#x27;M&#x27; comment &#x27;性别&#x27;; 修改表，修改列的命名，例如将学生表的 stu_sex 修改为 stu_gender。 1alter table `tb_student` change column `stu_sex` `stu_gender` boolean default 1 comment &#x27;性别&#x27;; 修改表，删除约束条件，例如删除学生表的 col_id 列的外键约束。 1alter table `tb_student` drop foreign key `fk_student_col_id`; 修改表，添加约束条件，例如给学生表的 col_id 列加上外键约束。 1alter table `tb_student` add foreign key (`col_id`) references `tb_college` (`col_id`); 或 1alter table `tb_student` add constraint `fk_student_col_id` foreign key (`col_id`) references `tb_college` (`col_id`); 说明：在添加外键约束时，还可以通过on update和on delete来指定在被引用的表发生删除和更新操作时，应该进行何种处理，二者的默认值都是restrict，表示如果存在外键约束，则不允许更新和删除被引用的数据。除了restrict之外，这里可能的取值还有cascade（级联操作）和set null（设置为空），有兴趣的读者可以自行研究。 修改表的名字，例如将学生表的名字修改为 tb_stu_info。 1alter table `tb_student` rename to `tb_stu_info`; 提示：一般情况下，请不要轻易修改数据库或表的名字。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day36-45/36.关系型数据库和MySQL概述","date":"2024-12-12T08:38:01.909Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day36-45/36.关系型数据库和MySQL概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day36-45/36.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8CMySQL%E6%A6%82%E8%BF%B0/","excerpt":"","text":"关系型数据库和MySQL概述关系型数据库概述 数据持久化 - 将数据保存到能够长久保存数据的存储介质中，在掉电的情况下数据也不会丢失。 数据库发展史 - 网状数据库、层次数据库、关系数据库、NoSQL 数据库、NewSQL 数据库。 1970年，IBM的研究员E.F.Codd在Communication of the ACM上发表了名为A Relational Model of Data for Large Shared Data Banks的论文，提出了关系模型的概念，奠定了关系模型的理论基础。后来Codd又陆续发表多篇文章，论述了范式理论和衡量关系系统的12条标准，用数学理论奠定了关系数据库的基础。 关系数据库特点。 理论基础：关系代数（集合论、一阶谓词、关系运算）。 具体表象：用二维表（有行和列）组织数据。 编程语言：结构化查询语言（SQL）。 DDL：数据定义语言 DML：数据操作语言 DCL：数据控制语言 TCL：事务控制语言 ER模型（实体关系模型）和概念模型图。 ER模型，全称为实体关系模型（Entity-Relationship Model），由美籍华裔计算机科学家陈品山先生提出，是概念数据模型的高层描述方式，如下图所示。 实体 - 矩形框 属性 - 椭圆框 关系 - 菱形框 重数 - 1:1（一对一） &#x2F; 1:N（一对多） &#x2F; M:N（多对多） 实际项目开发中，我们可以利用数据库建模工具（如：PowerDesigner）来绘制概念数据模型，然后再设置好目标数据库系统，将概念模型转换成物理模型（如下图所示），最终生成创建二维表的 SQL（很多工具都可以根据我们设计的物理模型图以及设定的目标数据库来导出 SQL 或直接生成数据表）。 关系数据库产品。 Oracle - 目前世界上使用最为广泛的数据库管理系统，作为一个通用的数据库系统，它具有完整的数据管理功能；作为一个关系数据库，它是一个完备关系的产品；作为分布式数据库，它实现了分布式处理的功能。在 Oracle 较新的版本中，还引入了多承租方架构，使用该架构可轻松部署和管理数据库云。 DB2 - IBM 公司开发的、主要运行于 Unix（包括 IBM 自家的 AIX）、Linux、以及 Windows 服务器版等系统的关系数据库产品。DB2 历史悠久且被认为是最早使用 SQL 的数据库产品，它拥有较为强大的商业智能功能。 SQL Server - 由 Microsoft 开发和推广的关系型数据库产品，最初适用于中小企业的数据管理，但是近年来它的应用范围有所扩展，部分大企业甚至是跨国公司也开始基于它来构建自己的数据管理系统。 MySQL - MySQL 是开放源代码的，任何人都可以在 GPL（General Public License）的许可下下载并根据个性化的需要对其进行修改。MySQL 因为其速度、可靠性和适应性而备受关注。 PostgreSQL - 在 BSD 许可证下发行的开放源代码的关系数据库产品。 MySQL 简介MySQL 最早是由瑞典的 MySQL AB 公司开发的一个开放源码的关系数据库管理系统，该公司于2008年被昇阳微系统公司（Sun Microsystems）收购。在2009年，甲骨文公司（Oracle）收购昇阳微系统公司，因此 MySQL 目前也是 Oracle 旗下产品。 MySQL 在过去由于性能高、成本低、可靠性好，已经成为最流行的开源数据库，因此被广泛地应用于中小型网站开发。随着 MySQL 的不断成熟，它也逐渐被应用于更多大规模网站和应用，比如维基百科、谷歌（Google）、脸书（Facebook）、百度、淘宝、腾讯、新浪、去哪儿等都使用了 MySQL 来提供数据持久化服务。 甲骨文公司收购后昇阳微系统公司，大幅调涨 MySQL 商业版的售价，且甲骨文公司不再支持另一个自由软件项目 OpenSolaris 的发展，因此导致自由软件社区对于 Oracle 是否还会持续支持 MySQL 社区版（MySQL 的各个发行版本中唯一免费的版本）有所担忧，MySQL 的创始人麦克尔·维德纽斯以 MySQL 为基础，创建了 MariaDB（以他女儿的名字命名的数据库）分支。有许多原来使用 MySQL 数据库的公司（例如：维基百科）已经陆续完成了从 MySQL 数据库到 MariaDB 数据库的迁移。 安装 MySQLWindows 环境 通过官方网站提供的下载链接下载“MySQL社区版服务器”安装程序，如下图所示，建议大家下载离线安装版的MySQL Installer。 运行 Installer，按照下面的步骤进行安装。 选择自定义安装。 选择需要安装的组件。 如果缺少依赖项，需要先安装依赖项。 准备开始安装。 安装完成。 准备执行配置向导。 执行安装后的配置向导。 配置服务器类型和网络。 配置认证方法（保护密码的方式）。 配置用户和角色。 配置Windows服务名以及是否开机自启。 配置日志。 配置高级选项。 应用配置。 可以在 Windows 系统的“服务”窗口中启动或停止 MySQL。 配置 PATH 环境变量，以便在命令行提示符窗口使用 MySQL 客户端工具。 打开 Windows 的“系统”窗口并点击“高级系统设置”。 在“系统属性”的“高级”窗口，点击“环境变量”按钮。 修改PATH环境变量，将MySQL安装路径下的bin文件夹的路径配置到PATH环境变量中。 配置完成后，可以尝试在“命令提示符”下使用 MySQL 的命令行工具。 Linux 环境下面以 CentOS 7.x 环境为例，演示如何安装 MySQL 5.7.x，如果需要在其他 Linux 系统下安装其他版本的 MySQL，请读者自行在网络上查找对应的安装教程。 安装 MySQL。 可以在 MySQL 官方网站下载安装文件。首先在下载页面中选择平台和版本，然后找到对应的下载链接，直接下载包含所有安装文件的归档文件，解归档之后通过包管理工具进行安装。 12wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.26-1.el7.x86_64.rpm-bundle.tartar -xvf mysql-5.7.26-1.el7.x86_64.rpm-bundle.tar 如果系统上有 MariaDB 相关的文件，需要先移除 MariaDB 相关的文件。 1yum list installed | grep mariadb | awk &#x27;&#123;print $1&#125;&#x27; | xargs yum erase -y 更新和安装可能用到的底层依赖库。 12yum updateyum install -y libaio libaio-devel 接下来可以按照如下所示的顺序用 RPM（Redhat Package Manager）工具安装 MySQL。 123456rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-libs-compat-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-devel-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpmrpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm 可以使用下面的命令查看已经安装的 MySQL 相关的包。 1rpm -qa | grep mysql 配置 MySQL。 MySQL 的配置文件在/etc目录下，名为my.cnf，默认的配置文件内容如下所示。 1cat /etc/my.cnf 123456789101112131415161718192021222324252627# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html[mysqld]## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M## Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.# log_bin## Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2Mdatadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid 通过配置文件，我们可以修改 MySQL 服务使用的端口、字符集、最大连接数、套接字队列大小、最大数据包大小、日志文件的位置、日志过期时间等配置。当然，我们还可以通过修改配置文件来对 MySQL 服务器进行性能调优和安全管控。 启动 MySQL 服务。 可以使用下面的命令来启动 MySQL。 1service mysqld start 在 CentOS 7 中，更推荐使用下面的命令来启动 MySQL。 1systemctl start mysqld 启动 MySQL 成功后，可以通过下面的命令来检查网络端口使用情况，MySQL 默认使用3306端口。 1netstat -ntlp | grep mysql 也可以使用下面的命令查找是否有名为mysqld的进程。 1pgrep mysqld 使用 MySQL 客户端工具连接服务器。 命令行工具： 1mysql -u root -p 说明：启动客户端时，-u参数用来指定用户名，MySQL 默认的超级管理账号为root；-p表示要输入密码（用户口令）；如果连接的是其他主机而非本机，可以用-h来指定连接主机的主机名或IP地址。 如果是首次安装 MySQL，可以使用下面的命令来找到默认的初始密码。 1cat /var/log/mysqld.log | grep password 上面的命令会查看 MySQL 的日志带有password的行，在显示的结果中root@localhost:后面的部分就是默认设置的初始密码。 进入客户端工具后，可以通过下面的指令来修改超级管理员（root）的访问口令为123456。 123set global validate_password_policy=0;set global validate_password_length=6;alter user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;; 说明：MySQL 较新的版本默认不允许使用弱口令作为用户口令，所以上面的代码修改了验证用户口令的策略和口令的长度。事实上我们不应该使用弱口令，因为存在用户口令被暴力破解的风险。近年来，攻击数据库窃取数据和劫持数据库勒索比特币的事件屡见不鲜，要避免这些潜在的风险，最为重要的一点是不要让数据库服务器暴露在公网上（最好的做法是将数据库置于内网，至少要做到不向公网开放数据库服务器的访问端口），另外要保管好root账号的口令，应用系统需要访问数据库时，通常不使用root账号进行访问，而是创建其他拥有适当权限的账号来访问。 再次使用客户端工具连接 MySQL 服务器时，就可以使用新设置的口令了。在实际开发中，为了方便用户操作，可以选择图形化的客户端工具来连接 MySQL 服务器，包括： MySQL Workbench（官方工具） Navicat for MySQL（界面简单友好） macOS环境macOS 系统安装 MySQL 是比较简单的，只需要从刚才说到的官方网站下载 DMG 安装文件并运行就可以了，下载的时候需要根据自己使用的是 Intel 的芯片还是苹果的 M1 芯片选择下载链接，如下图所示。 安装成功后，可以在“系统偏好设置”中找到“MySQL”，在如下所示的画面中，可以启动和停止 MySQL 服务器，也可以对 MySQL 核心文件的路径进行配置。 MySQL 基本命令查看命令 查看所有数据库 1show databases; 查看所有字符集 1show character set; 查看所有的排序规则 1show collation; 查看所有的引擎 1show engines; 查看所有日志文件 1show binary logs; 查看数据库下所有表 1show tables; 获取帮助在 MySQL 命令行工具中，可以使用help命令或?来获取帮助，如下所示。 查看show命令的帮助。 1? show 查看有哪些帮助内容。 1? contents 获取函数的帮助。 1? functions 获取数据类型的帮助。 1? data types 其他命令 新建&#x2F;重建服务器连接 - connect &#x2F; resetconnection。 清空当前输入 - \\c。在输入错误时，可以及时使用\\c清空当前输入并重新开始。 修改终止符（定界符）- delimiter。默认的终止符是;，可以使用该命令修改成其他的字符，例如修改为$符号，可以用delimiter $命令。 打开系统默认编辑器 - edit。编辑完成保存关闭之后，命令行会自动执行编辑的内容。 查看服务器状态 - status。 修改默认提示符 - prompt。 执行系统命令 - system。可以将系统命令跟在system命令的后面执行，system命令也可以缩写为\\!。 执行 SQL 文件 - source。source命令后面跟 SQL 文件路径。 重定向输出 - tee &#x2F; notee。可以将命令的输出重定向到指定的文件中。 切换数据库 - use。 显示警告信息 - warnings。 退出命令行 - quit或exit。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day31-35/31-35.玩转Linux操作系统","date":"2024-12-12T08:38:01.836Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day31-35/31-35.玩转Linux操作系统/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day31-35/31-35.%E7%8E%A9%E8%BD%ACLinux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"玩转Linux操作系统 说明：本文中对Linux命令的讲解都是基于名为CentOS的Linux发行版本，我自己使用的是阿里云服务器，系统版本为CentOS Linux release 7.6.1810。不同的Linux发行版本在Shell命令和工具程序上会有一些差别，但是这些差别是很小的。 操作系统发展史只有硬件没有软件的计算机系统被称之为“裸机”，我们很难用“裸机”来完成计算机日常的工作（如存储和运算），所以必须用特定的软件来控制硬件的工作。最靠近计算机硬件的软件是系统软件，其中最为重要的就是“操作系统”。“操作系统”是控制和管理整个计算机硬件和软件资源、实现资源分配和任务调配、为系统用户以及其他软件提供接口和环境的程序的集合。 没有操作系统（手工操作）在计算机诞生之初没有操作系统的年代，人们先把程序纸带（或卡片）装上计算机，然后启动输入机把程序送入计算机，接着通过控制台开关启动程序运行。当程序执行完毕，打印机输出计算的结果，用户卸下并取走纸带（或卡片）。第二个用户上机，重复同样的步骤。在整个过程中用户独占机器，CPU等待手工操作，资源利用率极低。 批处理系统首先启动计算机上的一个监督程序，在监督程序的控制下，计算机能够自动的、成批的处理一个或多个用户的作业。完成一批作业后，监督程度又从输入机读取作业存入磁带机。按照上面的步骤重复处理任务。监督程序不停的处理各个作业，实现了作业的自动转接，减少了作业的建立时间和手工操作时间，提高了计算机资源的利用率。 批处理系统又可以分为单道批处理系统、多道批处理系统、联机批处理系统、脱机批处理系统。 分时系统和实时系统分时系统是把处理器的运行时间分成很短的时间片，按时间片轮流把处理机分配给各联机作业使用。 若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时中断，把处理机让给另一作业使用，等待下一轮调度时再继续其运行。由于计算机速度很快，作业运行轮转得很快，给每个用户的感觉是他独占了一台计算机。而每个用户可以通过自己的终端向系统发出各种操作控制命令，在充分的人机交互情况下，完成作业的运行。为了解决分时系统不能及时响应用户指令的情况，又出现了能够在在严格的时间范围内完成事件处理，及时响应随机外部事件的实时系统。 通用操作系统 1960s：IBM的System&#x2F;360系列的机器有了统一的操作系统OS&#x2F;360。 1965年：AT&amp;T的贝尔实验室加入GE和MIT的合作计划开始开发MULTICS。 1969年：MULTICS项目失败，Ken Tompson赋闲在家，为了玩“Space Travel”游戏用汇编语言在当时已经被淘汰的PDP-7上开发了Unics。 注：很难想象，Unix这么伟大的操作系统，居然是一个赋闲在家的程序员（关键是老婆回娘家还带上了孩子）在一台被淘汰的设备上为了玩游戏开发出来的。 1970年~1971年：Ken Thompson和Dennis Ritchie用B语言在PDP-11上重写了Unics，并在Brian Kernighan的建议下将其更名为Unix。 1972年~1973年：Dennis Ritchie发明了C语言来取代可移植性较差的B语言，并开启了用C语言重写Unix的工作。 1974年：Unix推出了里程碑意义的第5版，几乎完全用C语言来实现。 1979年：从Unix第7版开始，AT&amp;T发布新的使用条款，将Unix私有化。 1987年：Andrew S. Tanenbaum教授为了能在课堂上为学生讲解操作系统运作的细节，决定在不使用任何AT&amp;T的源代码前提下，自行开发与Unix兼容的操作系统以避免版权上的争议，该系统被命名为Minix。 1991年：Linus Torvalds就读于芬兰赫尔辛基大学期间，尝试在Minix上做一些开发工作，但因为Minix只是作为教学用途的操作系统，功能并不强大，为了方便在学校的新闻组和邮件系统中读写和下载文件，Linus编写了磁盘驱动程序和文件系统，这些东西形成了Linux系统内核的雏形。 下图是Unix操作系统家族的图谱。 Linux概述Linux是一个通用操作系统。一个操作系统要负责任务调度、内存分配、处理外围设备I&#x2F;O等操作。操作系统通常由内核（运行其他程序，管理像磁盘、打印机等硬件设备的核心程序）和系统程序（设备驱动、底层库、shell、服务程序等）两部分组成。 Linux内核是芬兰人Linus Torvalds开发的，于1991年9月发布。而Linux操作系统作为Internet时代的产物，它是由全世界许多开发者共同合作开发的，是一个自由的操作系统（注意自由和免费并不是同一个概念，想了解二者的差别可以点击这里）。 Linux系统优点 通用操作系统，不跟特定的硬件绑定。 用C语言编写，可移植性强，有内核编程接口。 支持多用户和多任务，支持安全的分层文件系统。 大量的实用程序，完善的网络功能以及强大的支持文档。 可靠的安全性和良好的稳定性，对开发者更友好。 Linux系统发行版本 Redhat Ubuntu CentOS Fedora Debian openSUSE 基础命令Linux系统的命令通常都是如下所示的格式： 1命令名称 [命名参数] [命令对象] 获取登录信息 - w &#x2F; who &#x2F; last&#x2F; lastb。 1234567891011121314151617181920[root ~]# w 23:31:16 up 12:16, 2 users, load average: 0.00, 0.01, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 182.139.66.250 23:03 4.00s 0.02s 0.00s wjackfrue pts/1 182.139.66.250 23:26 3:56 0.00s 0.00s -bash[root ~]# whoroot pts/0 2018-04-12 23:03 (182.139.66.250)jackfrued pts/1 2018-04-12 23:26 (182.139.66.250)[root ~]# who am iroot pts/0 2018-04-12 23:03 (182.139.66.250)[root ~]# who mom likesroot pts/0 2018-04-12 23:03 (182.139.66.250)[root ~]# lastroot pts/0 117.136.63.184 Sun May 26 18:57 still logged in reboot system boot 3.10.0-957.10.1. Mon May 27 02:52 - 19:10 (-7:-42) root pts/4 117.136.63.184 Sun May 26 18:51 - crash (08:01) root pts/4 117.136.63.184 Sun May 26 18:49 - 18:49 (00:00) root pts/3 117.136.63.183 Sun May 26 18:35 - crash (08:17) root pts/2 117.136.63.183 Sun May 26 18:34 - crash (08:17) root pts/0 117.136.63.183 Sun May 26 18:10 - crash (08:42) 查看自己使用的Shell - ps。 Shell也被称为“壳”或“壳程序”，它是用户与操作系统内核交流的翻译官，简单的说就是人与计算机交互的界面和接口。目前很多Linux系统默认的Shell都是bash（Bourne Again SHell），因为它可以使用tab键进行命令和路径补全、可以保存历史命令、可以方便的配置环境变量以及执行批处理操作。 1234[root ~]# ps PID TTY TIME CMD 3531 pts/0 00:00:00 bash 3553 pts/0 00:00:00 ps 查看命令的说明和位置 - whatis &#x2F; which &#x2F; whereis。 123456789101112[root ~]# whatis psps (1) - report a snapshot of the current processes.[root ~]# whatis pythonpython (1) - an interpreted, interactive, object-oriented programming language[root ~]# whereis psps: /usr/bin/ps /usr/share/man/man1/ps.1.gz[root ~]# whereis pythonpython: /usr/bin/python /usr/bin/python2.7 /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz[root ~]# which ps/usr/bin/ps[root ~]# which python/usr/bin/python 清除屏幕上显示的内容 - clear。 查看帮助文档 - man &#x2F; info &#x2F; –help &#x2F; apropos。 123456789101112131415[root@izwz97tbgo9lkabnat2lo8z ~]# ps --helpUsage: ps [options] Try &#x27;ps --help &lt;simple|list|output|threads|misc|all&gt;&#x27; or &#x27;ps --help &lt;s|l|o|t|m|a&gt;&#x27; for additional help text.For more details see ps(1).[root@izwz97tbgo9lkabnat2lo8z ~]# man psPS(1) User Commands PS(1)NAME ps - report a snapshot of the current processes.SYNOPSIS ps [options]DESCRIPTION... 查看系统和主机名 - uname &#x2F; hostname。 123456[root@izwz97tbgo9lkabnat2lo8z ~]# unameLinux[root@izwz97tbgo9lkabnat2lo8z ~]# hostnameizwz97tbgo9lkabnat2lo8z[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat /etc/centos-releaseCentOS Linux release 7.6.1810 (Core) 说明：cat是连接文件内容并打印到标准输出的命令，后面会讲到该命令；/etc是Linux系统上的一个非常重要的目录，它保存了很多的配置文件；centos-release是该目录下的一个文件，因为我自己使用的Linux发行版本是CentOS 7.6，因此这里会有一个这样的文件。 时间和日期 - date &#x2F; cal。 123456789101112131415161718[root@iZwz97tbgo9lkabnat2lo8Z ~]# dateWed Jun 20 12:53:19 CST 2018[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal June 2018Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 910 11 12 13 14 15 1617 18 19 20 21 22 2324 25 26 27 28 29 30[root@iZwz97tbgo9lkabnat2lo8Z ~]# cal 5 2017 May 2017Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 17 18 19 2021 22 23 24 25 26 2728 29 30 31 重启和关机 - reboot &#x2F; shutdown。 123456789101112131415161718[root ~]# shutdown -h +5Shutdown scheduled for Sun 2019-05-26 19:34:27 CST, use &#x27;shutdown -c&#x27; to cancel.[root ~]# Broadcast message from root (Sun 2019-05-26 19:29:27 CST):The system is going down for power-off at Sun 2019-05-26 19:34:27 CST![root ~]# shutdown -cBroadcast message from root (Sun 2019-05-26 19:30:22 CST):The system shutdown has been cancelled at Sun 2019-05-26 19:31:22 CST![root ~]# shutdown -r 23:58Shutdown scheduled for Sun 2019-05-26 23:58:00 CST, use &#x27;shutdown -c&#x27; to cancel.[root ~]# shutdown -cBroadcast message from root (Sun 2019-05-26 19:31:06 CST):The system shutdown has been cancelled at Sun 2019-05-26 19:32:06 CST! 说明：在执行shutdown命令时会向登录系统的用户发出警告，可以在命令后面跟上警告消息来替换默认的警告消息，也可以在-h参数后通过now来表示立刻关机。 退出登录 - exit &#x2F; logout。 查看历史命令 - history。 1234567[root@iZwz97tbgo9lkabnat2lo8Z ~]# history...452 ls453 cd Python-3.6.5/454 clear455 history[root@iZwz97tbgo9lkabnat2lo8Z ~]# !454 说明：查看到历史命令之后，可以用!历史命令编号来重新执行该命令；通过history -c可以清除历史命令。 实用程序文件和文件夹操作 创建&#x2F;删除空目录 - mkdir &#x2F; rmdir。 123[root ~]# mkdir abc[root ~]# mkdir -p xyz/abc[root ~]# rmdir abc 创建&#x2F;删除文件 - touch &#x2F; rm。 12345[root ~]# touch readme.txt[root ~]# touch error.txt[root ~]# rm error.txtrm: remove regular empty file ‘error.txt’? y[root ~]# rm -rf xyz touch命令用于创建空白文件或修改文件时间。在Linux系统中一个文件有三种时间： 更改内容的时间 - mtime。 更改权限的时间 - ctime。 最后访问时间 - atime。 rm的几个重要参数： -i：交互式删除，每个删除项都会进行询问。 -r：删除目录并递归的删除目录中的文件和目录。 -f：强制删除，忽略不存在的文件，没有任何提示。 切换和查看当前工作目录 - cd &#x2F; pwd。 说明：cd命令后面可以跟相对路径（以当前路径作为参照）或绝对路径（以/开头）来切换到指定的目录，也可以用cd ..来返回上一级目录。请大家想一想，如果要返回到上上一级目录应该给cd命令加上什么样的参数呢？ 查看目录内容 - ls。 -l：以长格式查看文件和目录。 -a：显示以点开头的文件和目录（隐藏文件）。 -R：遇到目录要进行递归展开（继续列出目录下面的文件和目录）。 -d：只列出目录，不列出其他内容。 -S &#x2F; -t：按大小&#x2F;时间排序。 查看文件内容 - cat &#x2F; tac &#x2F; head &#x2F; tail &#x2F; more &#x2F; less &#x2F; rev &#x2F; od。 12345678910111213141516171819202122232425262728[root ~]# wget http://www.sohu.com/ -O sohu.html--2018-06-20 18:42:34-- http://www.sohu.com/Resolving www.sohu.com (www.sohu.com)... 14.18.240.6Connecting to www.sohu.com (www.sohu.com)|14.18.240.6|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 212527 (208K) [text/html]Saving to: ‘sohu.html’100%[==================================================&gt;] 212,527 --.-K/s in 0.03s2018-06-20 18:42:34 (7.48 MB/s) - ‘sohu.html’ saved [212527/212527][root ~]# cat sohu.html...[root ~]# head -10 sohu.html&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;搜狐&lt;/title&gt;&lt;meta name=&quot;Keywords&quot; content=&quot;搜狐,门户网站,新媒体,网络媒体,新闻,财经,体育,娱乐,时尚,汽车,房产,科技,图片,论坛,微博,博客,视频,电影,电视剧&quot;/&gt;&lt;meta name=&quot;Description&quot; content=&quot;搜狐网为用户提供24小时不间断的最新资讯，及搜索、邮件等网络服务。内容包括全球热点事件、突发新闻、时事评论、热播影视剧、体育赛事、行业动态、生活服务信息，以及论坛、博客、微博、我的搜狐等互动空间。&quot; /&gt;&lt;meta name=&quot;shenma-site-verification&quot; content=&quot;1237e4d02a3d8d73e96cbd97b699e9c3_1504254750&quot;&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge,chrome=1&quot;/&gt;[root ~]# tail -2 sohu.html&lt;/body&gt;&lt;/html&gt;[root ~]# less sohu.html...[root ~]# cat -n sohu.html | more... 说明：上面用到了一个名为wget的命令，它是一个网络下载器程序，可以从指定的URL下载资源。 拷贝&#x2F;移动文件 - cp &#x2F; mv。 12345678[root ~]# mkdir backup[root ~]# cp sohu.html backup/[root ~]# cd backup[root backup]# lssohu.html[root backup]# mv sohu.html sohu_index.html[root backup]# lssohu_index.html 文件重命名 - rename。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# rename .htm .html *.htm 查找文件和查找内容 - find &#x2F; grep。 1234567891011121314151617[root@iZwz97tbgo9lkabnat2lo8Z ~]# find / -name &quot;*.html&quot;/root/sohu.html/root/backup/sohu_index.html[root@izwz97tbgo9lkabnat2lo8z ~]# find . -atime 7 -type f -print[root@izwz97tbgo9lkabnat2lo8z ~]# find . -type f -size +2k[root@izwz97tbgo9lkabnat2lo8z ~]# find . -type f -name &quot;*.swp&quot; -delete[root@iZwz97tbgo9lkabnat2lo8Z ~]# grep &quot;&lt;script&gt;&quot; sohu.html -n20:&lt;script&gt;[root@iZwz97tbgo9lkabnat2lo8Z ~]# grep -E \\&lt;\\/?script.*\\&gt; sohu.html -n20:&lt;script&gt;22:&lt;/script&gt;24:&lt;script src=&quot;//statics.itc.cn/web/v3/static/js/es5-shim-08e41cfc3e.min.js&quot;&gt;&lt;/script&gt;25:&lt;script src=&quot;//statics.itc.cn/web/v3/static/js/es5-sham-1d5fa1124b.min.js&quot;&gt;&lt;/script&gt;26:&lt;script src=&quot;//statics.itc.cn/web/v3/static/js/html5shiv-21fc8c2ba6.js&quot;&gt;&lt;/script&gt;29:&lt;script type=&quot;text/javascript&quot;&gt;52:&lt;/script&gt;... 说明：grep在搜索字符串时可以使用正则表达式，如果需要使用正则表达式可以用grep -E或者直接使用egrep。 创建链接和查看链接 - ln &#x2F; readlink。 123456789101112131415[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 1 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln /root/sohu.html /root/backup/sohu_backup[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 2 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln /root/sohu.html /root/backup/sohu_backup2[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sohu.html-rw-r--r-- 3 root root 212131 Jun 20 19:15 sohu.html[root@iZwz97tbgo9lkabnat2lo8Z ~]# ln -s /etc/centos-release sysinfo[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls -l sysinfolrwxrwxrwx 1 root root 19 Jun 20 19:21 sysinfo -&gt; /etc/centos-release[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat sysinfoCentOS Linux release 7.4.1708 (Core)[root@iZwz97tbgo9lkabnat2lo8Z ~]# cat /etc/centos-releaseCentOS Linux release 7.4.1708 (Core) 说明：链接可以分为硬链接和软链接（符号链接）。硬链接可以认为是一个指向文件数据的指针，就像Python中对象的引用计数，每添加一个硬链接，文件的对应链接数就增加1，只有当文件的链接数为0时，文件所对应的存储空间才有可能被其他文件覆盖。我们平常删除文件时其实并没有删除硬盘上的数据，我们删除的只是一个指针，或者说是数据的一条使用记录，所以类似于“文件粉碎机”之类的软件在“粉碎”文件时除了删除文件指针，还会在文件对应的存储区域填入数据来保证文件无法再恢复。软链接类似于Windows系统下的快捷方式，当软链接链接的文件被删除时，软链接也就失效了。 压缩&#x2F;解压缩和归档&#x2F;解归档 - gzip &#x2F; gunzip &#x2F; xz。 1234567891011121314[root@iZwz97tbgo9lkabnat2lo8Z ~]# wget http://download.redis.io/releases/redis-4.0.10.tar.gz--2018-06-20 19:29:59-- http://download.redis.io/releases/redis-4.0.10.tar.gzResolving download.redis.io (download.redis.io)... 109.74.203.151Connecting to download.redis.io (download.redis.io)|109.74.203.151|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 1738465 (1.7M) [application/x-gzip]Saving to: ‘redis-4.0.10.tar.gz’100%[==================================================&gt;] 1,738,465 70.1KB/s in 74s2018-06-20 19:31:14 (22.9 KB/s) - ‘redis-4.0.10.tar.gz’ saved [1738465/1738465][root@iZwz97tbgo9lkabnat2lo8Z ~]# ls redis*redis-4.0.10.tar.gz[root@iZwz97tbgo9lkabnat2lo8Z ~]# gunzip redis-4.0.10.tar.gz[root@iZwz97tbgo9lkabnat2lo8Z ~]# ls redis*redis-4.0.10.tar 归档和解归档 - tar。 123456789101112131415[root@iZwz97tbgo9lkabnat2lo8Z ~]# tar -xvf redis-4.0.10.tarredis-4.0.10/redis-4.0.10/.gitignoreredis-4.0.10/00-RELEASENOTESredis-4.0.10/BUGSredis-4.0.10/CONTRIBUTINGredis-4.0.10/COPYINGredis-4.0.10/INSTALLredis-4.0.10/MANIFESTOredis-4.0.10/Makefileredis-4.0.10/README.mdredis-4.0.10/deps/redis-4.0.10/deps/Makefileredis-4.0.10/deps/README.md... 说明：归档（也称为创建归档）和解归档都使用tar命令，通常创建归档需要-cvf三个参数，其中c表示创建（create），v表示显示创建归档详情（verbose），f表示指定归档的文件（file）；解归档需要加上-xvf参数，其中x表示抽取（extract），其他两个参数跟创建归档相同。 将标准输入转成命令行参数 - xargs。 下面的命令会将查找当前路径下的html文件，然后通过xargs将这些文件作为参数传给rm命令，实现查找并删除文件的操作。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# find . -type f -name &quot;*.html&quot; | xargs rm -f 下面的命令将a.txt文件中的多行内容变成一行输出到b.txt文件中，其中&lt;表示从a.txt中读取输入，&gt;表示将命令的执行结果输出到b.txt中。 1[root@iZwz97tbgo9lkabnat2lo8Z ~]# xargs &lt; a.txt &gt; b.txt 说明：这个命令就像上面演示的那样常在管道（实现进程间通信的一种方式）和重定向（重新指定输入输出的位置）操作中用到，后面的内容中会讲到管道操作和输入输出重定向操作。 显示文件或目录 - basename &#x2F; dirname。 其他相关工具。 sort - 对内容排序 uniq - 去掉相邻重复内容 tr - 替换指定内容为新内容 cut &#x2F; paste - 剪切&#x2F;黏贴内容 split - 拆分文件 file - 判断文件类型 wc - 统计文件行数、单词数、字节数 iconv - 编码转换 1234567891011121314151617181920212223242526272829303132333435[root ~]# cat foo.txtgrapeapplepitaya[root ~]# cat bar.txt100200300400[root ~]# paste foo.txt bar.txtgrape 100apple 200pitaya 300 400[root ~]# paste foo.txt bar.txt &gt; hello.txt[root ~]# cut -b 4-8 hello.txtpe 10le 20aya 30[root ~]# cat hello.txt | tr &#x27;\\t&#x27; &#x27;,&#x27;grape,100apple,200pitaya,300,400[root ~]# split -l 100 sohu.html hello[root ~]# wget https://www.baidu.com/img/bd_logo1.png[root ~]# file bd_logo1.pngbd_logo1.png: PNG image data, 540 x 258, 8-bit colormap, non-interlaced[root ~]# wc sohu.html 2979 6355 212527 sohu.html[root ~]# wc -l sohu.html2979 sohu.html[root ~]# wget http://www.qq.com -O qq.html[root ~]# iconv -f gb2312 -t utf-8 qq.html 管道和重定向 管道的使用 - |。 例子：查找当前目录下文件个数。 12[root ~]# find ./ | wc -l6152 例子：列出当前路径下的文件和文件夹，给每一项加一个编号。 123456[root ~]# ls | cat -n 1 dump.rdb 2 mongodb-3.6.5 3 Python-3.6.5 4 redis-3.2.11 5 redis.conf 例子：查找record.log中包含AAA，但不包含BBB的记录的总数 1[root ~]# cat record.log | grep AAA | grep -v BBB | wc -l 输出重定向和错误重定向 - &gt; &#x2F; &gt;&gt; &#x2F; 2&gt;。 1234567891011121314151617[root ~]# cat readme.txtbananaapplegrapeapplegrapewatermelonpearpitaya[root ~]# cat readme.txt | sort | uniq &gt; result.txt[root ~]# cat result.txtapplebananagrapepearpitayawatermelon 输入重定向 - &lt;。 1234567891011[root ~]# echo &#x27;hello, world!&#x27; &gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:05 2018):hello, world![root ~]# echo &#x27;I will show you some code.&#x27; &gt;&gt; hello.txt[root ~]# wall &lt; hello.txt[root ~]#Broadcast message from root (Wed Jun 20 19:43:55 2018):hello, world!I will show you some code. 多重定向 - tee。 下面的命令除了在终端显示命令ls的结果之外，还会追加输出到ls.txt文件中。 1[root ~]# ls | tee -a ls.txt 别名 alias 1234567[root ~]# alias ll=&#x27;ls -l&#x27;[root ~]# alias frm=&#x27;rm -rf&#x27;[root ~]# ll...drwxr-xr-x 2 root root 4096 Jun 20 12:52 abc...[root ~]# frm abc unalias 123[root ~]# unalias frm[root ~]# frm sohu.html-bash: frm: command not found 文本处理 字符流编辑器 - sed。 sed是操作、过滤和转换文本内容的工具。假设有一个名为fruit.txt的文件，内容如下所示。 123456[root ~]# cat -n fruit.txt 1 banana 2 grape 3 apple 4 watermelon 5 orange 接下来，我们在第2行后面添加一个pitaya。 1234567[root ~]# sed &#x27;2a pitaya&#x27; fruit.txt bananagrapepitayaapplewatermelonorange 注意：刚才的命令和之前我们讲过的很多命令一样并没有改变fruit.txt文件，而是将添加了新行的内容输出到终端中，如果想保存到fruit.txt中，可以使用输出重定向操作。 在第2行前面插入一个waxberry。 1234567[root ~]# sed &#x27;2i waxberry&#x27; fruit.txtbananawaxberrygrapeapplewatermelonorange 删除第3行。 12345[root ~]# sed &#x27;3d&#x27; fruit.txtbananagrapewatermelonorange 删除第2行到第4行。 123[root ~]# sed &#x27;2,4d&#x27; fruit.txtbananaorange 将文本中的字符a替换为@。 123456[root ~]# sed &#x27;s#a#@#&#x27; fruit.txt b@nanagr@pe@pplew@termelonor@nge 将文本中的字符a替换为@，使用全局模式。 123456[root ~]# sed &#x27;s#a#@#g&#x27; fruit.txt b@n@n@gr@pe@pplew@termelonor@nge 模式匹配和处理语言 - awk。 awk是一种编程语言，也是Linux系统中处理文本最为强大的工具，它的作者之一和现在的维护者就是之前提到过的Brian Kernighan（ken和dmr最亲密的伙伴）。通过该命令可以从文本中提取出指定的列、用正则表达式从文本中取出我们想要的内容、显示指定的行以及进行统计和运算，总之它非常强大。 假设有一个名为fruit2.txt的文件，内容如下所示。 123456[root ~]# cat fruit2.txt 1 banana 1202 grape 5003 apple 12304 watermelon 805 orange 400 显示文件的第3行。 12[root ~]# awk &#x27;NR==3&#x27; fruit2.txt 3 apple 1230 显示文件的第2列。 123456[root ~]# awk &#x27;&#123;print $2&#125;&#x27; fruit2.txt bananagrapeapplewatermelonorange 显示文件的最后一列。 123456[root ~]# awk &#x27;&#123;print $NF&#125;&#x27; fruit2.txt 120500123080400 输出末尾数字大于等于300的行。 1234[root ~]# awk &#x27;&#123;if($3 &gt;= 300) &#123;print $0&#125;&#125;&#x27; fruit2.txt 2 grape 5003 apple 12305 orange 400 上面展示的只是awk命令的冰山一角，更多的内容留给读者自己在实践中去探索。 用户管理 创建和删除用户 - useradd &#x2F; userdel。 12[root home]# useradd hellokitty[root home]# userdel hellokitty -d - 创建用户时为用户指定用户主目录 -g - 创建用户时指定用户所属的用户组 创建和删除用户组 - groupadd &#x2F; groupdel。 说明：用户组主要是为了方便对一个组里面所有用户的管理。 修改密码 - passwd。 1234[root ~]# passwd hellokittyNew password: Retype new password: passwd: all authentication tokens updated successfully. 说明：输入密码和确认密码没有回显且必须一气呵成的输入完成（不能使用退格键），密码和确认密码需要一致。如果使用passwd命令时没有指定命令作用的对象，则表示要修改当前用户的密码。如果想批量修改用户密码，可以使用chpasswd命令。 -l &#x2F; -u - 锁定&#x2F;解锁用户。 -d - 清除用户密码。 -e - 设置密码立即过期，用户登录时会强制要求修改密码。 -i - 设置密码过期多少天以后禁用该用户。 查看和修改密码有效期 - chage。 设置hellokitty用户100天后必须修改密码，过期前15天通知该用户，过期后7天禁用该用户。 1chage -M 100 -W 15 -I 7 hellokitty 切换用户 - su。 12[root ~]# su hellokitty[hellokitty root]$ 以管理员身份执行命令 - sudo。 1234[hellokitty ~]$ ls /rootls: cannot open directory /root: Permission denied[hellokitty ~]$ sudo ls /root[sudo] password for hellokitty: 说明：如果希望用户能够以管理员身份执行命令，用户必须要出现在sudoers名单中，sudoers文件在 /etc目录下，如果希望直接编辑该文件也可以使用下面的命令。 编辑sudoers文件 - visudo。 这里使用的编辑器是vi，关于vi的知识在后面有讲解。该文件的部分内容如下所示： 123456789101112131415161718## Allow root to run any commands anywhere root ALL=(ALL) ALL## Allows members of the &#x27;sys&#x27; group to run networking, software, ## service management apps and more.# %sys ALL = NETWORKING, SOFTWARE, SERVICES, STORAGE, DELEGATING, PROCESSES, LOCATE, DRIVERS## Allows people in group wheel to run all commands%wheel ALL=(ALL) ALL## Same thing without a password# %wheel ALL=(ALL) NOPASSWD: ALL## Allows members of the users group to mount and unmount the## cdrom as root# %users ALL=/sbin/mount /mnt/cdrom, /sbin/umount /mnt/cdrom## Allows members of the users group to shutdown this system# %users localhost=/sbin/shutdown -h now 显示用户与用户组的信息 - id。 给其他用户发消息 -write &#x2F; wall。 发送方： 123[root ~]# write hellokittyDinner is on me.Call me at 6pm. 接收方： 12345[hellokitty ~]$ Message from root on pts/0 at 17:41 ...Dinner is on me.Call me at 6pm.EOF 查看&#x2F;设置是否接收其他用户发送的消息 - mesg。 12345[hellokitty ~]$ mesgis y[hellokitty ~]$ mesg n[hellokitty ~]$ mesgis n 文件系统文件和路径 命名规则：文件名的最大长度与文件系统类型有关，一般情况下，文件名不应该超过255个字符，虽然绝大多数的字符都可以用于文件名，但是最好使用英文大小写字母、数字、下划线、点这样的符号。文件名中虽然可以使用空格，但应该尽可能避免使用空格，否则在输入文件名时需要用将文件名放在双引号中或者通过\\对空格进行转义。 扩展名：在Linux系统下文件的扩展名是可选的，但是使用扩展名有助于对文件内容的理解。有些应用程序要通过扩展名来识别文件，但是更多的应用程序并不依赖文件的扩展名，就像file命令在识别文件时并不是依据扩展名来判定文件的类型。 隐藏文件：以点开头的文件在Linux系统中是隐藏文件（不可见文件）。 目录结构 &#x2F;bin - 基本命令的二进制文件。 &#x2F;boot - 引导加载程序的静态文件。 &#x2F;dev - 设备文件。 &#x2F;etc - 配置文件。 &#x2F;home - 普通用户主目录的父目录。 &#x2F;lib - 共享库文件。 &#x2F;lib64 - 共享64位库文件。 &#x2F;lost+found - 存放未链接文件。 &#x2F;media - 自动识别设备的挂载目录。 &#x2F;mnt - 临时挂载文件系统的挂载点。 &#x2F;opt - 可选插件软件包安装位置。 &#x2F;proc - 内核和进程信息。 &#x2F;root - 超级管理员用户主目录。 &#x2F;run - 存放系统运行时需要的东西。 &#x2F;sbin - 超级用户的二进制文件。 &#x2F;sys - 设备的伪文件系统。 &#x2F;tmp - 临时文件夹。 &#x2F;usr - 用户应用目录。 &#x2F;var - 变量数据目录。 访问权限 chmod - 改变文件模式比特。 1234567891011121314[root ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html...[root ~]# chmod g+w,o+w sohu.html[root ~]# ls -l...-rw-rw-rw- 1 root root 211878 Jun 19 16:06 sohu.html...[root ~]# chmod 644 sohu.html[root ~]# ls -l...-rw-r--r-- 1 root root 211878 Jun 19 16:06 sohu.html... 说明：通过上面的例子可以看出，用chmod改变文件模式比特有两种方式：一种是字符设定法，另一种是数字设定法。除了chmod之外，可以通过umask来设定哪些权限将在新文件的默认权限中被删除。 长格式查看目录或文件时显示结果及其对应权限的数值如下表所示。 chown - 改变文件所有者。 123456789[root ~]# ls -l...-rw-r--r-- 1 root root 54 Jun 20 10:06 readme.txt...[root ~]# chown hellokitty readme.txt[root ~]# ls -l...-rw-r--r-- 1 hellokitty root 54 Jun 20 10:06 readme.txt... chgrp - 改变用户组。 磁盘管理 列出文件系统的磁盘使用状况 - df。 12345678[root ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/vda1 40G 5.0G 33G 14% /devtmpfs 486M 0 486M 0% /devtmpfs 497M 0 497M 0% /dev/shmtmpfs 497M 356K 496M 1% /runtmpfs 497M 0 497M 0% /sys/fs/cgrouptmpfs 100M 0 100M 0% /run/user/0 磁盘分区表操作 - fdisk。 12345678910111213[root ~]# fdisk -lDisk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a42f4 Device Boot Start End Blocks Id System/dev/vda1 * 2048 83884031 41940992 83 LinuxDisk /dev/vdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 磁盘分区工具 - parted。 格式化文件系统 - mkfs。 1[root ~]# mkfs -t ext4 -v /dev/sdb -t - 指定文件系统的类型。 -c - 创建文件系统时检查磁盘损坏情况。 -v - 显示详细信息。 文件系统检查 - fsck。 转换或拷贝文件 - dd。 挂载&#x2F;卸载 - mount &#x2F; umount。 创建&#x2F;激活&#x2F;关闭交换分区 - mkswap &#x2F; swapon &#x2F; swapoff。 说明：执行上面这些命令会带有一定的风险，如果不清楚这些命令的用法，最好不用随意使用，在使用的过程中，最好对照参考资料进行操作，并在操作前确认是否要这么做。 编辑器 - vim 启动vim。可以通过vi或vim命令来启动vim，启动时可以指定文件名来打开一个文件，如果没有指定文件名，也可以在保存的时候指定文件名。 1[root ~]# vim guess.py 命令模式、编辑模式和末行模式：启动vim进入的是命令模式（也称为Normal模式），在命令模式下输入英文字母i会进入编辑模式（Insert模式），屏幕下方出现-- INSERT --提示；在编辑模式下按下Esc会回到命令模式，此时如果输入英文:会进入末行模式，在末行模式下输入q!可以在不保存当前工作的情况下强行退出vim；在命令模式下输入v会进入可视模式（Visual模式），可以用光标选择一个区域再完成对应的操作。 保存和退出vim：在命令模式下输入: 进入末行模式，输入wq可以实现保存退出；如果想放弃编辑的内容输入q!强行退出，这一点刚才已经提到过了；在命令模式下也可以直接输入ZZ实现保存退出。如果只想保存文件不退出，那么可以在末行模式下输入w；可以在w后面输入空格再指定要保存的文件名。 光标操作。 在命令模式下可以通过h、j、k、l来控制光标向左、下、上、右的方向移动，可以在字母前输入数字来表示移动的距离，例如：10h表示向左移动10个字符。 在命令模式下可以通过Ctrl+y和Ctrl+e来实现向上、向下滚动一行文本的操作，可以通过Ctrl+f和Ctrl+b来实现向前和向后翻页的操作。 在命令模式下可以通过输入英文字母G将光标移到文件的末尾，可以通过gg将光标移到文件的开始，也可以通过在G前输入数字来将光标移动到指定的行。 文本操作。 删除：在命令模式下可以用dd来删除整行；可以在dd前加数字来指定删除的行数；可以用d$来实现删除从光标处删到行尾的操作，也可以通过d0来实现从光标处删到行首的操作；如果想删除一个单词，可以使用dw；如果要删除全文，可以在输入:%d（其中:用来从命令模式进入末行模式）。 复制和粘贴：在命令模式下可以用yy来复制整行；可以在yy前加数字来指定复制的行数；可以通过p将复制的内容粘贴到光标所在的地方。 撤销和恢复：在命令模式下输入u可以撤销之前的操作；通过Ctrl+r可以恢复被撤销的操作。 对内容进行排序：在命令模式下输入%!sort。 查找和替换。 查找操作需要输入/进入末行模式并提供正则表达式来匹配与之对应的内容，例如：/doc.*\\.，输入n来向前搜索，也可以输入N来向后搜索。 替换操作需要输入:进入末行模式并指定搜索的范围、正则表达式以及替换后的内容和匹配选项，例如：:1,$s/doc.*/hello/gice，其中： g - global：全局匹配。 i - ignore case：忽略大小写匹配。 c - confirm：替换时需要确认。 e - error：忽略错误。 参数设定：在输入:进入末行模式后可以对vim进行设定。 设置Tab键的空格数：set ts=4 设置显示&#x2F;不显示行号：set nu &#x2F; set nonu 设置启用&#x2F;关闭高亮语法：syntax on &#x2F; syntax off 设置显示标尺（光标所在的行和列）： set ruler 设置启用&#x2F;关闭搜索结果高亮：set hls &#x2F; set nohls 说明：如果希望上面的这些设定在每次启动vim时都能自动生效，需要将这些设定写到用户主目录下的.vimrc文件中。 高级技巧 比较多个文件。 1[root ~]# vim -d foo.txt bar.txt 打开多个文件。 1[root ~]# vim foo.txt bar.txt hello.txt 启动vim后只有一个窗口显示的是foo.txt，可以在末行模式中输入ls查看到打开的三个文件，也可以在末行模式中输入b &lt;num&gt;来显示另一个文件，例如可以用:b 2将bar.txt显示出来，可以用:b 3将hello.txt显示出来。 拆分和切换窗口。 可以在末行模式中输入sp或vs来实现对窗口的水平或垂直拆分，这样我们就可以同时打开多个编辑窗口，通过按两次Ctrl+w就可以实现编辑窗口的切换，在一个窗口中执行退出操作只会关闭对应的窗口，其他的窗口继续保留。 映射快捷键：在vim下可以将一些常用操作映射为快捷键来提升工作效率。 例子1：在命令模式下输入F4执行从第一行开始删除10000行代码的操作。 :map &lt;F4&gt; gg10000dd。 例子2：在编辑模式下输入__main直接补全为if __name__ == &#39;__main__&#39;:。 :inoremap __main if __name__ == &#39;__main__&#39;: 说明：上面例子2的inoremap中的i表示映射的键在编辑模式使用， nore表示不要递归，这一点非常重要，否则如果键对应的内容中又出现键本身，就会引发递归（相当于进入了死循环）。如果希望映射的快捷键每次启动vim时都能生效，需要将映射写到用户主目录下的.vimrc文件中。 录制宏。 在命令模式下输入qa开始录制宏（其中a是寄存器的名字，也可以是其他英文字母或0-9的数字）。 执行你的操作（光标操作、编辑操作等），这些操作都会被录制下来。 如果录制的操作已经完成了，按q结束录制。 通过@a（a是刚才使用的寄存器的名字）播放宏，如果要多次执行宏可以在前面加数字，例如100@a表示将宏播放100次。 可以试一试下面的例子来体验录制宏的操作，该例子来源于Harttle Land网站，该网站上提供了很多关于vim的使用技巧，有兴趣的可以了解一下。 软件安装和配置使用包管理工具 yum - Yellowdog Updater Modified。 yum search：搜索软件包，例如yum search nginx。 yum list installed：列出已经安装的软件包，例如yum list installed | grep zlib。 yum install：安装软件包，例如yum install nginx。 yum remove：删除软件包，例如yum remove nginx。 yum update：更新软件包，例如yum update可以更新所有软件包，而yum update tar只会更新tar。 yum check-update：检查有哪些可以更新的软件包。 yum info：显示软件包的相关信息，例如yum info nginx。 rpm - Redhat Package Manager。 安装软件包：rpm -ivh &lt;packagename&gt;.rpm。 移除软件包：rpm -e &lt;packagename&gt;。 查询软件包：rpm -qa，例如可以用rpm -qa | grep mysql来检查是否安装了MySQL相关的软件包。 下面以Nginx为例，演示如何使用yum安装软件。 123456789101112131415161718192021222324252627282930313233[root ~]# yum -y install nginx...Installed: nginx.x86_64 1:1.12.2-2.el7Dependency Installed: nginx-all-modules.noarch 1:1.12.2-2.el7 nginx-mod-http-geoip.x86_64 1:1.12.2-2.el7 nginx-mod-http-image-filter.x86_64 1:1.12.2-2.el7 nginx-mod-http-perl.x86_64 1:1.12.2-2.el7 nginx-mod-http-xslt-filter.x86_64 1:1.12.2-2.el7 nginx-mod-mail.x86_64 1:1.12.2-2.el7 nginx-mod-stream.x86_64 1:1.12.2-2.el7Complete![root ~]# yum info nginxLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfileInstalled PackagesName : nginxArch : x86_64Epoch : 1Version : 1.12.2Release : 2.el7Size : 1.5 MRepo : installedFrom repo : epelSummary : A high performance web server and reverse proxy serverURL : http://nginx.org/License : BSDDescription : Nginx is a web server and a reverse proxy server for HTTP, SMTP, POP3 and : IMAP protocols, with a strong focus on high concurrency, performance and low : memory usage.[root ~]# nginx -vnginx version: nginx/1.12.2 移除Nginx。 1[root ~]# yum -y remove nginx 下面以MySQL为例，演示如何使用rpm安装软件。要安装MySQL需要先到MySQL官方网站下载对应的RPM文件，当然要选择和你使用的Linux系统对应的版本。MySQL现在是Oracle公司旗下的产品，在MySQL被收购后，MySQL的作者重新制作了一个MySQL的分支MariaDB，可以通过yum进行安装。 123456789101112131415[root mysql]# lsmysql-community-client-5.7.22-1.el7.x86_64.rpmmysql-community-common-5.7.22-1.el7.x86_64.rpmmysql-community-libs-5.7.22-1.el7.x86_64.rpmmysql-community-server-5.7.22-1.el7.x86_64.rpm[root mysql]# yum -y remove mariadb-libs[root mysql]# yum -y install libaio[root mysql]#rpm -ivh mysql-community-common-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-libs-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-client-5.7.26-1.el7.x86_64.rpm...[root mysql]#rpm -ivh mysql-community-server-5.7.26-1.el7.x86_64.rpm... 说明：由于MySQL和MariaDB的底层依赖库是有冲突的，所以上面我们首先用yum移除了名为mariadb-libs的依赖库并安装了名为libaio支持异步I&#x2F;O操作的依赖库。关于MySQL和MariaDB之间的关系，可以阅读维基百科上关于MariaDB的介绍。 移除安装的MySQL。 1[root ~]# rpm -qa | grep mysql | xargs rpm -e 下载解压配置环境变量下面以安装MongoDB为例，演示这类软件应该如何安装。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root ~]# wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.5.tgz--2018-06-21 18:32:53-- https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.5.tgzResolving fastdl.mongodb.org (fastdl.mongodb.org)... 52.85.83.16, 52.85.83.228, 52.85.83.186, ...Connecting to fastdl.mongodb.org (fastdl.mongodb.org)|52.85.83.16|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 100564462 (96M) [application/x-gzip]Saving to: ‘mongodb-linux-x86_64-rhel70-3.6.5.tgz’100%[==================================================&gt;] 100,564,462 630KB/s in 2m 9s2018-06-21 18:35:04 (760 KB/s) - ‘mongodb-linux-x86_64-rhel70-3.6.5.tgz’ saved [100564462/100564462][root ~]# gunzip mongodb-linux-x86_64-rhel70-3.6.5.tgz[root ~]# tar -xvf mongodb-linux-x86_64-rhel70-3.6.5.tarmongodb-linux-x86_64-rhel70-3.6.5/READMEmongodb-linux-x86_64-rhel70-3.6.5/THIRD-PARTY-NOTICESmongodb-linux-x86_64-rhel70-3.6.5/MPL-2mongodb-linux-x86_64-rhel70-3.6.5/GNU-AGPL-3.0mongodb-linux-x86_64-rhel70-3.6.5/bin/mongodumpmongodb-linux-x86_64-rhel70-3.6.5/bin/mongorestoremongodb-linux-x86_64-rhel70-3.6.5/bin/mongoexportmongodb-linux-x86_64-rhel70-3.6.5/bin/mongoimportmongodb-linux-x86_64-rhel70-3.6.5/bin/mongostatmongodb-linux-x86_64-rhel70-3.6.5/bin/mongotopmongodb-linux-x86_64-rhel70-3.6.5/bin/bsondumpmongodb-linux-x86_64-rhel70-3.6.5/bin/mongofilesmongodb-linux-x86_64-rhel70-3.6.5/bin/mongoreplaymongodb-linux-x86_64-rhel70-3.6.5/bin/mongoperfmongodb-linux-x86_64-rhel70-3.6.5/bin/mongodmongodb-linux-x86_64-rhel70-3.6.5/bin/mongosmongodb-linux-x86_64-rhel70-3.6.5/bin/mongomongodb-linux-x86_64-rhel70-3.6.5/bin/install_compass[root ~]# vim .bash_profile...PATH=$PATH:$HOME/bin:$HOME/mongodb-linux-x86_64-rhel70-3.6.5/binexport PATH...[root ~]# source .bash_profile[root ~]# mongod --versiondb version v3.6.5git version: a20ecd3e3a174162052ff99913bc2ca9a839d618OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment: distmod: rhel70 distarch: x86_64 target_arch: x86_64[root ~]# mongo --versionMongoDB shell version v3.6.5git version: a20ecd3e3a174162052ff99913bc2ca9a839d618OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment: distmod: rhel70 distarch: x86_64 target_arch: x86_64 说明：当然也可以通过yum来安装MongoDB，具体可以参照官方网站上给出的说明。 源代码构建安装 安装Python 3.6。 1234567891011121314[root ~]# yum install gcc[root ~]# wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz[root ~]# gunzip Python-3.6.5.tgz[root ~]# tar -xvf Python-3.6.5.tar[root ~]# cd Python-3.6.5[root ~]# ./configure --prefix=/usr/local/python36 --enable-optimizations[root ~]# yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel[root ~]# make &amp;&amp; make install...[root ~]# ln -s /usr/local/python36/bin/python3.6 /usr/bin/python3[root ~]# python3 --versionPython 3.6.5[root ~]# python3 -m pip install -U pip[root ~]# pip3 --version 说明：上面在安装好Python之后还需要注册PATH环境变量，将Python安装路径下bin文件夹的绝对路径注册到PATH环境变量中。注册环境变量可以修改用户主目录下的.bash_profile或者&#x2F;etc目录下的profile文件，二者的区别在于前者相当于是用户环境变量，而后者相当于是系统环境变量。 安装Redis-3.2.12。 123456789[root ~]# wget http://download.redis.io/releases/redis-3.2.12.tar.gz[root ~]# gunzip redis-3.2.12.tar.gz[root ~]# tar -xvf redis-3.2.12.tar[root ~]# cd redis-3.2.12[root ~]# make &amp;&amp; make install[root ~]# redis-server --versionRedis server v=3.2.12 sha=00000000:0 malloc=jemalloc-4.0.3 bits=64 build=5bc5cd3c03d6ceb6[root ~]# redis-cli --versionredis-cli 3.2.12 配置服务我们可以Linux系统下安装和配置各种服务，也就是说我们可以把Linux系统打造成数据库服务器、Web服务器、缓存服务器、文件服务器、消息队列服务器等等。Linux下的大多数服务都被设置为守护进程（驻留在系统后台运行，但不会因为服务还在运行而导致Linux无法停止运行），所以我们安装的服务通常名字后面都有一个字母d，它是英文单词daemon的缩写，例如：防火墙服务叫firewalld，我们之前安装的MySQL服务叫mysqld，Apache服务器叫httpd等。在安装好服务之后，可以使用systemctl命令或service命令来完成对服务的启动、停止等操作，具体操作如下所示。 启动防火墙服务。 1[root ~]# systemctl start firewalld 终止防火墙服务。 1[root ~]# systemctl stop firewalld 重启防火墙服务。 1[root ~]# systemctl restart firewalld 查看防火墙服务状态。 1[root ~]# systemctl status firewalld 设置&#x2F;禁用防火墙服务开机自启。 123456[root ~]# systemctl enable firewalldCreated symlink from /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service to /usr/lib/systemd/system/firewalld.service.Created symlink from /etc/systemd/system/multi-user.target.wants/firewalld.service to /usr/lib/systemd/system/firewalld.service.[root ~]# systemctl disable firewalldRemoved symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. 计划任务 在指定的时间执行命令。 at - 将任务排队，在指定的时间执行。 atq - 查看待执行的任务队列。 atrm - 从队列中删除待执行的任务。 指定3天以后下午5点要执行的任务。 1234[root ~]# at 5pm+3daysat&gt; rm -f /root/*.htmlat&gt; &lt;EOT&gt;job 9 at Wed Jun 5 17:00:00 2019 查看待执行的任务队列。 12[root ~]# atq9 Wed Jun 5 17:00:00 2019 a root 从队列中删除指定的任务。 1[root ~]$ atrm 9 计划任务表 - crontab。 123[root ~]# crontab -e* * * * * echo &quot;hello, world!&quot; &gt;&gt; /root/hello.txt59 23 * * * rm -f /root/*.log 说明：输入crontab -e命令会打开vim来编辑Cron表达式并指定触发的任务，上面我们定制了两个计划任务，一个是每分钟向&#x2F;root目录下的hello.txt中追加输出hello, world!；另一个是每天23时59分执行删除&#x2F;root目录下以log为后缀名的文件。如果不知道Cron表达式如何书写，可以参照&#x2F;etc&#x2F;crontab文件中的提示（下面会讲到）或者用搜索引擎找一下“Cron表达式在线生成器”来生成Cron表达式。 和crontab相关的文件在/etc目录下，通过修改/etc目录下的crontab文件也能够定制计划任务。 12345678910111213141516171819202122232425[root ~]# cd /etc[root etc]# ls -l | grep cron-rw-------. 1 root root 541 Aug 3 2017 anacrontabdrwxr-xr-x. 2 root root 4096 Mar 27 11:56 cron.ddrwxr-xr-x. 2 root root 4096 Mar 27 11:51 cron.daily-rw-------. 1 root root 0 Aug 3 2017 cron.denydrwxr-xr-x. 2 root root 4096 Mar 27 11:50 cron.hourlydrwxr-xr-x. 2 root root 4096 Jun 10 2014 cron.monthly-rw-r--r-- 1 root root 493 Jun 23 15:09 crontabdrwxr-xr-x. 2 root root 4096 Jun 10 2014 cron.weekly[root etc]# vim crontab 1 SHELL=/bin/bash 2 PATH=/sbin:/bin:/usr/sbin:/usr/bin 3 MAILTO=root 4 5 # For details see man 4 crontabs 6 7 # Example of job definition: 8 # .---------------- minute (0 - 59) 9 # | .------------- hour (0 - 23) 10 # | | .---------- day of month (1 - 31) 11 # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... 12 # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat 13 # | | | | | 14 # * * * * * user-name command to be executed 网络访问和管理 安全远程连接 - ssh。 1234567[root ~]$ ssh root@120.77.222.217The authenticity of host &#x27;120.77.222.217 (120.77.222.217)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:BhUhykv+FvnIL03I9cLRpWpaCxI91m9n7zBWrcXRa8w.ECDSA key fingerprint is MD5:cc:85:e9:f0:d7:07:1a:26:41:92:77:6b:7f:a0:92:65.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &#x27;120.77.222.217&#x27; (ECDSA) to the list of known hosts.root@120.77.222.217&#x27;s password: 通过网络获取资源 - wget。 -b 后台下载模式 -O 下载到指定的目录 -r 递归下载 发送和接收邮件 - mail。 网络配置工具（旧） - ifconfig。 12345678[root ~]# ifconfig eth0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.18.61.250 netmask 255.255.240.0 broadcast 172.18.63.255 ether 00:16:3e:02:b6:46 txqueuelen 1000 (Ethernet) RX packets 1067841 bytes 1296732947 (1.2 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 409912 bytes 43569163 (41.5 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 网络配置工具（新） - ip。 123456789[root ~]# ip address1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:16:3e:02:b6:46 brd ff:ff:ff:ff:ff:ff inet 172.18.61.250/20 brd 172.18.63.255 scope global eth0 valid_lft forever preferred_lft forever 网络可达性检查 - ping。 12345678[root ~]# ping www.baidu.com -c 3PING www.a.shifen.com (220.181.111.188) 56(84) bytes of data.64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=1 ttl=51 time=36.3 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=2 ttl=51 time=36.4 ms64 bytes from 220.181.111.188 (220.181.111.188): icmp_seq=3 ttl=51 time=36.4 ms--- www.a.shifen.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2002msrtt min/avg/max/mdev = 36.392/36.406/36.427/0.156 ms 显示或管理路由表 - route。 查看网络服务和端口 - netstat &#x2F; ss。 1[root ~]# netstat -nap | grep nginx 网络监听抓包 - tcpdump。 安全文件拷贝 - scp。 1[root ~]# scp root@1.2.3.4:/root/guido.jpg hellokitty@4.3.2.1:/home/hellokitty/pic.jpg 文件同步工具 - rsync。 说明：使用rsync可以实现文件的自动同步，这个对于文件服务器来说相当重要。关于这个命令的用法，我们在后面讲项目部署的时候为大家详细说明。 安全文件传输 - sftp。 1234[root ~]# sftp root@1.2.3.4root@1.2.3.4&#x27;s password:Connected to 1.2.3.4.sftp&gt; help：显示帮助信息。 ls&#x2F;lls：显示远端&#x2F;本地目录列表。 cd&#x2F;lcd：切换远端&#x2F;本地路径。 mkdir&#x2F;lmkdir：创建远端&#x2F;本地目录。 pwd&#x2F;lpwd：显示远端&#x2F;本地当前工作目录。 get：下载文件。 put：上传文件。 rm：删除远端文件。 bye&#x2F;exit&#x2F;quit：退出sftp。 进程管理 查看进程 - ps。 12345678[root ~]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Jun23 ? 00:00:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 21root 2 0 0 Jun23 ? 00:00:00 [kthreadd]...[root ~]# ps -ef | grep mysqldroot 4943 4581 0 22:45 pts/0 00:00:00 grep --color=auto mysqldmysql 25257 1 0 Jun25 ? 00:00:39 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid 显示进程状态树 - pstree。 123456789101112131415161718192021[root ~]# pstreesystemd─┬─AliYunDun───18*[&#123;AliYunDun&#125;] ├─AliYunDunUpdate───3*[&#123;AliYunDunUpdate&#125;] ├─2*[agetty] ├─aliyun-service───2*[&#123;aliyun-service&#125;] ├─atd ├─auditd───&#123;auditd&#125; ├─dbus-daemon ├─dhclient ├─irqbalance ├─lvmetad ├─mysqld───28*[&#123;mysqld&#125;] ├─nginx───2*[nginx] ├─ntpd ├─polkitd───6*[&#123;polkitd&#125;] ├─rsyslogd───2*[&#123;rsyslogd&#125;] ├─sshd───sshd───bash───pstree ├─systemd-journal ├─systemd-logind ├─systemd-udevd └─tuned───4*[&#123;tuned&#125;] 查找与指定条件匹配的进程 - pgrep。 12[root ~]$ pgrep mysqld3584 通过进程号终止进程 - kill。 12345678910111213141516[root ~]$ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX[root ~]# kill 1234[root ~]# kill -9 1234 通过进程名终止进程 - killall &#x2F; pkill。 结束名为mysqld的进程。 1[root ~]# pkill mysqld 结束hellokitty用户的所有进程。 1[root ~]# pkill -u hellokitty 说明：这样的操作会让hellokitty用户和服务器断开连接。 将进程置于后台运行。 Ctrl+Z - 快捷键，用于停止进程并置于后台。 &amp; - 将进程置于后台运行。 12345[root ~]# mongod &amp;[root ~]# redis-server...^Z[4]+ Stopped redis-server 查询后台进程 - jobs。 1234[root ~]# jobs[2] Running mongod &amp;[3]- Stopped cat[4]+ Stopped redis-server 让进程在后台继续运行 - bg。 123456[root ~]# bg %4[4]+ redis-server &amp;[root ~]# jobs[2] Running mongod &amp;[3]+ Stopped cat[4]- Running redis-server &amp; 将后台进程置于前台 - fg。 12[root ~]# fg %4redis-server 说明：置于前台的进程可以使用Ctrl+C来终止它。 调整程序&#x2F;进程运行时优先级 - nice &#x2F; renice。 用户登出后进程继续工作 - nohup。 1[root ~]# nohup ping www.baidu.com &gt; result.txt &amp; 跟踪进程系统调用情况 - strace。 1234567891011121314[root ~]# pgrep mysqld8803[root ~]# strace -c -p 8803strace: Process 8803 attached^Cstrace: Process 8803 detached% time seconds usecs/call calls errors syscall------ ----------- ----------- --------- --------- ---------------- 99.18 0.005719 5719 1 restart_syscall 0.49 0.000028 28 1 mprotect 0.24 0.000014 14 1 clone 0.05 0.000003 3 1 mmap 0.03 0.000002 2 1 accept------ ----------- ----------- --------- --------- ----------------100.00 0.005766 5 total 说明：这个命令的用法和参数都比较复杂，建议大家在真正用到这个命令的时候再根据实际需要进行了解。 查看当前运行级别 - runlevel。 12[root ~]# runlevelN 3 实时监控进程占用资源状况 - top。 1234567[root ~]# toptop - 23:04:23 up 3 days, 14:10, 1 user, load average: 0.00, 0.01, 0.05Tasks: 65 total, 1 running, 64 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.3 us, 0.3 sy, 0.0 ni, 99.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1016168 total, 191060 free, 324700 used, 500408 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 530944 avail Mem... -c - 显示进程的整个路径。 -d - 指定两次刷屏之间的间隔时间（秒为单位）。 -i - 不显示闲置进程或僵尸进程。 -p - 显示指定进程的信息。 系统诊断 系统启动异常诊断 - dmesg。 查看系统活动信息 - sar。 12345678[root ~]# sar -u -r 5 10Linux 3.10.0-957.10.1.el7.x86_64 (izwz97tbgo9lkabnat2lo8z) 06/02/2019 _x86_64_ (2 CPU)06:48:30 PM CPU %user %nice %system %iowait %steal %idle06:48:35 PM all 0.10 0.00 0.10 0.00 0.00 99.8006:48:30 PM kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty06:48:35 PM 1772012 2108392 54.33 102816 1634528 784940 20.23 793328 1164704 0 -A - 显示所有设备（CPU、内存、磁盘）的运行状况。 -u - 显示所有CPU的负载情况。 -d - 显示所有磁盘的使用情况。 -r - 显示内存的使用情况。 -n - 显示网络运行状态。 查看内存使用情况 - free。 1234[root ~]# free total used free shared buff/cache availableMem: 1016168 323924 190452 356 501792 531800Swap: 0 0 0 虚拟内存统计 - vmstat。 1234[root ~]# vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 2 0 0 204020 79036 667532 0 0 5 18 101 58 1 0 99 0 0 CPU信息统计 - mpstat。 12345[root ~]# mpstatLinux 3.10.0-957.5.1.el7.x86_64 (iZ8vba0s66jjlfmo601w4xZ) 05/30/2019 _x86_64_ (1 CPU)01:51:54 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01:51:54 AM all 0.71 0.00 0.17 0.04 0.00 0.00 0.00 0.00 0.00 99.07 查看进程使用内存状况 - pmap。 1234567891011121314[root ~]# ps PID TTY TIME CMD 4581 pts/0 00:00:00 bash 5664 pts/0 00:00:00 ps[root ~]# pmap 45814581: -bash0000000000400000 884K r-x-- bash00000000006dc000 4K r---- bash00000000006dd000 36K rw--- bash00000000006e6000 24K rw--- [ anon ]0000000001de0000 400K rw--- [ anon ]00007f82fe805000 48K r-x-- libnss_files-2.17.so00007f82fe811000 2044K ----- libnss_files-2.17.so... 报告设备CPU和I&#x2F;O统计信息 - iostat。 1234567[root ~]# iostatLinux 3.10.0-693.11.1.el7.x86_64 (iZwz97tbgo9lkabnat2lo8Z) 06/26/2018 _x86_64_ (1 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.79 0.00 0.20 0.04 0.00 98.97Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnvda 0.85 6.78 21.32 2106565 6623024vdb 0.00 0.01 0.00 2088 0 显示所有PCI设备 - lspci。 123456789101112[root ~]# lspci00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)00:02.0 VGA compatible controller: Cirrus Logic GD 544600:03.0 Ethernet controller: Red Hat, Inc. Virtio network device00:04.0 Communication controller: Red Hat, Inc. Virtio console00:05.0 SCSI storage controller: Red Hat, Inc. Virtio block device00:06.0 SCSI storage controller: Red Hat, Inc. Virtio block device00:07.0 Unclassified device [00ff]: Red Hat, Inc. Virtio memory balloon 显示进程间通信设施的状态 - ipcs。 12345678910[root ~]# ipcs------ Message Queues --------key msqid owner perms used-bytes messages ------ Shared Memory Segments --------key shmid owner perms bytes nattch status ------ Semaphore Arrays --------key semid owner perms nsems Shell编程之前我们提到过，Shell是一个连接用户和操作系统的应用程序，它提供了人机交互的界面（接口），用户通过这个界面访问操作系统内核的服务。Shell脚本是一种为Shell编写的脚本程序，我们可以通过Shell脚本来进行系统管理，同时也可以通过它进行文件操作。总之，编写Shell脚本对于使用Linux系统的人来说，应该是一项标配技能。 互联网上有大量关于Shell脚本的相关知识，我不打算再此对Shell脚本做一个全面系统的讲解，我们通过下面的代码来感性的认识下Shell脚本就行了。 例子1：输入两个整数m和n，计算从m到n的整数求和的结果。 12345678910111213#!/usr/bin/bashprintf &#x27;m = &#x27;read mprintf &#x27;n = &#x27;read na=$msum=0while [ $a -le $n ]do sum=$[ sum + a ] a=$[ a + 1 ]doneecho &#x27;结果: &#x27;$sum 例子2：自动创建文件夹和指定数量的文件。 12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/bashprintf &#x27;输入文件夹名: &#x27;read dirprintf &#x27;输入文件名: &#x27;read fileprintf &#x27;输入文件数量(&lt;1000): &#x27;read numif [ $num -ge 1000 ]then echo &#x27;文件数量不能超过1000&#x27;else if [ -e $dir -a -d $dir ] then rm -rf $dir else if [ -e $dir -a -f $dir ] then rm -f $dir fi fi mkdir -p $dir index=1 while [ $index -le $num ] do if [ $index -lt 10 ] then pre=&#x27;00&#x27; elif [ $index -lt 100 ] then pre=&#x27;0&#x27; else pre=&#x27;&#x27; fi touch $dir&#x27;/&#x27;$file&#x27;_&#x27;$pre$index index=$[ index + 1 ] donefi 例子3：自动安装指定版本的Redis。 123456789101112131415161718#!/usr/bin/bashinstall_redis() &#123; if ! which redis-server &gt; /dev/null then cd /root wget $1$2&#x27;.tar.gz&#x27; &gt;&gt; install.log gunzip /root/$2&#x27;.tar.gz&#x27; tar -xf /root/$2&#x27;.tar&#x27; cd /root/$2 make &gt;&gt; install.log make install &gt;&gt; install.log echo &#x27;安装完成&#x27; else echo &#x27;已经安装过Redis&#x27; fi&#125;install_redis &#x27;http://download.redis.io/releases/&#x27; $1 相关资源 Linux命令行常用快捷键 快捷键 功能说明 tab 自动补全命令或路径 Ctrl+a 将光标移动到命令行行首 Ctrl+e 将光标移动到命令行行尾 Ctrl+f 将光标向右移动一个字符 Ctrl+b 将光标向左移动一个字符 Ctrl+k 剪切从光标到行尾的字符 Ctrl+u 剪切从光标到行首的字符 Ctrl+w 剪切光标前面的一个单词 Ctrl+y 复制剪切命名剪切的内容 Ctrl+c 中断正在执行的任务 Ctrl+h 删除光标前面的一个字符 Ctrl+d 退出当前命令行 Ctrl+r 搜索历史命令 Ctrl+g 退出历史命令搜索 Ctrl+l 清除屏幕上所有内容在屏幕的最上方开启一个新行 Ctrl+s 锁定终端使之暂时无法输入内容 Ctrl+q 退出终端锁定 Ctrl+z 将正在终端执行的任务停下来放到后台 !! 执行上一条命令 !数字 执行数字对应的历史命令 !字母 执行最近的以字母打头的命令 !$ &#x2F; Esc+. 获得上一条命令最后一个参数 Esc+b 移动到当前单词的开头 Esc+f 移动到当前单词的结尾 man查阅命令手册的内容说明 手册中的标题 功能说明 NAME 命令的说明和介绍 SYNOPSIS 使用该命令的基本语法 DESCRIPTION 使用该命令的详细描述，各个参数的作用，有时候这些信息会出现在OPTIONS中 OPTIONS 命令相关参数选项的说明 EXAMPLES 使用该命令的参考例子 EXIT STATUS 命令结束的退出状态码，通常0表示成功执行 SEE ALSO 和命令相关的其他命令或信息 BUGS 和命令相关的缺陷的描述 AUTHOR 该命令的作者介绍","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/垃圾分类查询/index","date":"2024-12-12T08:38:01.763Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/垃圾分类查询/index/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E6%9F%A5%E8%AF%A2/index/","excerpt":"","text":"垃圾分类查询助手 .search, .result { width: 720px; margin: 50px auto; } .search > input { width: 520px; border: none; outline: none; text-align: center; font-size: 36px; line-height: 36px; border-bottom: 1px solid gray; margin: 0 20px; } .search button { background-color: red; color: white; font-size: 28px; border: none; outline: none; width: 120px; } .result > p, .result > div { width: 640px; margin: 0 auto; } .result > p, .result span { text-align: left; font-size: 28px; } .result img { vertical-align: middle; } .explain { font-size: 12px; color: darkgray; } .result .pre { font-size: 16px; } 查询 没有对应的查询结果 &nbsp;&nbsp; &nbsp;&nbsp; （预测结果） 说明： new Vue({ el: '#app', data: { word: '', searched: false, types: ['可回收物', '有害垃圾', '厨余垃圾', '其他垃圾'], pictures: ['recyclable.png', 'harmful-waste.png', 'kitchen-waste.png', 'other-waste.png'], results: [] }, methods: { // 查询垃圾分类的函数 search() { if (this.word.trim().length > 0) { let key = 'e8c5524dd2a365f20908ced735f8e480' let url = `http://api.tianapi.com/txapi/lajifenlei/?key=${key}&word=${this.word}` fetch(url) .then(resp => resp.json()) .then(json => { // 处理返回的JSON格式数据 this.searched = true this.results = json.newslist }) } } } })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/垃圾分类查询/index-2","date":"2024-12-12T08:38:01.759Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/垃圾分类查询/index-2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/%E5%9E%83%E5%9C%BE%E5%88%86%E7%B1%BB%E6%9F%A5%E8%AF%A2/index-2/","excerpt":"","text":"垃圾分类查询助手 .search, .result { width: 720px; margin: 50px auto; } .search > input { width: 520px; border: none; outline: none; text-align: center; font-size: 36px; line-height: 36px; border-bottom: 1px solid gray; margin: 0 20px; } .search button { background-color: red; color: white; font-size: 28px; border: none; outline: none; width: 120px; } .result > p, .result > div { width: 640px; margin: 0 auto; } .result > p, .result span { text-align: left; font-size: 28px; } .result img { vertical-align: middle; } .explain { font-size: 12px; color: darkgray; } .result .pre { font-size: 16px; } 查询 没有对应的查询结果 &nbsp;&nbsp; &nbsp;&nbsp; （预测结果） 说明： new Vue({ el: '#app', data: { word: '', searched: false, types: ['可回收物', '有害垃圾', '厨余垃圾', '其他垃圾'], pictures: ['recyclable.png', 'harmful-waste.png', 'kitchen-waste.png', 'other-waste.png'], results: [] }, methods: { search() { if (this.word.trim().length > 0) { let key = 'e8c5524dd2a365f20908ced735f8e480' let url = `http://api.tianapi.com/txapi/lajifenlei/?key=${key}&word=${this.word}` fetch(url) .then(resp => resp.json()) .then(json => { this.searched = true this.results = json.newslist }) } } } })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/js/mylib","date":"2024-12-12T08:38:01.747Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/js/mylib/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/js/mylib/","excerpt":"","text":"function randomColor() { var r = parseInt(Math.random() * 128 + 128); var g = parseInt(Math.random() * 128 + 128); var b = parseInt(Math.random() * 128 + 128); return 'rgb(' + r + ', ' + g + ', ' + b + ')'; }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/js/jquery.min","date":"2024-12-12T08:38:01.744Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/js/jquery.min/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/js/jquery.min/","excerpt":"","text":"/*! jQuery v3.3.1 | (c) JS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(e,t){\"use strict\";var n=[],r=e.document,i=Object.getPrototypeOf,o=n.slice,a=n.concat,s=n.push,u=n.indexOf,l={},c=l.toString,f=l.hasOwnProperty,p=f.toString,d=p.call(Object),h={},g=function e(t){return\"function\"==typeof t&&\"number\"!=typeof t.nodeType},y=function e(t){return null!=t&&t===t.window},v={type:!0,src:!0,noModule:!0};function m(e,t,n){var i,o=(t=t||r).createElement(\"script\");if(o.text=e,n)for(i in v)n[i]&&(o[i]=n[i]);t.head.appendChild(o).parentNode.removeChild(o)}function x(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?l[c.call(e)]||\"object\":typeof e}var b=\"3.3.1\",w=function(e,t){return new w.fn.init(e,t)},T=/^[\\s\\uFEFF\\xA0]+|[\\s\\uFEFF\\xA0]+$/g;w.fn=w.prototype={jquery:\"3.3.1\",constructor:w,length:0,toArray:function(){return o.call(this)},get:function(e){return null==e?o.call(this):e1?(n=[e,e,\"\",t],r.setFilters.hasOwnProperty(e.toLowerCase())?se(function(e,n){var r,o=i(e,t),a=o.length;while(a--)e[r=O(e,o[a])]=!(n[r]=o[a])}):function(e){return i(e,0,n)}):i}},pseudos:{not:se(function(e){var t=[],n=[],r=s(e.replace(B,\"$1\"));return r[b]?se(function(e,t,n,i){var o,a=r(e,null,i,[]),s=e.length;while(s--)(o=a[s])&&(e[s]=!(t[s]=o))}):function(e,i,o){return t[0]=e,r(t,null,o,n),t[0]=null,!n.pop()}}),has:se(function(e){return function(t){return oe(e,t).length>0}}),contains:se(function(e){return e=e.replace(Z,ee),function(t){return(t.textContent||t.innerText||i(t)).indexOf(e)>-1}}),lang:se(function(e){return U.test(e||\"\")||oe.error(\"unsupported lang: \"+e),e=e.replace(Z,ee).toLowerCase(),function(t){var n;do{if(n=g?t.lang:t.getAttribute(\"xml:lang\")||t.getAttribute(\"lang\"))return(n=n.toLowerCase())===e||0===n.indexOf(e+\"-\")}while((t=t.parentNode)&&1===t.nodeType);return!1}}),target:function(t){var n=e.location&&e.location.hash;return n&&n.slice(1)===t.id},root:function(e){return e===h},focus:function(e){return e===d.activeElement&&(!d.hasFocus||d.hasFocus())&&!!(e.type||e.href||~e.tabIndex)},enabled:de(!1),disabled:de(!0),checked:function(e){var t=e.nodeName.toLowerCase();return\"input\"===t&&!!e.checked||\"option\"===t&&!!e.selected},selected:function(e){return e.parentNode&&e.parentNode.selectedIndex,!0===e.selected},empty:function(e){for(e=e.firstChild;e;e=e.nextSibling)if(e.nodeType=3?[null,e,null]:L.exec(e))||!i[1]&&t)return!t||t.jquery?(t||n).find(e):this.constructor(t).find(e);if(i[1]){if(t=t instanceof w?t[0]:t,w.merge(this,w.parseHTML(i[1],t&&t.nodeType?t.ownerDocument||t:r,!0)),A.test(i[1])&&w.isPlainObject(t))for(i in t)g(this[i])?this[i](t[i]):this.attr(i,t[i]);return this}return(o=r.getElementById(i[2]))&&(this[0]=o,this.length=1),this}return e.nodeType?(this[0]=e,this.length=1,this):g(e)?void 0!==n.ready?n.ready(e):e(w):w.makeArray(e,this)}).prototype=w.fn,q=w(r);var H=/^(?:parents|prev(?:Until|All))/,O={children:!0,contents:!0,next:!0,prev:!0};w.fn.extend({has:function(e){var t=w(e,this),n=t.length;return this.filter(function(){for(var e=0;e1&&(O[e]||w.uniqueSort(i),H.test(e)&&i.reverse()),this.pushStack(i)}});var M=/[^\\x20\\t\\r\\n\\f]+/g;function R(e){var t={};return w.each(e.match(M)||[],function(e,n){t[n]=!0}),t}w.Callbacks=function(e){e=\"string\"==typeof e?R(e):w.extend({},e);var t,n,r,i,o=[],a=[],s=-1,u=function(){for(i=i||e.once,r=t=!0;a.length;s=-1){n=a.shift();while(++s-1)o.splice(n,1),n-1:o.length>0},empty:function(){return o&&(o=[]),this},disable:function(){return i=a=[],o=n=\"\",this},disabled:function(){return!o},lock:function(){return i=a=[],n||t||(o=n=\"\"),this},locked:function(){return!!i},fireWith:function(e,n){return i||(n=[e,(n=n||[]).slice?n.slice():n],a.push(n),t||u()),this},fire:function(){return l.fireWith(this,arguments),this},fired:function(){return!!r}};return l};function I(e){return e}function W(e){throw e}function $(e,t,n,r){var i;try{e&&g(i=e.promise)?i.call(e).done(t).fail(n):e&&g(i=e.then)?i.call(e,t,n):t.apply(void 0,[e].slice(r))}catch(e){n.apply(void 0,[e])}}w.extend({Deferred:function(t){var n=[[\"notify\",\"progress\",w.Callbacks(\"memory\"),w.Callbacks(\"memory\"),2],[\"resolve\",\"done\",w.Callbacks(\"once memory\"),w.Callbacks(\"once memory\"),0,\"resolved\"],[\"reject\",\"fail\",w.Callbacks(\"once memory\"),w.Callbacks(\"once memory\"),1,\"rejected\"]],r=\"pending\",i={state:function(){return r},always:function(){return o.done(arguments).fail(arguments),this},\"catch\":function(e){return i.then(null,e)},pipe:function(){var e=arguments;return w.Deferred(function(t){w.each(n,function(n,r){var i=g(e[r[4]])&&e[r[4]];o[r[1]](function(){var e=i&&i.apply(this,arguments);e&&g(e.promise)?e.promise().progress(t.notify).done(t.resolve).fail(t.reject):t[r[0]+\"With\"](this,i?[e]:arguments)})}),e=null}).promise()},then:function(t,r,i){var o=0;function a(t,n,r,i){return function(){var s=this,u=arguments,l=function(){var e,l;if(!(t=o&&(r!==W&&(s=void 0,u=[e]),n.rejectWith(s,u))}};t?c():(w.Deferred.getStackHook&&(c.stackTrace=w.Deferred.getStackHook()),e.setTimeout(c))}}return w.Deferred(function(e){n[0][3].add(a(0,e,g(i)?i:I,e.notifyWith)),n[1][3].add(a(0,e,g(t)?t:I)),n[2][3].add(a(0,e,g(r)?r:W))}).promise()},promise:function(e){return null!=e?w.extend(e,i):i}},o={};return w.each(n,function(e,t){var a=t[2],s=t[5];i[t[1]]=a.add,s&&a.add(function(){r=s},n[3-e][2].disable,n[3-e][3].disable,n[0][2].lock,n[0][3].lock),a.add(t[3].fire),o[t[0]]=function(){return o[t[0]+\"With\"](this===o?void 0:this,arguments),this},o[t[0]+\"With\"]=a.fireWith}),i.promise(o),t&&t.call(o,o),o},when:function(e){var t=arguments.length,n=t,r=Array(n),i=o.call(arguments),a=w.Deferred(),s=function(e){return function(n){r[e]=this,i[e]=arguments.length>1?o.call(arguments):n,--t||a.resolveWith(r,i)}};if(t0||F.resolveWith(r,[w]))}}),w.ready.then=F.then;function _(){r.removeEventListener(\"DOMContentLoaded\",_),e.removeEventListener(\"load\",_),w.ready()}\"complete\"===r.readyState||\"loading\"!==r.readyState&&!r.documentElement.doScroll?e.setTimeout(w.ready):(r.addEventListener(\"DOMContentLoaded\",_),e.addEventListener(\"load\",_));var z=function(e,t,n,r,i,o,a){var s=0,u=e.length,l=null==n;if(\"object\"===x(n)){i=!0;for(s in n)z(e,t,s,n[s],!0,o,a)}else if(void 0!==r&&(i=!0,g(r)||(a=!0),l&&(a?(t.call(e,r),t=null):(l=t,t=function(e,t,n){return l.call(w(e),n)})),t))for(;s1,null,!0)},removeData:function(e){return this.each(function(){K.remove(this,e)})}}),w.extend({queue:function(e,t,n){var r;if(e)return t=(t||\"fx\")+\"queue\",r=J.get(e,t),n&&(!r||Array.isArray(n)?r=J.access(e,t,w.makeArray(n)):r.push(n)),r||[]},dequeue:function(e,t){t=t||\"fx\";var n=w.queue(e,t),r=n.length,i=n.shift(),o=w._queueHooks(e,t),a=function(){w.dequeue(e,t)};\"inprogress\"===i&&(i=n.shift(),r--),i&&(\"fx\"===t&&n.unshift(\"inprogress\"),delete o.stop,i.call(e,a,o)),!r&&o&&o.empty.fire()},_queueHooks:function(e,t){var n=t+\"queueHooks\";return J.get(e,n)||J.access(e,n,{empty:w.Callbacks(\"once memory\").add(function(){J.remove(e,[t+\"queue\",n])})})}}),w.fn.extend({queue:function(e,t){var n=2;return\"string\"!=typeof e&&(t=e,e=\"fx\",n--),arguments.length","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/message","date":"2024-12-12T08:38:01.658Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/message/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/message/","excerpt":"","text":"{\"from\":\"骆昊\",\"to\":\"王大锤\",\"content\":\"今天晚上你请我吃饭!\"}","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/index","date":"2024-12-12T08:38:01.655Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/index/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/index/","excerpt":"","text":"* { font-size: 18px; } h2 { font-size: 22px; } h3 { font-size: 20px; } ul li { list-style: circle; } JavaScript课堂案例 Make English as your working language!!! 浏览器中的JavaScript： ECMAScript: JavaScript语法规范 BOM: 浏览器对象模型（Browser Object Model）,把浏览器当成一个对象（window），通过这个对象可以操控浏览器 DOM: 文档对象模型（Document Object Model），把整个页面当成一个对象（document），通过这个对象可以操作整个页面 课堂案例 例子1：BOM和DOM的感性认识 例子2：成都机动车限行查询 例子3：延迟跳转到百度 例子4：轮播广告 完整效果请参考作业1 例子5：事件冒泡和事件捕获 例子6：获取事件源和访问相关元素 例子7：动态添加和删除元素 例子8：流氓浮动广告 例子9：jQuery实现表格效果 例子10：jQuery实现动态列表 例子11：Ajax加载美女图片（原生JavaScript） 例子12：Ajax加载美女图片（jQuery） 课后练习 练习1：轮播广告 练习2：缩略图效果 练习3：闪烁的方块 练习4：表格效果 练习5：购物车效果（仿京东） 练习6：可拖拽的元素 练习7：周公解梦（Ajax） 练习7：表单验证（正则表达式）","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework08","date":"2024-12-12T08:38:01.653Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework08/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework08/","excerpt":"","text":"* { margin: 0; padding: 0; font-size: 18px; } #login label { display: inline-block; width: 150px; text-align: right; margin-right: 20px; } .formitem { margin: 20px 0; } .hint { display: inline-block; width: 320px; font-size: 14px; margin-left: 10px; } .correct { color: green; } .incorrect { color: red; } #login input[type=\"submit\"] { display: inline-block; width: 120px; height: 30px; background-color: darkred; color: white; font-size: 20px; line-height: 30px; border: none; cursor: pointer; margin-left: 200px; } 用户名: 密码: 确认密码: 手机号: 验证码: 我同意《XYZ服务协议》","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework07","date":"2024-12-12T08:38:01.650Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework07/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework07/","excerpt":"","text":"#container { width: 400px; margin: 0 auto; padding-top: 120px; text-align: center; } #container input { font-size: 22px; line-height: 30px; height: 30px; outline: none; } #keyword { width: 300px; border: none; text-align: center; border-bottom: 1px solid gray; } #search { width: 80px; color: white; border: none; background-color: red; } #result { width: 400px; margin: 10px auto; font-size: 18px; } $(function() { $(\"#search\").on(\"click\", function() { var keyword = $(\"#keyword\").val().trim(); if (keyword.length > 0) { var url = \"http://api.tianapi.com/txapi/dream/\"; $.ajax({ \"url\": url, \"type\": \"get\", \"data\": { \"key\": \"772a81a51ae5c780251b1f98ea431b84\", \"word\": keyword, }, \"dataType\": \"json\", \"success\": function(jsonObj) { if (jsonObj.code == 250) { $(\"#result\").text(jsonObj.msg); } else { $(\"#result\").text(jsonObj.newslist[0].result); } } }); } }); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework06","date":"2024-12-12T08:38:01.648Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework06/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework06/","excerpt":"","text":"#one, #two, #three { width: 200px; height: 200px; position: fixed; } #one { left: 50px; top: 50px; background-color: lightpink; } #two { left: 200px; top: 150px; background-color: lightgreen; } #three { right: 30px; top: 100px; background-color: lightgoldenrodyellow; } $(function() { makeDraggable($('#one')); makeDraggable($('#two')); makeDraggable($('#three')); }); var draggables = []; function makeDraggable(jqElem) { draggables.push(jqElem); jqElem.on('mousedown', function(evt) { this.isMouseDown = true; this.oldX = evt.clientX; this.oldY = evt.clientY; this.oldLeft = parseInt($(evt.target).css('left')); this.oldTop = parseInt($(evt.target).css('top')); $.each(draggables, function(index, elem) { elem.css('z-index', '0'); }); $(evt.target).css('z-index', '99'); }) .on('mousemove', function(evt) { if (this.isMouseDown) { var dx = evt.clientX - this.oldX; var dy = evt.clientY - this.oldY; $(evt.target).css('left', this.oldLeft + dx + 'px'); $(evt.target).css('top', this.oldTop + dy + 'px'); } }) .on('mouseup', function(evt) { this.isMouseDown = false; }) .on('mouseout', function(evt) { this.isMouseDown = false; }); }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework05","date":"2024-12-12T08:38:01.646Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework05/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework05/","excerpt":"","text":"* { margin: 0; padding: 0; } body { width: 960px; margin: 20px auto; } #cart { margin: 0 auto; width: 850px; } #cart-header { height: 40px; background-color: lightgray; margin-bottom: 20px; } #cart-header div { line-height: 40px; } .left { float: left; } .right { float: right; } .w110 { width: 100px; } .ml10 { margin-left: 10px; } .w120 { width: 120px; } .w250 { width: 250px; } .center { text-align: center; } .w20 { width: 20px; } .w90 { width: 90px; } .clear { clear: both; } #cart-items>div { height: 100px; } #cart-items>div>div { line-height: 100px; } .w250 span { display: inline-block; font-size: 12px; line-height: 16px !important; } .single-item { border-bottom: 1px solid gray; } .small-button { display: inline-block; width: 20px; height: 20px; border: none; } .big-button { color: white; background-color: red; display: inline-block; width: 120px; height: 40px; border: none; font-size: 22px; } #totalCount, #totalPrice { color: red; } #totalPrice { font: bolder 20px Arial; display: inline-block; width: 150px; } #cart a { text-decoration: none; } #cart a:link, #cart a:visited, #cart a:active { color: gray; } 全选 商品 单价 数量 小计 操作 海澜之家/Heilan Home春装商务白衬衫男修身HNCAD3A067Y 漂白(69) 漂 &yen;138.00 - + &yen;138.00 删除 HLA海澜之家长袖衬衫男牛津纺休闲干净透气HNEAJ1E048A浅灰 &yen;128.00 - + &yen;128.00 删除 HLA海澜之家牛津纺清新休闲衬衫2018春季新品质感柔软长袖衬衫男 &yen;99.00 - + &yen;99.00 删除 删除选中商品 总共选中了0件商品 总计: &yen;0.00 去结算 function calcTotal() { var amountsInput = $('.single-item input[type=text]'); var pricesSpan = $('.single-item .price'); var checkboxes = $('.single-item input[type=checkbox]'); var totalAmount = 0; var totalPrice = 0; amountsInput.each(function(index) { if (checkboxes[index].checked) { var amount = parseInt($(this).val()); totalAmount += amount; var price = parseFloat($(pricesSpan[index]).text()); var currentPrice = (price * amount).toFixed(2); $(this).parent().next().find('span').text(currentPrice); totalPrice += parseFloat(currentPrice); } }); $('#totalCount').text(totalAmount); $('#totalPrice').text('￥' + totalPrice.toFixed(2)); } $(function() { $('#selectAll').on('click', function(evt) { $('.single-item input[type=checkbox]').prop('checked', evt.target.checked); calcTotal(); }); $('.single-item button').on('click', function(evt) { var op = $(evt.target).text(); if (op == '-') { var numInput = $(evt.target).next(); var num = parseInt(numInput.val()); if (num > 1) { numInput.val(num - 1); } } else { var numInput = $(evt.target).prev(); var num = parseInt(numInput.val()); if (num < 200) { numInput.val(num + 1); } } $(evt.target).parent().parent().find('input[type=checkbox]').prop('checked', true); calcTotal(); }); $('.single-item input[type=checkbox]').on('click', function() { calcTotal(); }); $('.single-item a').on('click', function(evt) { if (confirm('确定要删除该商品吗?')) { $(evt.target).parent().parent().remove(); calcTotal(); } }); $('#clearSelected').on('click', function() { if (confirm('确定要删除选中的商品吗?')) { $('.single-item').each(function() { if ($(this).find('input:checkbox').prop('checked')) { $(this).remove(); } }); calcTotal(); } }); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework04","date":"2024-12-12T08:38:01.644Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework04/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework04/","excerpt":"","text":"#data { border-collapse: collapse; } #data td, #data th { width: 120px; height: 40px; text-align: center; border: 1px solid black; } #buttons { margin: 10px 0; } 数据统计表 姓名 年龄 性别 身高 体重 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 隔行换色 清除数据 删单元格 隐藏表格 function prettify() { var trs = document.querySelectorAll('#data tr'); for (var i = 1; i < trs.length; i += 1) { trs[i].style.backgroundColor = i % 2 == 0 ? 'lightgray' : 'lightsteelblue'; } } function clear() { var tds = document.querySelectorAll('#data td'); for (var i = 0; i < tds.length; i += 1) { tds[i].textContent = ''; } } function removeLastRow() { var table = document.getElementById('data'); if (table.rows.length > 1) { table.deleteRow(table.rows.length - 1); } } function hideTable() { var table = document.getElementById('data'); table.style.visibility = 'hidden'; } +function() { var prettyBtn = document.querySelector('#pretty'); prettyBtn.addEventListener('click', prettify) var clearBtn = document.querySelector('#clear'); clearBtn.addEventListener('click', clear); var removeBtn = document.querySelector('#remove'); removeBtn.addEventListener('click', removeLastRow); var hideBtn = document.querySelector('#hide'); hideBtn.addEventListener('click', hideTable); }();","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework03","date":"2024-12-12T08:38:01.642Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework03/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework03/","excerpt":"","text":"#container { width: 800px; height: 400px; margin: 10px auto; border: 1px solid black; overflow: hidden; } #buttons { width: 800px; margin: 10px auto; text-align: center; } #add, #fla { border: none; outline: none; width: 80px; height: 30px; background-color: red; color: white; font-size: 16px; cursor: pointer; } .small { width: 80px; height: 80px; float: left; } 添加 闪烁 var bigDiv = document.querySelector('#container'); var addButton = document.querySelector('#add'); addButton.addEventListener('click', function() { var smallDiv = document.createElement('div'); smallDiv.className = 'small'; // smallDiv.style.width = '80px'; // smallDiv.style.height = '80px'; // smallDiv.style.float = 'left'; smallDiv.style.backgroundColor = randomColor(); bigDiv.insertBefore(smallDiv, bigDiv.firstChild); }); var flaButton = document.querySelector('#fla'); var isFlashing = false; var timerId; flaButton.addEventListener('click', function(evt) { isFlashing = !isFlashing; if (isFlashing) { timerId = window.setInterval(function() { var divs = document.querySelectorAll('#container>div'); for (var i = 0; i < divs.length; i += 1) { divs[i].style.backgroundColor = randomColor(); } }, 200); flaButton.textContent = '暂停'; } else { window.clearInterval(timerId); flaButton.textContent = '闪烁'; } });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework02","date":"2024-12-12T08:38:01.640Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework02/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework02/","excerpt":"","text":"* { margin: 0; padding: 0; } #container { margin: 10px 20px; } #container li { float: left; list-style: none; width: 60px; height: 60px; } var img = document.querySelector('#container>img'); var images = document.querySelectorAll('#items img'); for (var i = 0; i < images.length; i += 1) { // 事件回调函数在for循环的时候并没有执行所以也取不到循环变量i当前的值 // JavaScript是动态弱类型语言可以在运行时动态的添加(或删除)对象的属性 images[i].picture = 'img/picture-' + (i + 1) + '.jpg'; images[i].addEventListener('mouseover', function(evt) { img.src = evt.target.picture; }); }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/homework01","date":"2024-12-12T08:38:01.637Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework01/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/homework01/","excerpt":"","text":"* { margin: 0; padding: 0; } #adv { width: 940px; margin: 0 auto; } #adv ul { width: 120px; height: 30px; margin: 0 auto; position: relative; top: -30px; } #adv li { width: 30px; height: 30px; list-style: none; float: left; color: #ccc; cursor: pointer; } #adv li:first-child { color: lightseagreen; } ● ● ● ● var img = document.querySelector('#adv>img'); var items = document.querySelectorAll('#adv li'); var timerId = 0; for (var i = 0; i < items.length; i += 1) { items[i].index = i; items[i].addEventListener('mouseover', function(evt) { index = evt.target.index; changeItemsColor(index); img.src = 'img/' + images[index]; if (timerId != 0) { window.clearInterval(timerId); timerId = 0; } }); items[i].addEventListener('mouseout', startIt); } var images = ['slide-1.jpg', 'slide-2.jpg', 'slide-3.jpg', 'slide-4.jpg']; var index = 0; startIt(); function startIt() { if (timerId == 0) { timerId = window.setInterval(function() { index += 1; index %= images.length; changeItemsColor(index); img.src = 'img/' + images[index]; }, 2000); } } function changeItemsColor(index) { for (var i = 0; i < items.length; i += 1) { items[i].style.color = '#ccc'; } items[index].style.color = 'lightseagreen'; }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example12","date":"2024-12-12T08:38:01.635Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example12/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example12/","excerpt":"","text":"换一组 $(function() { var page = 0; $('#ok').on('click', function() { page += 1; $.ajax({ 'url': 'http://api.tianapi.com/meinv/', 'type': 'get', 'data': { 'key': '772a81a51ae5c780251b1f98ea431b84', 'num': 12, 'page': page }, 'dataType': 'json', 'success': function(jsonObj) { $('#container').empty(); $.each(jsonObj.newslist, function(index, mmObj) { var img = $('').attr('width', '250') .attr('src', mmObj.picUrl); $('#container').append(img); }); } }); /* var url = 'http://api.tianapi.com/meinv/?key=772a81a51ae5c780251b1f98ea431b84&num=10'; $.getJSON(url, function(jsonObj) { $('#container').empty(); $.each(jsonObj.newslist, function(index, mm) { $('#container').append( $('').attr('width', '250').attr('src', mm.picUrl) ); }); }); */ }); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example11","date":"2024-12-12T08:38:01.633Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example11/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example11/","excerpt":"","text":"换一组 (function() { var page = 0; var div = document.getElementById('container'); var button = document.getElementById('ok'); button.addEventListener('click', function() { // 1. 创建异步请求对象 var xhr = new XMLHttpRequest(); if (xhr) { page += 1 var url = 'http://api.tianapi.com/meinv/?key=772a81a51ae5c780251b1f98ea431b84&num=10&page=' + page; // 2. 配置异步请求 xhr.open('get', url, true); // 3. 绑定事件回调函数（服务器成功响应后要干什么） xhr.onreadystatechange = function() { if (xhr.readyState == 4 && xhr.status == 200) { div.innerHTML = ''; // 5. 解析服务器返回的JSON格式的数据 var jsonObj = JSON.parse(xhr.responseText); var array = jsonObj.newslist; // 6. 通过DOM操作实现页面的局部刷新 for (var i = 0; i < array.length; i += 1) { var img = document.createElement('img'); img.src = array[i].picUrl; img.width = '250'; div.appendChild(img); } } }; // 4. 发出请求 xhr.send(); } else { alert('使用垃圾浏览器还想看美女，做梦！'); } }); })();","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example10","date":"2024-12-12T08:38:01.631Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example10/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example10/","excerpt":"","text":"* { margin: 0; padding: 0; } #container { margin: 20px 50px; } #fruits li { list-style: none; width: 200px; height: 50px; font-size: 20px; line-height: 50px; background-color: cadetblue; color: white; text-align: center; margin: 2px 0; } #fruits>li>a { float: right; text-decoration: none; color: white; position: relative; right: 5px; } #fruits~input { border: none; outline: none; font-size: 18px; } #fruits~input[type=text] { border-bottom: 1px solid darkgray; width: 200px; height: 50px; text-align: center; } #fruits~input[type=button] { width: 80px; height: 30px; background-color: coral; color: white; vertical-align: bottom; cursor: pointer; } 苹果× 香蕉× 火龙果× 西瓜× function removeItem(evt) { evt.preventDefault(); // $函数的第四种用法：参数是原生的JS对象 // 将原生的JS对象包装成对应的jQuery对象 $(evt.target).parent().remove(); } // $函数的第一种用法: 参数是另一个函数 // 传入的函数是页面加载完成之后要执行的回调函数 // $(document).ready(function() {}); $(function() { // $函数的第二种用法：参数是一个选择器字符串 // 获取元素并得到与之对应的jQuery对象（伪数组） $('#fruits a').on('click', removeItem); $('#ok').on('click', function() { var fruitName = $('#name').val().trim(); if (fruitName.length > 0) { $('#fruits').append( // $函数的第三种用法：参数是一个标签字符串 // 创建新元素并得到与之对应的jQuery对象 $('').text(fruitName).append( $('').attr('href', '').text('×').on('click', removeItem) ) ); } // 对jQuery对象使用下标运算或调用get()方法会得到原生JS对象 $('#name').val('').get(0).focus(); }); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example09","date":"2024-12-12T08:38:01.629Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example09/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example09/","excerpt":"","text":"#data { border-collapse: collapse; } #data td, #data th { width: 120px; height: 40px; text-align: center; border: 1px solid black; } #buttons { margin: 10px 0; } 数据统计表 姓名 年龄 性别 身高 体重 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 隔行换色 清除数据 删单元格 隐藏表格 $(function() { $('#hide').on('click', function() { // 根据样式表选择器获取元素获取到的不是原生的JS对象 // 而是经过jQuery封装过后的对象（有更多的方法方便操作） $('#data').fadeOut(2000); }); $('#remove').on('click', function() { $('#data tr:gt(0):last-child').remove(); }); $('#clear').on('click', function() { $('#data tr:gt(0)>td').empty(); }); $('#pretty').on('click', function() { $('#data tr:gt(0):odd').css({ 'background-color': '#ccc', 'font-size': '36px', 'font-weight': 'bolder' }); $('#data tr:gt(0):even').css('background-color', '#abc'); }); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example08","date":"2024-12-12T08:38:01.627Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example08/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example08/","excerpt":"","text":"#adv { width: 200px; height: 200px; color: yellow; position: fixed; right: 10px; top: 10px; background-color: blue; } #adv button { float: right; border: none; outline: none; color: white; background-color: gray; } 此广告位招租 关闭 +function() { var advDiv = document.querySelector('#adv'); var button = document.querySelector('#adv button'); var counter = 0; button.addEventListener('click', function() { counter += 1; if (counter < 3) { var currentStyle = document.defaultView.getComputedStyle(advDiv); var newTop = parseInt(currentStyle.top) + 20; var newRight = parseInt(currentStyle.right) + 20; advDiv.style.top = newTop + 'px'; advDiv.style.right = newRight + 'px'; } else { advDiv.style.display = 'none'; } }); }(); // 鼠标按下 - mousedown // 鼠标移动 - mousemove // 鼠标松开 - mouseup // clientX / clientY - 鼠标的横纵坐标","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example07","date":"2024-12-12T08:38:01.625Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example07/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example07/","excerpt":"","text":"* { margin: 0; padding: 0; } #container { margin: 20px 50px; } #fruits li { list-style: none; width: 200px; height: 50px; font-size: 20px; line-height: 50px; background-color: cadetblue; color: white; text-align: center; margin: 2px 0; } #fruits>li>a { float: right; text-decoration: none; color: white; position: relative; right: 5px; } #fruits~input { border: none; outline: none; font-size: 18px; } #fruits~input[type=text] { border-bottom: 1px solid darkgray; width: 200px; height: 50px; text-align: center; } #fruits~input[type=button] { width: 80px; height: 30px; background-color: coral; color: white; vertical-align: bottom; cursor: pointer; } 苹果× 香蕉× 火龙果× 西瓜× function removeItem(evt) { evt.preventDefault(); var a = evt.target || evt.srcElement; var li = a.parentNode; li.parentNode.removeChild(li); } function addItem() { var fruitName = input.value.trim(); if (fruitName.length > 0) { var li = document.createElement('li'); li.textContent = fruitName; li.style.backgroundColor = randomColor(); var a = document.createElement('a'); a.href = ''; a.textContent = '×'; a.addEventListener('click', removeItem); li.appendChild(a); ul.insertBefore(li, ul.firstChild); } input.value = ''; input.focus(); } var anchors = document.querySelectorAll('#fruits a'); for (var i = 0; i < anchors.length; i += 1) { anchors[i].addEventListener('click', removeItem); } var ul = document.getElementById('fruits'); var input = document.querySelector('#container input[type=text]'); input.addEventListener('keypress', function(evt) { var key = evt.keyCode || evt.which; if (key == 13) { addItem(); } }); var okButton = document.querySelector('#ok'); okButton.addEventListener('click', addItem);","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example06","date":"2024-12-12T08:38:01.622Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example06/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example06/","excerpt":"","text":"#buttons>button { border: none; outline: none; width: 120px; height: 40px; font: 22px/40px Arial; background-color: red; color: white; } 苹果 香蕉 草莓 蓝莓 榴莲 西瓜 芒果 柠檬 var buttons = document.querySelectorAll('#buttons>button'); for (var i = 0; i < buttons.length; i += 1) { buttons[i].firstChild.addEventListener('click', function(evt) { var checkbox = evt.target || evt.srcElement; if (checkbox.checked) { checkbox.parentNode.style.backgroundColor = 'lightseagreen'; } else { checkbox.parentNode.style.backgroundColor = 'red'; } evt.stopPropagation(); }); buttons[i].addEventListener('click', function(evt) { // 通过事件对象的target属性可以获取事件源（谁引发了事件） // 但是有的浏览器是通过srcElement属性获取事件源的 // 可以通过短路或运算来解决这个兼容性问题 var button = evt.target || evt.srcElement; // 当获取到一个元素之后可以通过它的属性来获取它的父元素、子元素以及兄弟元素 // parentNode - 父元素 // firstChild / lastChild / children - 第一个子元素 / 最后一个子元素 / 所有子元素 // previousSibling / nextSibling - 前一个兄弟元素 / 后一个兄弟元素 var checkbox = button.firstChild; checkbox.checked = !checkbox.checked; if (checkbox.checked) { button.style.backgroundColor = 'lightseagreen'; } else { button.style.backgroundColor = 'red'; } }); }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example05","date":"2024-12-12T08:38:01.620Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example05/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example05/","excerpt":"","text":"#one { width: 400px; height: 400px; background-color: indianred; margin: 60px auto; } #two { width: 300px; height: 300px; background-color: darkseagreen; } #three { width: 200px; height: 200px; background-color: lightsteelblue; } #two, #three { position: relative; left: 50px; top: 50px; } var one = document.querySelector('#one'); var two = document.querySelector('#two'); var three = document.querySelector('#three'); // addEventListener方法的第一个参数是事件名 // 第二个参数是事件发生时需要执行的回调函数 // 第三个参数是一个布尔值 // 如果是true表示事件捕获 - 从外层向内层传递事件 // 如果是false表示事件冒泡 - 从内存向外层传递事件 // 一般情况下事件处理的方式都是事件冒泡（默认行为） // 如果想阻止事件的传播行为可以调用事件对象的stopPropagation方法 one.addEventListener('click', function() { window.alert('I am one!'); }); two.addEventListener('click', function() { window.alert('I am two!'); }); // 事件回调函数中的第一个参数是事件对象（封装了和事件相关的信息） three.addEventListener('click', function(evt) { window.alert('I am three!'); evt.stopPropagation(); });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example04","date":"2024-12-12T08:38:01.618Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example04/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example04/","excerpt":"","text":"#adv { width: 705px; margin: 0 auto; } var index = 0; var images = ['slide-1.jpg', 'slide-2.jpg', 'slide-3.jpg', 'slide-4.jpg'] // 通过document对象获取页面元素的常用方法有5个: // document.getElementById('...') ==> 通过ID获取单个元素 // document.getElementsByTagName('...') ==> 通过标签名获取元素的列表 // document.getElementsByClassName('...') ==> 通过类名获取元素的列表 // document.querySelector('...') ==> 通过样式表选择器获取单个元素 // document.querySelectorAll('...') ==> 通过样式表选择器获取元素的列表 var img = document.querySelector('img'); // var img = document.getElementsByTagName('img')[0]; var timerId; startIt(); var div = document.querySelector('#adv'); div.addEventListener('mouseover', stopIt); div.addEventListener('mouseout', startIt); function startIt() { timerId = window.setInterval(function() { index += 1; index %= images.length; img.src = 'img/' + images[index]; }, 2000); } function stopIt() { window.clearInterval(timerId); }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example03","date":"2024-12-12T08:38:01.615Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example03/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example03/","excerpt":"","text":"延迟跳转 5秒钟以后自动跳转到百度 var countDown = 5; var span = document.getElementById('counter'); window.setTimeout(function() { countDown -= 1; if (countDown == 0) { // window对象的location属性代表浏览器地址栏 window.location.href = 'https://www.baidu.com'; } else { span.textContent = countDown; // arguments是函数中的隐含对象 // 通过arguments[0]、arguments[1]可以获得函数的参数 // 通过arguments.callee可以获得正在被调用的函数 window.setTimeout(arguments.callee, 1000); } }, 1000);","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example02","date":"2024-12-12T08:38:01.613Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example02/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example02/","excerpt":"","text":"成都机动车限行查询 #search { width: 640px; margin: 0 auto; text-align: center; margin-top: 150px; } #carno { display: inline-block; width: 460px; height: 36px; font: 36px/36px arial; text-align: center; vertical-align: middle; border: none; outline: none; border-bottom: 1px dotted darkgray; } #search input[type=button] { width: 80px; height: 36px; font: 28px/36px arial; border: none; color: white; background-color: red; vertical-align: middle; } #result { width: 640px; margin: 0 auto; text-align: center; font: 32px/36px arial; } var p = document.getElementById('result'); function clearResult() { p.textContent = ''; } function showResult() { var input = document.getElementById('carno'); var carNo = input.value; var regex = /^[京津沪渝辽吉黑冀鲁豫晋陕甘闽粤桂川云贵苏浙皖湘鄂赣青新宁蒙藏琼][A-Z]\\s*[0-9A-Z]{5}$/; if (regex.test(carNo)) { var digitStr = lastDigit(carNo); if (digitStr) { var digit = parseInt(digitStr); var day = new Date().getDay(); if (digit % 5 == day || digit % 5 == day - 5) { p.innerHTML = carNo + '今日限行' + p.innerHTML; } else { p.innerHTML = carNo + '今日不限行' + p.innerHTML; } } else { p.innerHTML = carNo + '不是有效的车牌号' + p.innerHTML; } } else { p.innerHTML = carNo + '不是有效的车牌号' + p.innerHTML; } input.value = ''; } function lastDigit(str) { for (var index = str.length - 1; index >= 0; index -= 1) { var digitStr = str[index]; if (digitStr >= '0' && digitStr","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/javascript/example01","date":"2024-12-12T08:38:01.608Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example01/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/javascript/example01/","excerpt":"","text":"h1 { font: 72px arial; } #bar { color: red; } .foo { color: green; } h1 { color: blue !important; } #timer { width: 250px; height: 30px; line-height: 30px; text-align: center; color: yellow; background-color: blue; float: right; } Hello, world! 关闭 打开百度 function openBaidu() { window.open('https://www.baidu.com', '', 'width=300,height=200'); } function shutdown() { if (window.confirm('确定要退出吗？')) { window.close(); } } var weekdays = ['日', '一', '二', '三', '四', '五', '六']; function showTime() { var now = new Date(); var year = now.getFullYear(); var month = now.getMonth() + 1; var date = now.getDate(); var hour = now.getHours(); var minute = now.getMinutes(); var second = now.getSeconds(); var day = now.getDay(); var timeStr = year + '年' + (month < 10 ? '0' : '') + month + '月' + (date < 10 ? '0' : '') + date + '日 ' + (hour < 10 ? '0' : '') + hour + ':' + (minute < 10 ? '0' : '') + minute + ':' + (second < 10 ? '0' : '') + second + ' 星期' + weekdays[day] + ''; var div = document.getElementById('timer'); div.innerHTML = timeStr; } showTime(); window.setInterval(showTime, 1000); // 1TBS风格 - C/Unix - Dennis M. Ritchie // Allman风格 - FreeBSD - Allman // while循环 / do-while循环 / for循环 // while循环 - 不确定循环的次数 // for循环 - 循环的次数是确定的 // do-while循环 - 至少执行一次 // var flag = true; // do { // var yearStr = window.prompt('请输入年份: '); // var year = parseInt(yearStr); // if (year == yearStr && year > 0) { // if (year % 4 == 0 && year % 100 != 0 // || year % 400 == 0) { // window.alert(year + '年是闰年'); // } else { // window.alert(year + '年不是闰年'); // } // flag = window.confirm('是否继续？'); // } else { // window.alert('请输入有效的年份!'); // } // } while (flag);","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/html+css/qq_link","date":"2024-12-12T08:38:01.600Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/qq_link/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/qq_link/","excerpt":"","text":"联系站长 关于 function showMsg() { alert('Hello, world!'); }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/html+css/form_and_table","date":"2024-12-12T08:38:01.597Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/form_and_table/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/form_and_table/","excerpt":"","text":"用户登录 body { width: 90%; margin: 0 auto; font-size: 16px; } #login { width: 290px; margin: 20px auto; } #login fieldset { border-radius: 5px; } #login legend { background-color: lightgray; padding: 2px 15px; border-radius: 5px; } #login span { display: inline-block; width: 60px; text-align: right; } #login input { margin: 12px 5px; border: none; } #login input[name^=\"user\"] { width: 175px; outline: none; border-bottom: 1px dotted darkgray; } #login input[type=\"submit\"] { margin-left: 195px; color: white; background-color: chocolate; border-radius: 5px; } #login input[type=\"submit\"]:hover { background-color: darkgreen; cursor: pointer; } #data { margin: 10px auto; border-collapse: collapse; } #data td { border-bottom: 1px solid gray; border-right: 1px solid gray; width: 160px; height: 60px; } #data td.tl { border-top-left-radius: 10px; } #data td.tr { border-top-right-radius: 10px; } #data td.bl { border-bottom-left-radius: 10px; } #data td.br { border-bottom-right-radius: 10px; } #data td.last { border-right: none; } #data td.first { width: 250px; padding-left: 10px; } #data td.center { color: white; text-align: center; } #data td.bottom { border-bottom: none; } #data tr.head { background-color:lightblue; } #data tr.odd { background-color: beige; } #data tr.even { background-color: blanchedalmond; } 用户登录 用户名: 密码: 邮箱: 成都 北京 杭州 Python从入门到住院全国巡演 2018年2月28日 上午9:30 2018年3月28日 上午9:30 2018年4月28日 上午9:30 MySQL从删库到跑路公开课 2018年2月27日 上午9:30 2018年3月5日 上午9:30 2018年4月2日 上午9:30 Django从学习到上吊交流会 2018年2月28日 上午9:30 2018年5月21日 上午9:30 爬虫从精通到坐牢无遮大会 2018年3月3日 上午9:30 2018年4月17日 上午9:30 2018年1月15日 上午9:30","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/html+css/example","date":"2024-12-12T08:38:01.594Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/example/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/example/","excerpt":"","text":"* { margin: 0; padding: 0; } .menu { margin: 20px 50px; } .menu li { list-style: none; width: 120px; height: 35px; line-height: 35px; color: white; text-align: center; border-bottom: 1px solid lightgray; } .menu>li { background-color: #8FBC8F; overflow: hidden; } .menu>li:hover { height: auto; } .menu li ul li { background-color: lightsteelblue; } Menu 1 Menu 1-1 Menu 1-2 Menu 1-3 Menu 2 Menu 2-1 Menu 2-2 Menu 3 Menu 3-1 Menu 3-2 Menu 3-3 Menu 3-4 Menu 3-5 Menu 4 Menu 4-1 Menu 4-2 Menu 5 Menu 5-1 Menu 5-2 Menu 5-3 Menu 5-4","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/old/html+css/classical_layout","date":"2024-12-12T08:38:01.590Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/classical_layout/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/old/html+css/classical_layout/","excerpt":"","text":"* { margin: 0; padding: 0; font-family: Verdana, '宋体'; } body { width: 960px; margin: 10px auto; background-image: url(img/dark-wood.jpg); } #header, #main, #footer { background-color: lightgray; margin: 10px 0; clear: both; } #header h1 { height: 60px; line-height: 60px; text-align: center; } #nav { height: 35px; text-align: center; } #nav ul li { line-height: 35px; width: 100px; margin:0 10px; display: inline-block; } a { text-decoration: none; } a:link, a:visited, a:active { color: black; } #nav ul li:hover { border-bottom: 4px solid red; } .feature { height: 250px; text-align: center; background-color: #ADD8E6; } .one, .two, .three { width: 300px; height: 150px; float: left; } .one { margin: 10px 15px 0 0; background-color: #FFEBCD; } .two { margin: 10px 15px; background-color: coral; } .three { margin: 10px 0 0 15px; background-color: darkseagreen; } #footer { text-align: center; line-height: 45px; height: 45px; font-size: 1.2em; } Logo Home Products Services About Contact &copy; Copyright 2011","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js/jquery.min","date":"2024-12-12T08:38:01.497Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js/jquery.min/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js/jquery.min/","excerpt":"","text":"/*! jQuery v3.3.1 | (c) JS Foundation and other contributors | jquery.org/license */ !function(e,t){\"use strict\";\"object\"==typeof module&&\"object\"==typeof module.exports?module.exports=e.document?t(e,!0):function(e){if(!e.document)throw new Error(\"jQuery requires a window with a document\");return t(e)}:t(e)}(\"undefined\"!=typeof window?window:this,function(e,t){\"use strict\";var n=[],r=e.document,i=Object.getPrototypeOf,o=n.slice,a=n.concat,s=n.push,u=n.indexOf,l={},c=l.toString,f=l.hasOwnProperty,p=f.toString,d=p.call(Object),h={},g=function e(t){return\"function\"==typeof t&&\"number\"!=typeof t.nodeType},y=function e(t){return null!=t&&t===t.window},v={type:!0,src:!0,noModule:!0};function m(e,t,n){var i,o=(t=t||r).createElement(\"script\");if(o.text=e,n)for(i in v)n[i]&&(o[i]=n[i]);t.head.appendChild(o).parentNode.removeChild(o)}function x(e){return null==e?e+\"\":\"object\"==typeof e||\"function\"==typeof e?l[c.call(e)]||\"object\":typeof e}var b=\"3.3.1\",w=function(e,t){return new w.fn.init(e,t)},T=/^[\\s\\uFEFF\\xA0]+|[\\s\\uFEFF\\xA0]+$/g;w.fn=w.prototype={jquery:\"3.3.1\",constructor:w,length:0,toArray:function(){return o.call(this)},get:function(e){return null==e?o.call(this):e1?(n=[e,e,\"\",t],r.setFilters.hasOwnProperty(e.toLowerCase())?se(function(e,n){var r,o=i(e,t),a=o.length;while(a--)e[r=O(e,o[a])]=!(n[r]=o[a])}):function(e){return i(e,0,n)}):i}},pseudos:{not:se(function(e){var t=[],n=[],r=s(e.replace(B,\"$1\"));return r[b]?se(function(e,t,n,i){var o,a=r(e,null,i,[]),s=e.length;while(s--)(o=a[s])&&(e[s]=!(t[s]=o))}):function(e,i,o){return t[0]=e,r(t,null,o,n),t[0]=null,!n.pop()}}),has:se(function(e){return function(t){return oe(e,t).length>0}}),contains:se(function(e){return e=e.replace(Z,ee),function(t){return(t.textContent||t.innerText||i(t)).indexOf(e)>-1}}),lang:se(function(e){return U.test(e||\"\")||oe.error(\"unsupported lang: \"+e),e=e.replace(Z,ee).toLowerCase(),function(t){var n;do{if(n=g?t.lang:t.getAttribute(\"xml:lang\")||t.getAttribute(\"lang\"))return(n=n.toLowerCase())===e||0===n.indexOf(e+\"-\")}while((t=t.parentNode)&&1===t.nodeType);return!1}}),target:function(t){var n=e.location&&e.location.hash;return n&&n.slice(1)===t.id},root:function(e){return e===h},focus:function(e){return e===d.activeElement&&(!d.hasFocus||d.hasFocus())&&!!(e.type||e.href||~e.tabIndex)},enabled:de(!1),disabled:de(!0),checked:function(e){var t=e.nodeName.toLowerCase();return\"input\"===t&&!!e.checked||\"option\"===t&&!!e.selected},selected:function(e){return e.parentNode&&e.parentNode.selectedIndex,!0===e.selected},empty:function(e){for(e=e.firstChild;e;e=e.nextSibling)if(e.nodeType=3?[null,e,null]:L.exec(e))||!i[1]&&t)return!t||t.jquery?(t||n).find(e):this.constructor(t).find(e);if(i[1]){if(t=t instanceof w?t[0]:t,w.merge(this,w.parseHTML(i[1],t&&t.nodeType?t.ownerDocument||t:r,!0)),A.test(i[1])&&w.isPlainObject(t))for(i in t)g(this[i])?this[i](t[i]):this.attr(i,t[i]);return this}return(o=r.getElementById(i[2]))&&(this[0]=o,this.length=1),this}return e.nodeType?(this[0]=e,this.length=1,this):g(e)?void 0!==n.ready?n.ready(e):e(w):w.makeArray(e,this)}).prototype=w.fn,q=w(r);var H=/^(?:parents|prev(?:Until|All))/,O={children:!0,contents:!0,next:!0,prev:!0};w.fn.extend({has:function(e){var t=w(e,this),n=t.length;return this.filter(function(){for(var e=0;e1&&(O[e]||w.uniqueSort(i),H.test(e)&&i.reverse()),this.pushStack(i)}});var M=/[^\\x20\\t\\r\\n\\f]+/g;function R(e){var t={};return w.each(e.match(M)||[],function(e,n){t[n]=!0}),t}w.Callbacks=function(e){e=\"string\"==typeof e?R(e):w.extend({},e);var t,n,r,i,o=[],a=[],s=-1,u=function(){for(i=i||e.once,r=t=!0;a.length;s=-1){n=a.shift();while(++s-1)o.splice(n,1),n-1:o.length>0},empty:function(){return o&&(o=[]),this},disable:function(){return i=a=[],o=n=\"\",this},disabled:function(){return!o},lock:function(){return i=a=[],n||t||(o=n=\"\"),this},locked:function(){return!!i},fireWith:function(e,n){return i||(n=[e,(n=n||[]).slice?n.slice():n],a.push(n),t||u()),this},fire:function(){return l.fireWith(this,arguments),this},fired:function(){return!!r}};return l};function I(e){return e}function W(e){throw e}function $(e,t,n,r){var i;try{e&&g(i=e.promise)?i.call(e).done(t).fail(n):e&&g(i=e.then)?i.call(e,t,n):t.apply(void 0,[e].slice(r))}catch(e){n.apply(void 0,[e])}}w.extend({Deferred:function(t){var n=[[\"notify\",\"progress\",w.Callbacks(\"memory\"),w.Callbacks(\"memory\"),2],[\"resolve\",\"done\",w.Callbacks(\"once memory\"),w.Callbacks(\"once memory\"),0,\"resolved\"],[\"reject\",\"fail\",w.Callbacks(\"once memory\"),w.Callbacks(\"once memory\"),1,\"rejected\"]],r=\"pending\",i={state:function(){return r},always:function(){return o.done(arguments).fail(arguments),this},\"catch\":function(e){return i.then(null,e)},pipe:function(){var e=arguments;return w.Deferred(function(t){w.each(n,function(n,r){var i=g(e[r[4]])&&e[r[4]];o[r[1]](function(){var e=i&&i.apply(this,arguments);e&&g(e.promise)?e.promise().progress(t.notify).done(t.resolve).fail(t.reject):t[r[0]+\"With\"](this,i?[e]:arguments)})}),e=null}).promise()},then:function(t,r,i){var o=0;function a(t,n,r,i){return function(){var s=this,u=arguments,l=function(){var e,l;if(!(t=o&&(r!==W&&(s=void 0,u=[e]),n.rejectWith(s,u))}};t?c():(w.Deferred.getStackHook&&(c.stackTrace=w.Deferred.getStackHook()),e.setTimeout(c))}}return w.Deferred(function(e){n[0][3].add(a(0,e,g(i)?i:I,e.notifyWith)),n[1][3].add(a(0,e,g(t)?t:I)),n[2][3].add(a(0,e,g(r)?r:W))}).promise()},promise:function(e){return null!=e?w.extend(e,i):i}},o={};return w.each(n,function(e,t){var a=t[2],s=t[5];i[t[1]]=a.add,s&&a.add(function(){r=s},n[3-e][2].disable,n[3-e][3].disable,n[0][2].lock,n[0][3].lock),a.add(t[3].fire),o[t[0]]=function(){return o[t[0]+\"With\"](this===o?void 0:this,arguments),this},o[t[0]+\"With\"]=a.fireWith}),i.promise(o),t&&t.call(o,o),o},when:function(e){var t=arguments.length,n=t,r=Array(n),i=o.call(arguments),a=w.Deferred(),s=function(e){return function(n){r[e]=this,i[e]=arguments.length>1?o.call(arguments):n,--t||a.resolveWith(r,i)}};if(t0||F.resolveWith(r,[w]))}}),w.ready.then=F.then;function _(){r.removeEventListener(\"DOMContentLoaded\",_),e.removeEventListener(\"load\",_),w.ready()}\"complete\"===r.readyState||\"loading\"!==r.readyState&&!r.documentElement.doScroll?e.setTimeout(w.ready):(r.addEventListener(\"DOMContentLoaded\",_),e.addEventListener(\"load\",_));var z=function(e,t,n,r,i,o,a){var s=0,u=e.length,l=null==n;if(\"object\"===x(n)){i=!0;for(s in n)z(e,t,s,n[s],!0,o,a)}else if(void 0!==r&&(i=!0,g(r)||(a=!0),l&&(a?(t.call(e,r),t=null):(l=t,t=function(e,t,n){return l.call(w(e),n)})),t))for(;s1,null,!0)},removeData:function(e){return this.each(function(){K.remove(this,e)})}}),w.extend({queue:function(e,t,n){var r;if(e)return t=(t||\"fx\")+\"queue\",r=J.get(e,t),n&&(!r||Array.isArray(n)?r=J.access(e,t,w.makeArray(n)):r.push(n)),r||[]},dequeue:function(e,t){t=t||\"fx\";var n=w.queue(e,t),r=n.length,i=n.shift(),o=w._queueHooks(e,t),a=function(){w.dequeue(e,t)};\"inprogress\"===i&&(i=n.shift(),r--),i&&(\"fx\"===t&&n.unshift(\"inprogress\"),delete o.stop,i.call(e,a,o)),!r&&o&&o.empty.fire()},_queueHooks:function(e,t){var n=t+\"queueHooks\";return J.get(e,n)||J.access(e,n,{empty:w.Callbacks(\"once memory\").add(function(){J.remove(e,[t+\"queue\",n])})})}}),w.fn.extend({queue:function(e,t){var n=2;return\"string\"!=typeof e&&(t=e,e=\"fx\",n--),arguments.length","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js/hello","date":"2024-12-12T08:38:01.494Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js/hello/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js/hello/","excerpt":"","text":"function randomColor(opacity=1) { let r = parseInt(Math.random() * 256) let g = parseInt(Math.random() * 256) let b = parseInt(Math.random() * 256) return `rgba(${r}, ${g}, ${b}, ${opacity})` }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css/style","date":"2024-12-12T08:38:01.282Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css/style/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css/style/","excerpt":"","text":"/* 通配符选择器 */ * { margin: 0; padding: 0; } /* 标签选择器 */ h1 { width: 960px; height: 40px; margin: 5px auto; } /* 类选择器 */ .a { background-color: red; } .b { background-color: orange; } .c { background-color: yellow; } .d { background-color: green; } .e { background-color: cyan; } .f { background-color: blue; } .g { background-color: purple; } .h { color: blue; text-align: center; width: 100px; height: 38px; overflow: hidden; } .big { font-size: 32px; } .normal { font-size: 18px; } .small { font-size: 12px; } /* ID选择器 */ #header, #footer { width: 800px; height: 120px; border: 1px solid red; margin: 10px auto; }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/shopping_cart","date":"2024-12-12T08:38:01.239Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/shopping_cart/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/shopping_cart/","excerpt":"","text":"* { margin: 0; padding: 0; } body { width: 960px; margin: 20px auto; } #cart { margin: 0 auto; width: 850px; } #cart-header { height: 40px; background-color: lightgray; margin-bottom: 20px; } #cart-header div { line-height: 40px; } .left { float: left; } .right { float: right; } .w110 { width: 100px; } .ml10 { margin-left: 10px; } .w120 { width: 120px; } .w250 { width: 250px; } .center { text-align: center; } .w20 { width: 20px; } .w90 { width: 90px; } .clear { clear: both; } #cart-items>div { height: 100px; } #cart-items>div>div { line-height: 100px; } .w250 span { display: inline-block; font-size: 12px; line-height: 16px !important; } .single-item { border-bottom: 1px solid gray; } .small-button { display: inline-block; width: 20px; height: 20px; border: none; } .big-button { color: white; background-color: red; display: inline-block; width: 120px; height: 40px; border: none; font-size: 22px; } #totalCount, #totalPrice { color: red; } #totalPrice { font: bolder 20px Arial; display: inline-block; width: 150px; } #cart a { text-decoration: none; } #cart a:link, #cart a:visited, #cart a:active { color: gray; } 全选 商品 单价 数量 小计 操作 海澜之家/Heilan Home春装商务白衬衫男修身HNCAD3A067Y 漂白(69) 漂 &yen;138.00 - + &yen;138.00 删除 HLA海澜之家长袖衬衫男牛津纺休闲干净透气HNEAJ1E048A浅灰 &yen;128.00 - + &yen;128.00 删除 HLA海澜之家牛津纺清新休闲衬衫2018春季新品质感柔软长袖衬衫男 &yen;99.00 - + &yen;99.00 删除 删除选中商品 总共选中了0件商品 总计: &yen;0.00 去结算 function calcTotal() { let amountsInput = $('.single-item input[type=text]') let pricesSpan = $('.single-item .price') let checkboxes = $('.single-item input[type=checkbox]') let totalAmount = 0 let totalPrice = 0 amountsInput.each(function(index) { if (checkboxes[index].checked) { let amount = parseInt($(this).val()) totalAmount += amount let price = parseFloat($(pricesSpan[index]).text()) let currentPrice = (price * amount).toFixed(2) $(this).parent().next().find('span').text(currentPrice) totalPrice += parseFloat(currentPrice) } }) $('#totalCount').text(totalAmount) $('#totalPrice').text('￥' + totalPrice.toFixed(2)) } $(function() { $('#selectAll').on('click', function(evt) { $('.single-item input[type=checkbox]').prop('checked', evt.target.checked) calcTotal() }) $('.single-item button').on('click', function(evt) { let op = $(evt.target).text() if (op == '-') { let numInput = $(evt.target).next() let num = parseInt(numInput.val()) if (num > 1) { numInput.val(num - 1) } } else { let numInput = $(evt.target).prev() let num = parseInt(numInput.val()) if (num < 200) { numInput.val(num + 1) } } $(evt.target).parent().parent().find('input[type=checkbox]').prop('checked', true) calcTotal() }) $('.single-item input[type=checkbox]').on('click', function() { calcTotal() }) $('.single-item a').on('click', function(evt) { if (confirm('确定要删除该商品吗?')) { $(evt.target).parent().parent().remove() calcTotal() } }) $('#clearSelected').on('click', function() { if (confirm('确定要删除选中的商品吗?')) { $('.single-item').each(function() { if ($(this).find('input:checkbox').prop('checked')) { $(this).remove() } }) calcTotal() } }) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/problem_of_float","date":"2024-12-12T08:38:01.236Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/problem_of_float/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/problem_of_float/","excerpt":"","text":"Parent Float - Problem body { width: 752px; font-family: Arial, Verdana, sans-serif; color: #665544; } /* CSS hack */ /* 问题：因为p标签的浮动边框会变成一条线 */ .container { border: 1px solid #665544; overflow: auto; } p { width: 230px; float: left; margin: 10px; } The Evolution of the Bicycle In 1817 Baron von Drais invented a walking machine that would help him get around the royal gardens faster. The device know as the Draisienne (or \"hobby horse\") was made of wood, and propelled by pushing your feed on the ground in a gliding movement. It was not seen a suitable for any place other than a well maintained pathway. In 1865, the velocipede (meaning \"fast foot\") attached pedals to the front wheel, but its wooden structure made it extremely uncomfortable. In 1870 the first all-metal machine appeared. The pedals were attached directly to the front wheel. Solid rubber tires and the long spokes of the large front wheel provided a much smoother ride than its predecessor. In 1817 Baron von Drais invented a walking machine that would help him get around the royal gardens faster. The device know as the Draisienne (or \"hobby horse\") was made of wood, and propelled by pushing your feed on the ground in a gliding movement.","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_7","date":"2024-12-12T08:38:01.234Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_7/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_7/","excerpt":"","text":"#container { width: 800px; height: 400px; margin: 10px auto; border: 1px solid black; overflow: hidden; } #buttons { width: 800px; margin: 10px auto; text-align: center; } #add, #fla { border: none; outline: none; width: 80px; height: 30px; background-color: red; color: white; font-size: 16px; cursor: pointer; } .small { width: 80px; height: 80px; float: left; } 添加 闪烁 $(() => { $('#add').on('click', (evt) => { $('#container').prepend( $('') .css('background-color', randomColor()) ) }) $('#fla').on('click', (evt) => { setInterval(() => { $('#container>div').each((index, div) => { $(div).css('background-color', randomColor()) }) }, 200) }) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_6","date":"2024-12-12T08:38:01.232Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_6/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_6/","excerpt":"","text":"#data { border-collapse: collapse; } #data td, #data th { width: 120px; height: 40px; text-align: center; border: 1px solid black; } #buttons { margin: 10px 0; } #adv { width: 200px; height: 200px; position: absolute; top: 10px; right: 10px; background-color: blue; } 数据统计表 姓名 年龄 性别 身高 体重 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 隔行换色 清除数据 删除一行 表格淡出 const pretty = document.querySelector('#pretty') pretty.addEventListener('click', (evt) => { let rows = document.querySelectorAll('#data>tbody>tr') rows.forEach((row, i) => { let color = (i % 2 == 0 ? 'lightgray' : 'lightseagreen') row.style.backgroundColor = color }) }) const clear = document.querySelector('#clear') clear.addEventListener('click', (evt) => { let cols = document.querySelectorAll('#data>tbody>tr>td') cols.forEach((col) => { col.innerHTML = '' }) }) const remove = document.querySelector('#remove') remove.addEventListener('click', (evt) => { let tbody = document.querySelector('#data>tbody') let lastRow = tbody.lastElementChild if (lastRow) { tbody.removeChild(lastRow) } }) var opacity = 100 var delta = -5 const table = document.querySelector('#data') const hide = document.querySelector('#hide') hide.addEventListener('click', (evt) => { let button = evt.target setTimeout(function() { opacity += delta table.style.opacity = opacity / 100 if (opacity == 0 || opacity == 100) { delta = -delta button.textContent = opacity == 0? '表格淡入' : '表格淡出' } else { setTimeout(arguments.callee, 50) } }, 50) }) let adv = document.querySelector('#adv') adv.addEventListener('click', (evt) => { // 读取样式 let currentStyle = document.defaultView.getComputedStyle(adv) let top = parseInt(currentStyle.top) + 5 // 修改样式 adv.style.top = top + 'px' let right = parseInt(currentStyle.right) + 5 adv.style.right = right + 'px' })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_5","date":"2024-12-12T08:38:01.230Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_5/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_5/","excerpt":"","text":"给多个元素绑定事件 #buttons>button { border: none; outline: none; width: 120px; height: 40px; font: 22px/40px Arial; background-color: red; color: white; } 苹果 香蕉 草莓 蓝莓 榴莲 西瓜 芒果 柠檬 const allButtons = document.querySelectorAll('#buttons>button') allButtons.forEach(button => { button.addEventListener('click', () => { let checkbox = button.firstChild checkbox.checked = !checkbox.checked // button.style.backgroundColor = // checkbox.checked? 'green' : 'red' button.style.backgroundColor = randomColor() }) }) // textContent / innerHTML // image.src / image.title / image.style / checkbox.checked // ES6的做法: // for (let i = 0; i < allButtons.length; i += 1) { // // 闭包(closure) - 将一个块级作用域的变量生命周期延长至了全局 // allButtons[i].addEventListener('click', () => { // let checkbox = allButtons[i].firstChild // checkbox.checked = !checkbox.checked // allButtons[i].style.backgroundColor = // checkbox.checked? 'green' : 'red' // }) // } // ES5的做法: // for (var i = 0; i < allButtons.length; i += 1) { // allButtons[i].addEventListener('click', function(evt) { // // 我们知道事件发生时要做什么但我们不知道事件什么时候发生 // // 在这种情况下就需要放置一个回调函数(callback function) // // 绑定事件时传入的函数通常我们称之为回调函数 // // 当浏览器执行该回调函数时会传入一个代表事件的对象 // 通过事件对象的target属性就可以获取到事件源(谁引发了事件) // var checkbox = evt.target.firstChild // checkbox.checked = !checkbox.checked // evt.target.style.backgroundColor = // checkbox.checked? 'green' : 'red' // }) //}","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_4","date":"2024-12-12T08:38:01.228Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_4/","excerpt":"","text":"轮播广告 #adv { width: 600px; margin: 0 auto; } // document.getElementById('标识') ---> HTMLElement [!] // document.getElementsByTagName('标签名') ---> NodeList // document.getElementsByClassName('类名') ---> NodeList // document.querySelector('样式表选择器') ---> HTMLElement [!] // document.querySelectorAll('样式表选择器') ---> NodeList [!] // firstChild / lastChild / children --- 获取子节点 // parentNode --- 获取父节点 // previousSibling / nextSibling --- 获取兄弟 const img = document.querySelector('#adv>img') const imageNames = ['slide-1', 'slide-2', 'slide-3', 'slide-4'] var imageIndex = 0 function switchImage() { imageIndex += 1 imageIndex %= imageNames.length img.src = 'images/' + imageNames[imageIndex] + '.jpg' } var timerId = setInterval(switchImage, 2000) img.addEventListener('mouseover', () => clearInterval(timerId)) img.addEventListener('mouseout', () => { timerId = setInterval(switchImage, 2000) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_3","date":"2024-12-12T08:38:01.226Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_3/","excerpt":"","text":"成都机动车限行查询 #search { width: 640px; margin: 0 auto; text-align: center; margin-top: 150px; } #carno { display: inline-block; width: 460px; height: 36px; font: 36px/36px arial; text-align: center; vertical-align: middle; border: none; outline: none; border-bottom: 1px dotted darkgray; } #search input[type=button] { width: 80px; height: 36px; font: 28px/36px arial; border: none; color: white; background-color: red; vertical-align: middle; } #result { width: 640px; margin: 0 auto; text-align: center; font: 32px/36px arial; } const carnoInput = document.getElementById('carno') const result = document.getElementById('result') const pattern = /^[京津沪渝辽吉黑冀鲁豫晋陕甘闽粤桂川云贵苏浙皖湘鄂赣青新宁蒙藏琼][A-Z]\\s*[0-9A-Z]{5}$/i function clearResult() { carnoInput.value = '' result.textContent = '' } function showResult() { let carno = carnoInput.value.trim() if (pattern.test(carno)) { let num = getLastDigit(carno) if (num) { let day = new Date().getDay() if (day == 0 || day == 6) { result.textContent = '节假日不限行' } else if (num % 5 == day || num % 5 == day - 5) { result.textContent = carno + '今日限行' } else { result.textContent = carno + '今日不限行' } return } } alert('请输入有效的车牌号') } function getLastDigit(carno) { let len = carno.length for (let i = len - 1; i >= len - 5; i -= 1) { if ('0'","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_2","date":"2024-12-12T08:38:01.224Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_2/","excerpt":"","text":"h1 { width: 600px; height: 40px; font-size: 36px; line-height: 40px; margin: 0 auto; border: 1px solid red; } 欢迎来到千锋教育成都校区学习&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; let content = '马化腾，fuck you, 操你大爷，日你二爷' // let pattern = new RegExp('马化腾|[日操]', 'g') let pattern = /fuck|马化腾|[日操]/gi content = content.replace(pattern, '*') alert(content) const welcome = document.getElementById('welcome') function move() { let str = welcome.textContent str = str.substring(1) + str.charAt(0) welcome.textContent = str } setInterval(move, 500)","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/js_practice_1","date":"2024-12-12T08:38:01.222Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/js_practice_1/","excerpt":"","text":"乘法口诀表 table { border: 730px; border-collapse: collapse; margin: 0 auto; } td { border: 1px solid black; width: 80px; text-align: center; } +function showTable() { document.write('') for (let i = 1; i","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/index","date":"2024-12-12T08:38:01.219Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/index/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/index/","excerpt":"","text":"骆昊的技术专栏 h1 { color: purple; /* 颜色 */ font-size: 2cm; /* 字体大小 */ font-family: \"华文宋体\"; /* 字体 */ } p { font-size: 1cm; } p:first-letter { font-size: 1.5cm; color: blue; } .a { color: green; } .b { color: red; } a { color: red; text-decoration: none; } a:hover { color: green; } 革命理想 坚持马克思 &lt;jackfrued&gt;的博客 - H2O 骆昊喜欢的网站 百度 京东 视频网站 优酷 爱奇艺 腾讯视频 Python 面向对象的编程语言 基本概念：类、对象 三大支柱：封装、继承、多态 动态弱类型语言，需要Python解释器才能执行 床&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前明月光1 疑似地上霜&copy; 举头&trade;望明月2 低头思故乡 确定 关闭 回顶部 联系站长 function shutdown() { if (window.confirm('确定要关闭吗?')) { window.close() } } // 驼峰命名法 - Camel Notation function showWriter() { for (var i = 1; i","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_vue_element","date":"2024-12-12T08:38:01.216Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_vue_element/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_vue_element/","excerpt":"","text":"Vue入门 /* #emp { border-collapse: collapse; } #emp td, #emp th { border-bottom: 1px solid black; width: 120px; text-align: center; } */ #page { width: 100%; text-align: center; margin-top: 20px; } /* #page a { text-decoration: none; color: #67C23A; cursor: pointer; margin: 20px 20px; } */ h1 { color:#909399; } 员工信息表 上一页 下一页 let pageSize = 5 const app = new Vue({ el: '#app', data: { emps: [], currentPage: 1, totalPage: 0, loading: true }, created() { this.getEmpData() }, methods: { getEmpData() { this.loading = true const url = `http://120.77.222.217/api/emps/?page=${this.currentPage}` fetch(url, { headers: { \"token\": \"35ad60445cea11e99e1000163e02b646\", } }) .then(resp => resp.json()) .then(json => { setTimeout(() => { this.emps = json.results this.totalPage = parseInt((json.count - 1) / pageSize) + 1 this.loading = false }, 1000) }) }, prevPage() { if (this.currentPage > 1) { this.currentPage -= 1 this.getEmpData() } }, nextPage() { if (this.currentPage < this.totalPage) { this.currentPage += 1 this.getEmpData() } } } })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_table","date":"2024-12-12T08:38:01.214Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_table/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_table/","excerpt":"","text":"表格 table { border-spacing: 0; } td, th { width: 150px; height: 30px; border-bottom: 1px solid black; border-right: 1px solid black; text-align: center; empty-cells: hide; } .bottom>td { border-bottom: none; } .right { border-right: none; } thead>tr { background-color: lightgoldenrodyellow; } .even { background-color: lightgrey; } .odd { background-color: lightblue; } th:first-child { border-top-left-radius: 10px; } th:last-child { border-top-right-radius: 10px; } tr:last-child>td:first-child { border-bottom-left-radius: 10px; } tr:last-child>td:last-child { border-bottom-right-radius: 10px; } .number li { /* list-style-type: none; */ list-style-position: inside; list-style-image: url(images/add.gif); width: 120px; height: 30px; background-color: #006633; color: white; text-align: center; margin: 5px 5px; float: left; } .number li:hover { color: orange; cursor: pointer; } 学生考试成绩表 姓名 语文 数学 英语 体育 王大锤 90 80 60 王大锤 90 70 60 王大锤 90 80 70 60 One Two Three Four Five","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_layout","date":"2024-12-12T08:38:01.212Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_layout/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_layout/","excerpt":"","text":"经典布局 #page { width: 960px; margin: 0 auto; } header, footer, #main { margin: 10px 0; } header { height: 150px; background-color: lightgoldenrodyellow; } #main { height: 600px; background-color: lightpink; } footer { height: 150px; background-color: lightsalmon; } #logo { height: 70%; } nav { height: 30%; background-color: lightgray; } #content { float: left; width: 74%; height: 100%; margin-right: 1%; background-color: lightgreen; } article { height: 33%; border: 1px dashed black; } aside { float: left; width: 25%; height: 100%; background-color: lightcyan; } 页眉 导航菜单 文章1 文章2 文章3 侧边栏 页脚","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_7","date":"2024-12-12T08:38:01.211Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_7/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_7/","excerpt":"","text":"function add() { alert(arguments.callee) alert(arguments.callee.caller) return arguments[0] + arguments[1] } alert(add(1, 2))","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_6","date":"2024-12-12T08:38:01.209Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_6/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_6/","excerpt":"","text":"* { margin: 0; padding: 0; } #fruits { width: 120px; margin: 20px 20px; } #fruits>li { list-style-type: none; height: 40px; color: white; background-color: #009966; line-height: 40px; text-align: center; margin-top: 2px; } #fruits>li>a { float: right; color: white; text-decoration: none; } #fruits~input { border: none; outline: none; text-align: center; margin: 20px 15px; } input[type=text] { border-bottom: 1px solid gray !important; } #ok { width: 80px; height: 30px; background-color: #CC3333; color: white; } 苹果× 香蕉× 火龙果× 西瓜× function addListItem() { let name = input.value.trim() if (name) { // 通过document对象的createElement方法创建新元素 let li = document.createElement('li') // 可以用textContent或innerHTML属性来修改标签的内容 li.innerHTML = name let a = document.createElement('a') a.innerHTML = '&times;' a.href = '' a.addEventListener('click', removeListItem) li.appendChild(a) // 通过父元素的appendChild或insertBefore可以添加子元素 ul.appendChild(li) input.value = '' // 元素的focus和blur方法可以让元素获得或失去焦点 input.focus() } } function removeListItem(evt) { // evt.stopPropagation() // 通过事件对象的preventDefault方法阻止事件的默认行为 evt.preventDefault() // 通过元素获取父元素 - parentNode // 通过元素获取子元素 - children / firstElementChild / lastElementChild // 通过元素获取兄弟元素 - previousElementSibling / nextElementSibling let li = evt.target.parentNode // 通过父元素的removeChild方法可以删除指定的子元素 ul.removeChild(li) } const ul = document.querySelector('#fruits') const input = ul.nextElementSibling input.addEventListener('keypress', (evt) => { let code = evt.keyCode || evt.which if (code == 13) { addListItem() } }) const ok = input.nextElementSibling ok.addEventListener('click', addListItem) let anchors = document.querySelectorAll('#fruits>li>a') for (let i = 0; i < anchors.length; i += 1) { // addEventListener方法有三个参数 // 第一个参数是事件的名称，例如: click / dbclick / mouseover / mouseout // 第二个参数是事件发生时要执行的回调函数，函数的参数是事件对象: // ~ 传入一个已有函数的名字 // ~ 写一个匿名函数 function(evt) {} // ~ 写一个箭头函数 (evt) => {} // 第三个参数表示使用事件捕获还是事件冒泡,如果不写表示事件冒泡(从里向外传播事件) // ~ 如果设置为true表示事件捕获(从外向里传播事件) // ~ 如果希望阻止事件的传播行为可以调用事件对象的stopPropagation方法 anchors[i].addEventListener('click', removeListItem) }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_5","date":"2024-12-12T08:38:01.208Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_5/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_5/","excerpt":"","text":"类和对象 class Person { constructor(name, age) { this.name = name this.age = age } eat(food) { alert(`${this.name}正在吃${food}`) } watchAv() { if (this.age < 18) { alert(`${this.name}只能看《熊出没》`) } else { alert(`${this.name}正在观看岛国爱情动作片`) } } } let person = new Person('骆昊', 39) person.eat('蛋炒饭') let person2 = new Person('王大锤', 15) person2.watchAv() // 构造器函数 /* function Person(name, age) { this.name = name this.age = age } Person.prototype.eat = function(food) { alert(this.name + '正在吃' + food) } Person.prototype.watchAv = function() { if (this.age < 18) { alert(this.name + '只能看《熊出没》') } else { alert(this.name + '正在观看岛国爱情动作片') } } let person = new Person('骆昊', 39) person.eat('蛋炒饭') let person2 = new Person('王大锤', 15) person2.watchAv() */ // JSON - JavaScript Object Notation // JavaScript对象表达式 - 创建对象的字面量语法 /* const person = { name: '骆昊', age: 39, eat: function(food) { alert(this.name + '正在吃' + food) }, watchAv: function() { if (this.age < 18) { alert(this.name + '只能看《熊出没》') } else { alert(this.name + '正在观看岛国爱情动作片') } } } // person.age = 15 person.eat('蛋炒饭') person.watchAv() */","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_4","date":"2024-12-12T08:38:01.206Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_4/","excerpt":"","text":"显示时间日期 #clock { color: white; text-align: center; position: absolute; right: 10px; top: 10px; width: 280px; height: 40px; background-color: blue; font: 16px/40px \"pt mono\"; } // JavaScript - 面向对象编程 // 定义类创建对象 const clockDiv = document.getElementById('clock') const weekdays = ['日', '一', '二', '三', '四', '五', '六'] function showClock() { let now = new Date() let year = now.getFullYear() let mon = now.getMonth() + 1 mon = mon < 10 ? '0' + mon : '' + mon let date = now.getDate() date = date < 10 ? '0' + date : '' + date let hour = now.getHours() hour = hour < 10 ? '0' + hour : '' + hour let min = now.getMinutes() min = min < 10 ? '0' + min : '' + min let sec = now.getSeconds() sec = sec < 10 ? '0' + sec : '' + sec let day = now.getDay() let fullStr = `${year}年${mon}月${date}日 ${hour}:${min}:${sec} 星期${weekdays[day]}` clockDiv.textContent = fullStr } showClock() setInterval(showClock, 1000) // var timerId = setInterval(showClock, 1000) // clearInterval(timerId)","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_3","date":"2024-12-12T08:38:01.204Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_3/","excerpt":"","text":"双色球随机选号 p { width: 100px; height: 100px; color: white; font: 60px/100px Arial; border-radius: 50px; text-align: center; float: left; margin-left: 10px; } .red { background-color: red; } .blue { background-color: blue; } function outputBall(num, color='red') { document.write(``) if (num < 10) { document.write('0') } document.write(num) document.write('') } var selectedBalls = [] while (selectedBalls.length < 6) { let num = parseInt(Math.random() * 33 + 1) if (selectedBalls.indexOf(num) == -1) { selectedBalls.push(num) } } // 给红色球排序 - 需要传入一个匿名函数给出比较元素的规则 // ES6开始支持使用箭头函数(Lambda函数 - 匿名函数) selectedBalls.sort((x, y) => x - y) // 代码有很多种坏味道 重复是最坏的一种 - Martin Fowler selectedBalls.forEach(item => outputBall(item)) let num = parseInt(Math.random() * 16 + 1) outputBall(num, 'blue')","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_2","date":"2024-12-12T08:38:01.201Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_2/","excerpt":"","text":"判断闰年 // var / let / const // JavaScript中的比较运算符带有隐式的类型转换 // === / !== / && / || / ! // 分支结构 - if...else... / switch...case...default... // 循环结构 - while / for / do...while // JavaScript中的数据类型: // - 简单数据类型: string / number / boolean / undefined / symbol / null // - 复杂数据类型: object // 检查变量或常量的数据类型可以使用typeof关键字 var yearStr = prompt('请输入年份: ') var year = parseInt(yearStr) // && - and - 短路与运算 // 一对花括号可以构成一个块级作用域 - let定义的变量就是块级作用域变量 if (year == yearStr && year > 0) { // || - or - 短路或运算 // 逻辑运算(短路运算)会产生一个布尔值要么是true要么是false let isLeap = (year % 4 == 0 && year % 100 != 0) || year % 400 == 0 let yesOrNo = isLeap? '是' : '不是' // ES6语法中的带占位符的字符串 alert(`${year}年${yesOrNo}闰年`) } else { alert('请输入有效的年份') }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_1","date":"2024-12-12T08:38:01.199Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_js_1/","excerpt":"","text":"猜数字 var answer = parseInt(Math.random() * 100 + 1) var counter = 0 var yourInput do { counter += 1 yourInput = prompt('请输入你猜的数字: ') if (yourInput > answer) { alert('小一点') } else if (yourInput < answer) { alert('大一点') } else { alert('恭喜你猜对了') } } while (answer != yourInput) if (counter > 7) { alert('智商余额不足') }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_5","date":"2024-12-12T08:38:01.197Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_5/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_5/","excerpt":"","text":"Ajax请求 #emp { border-collapse: collapse; } #emp td, #emp th { border-bottom: 1px solid black; width: 120px; text-align: center; } #page { width: 720px; text-align: center; } #page a { text-decoration: none; color: darkcyan; } 编号 姓名 职位 工资 补贴 所在部门 上一页&nbsp;&nbsp; 下一页 $(() => { function getEmpData(url) { $.ajax({ url: url, type: 'get', data: {}, dataType: 'json', headers: { \"token\": \"35ad60445cea11e99e1000163e02b646\" }, success: (json) => { $('#emp>tbody').empty() json.results.forEach((emp) => { let tr = $('') .append($('').text(emp.no)) .append($('').text(emp.name)) .append($('').text(emp.job)) .append($('').text(emp.sal)) .append($('').text(emp.comm)) .append($('').text(emp.dept.name)) $('#emp>tbody').append(tr) }) $('#prev').attr('href', json.previous? json.previous : '') $('#next').attr('href', json.next? json.next : '') } }) } function changePage(evt) { evt.preventDefault() let href = $(evt.target).attr('href') if (href) { getEmpData(href) } } getEmpData('http://120.77.222.217/api/emps/') $('#prev').on('click', changePage) $('#next').on('click', changePage) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_4","date":"2024-12-12T08:38:01.195Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_4/","excerpt":"","text":"Ajax请求 加载更多 $(() => { const url = 'http://api.tianapi.com/meinv/' var page = 0 $('#load').on('click', (evt) => { page += 1 let data = {\"key\": \"772a81a51ae5c780251b1f98ea431b84\", \"page\": page} $.ajax({ \"url\": url, // 请求的地址(统一资源定位符) \"type\": \"get\", // 请求的方法(get/post/delete/put) \"data\": data, // 发送给服务器的数据 \"dataType\": \"json\", // 服务器返回的数据类型 \"headers\": {}, // 请求头 \"success\": (json) => { // 请求成功后要执行的回调函数 json.newslist.forEach((mm) => { $('#photos').prepend($('').attr('src', mm.picUrl)) }) }, \"error\": (error) => { // 请求失败后要执行的回调函数 } }) // $对象的getJSON方法可以执行异步请求(get请求)获得服务器返回的JSON格式的数据 // 第一个参数是请求的URL(统一资源定位符) // 第二个参数是要发送给服务器的数据(JSON格式), 如果没有数据发给服务器可以省略不写 // 第三个参数是请求成功服务器返回数据之后要执行的回调函数, 其参数为服务器返回的内容处理后的JSON对象 // $.getJSON(url, data, (json) => { // json.newslist.forEach((mm) => { // $('#photos').prepend($('').attr('src', mm.picUrl).attr('width', '300')) // }) // }) }) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_3","date":"2024-12-12T08:38:01.192Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_3/","excerpt":"","text":"Ajax请求 加载更多 (() => { const photos = document.querySelector('#photos') const loadBtn = document.querySelector('#load') const url = 'http://api.tianapi.com/meinv/?key=772a81a51ae5c780251b1f98ea431b84&page=' var page = 0 loadBtn.addEventListener('click', (evt) => { page += 1 // 创建异步请求对象 let xhr = new XMLHttpRequest() // open方法的第一个参数是请求类型, 第二个参数是请求的URL, 第三个参数必须设置为true(异步请求) xhr.open('get', url + page, true) // 绑定事件回调函数,在收到服务器返回的数据后要对页面进行局部刷新 xhr.addEventListener('readystatechange', () => { if (xhr.readyState == 4 && xhr.status == 200) { // 将返回的JSON字符串解析成JavaScript的对象 let json = JSON.parse(xhr.responseText) json.newslist.forEach((mm) => { let img = document.createElement('img') img.src = mm.picUrl img.width = '300' photos.insertBefore(img, photos.firstElementChild) }) } }) // 发送异步请求 xhr.send() }) })()","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_2","date":"2024-12-12T08:38:01.191Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_2/","excerpt":"","text":"* { margin: 0; padding: 0; } #fruits { width: 120px; margin: 20px 20px; } #fruits>li { list-style-type: none; height: 40px; color: white; background-color: #009966; line-height: 40px; text-align: center; margin-top: 2px; } #fruits>li>a { float: right; color: white; text-decoration: none; } #fruits~input { border: none; outline: none; text-align: center; margin: 20px 15px; } input[type=text] { border-bottom: 1px solid gray !important; } #ok { width: 80px; height: 30px; background-color: #CC3333; color: white; } 苹果× 香蕉× 火龙果× 西瓜× function removeListItem(evt) { evt.preventDefault() // $函数的参数是一个原生的JavaScript对象,返回与原生JavaScript对象对应的jQuery对象 $(evt.target).parent().remove() } // $函数的四种用法: // $函数的参数是一个匿名函数或箭头函数,传入的函数是页面加载完成后要执行的回调函数 $(() => { // $函数的参数是一个样式表选择器字符串,获取页面元素得到一个jQuery对象(伪数组 - 数组中装的是原生JavaScript对象) $('#fruits>li>a').on('click', removeListItem) $('#ok').on('click', (evt) => { let input = $('#ok').prev(); let name = input.val().trim() if (name) { $('#fruits').append( // $函数的参数是一个标签字符串,创建一个新的元素并获得对应的jQuery对象 $('').text(name).append( $('').html('&times;').on('click', removeListItem) ) ) } input.val('').get(0).focus() }) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_1","date":"2024-12-12T08:38:01.189Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_jquery_1/","excerpt":"","text":"#data { border-collapse: collapse; } #data td, #data th { width: 120px; height: 40px; text-align: center; border: 1px solid black; } #buttons { margin: 10px 0; } 数据统计表 姓名 年龄 性别 身高 体重 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 Item1 Item2 Item3 Item4 Item5 隔行换色 清除数据 删除一行 表格淡出 // $('选择器') --> jQuery对象 --> 封装了很多的方法 $('#pretty').on('click', () => { $('#data>tbody>tr:odd').css('background-color', 'lightgray') $('#data>tbody>tr:even').css('background-color', 'lightgreen') }) $('#clear').on('click', () => { $('#data>tbody>tr>td').empty() }) $('#remove').on('click', () => { $('#data>tbody>tr:last-child').remove() }) $('#hide').on('click', () => { let title = $('#hide').text() if (title == '表格淡出') { $('#data').fadeOut(1000, () => { $('#hide').text('表格淡入') }) } else { $('#data').fadeIn(2000, () => { $('#hide').text('表格淡出') }) } })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_iframe","date":"2024-12-12T08:38:01.187Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_iframe/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_iframe/","excerpt":"","text":"内部窗口 1 2 3 1 2 3 1 2 3 确定 取消 百度&nbsp;&nbsp;京东","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_form","date":"2024-12-12T08:38:01.185Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_form/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_form/","excerpt":"","text":"表单的例子 - 注册 /* 属性选择器 */ /* form input 后代选择器 */ /* form>input 儿子选择器 */ form input[type=text], form input[type=password] { border: none; outline: none; border-bottom: 1px dotted darkgray; } /* form~input 兄弟选择器 */ /* form+input 相邻兄弟选择器 */ form~p>input[type=text] { outline: none; border: 1px solid lightgray; } form~p>input[type=text]:focus { outline: none; border: 1px solid #00FFFF; } .button { display: inline-block; color: white; background-color: red; border: none; width: 120px; height: 40px; } 图1. 这是一个图片 用户基本信息 用户名: 密码： 确认密码： 性别： 男 女 爱好： 阅读 旅游 美食 运动 省份： 北京 天津 上海 重庆 四川省 生日： 用户附加信息 邮箱： 头像： 自我介绍：","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_5","date":"2024-12-12T08:38:01.183Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_5/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_5/","excerpt":"","text":"CSS - 动画效果 #one, #two, #three { position: fixed; width: 200px; height: 200px; top: 100px; } #one { background-color: purple; left: 100px; animation: foo 10s; } #two { background-color: gold; left: 400px; animation: foo 2s infinite; } #three { background-color: darkgreen; left: 700px; animation: foo 0.5s infinite; } @keyframes foo { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_4","date":"2024-12-12T08:38:01.181Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_4/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_4/","excerpt":"","text":"CSS - 定位 .one { background-color: red; left: 50px; top: 50px; z-index: 100; } .two { background-color: green; left: 100px; top: 100px; z-index: 20; } .three { background-color: blue; left: 150px; top: 150px; z-index: 10; } .one, .two, .three { position: absolute; width: 200px; height: 200px; }","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_3","date":"2024-12-12T08:38:01.179Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_3/","excerpt":"","text":"CSS - 盒子模型 @font-face { font-family: \"FatDog\"; src: url('fonts/chunkfive.ttf'); } body { width: 960px; margin: 0 auto; } .ms { letter-spacing: 10px; width: 320px; height: 100px; border: 5px dotted gray; line-height: 100px; text-align: center; margin: 10px auto; text-decoration: underline; text-shadow: 2px 2px gray; font-size: 400%; } .one { text-indent: 60px; } .two { text-indent: 40px; } .three { text-indent: 20px; } h3 { /* block / inline-block / none */ /* display: none; */ visibility: hidden; text-transform: uppercase; font: italic bolder 2cm/60px \"FatDog\"; /* font-size: 2cm; font-family: \"FatDog\"; font-stretch: condensed; font-style: oblique; */ } .box { color: rgb(64, 128, 192); height: 220px; border: 4px double #00CC33; border-left-color: blue; border-right-color: red; border-radius: 10px 20px 40px 80px; width: 50%; margin: 50px auto; margin-top: 10px; text-align: center; background-color: #749ABE; background-image: url(images/bird.gif); /* background-repeat: no-repeat no-repeat; background-position: -25px -50px; */ } .box p { background-color: white; width: 200px; height: 40px; line-height: 40px; margin: 10px auto; } #photo { border: 10px solid gold; border-radius: 60px 60px 60px 60px; /* border-image: url(\"images/dots.gif\") 10 10 10 10 round; */ } #icon { width: 100px; height: 100px; background: url(images/icons.jpg) no-repeat no-repeat -570px -280px; background-color: #00FFFF; /* background-image: url(images/icons.jpg); background-repeat: no-repeat no-repeat; background-position: -570px -280px; */ } #python { width: 290px; height: 82px; background-image: url(images/python-logo.png); background-color: #009966; } /* body { background-image: url(images/bird.gif); } */ Hello, world! 静夜思 床前明月光 疑似地上霜 举头望明月 低头思故乡","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_2","date":"2024-12-12T08:38:01.177Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_2/","excerpt":"","text":"CSS - 优先级 #h1 { color: blue; } .h1 { color: green !important; } .h1 { color: pink !important; border: 5px dotted #FFA500; width: 300px; height: 80px; line-height: 80px; text-align: center; margin-top: 50px; padding: 100px 100px; } h1 { color: red; } Hello, world! Goodbye world!","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_1","date":"2024-12-12T08:38:01.176Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_css_1/","excerpt":"","text":"CSS - 内部和外部样式表 * { border: 1px dashed black; } 静夜思 - 李白 床前明月光","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_bootstrap","date":"2024-12-12T08:38:01.174Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_bootstrap/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_bootstrap/","excerpt":"","text":"h3. 这是一套可视化布局系统. 首页 简介 信息 下拉 操作 设置栏目 更多设置 分割线 First Thumbnail label Cras justo odio, dapibus ac facilisis in, egestas eget quam. Donec id elit non mi porta gravida at eget metus. Nullam id dolor id nibh ultricies vehicula ut id elit. Second Thumbnail label Cras justo odio, dapibus ac facilisis in, egestas eget quam. Donec id elit non mi porta gravida at eget metus. Nullam id dolor id nibh ultricies vehicula ut id elit. Third Thumbnail label Cras justo odio, dapibus ac facilisis in, egestas eget quam. Donec id elit non mi porta gravida at eget metus. Nullam id dolor id nibh ultricies vehicula ut id elit.","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_bom_1","date":"2024-12-12T08:38:01.172Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_bom_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_bom_1/","excerpt":"","text":"BOM - 延迟跳转 5秒钟以后自动跳转到百度 取消跳转 打开百度 // alert(localStorage.sport) // alert(localStorage.fruit) // localStorage.sport = '乒乓球' // localStorage.fruit = '榴莲' const openBtn = document.getElementById('openBtn') openBtn.addEventListener('click', () => print()) const cancelBtn = document.getElementById('cancelBtn') cancelBtn.addEventListener('click', () => clearInterval(timerId)) const span = document.getElementById('countdown') var counter = 5 var timerId = setInterval(function () { counter -= 1 if (counter == 0) { // location对象代表了浏览器的地址栏 location.href = 'http://www.baidu.com' } else { span.textContent = counter } }, 1000)","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_audio_video","date":"2024-12-12T08:38:01.169Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_audio_video/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_audio_video/","excerpt":"","text":"音视频","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/example_of_anchor","date":"2024-12-12T08:38:01.167Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_anchor/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/example_of_anchor/","excerpt":"","text":"锚点连接 h1 { margin-left: 100px } p { margin: 40px 120px; } #adv { width: 200px; height: 200px; color: yellow; background-color: rgba(0, 0, 255, 0.75); /* background-color: blue; */ /* opacity: 0.5; */ position: fixed; right: 50px; top: 20px; } /* position属性的取值 */ /* 1. static - 静态定位 - 正常的文档流 */ /* 2. relative - 相对定位 - 相对于原来正常的位置 */ /* 3. absolute - 绝对定位 - 相对于父元素来设置位置 */ /* 4. fixed - 固定定位 - 相对于浏览器窗口来设置位置 */ 此广告位招租 关闭 坚持中国特色社会主义 第一，中国特色社会主义是社会主义而不是其他什么主义，科学社会主义基本原则不能丢，丢了就不是社会主义。我们党始终强调，中国特色社会主义，既坚持了科学社会主义基本原则，又根据时代条件赋予其鲜明的中国特色。这就是说，中国特色社会主义是社会主义，不是别的什么主义。一个国家实行什么样的主义，关键要看这个主义能否解决这个国家面临的历史性课题。在中华民族积贫积弱、任人宰割的时期，各种主义和思潮都进行过尝试，资本主义道路没有走通，改良主义、自由主义、社会达尔文主义、无政府主义、实用主义、民粹主义、工团主义等也都“你方唱罢我登场”，但都没能解决中国的前途和命运问题。是马克思列宁主义、毛泽东思想引导中国人民走出了漫漫长夜、建立了新中国，是中国特色社会主义使中国快速发展起来了。不说更早的时期，就从改革开放开始，特别是苏联解体、东欧剧变以后，唱衰中国的舆论在国际上不绝于耳，各式各样的“中国崩溃论”从来没有中断过。但是，中国非但没有崩溃，反而综合国力与日俱增，人民生活水平不断提高，“风景这边独好”。历史和现实都告诉我们，只有社会主义才能救中国，只有中国特色社会主义才能发展中国，这是历史的结论、人民的选择。 近些年来，国内外有些舆论提出中国现在搞的究竟还是不是社会主义的疑问，有人说是“资本社会主义”，还有人干脆说是“国家资本主义”、“新官僚资本主义”。这些都是完全错误的。我们说中国特色社会主义是社会主义，那就是不论怎么改革、怎么开放，我们都始终要坚持中国特色社会主义道路、中国特色社会主义理论体系、中国特色社会主义制度，坚持党的十八大提出的夺取中国特色社会主义新胜利的基本要求。这就包括在中国共产党领导下，立足基本国情，以经济建设为中心，坚持四项基本原则，坚持改革开放，解放和发展社会生产力，建设社会主义市场经济、社会主义民主政治、社会主义先进文化、社会主义和谐社会、社会主义生态文明，促进人的全面发展，逐步实现全体人民共同富裕，建设富强民主文明和谐的社会主义现代化国家；包括坚持人民代表大会制度的根本政治制度，中国共产党领导的多党合作和政治协商制度、民族区域自治制度以及基层群众自治制度等基本政治制度，中国特色社会主义法律体系，公有制为主体、多种所有制经济共同发展的基本经济制度。这些都是在新的历史条件下体现科学社会主义基本原则的内容，如果丢掉了这些，那就不成其为社会主义了。 邓小平同志曾经深刻地、总结性地指出：“我们的现代化建设，必须从中国的实际出发。无论是革命还是建设，都要注意学习和借鉴外国经验。但是，照抄照搬别国经验、别国模式，从来不能得到成功。这方面我们有过不少教训。”过去不能搞全盘苏化，现在也不能搞全盘西化或者其他什么化。冷战结束后，不少发展中国家被迫采纳了西方模式，结果党争纷起、社会动荡、人民流离失所，至今都难以稳定下来。《庄子·秋水》中写道：“且子独不闻夫寿陵余子之学行于邯郸与？未得国能，又失其故行矣，直匍匐而归耳。”我们千万不能“邯郸学步，失其故行”。我们就是把马克思主义中国化，就是搞中国特色社会主义。近年来，随着我国综合国力和国际地位上升，国际上关于“北京共识”、“中国模式”、“中国道路”等议论和研究也多了起来，其中不乏赞扬者。一些外国学者认为，中国的快速发展，导致一些西方理论正在被质疑，一种新版的马克思主义理论正在颠覆西方的传统理论。我们始终认为，各国的发展道路应由各国人民选择。所谓的“中国模式”是中国人民在自己的奋斗实践中创造的中国特色社会主义道路。我们坚信，随着中国特色社会主义不断发展，我们的制度必将越来越成熟，我国社会主义制度的优越性必将进一步显现，我们的道路必将越走越宽广，我国发展道路对世界的影响必将越来越大。我们就是要有这样的道路自信、理论自信、制度自信，真正做到“千磨万击还坚劲，任尔东西南北风”。 第二，我们党领导人民进行社会主义建设，有改革开放前和改革开放后两个历史时期，这是两个相互联系又有重大区别的时期，但本质上都是我们党领导人民进行社会主义建设的实践探索。中国特色社会主义是在改革开放历史新时期开创的，但也是在新中国已经建立起社会主义基本制度并进行了20多年建设的基础上开创的。正确认识这个问题，要把握3个方面。一是，如果没有1978年我们党果断决定实行改革开放，并坚定不移推进改革开放，坚定不移把握改革开放的正确方向，社会主义中国就不可能有今天这样的大好局面，就可能面临严重危机，就可能遇到像苏联、东欧国家那样的亡党亡国危机。同时，如果没有1949年建立新中国并进行社会主义革命和建设，积累了重要的思想、物质、制度条件，积累了正反两方面经验，改革开放也很难顺利推进。二是，虽然这两个历史时期在进行社会主义建设的思想指导、方针政策、实际工作上有很大差别，但两者决不是彼此割裂的，更不是根本对立的。我们党在社会主义建设实践中提出了许多正确主张，当时没有真正落实，改革开放后得到了真正贯彻，将来也还是要坚持和发展的。马克思早就说过：“人们自己创造自己的历史，但是他们并不是随心所欲地创造，并不是在他们自己选定的条件下创造，而是在直接碰到的、既定的、从过去承继下来的条件下创造。”三是，对改革开放前的历史时期要正确评价，不能用改革开放后的历史时期否定改革开放前的历史时期，也不能用改革开放前的历史时期否定改革开放后的历史时期。改革开放前的社会主义实践探索为改革开放后的社会主义实践探索积累了条件，改革开放后的社会主义实践探索是对前一个时期的坚持、改革、发展。对改革开放前的社会主义实践探索，要坚持实事求是的思想路线，分清主流和支流，坚持真理，修正错误，发扬经验，吸取教训，在这个基础上把党和人民事业继续推向前进。 我之所以强调这个问题，是因为这个重大政治问题处理不好，就会产生严重政治后果。古人说：“灭人之国，必先去其史。”国内外敌对势力往往就是拿中国革命史、新中国历史来做文章，竭尽攻击、丑化、污蔑之能事，根本目的就是要搞乱人心，煽动推翻中国共产党的领导和我国社会主义制度。苏联为什么解体？苏共为什么垮台？一个重要原因就是意识形态领域的斗争十分激烈，全面否定苏联历史、苏共历史，否定列宁，否定斯大林，搞历史虚无主义，思想搞乱了，各级党组织几乎没任何作用了，军队都不在党的领导之下了。最后，苏联共产党偌大一个党就作鸟兽散了，苏联偌大一个社会主义国家就分崩离析了。这是前车之鉴啊！邓小平同志指出：“毛泽东思想这个旗帜丢不得。丢掉了这个旗帜，实际上就否定了我们党的光辉历史。总的来说，我们党的历史还是光辉的历史。虽然我们党在历史上，包括建国以后的30年中，犯过一些大错误，甚至犯过搞‘文化大革命’这样的大错误，但是我们党终究把革命搞成功了。中国在世界上的地位，是在中华人民共和国成立以后才大大提高的。只有中华人民共和国的成立，才使我们这个人口占世界总人口近1/4的大国，在世界上站起来，而且站住了。”他还强调：“对毛泽东同志的评价，对毛泽东思想的阐述，不是仅仅涉及毛泽东同志个人的问题，这同我们党、我们国家的整个历史是分不开的。要看到这个全局。”“这不只是个理论问题，尤其是个政治问题，是国际国内的很大的政治问题。”这就是一个伟大马克思主义政治家的眼界和胸怀。试想一下，如果当时全盘否定了毛泽东同志，那我们党还能站得住吗？我们国家的社会主义制度还能站得住吗？那就站不住了，站不住就会天下大乱。所以，正确处理改革开放前后的社会主义实践探索的关系，不只是一个历史问题，更主要的是一个政治问题。建议大家把《关于建国以来党的若干历史问题的决议》找出来再看看。 第三，马克思主义必定随着时代、实践和科学的发展而不断发展，不可能一成不变，社会主义从来都是在开拓中前进的。坚持和发展中国特色社会主义是一篇大文章，邓小平同志为它确定了基本思路和基本原则，以江泽民同志为核心的党的第三代中央领导集体、以胡锦涛同志为总书记的党中央在这篇大文章上都写下了精彩的篇章。现在，我们这一代共产党人的任务，就是继续把这篇大文章写下去。30多年来，中国特色社会主义取得了巨大成就，加之新中国成立以后打下的基础，这是它得以站得住、行得远的重要基础。我们对社会主义的认识，对中国特色社会主义规律的把握，已经达到了一个前所未有的新的高度，这一点不容置疑。同时，也要看到，我国社会主义还处在初级阶段，我们还面临很多没有弄清楚的问题和待解的难题，对许多重大问题的认识和处理都还处在不断深化的过程之中，这一点也不容置疑。对事物的认识是需要一个过程的，而对社会主义这个我们只搞了几十年的东西，我们的认识和把握也还是非常有限的，还需要在实践中不断深化和发展。 坚持马克思主义，坚持社会主义，一定要有发展的观点，一定要以我国改革开放和现代化建设的实际问题、以我们正在做的事情为中心，着眼于马克思主义理论的运用，着眼于对实际问题的理论思考，着眼于新的实践和新的发展。我们说过，世界上没有放之四海而皆准的发展道路和发展模式，也没有一成不变的发展道路和发展模式。我们过去取得的实践和理论成果，能够帮助我们更好面对和解决前进中的问题，但不能成为我们骄傲自满的理由，更不能成为我们继续前进的包袱。我们的事业越前进、越发展，新情况新问题就会越多，面临的风险和挑战就会越多，面对的不可预料的事情就会越多。我们必须增强忧患意识，做到居安思危。解放思想、实事求是、与时俱进，是马克思主义活的灵魂，是我们适应新形势、认识新事物、完成新任务的根本思想武器。全党同志首先是各级领导干部必须坚持马克思主义的发展观点，坚持实践是检验真理的唯一标准，发挥历史的主动性和创造性，清醒认识世情、国情、党情的变和不变，永远要有逢山开路、遇河架桥的精神，锐意进取，大胆探索，敢于和善于分析回答现实生活中和群众思想上迫切需要解决的问题，不断深化改革开放，不断有所发现、有所创造、有所前进，不断推进理论创新、实践创新、制度创新。 第四，我们党始终坚持共产主义远大理想，共产党员特别是党员领导干部要做共产主义远大理想和中国特色社会主义共同理想的坚定信仰者和忠实践行者。对马克思主义的信仰，对社会主义和共产主义的信念，是共产党人的政治灵魂，是共产党人经受住任何考验的精神支柱。党章明确规定，党的最高理想和最终目标是实现共产主义。党章同时明确规定，中国共产党人追求的共产主义最高理想，只有在社会主义社会充分发展和高度发达的基础上才能实现。想一下子、两下子就进入共产主义，那是不切实际的。邓小平同志说，巩固和发展社会主义制度，还需要一个很长的历史阶段，需要我们几代人、十几代人、甚至几十代人坚持不懈地努力奋斗。几十代人，那是多么长啊！从孔老夫子到现在也不过七十几代人。这样看问题，充分说明了我们中国共产党人政治上的清醒。必须认识到，我们现在的努力以及将来多少代人的持续努力，都是朝着最终实现共产主义这个大目标前进的。同时，必须认识到，实现共产主义是一个非常漫长的历史过程，我们必须立足党在现阶段的奋斗目标，脚踏实地推进我们的事业。如果丢失了我们共产党人的远大目标，就会迷失方向，变成功利主义、实用主义。中国特色社会主义是党的最高纲领和基本纲领的统一。中国特色社会主义的基本纲领，概言之，就是建立富强民主文明和谐的社会主义现代化国家。这既是从我国正处于并将长期处于社会主义初级阶段的基本国情出发的，也没有脱离党的最高理想。我们既要坚定走中国特色社会主义道路的信念，也要胸怀共产主义的崇高理想，矢志不移贯彻执行党在社会主义初级阶段的基本路线和基本纲领，做好当前每一项工作。 返回 革命理想高于天。没有远大理想，不是合格的共产党员；离开现实工作而空谈远大理想，也不是合格的共产党员。在我们党90多年的历史中，一代又一代共产党人为了追求民族独立和人民解放，不惜流血牺牲，靠的就是一种信仰，为的就是一个理想。尽管他们也知道，自己追求的理想并不会在自己手中实现，但他们坚信，只要一代又一代人为之持续努力，一代又一代人为此作出牺牲，崇高的理想就一定能实现，正所谓“砍头不要紧，只要主义真”。今天，衡量一名共产党员、一名领导干部是否具有共产主义远大理想，是有客观标准的，那就要看他能否坚持全心全意为人民服务的根本宗旨，能否吃苦在前、享受在后，能否勤奋工作、廉洁奉公，能否为理想而奋不顾身去拼搏、去奋斗、去献出自己的全部精力乃至生命。一切迷惘迟疑的观点，一切及时行乐的思想，一切贪图私利的行为，一切无所作为的作风，都是与此格格不入的。一些人认为共产主义是可望而不可及的，甚至认为是望都望不到、看都看不见的，是虚无缥缈的。这就涉及是唯物史观还是唯心史观的世界观问题。我们一些同志之所以理想渺茫、信仰动摇，根本的就是历史唯物主义观点不牢固。要教育引导广大党员、干部把践行中国特色社会主义共同理想和坚定共产主义远大理想统一起来，做到虔诚而执着、至信而深厚。有了坚定的理想信念，站位就高了，眼界就宽了，心胸就开阔了，就能坚持正确政治方向，在胜利和顺境时不骄傲不急躁，在困难和逆境时不消沉不动摇，经受住各种风险和困难考验，自觉抵御各种腐朽思想的侵蚀，永葆共产党人政治本色。 var closeBtn = document.getElementById('closeBtn') closeBtn.addEventListener('click', function () { // var advDiv = document.getElementById('adv') // advDiv.style.display = 'none' location.href = 'http://youku.com' })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3.result","date":"2024-12-12T08:38:01.165Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3.result/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3.result/","excerpt":"","text":"HTML5 Layout header, section, footer, aside, nav, article, figure, figcaption { display: block; } body { color: #666666; background-color: #f9f8f6; background-image: url(\"images/dark-wood.jpg\"); background-position: center; font-family: Georgia, Times, serif; line-height: 1.4em; margin: 0px; } .wrapper { width: 940px; margin: 20px auto 20px auto; border: 2px solid #000000; background-color: #ffffff; } header { height: 160px; background-image: url(images/header.jpg); } h1 { text-indent: -9999px; width: 940px; height: 130px; margin: 0px; } nav, footer { clear: both; color: #ffffff; background-color: #aeaca8; height: 30px; } nav ul { margin: 0px; padding: 5px 0px 5px 30px; } nav li { display: inline; margin-right: 40px; } nav li a { color: #ffffff; } nav li a:hover, nav li a.current { color: #000000; } section.courses { float: left; width: 659px; border-right: 1px solid #eeeeee; } article { clear: both; overflow: auto; width: 100%; } hgroup { margin-top: 40px; } figure { float: left; width: 290px; height: 220px; padding: 5px; margin: 20px; border: 1px solid #eeeeee; } figcaption { font-size: 90%; text-align: left; } aside { width: 230px; float: left; padding: 0px 0px 0px 20px; } aside section a { display: block; padding: 10px; border-bottom: 1px solid #eeeeee; } aside section a:hover { color: #985d6a; background-color: #efefef; } a { color: #de6581; text-decoration: none; } h1, h2, h3 { font-weight: normal; } h2 { margin: 10px 0px 5px 0px; padding: 0px; } h3 { margin: 0px 0px 10px 0px; color: #de6581; } aside h2 { padding: 30px 0px 10px 0px; color: #de6581; } footer { font-size: 80%; padding: 7px 0px 0px 20px; } Yoko's Kitchen home classes catering about contact Bok Choi Japanese Vegetarian Five week course in London A five week introduction to traditional Japanese vegetarian meals, teaching you a selection of rice and noodle dishes. Teriyaki Sauce Sauces Masterclass One day workshop An intensive one-day course looking at how to create the most delicious sauces for use in a range of Japanese cookery. Popular Recipes Yakitori (grilled chicken) Tsukune (minced chicken patties) Okonomiyaki (savory pancakes) Mizutaki (chicken stew) Contact Yoko's Kitchen 27 Redchurch Street Shoreditch London E2 7DP &copy; 2011 Yoko's Kitchen","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3","date":"2024-12-12T08:38:01.164Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_3/","excerpt":"","text":"HTML5 Layout Yoko's Kitchen home classes catering about contact Bok Choi Japanese Vegetarian Five week course in London A five week introduction to traditional Japanese vegetarian meals, teaching you a selection of rice and noodle dishes. Teriyaki Sauce Sauces Masterclass One day workshop An intensive one-day course looking at how to create the most delicious sauces for use in a range of Japanese cookery. Popular Recipes Yakitori (grilled chicken) Tsukune (minced chicken patties) Okonomiyaki (savory pancakes) Mizutaki (chicken stew) Contact Yoko's Kitchen 27 Redchurch Street Shoreditch London E2 7DP &copy; 2011 Yoko's Kitchen","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2.result","date":"2024-12-12T08:38:01.162Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2.result/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2.result/","excerpt":"","text":"Lists, Tables and Forms body { font-family: Arial, Verdana, sans-serif; font-size: 90%; color: #666; background-color: #f8f8f8; } li { list-style-image: url(\"images/icon-plus.png\"); line-height: 1.6em; } table { border-spacing: 0px; } th, td { padding: 5px 30px 5px 10px; border-spacing: 0px; font-size: 90%; margin: 0px; } th, td { text-align: left; background-color: #e0e9f0; border-top: 1px solid #f1f8fe; border-bottom: 1px solid #cbd2d8; border-right: 1px solid #cbd2d8; } tr.head th { color: #fff; background-color: #90b4d6; border-bottom: 2px solid #547ca0; border-right: 1px solid #749abe; border-top: 1px solid #90b4d6; text-align: center; text-shadow: -1px -1px 1px #666666; letter-spacing: 0.15em; } td { text-shadow: 1px 1px 1px #ffffff; } tr.even td, tr.even th { background-color: #e8eff5; } tr.head th:first-child { -webkit-border-top-left-radius: 5px; -moz-border-radius-topleft: 5px; border-top-left-radius: 5px; } tr.head th:last-child { -webkit-border-top-right-radius: 5px; -moz-border-radius-topright: 5px; border-top-right-radius: 5px; } fieldset { width: 310px; margin-top: 20px; border: 1px solid #d6d6d6; background-color: #ffffff; line-height: 1.6em; } legend { font-style: italic; color: #666666; } input[type=\"text\"] { width: 120px; border: 1px solid #d6d6d6; padding: 2px; outline: none; } input[type=\"text\"]:focus, input[type=\"text\"]:hover { background-color: #d0e2f0; border: 1px solid #999999; } input[type=\"submit\"] { border: 1px solid #006633; background-color: #009966; color: #ffffff; border-radius: 5px; padding: 5px; margin-top: 10px; } input[type=\"submit\"]:hover { border: 1px solid #006633; background-color: #00CC33; color: #ffffff; cursor: pointer; } .title { float: left; width: 160px; clear: left; } .submit { width: 310px; text-align: right; } Poetry Workshops We will be conducting a number of poetry workshops and symposiums throughout the year. Please note that the following events are free to members: A Poetic Perspective Walt Whitman at War Found Poems and Outsider Poetry New York Chicago San Francisco A Poetic Perspective Sat, 4 Feb 201211am - 2pm Sat, 3 Mar 201211am - 2pm Sat, 17 Mar 201211am - 2pm Walt Whitman at War Sat, 7 Apr 201211am - 1pm Sat, 5 May 201211am - 1pm Sat, 19 May 201211am - 1pm Found Poems &amp; Outsider Poetry Sat, 9 Jun 201211am - 2pm Sat, 7 Jul 201211am - 2pm Sat, 21 Jul 201211am - 2pm Natural Death: An Exploration Sat, 4 Aug 201211am - 4pm Sat, 8 Sep 201211am - 4pm Sat, 15 Sep 201211am - 4pm Register your interest Your name: Your email: Your closest center: New York Chicago San Francisco Are you a member? Yes No","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2","date":"2024-12-12T08:38:01.159Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_2/","excerpt":"","text":"Lists, Tables and Forms .books>li { list-style-image: url(images/icon-plus.png); } .schedule { border-spacing: 0; } .header { background-color: rgb(126, 164, 204); } .header>th:first-child { border-top-left-radius: 5px; } .header>th:last-child { border-top-right-radius: 5px; } .schedule td, .schedule th { border: 1px solid lightgray; } .header th { border-bottom: 2px solid rgb(68, 104, 142); } label { display: inline-block; width: 160px; text-align: right; } Poetry Workshops We will be conducting a number of poetry workshops and symposiums throughout the year. Please note that the following events are free to members: A Poetic Perspective Walt Whitman at War Found Poems and Outsider Poetry New York Chicago San Francisco A Poetic Perspective Sat, 4 Feb 201211am - 2pm Sat, 3 Mar 201211am - 2pm Sat, 17 Mar 201211am - 2pm Walt Whitman at War Sat, 7 Apr 201211am - 1pm Sat, 5 May 201211am - 1pm Sat, 19 May 201211am - 1pm Found Poems &amp; Outsider Poetry Sat, 9 Jun 201211am - 2pm Sat, 7 Jul 201211am - 2pm Sat, 21 Jul 201211am - 2pm Natural Death: An Exploration Sat, 4 Aug 201211am - 4pm Sat, 8 Sep 201211am - 4pm Sat, 15 Sep 201211am - 4pm Register your interest Your name: Your email: Your closest center: New York Chicago San Francisco Are you a member? Yes No","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1.result","date":"2024-12-12T08:38:01.157Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1.result/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1.result/","excerpt":"","text":"Boxes body { font-size: 80%; font-family: \"Courier New\", Courier, monospace; letter-spacing: 0.15em; background-color: #efefef; } #page { max-width: 940px; min-width: 720px; margin: 10px auto 10px auto; padding: 20px; border: 4px double #000; background-color: #ffffff; } #logo { width: 150px; margin: 10px auto 25px auto; } ul { width: 570px; padding: 15px; margin: 0px auto 0px auto; border-top: 2px solid #000; border-bottom: 1px solid #000; text-align: center; } li { display: inline; margin: 0px 3px; } p { text-align: center; width: 600px; margin: 20px auto 20px auto; font-weight: normal; } a { color: #000000; text-transform: uppercase; text-decoration: none; padding: 6px 18px 5px 18px; } a:hover, a.on { color: #cc3333; background-color: #ffffff; } Home For Sale Repairs About Contact We specialize in the sale and repair of classic keyboards, in particular the Fender Rhodes, Wurlitzer EP200, and Hohner Clavinet.","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1","date":"2024-12-12T08:38:01.154Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/web1901/css_practice_1/","excerpt":"","text":"Boxes * { /* border: 1px solid black; */ font-family: \"Courier New\"; margin: 0; padding: 0; } body { background-color: rgba(192, 192, 192, 0.3); } #page { width: 880px; border: 4px double black; background-color: white; margin: 10px auto; padding: 20px 50px; } #logo { width: 100%; text-align: center; } #navigation { width: 600px; height: 60px; margin: 10px auto; border-top: 2px solid black; border-bottom: 1px solid black; } #navigation>li { float: left; width: 120px; height: 40px; list-style-type:none; text-align: center; line-height: 40px; padding-top: 10px; } #navigation>li>a { text-decoration: none; letter-spacing: 0.15em; color: black; } #navigation>li:first-child>a{ color: red; } #navigation>li>a:hover { color: red; } #navigation~p { width: 600px; margin: 20px auto; text-align: center; line-height: 20px; } Home For Sale Repairs About Contact We specialize in the sale and repair of classic keyboards, in particular the Fender Rhodes, Wurlitzer EP200, and Hohner Clavinet.","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/new/vue/vue.demo","date":"2024-12-12T08:38:01.145Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/new/vue/vue.demo/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/new/vue/vue.demo/","excerpt":"","text":"库存信息 - 已经售罄 增加库存 库存总量：台 const app = new Vue({ el: '#app', data: { products: [ {\"id\": 1, \"name\": \"iPhone X\", \"quantity\": 20}, {\"id\": 2, \"name\": \"华为 Mate20\", \"quantity\": 0}, {\"id\": 3, \"name\": \"小米 Mix3\", \"quantity\": 50} ] }, computed: { totalQuantity() { return this.products.reduce((sum, product) => { return sum + product.quantity }, 0); } } });","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/list_by_vue","date":"2024-12-12T08:38:01.130Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/list_by_vue/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/list_by_vue/","excerpt":"","text":"动态列表 * { margin: 0; padding: 0; } body { background-color: #000; color: #fff; } #app { width: 40%; margin: 20px auto; } #fruits>li { width: 90%; height: 50px; background-color: #6ca; margin: 4px 0; text-align: center; font-size: 20px; list-style-type: none; line-height: 50px; } #fruits>li>a { float: right; color: #fff; text-decoration: none; margin-right: 10px; } #fruits+div { margin-top: 20px; } #fname { width: 70%; height: 40px; color: #fff; border-radius: 8px; border: none; outline: none; font-size: 20px; text-align: center; vertical-align: middle; background-color: #999; } #ok { width: 19%; height: 40px; color: #fff; background-color: #a45; border: none; outline: none; font-size: 16px; vertical-align: middle; } × 确定 const app = new Vue({ el: '#app', data: { fruits: ['苹果', '香蕉', '榴莲', '火龙果'], fname: '' }, methods: { addItem() { if (this.fname.length > 0) { this.fruits.push(this.fname) } this.fname = '' }, removeItem(fruit) { let index = this.fruits.indexOf(fruit) if (index >= 0) { this.fruits.splice(index, 1) } } } })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/list_by_jquery","date":"2024-12-12T08:38:01.127Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/list_by_jquery/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/list_by_jquery/","excerpt":"","text":"动态列表 * { margin: 0; padding: 0; } body { background-color: #000; color: #fff; } #app { width: 40%; margin: 20px auto; } #fruits>li { width: 90%; height: 50px; background-color: #6ca; margin: 4px 0; text-align: center; font-size: 20px; list-style-type: none; line-height: 50px; } #fruits>li>a { float: right; color: #fff; text-decoration: none; margin-right: 10px; } #fruits+div { margin-top: 20px; } #fname { width: 70%; height: 40px; color: #fff; border-radius: 8px; border: none; outline: none; font-size: 20px; text-align: center; vertical-align: middle; background-color: #999; } #ok { width: 19%; height: 40px; color: #fff; background-color: #a45; border: none; outline: none; font-size: 16px; vertical-align: middle; } 苹果× 香蕉× 榴莲× 火龙果× 确定 // 1. $函数的参数是一个函数，该函数是页面加载完成后执行的回调函数 $(() => { function removeItem(evt) { evt.preventDefault() // 4. $函数的参数是原生JavaScript对象，返回该原生JavaScript对象对应的jQuery对象 $(evt.target).parent().remove() } function addItem(evt) { let fname = $('#fname').val().trim() if (fname.length > 0) { $('#fruits').append( // 3. $函数的参数是标签字符串，创建对应的标签元素并返回jQuery对象 $('').text(fname).append( $('').attr('href', '').text('×') .on('click', removeItem) ) ) } $('#fname').val('') // jQuery对象通过下标运算或get方法可以获得与之对应的原生JavaScript对象 // input.get(0).focus() $('#fname')[0].focus() } // 2. $函数的参数是选择器字符串，返回对应元素的jQuery对象 $('#fruits a').on('click', removeItem) $('#ok').on('click', addItem) $('#fname').on('keydown', (evt) => { let code = evt.keyCode || evt.which if (code == 13) { addItem(evt) } }) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/code/list_by_javascript","date":"2024-12-12T08:38:01.124Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/code/list_by_javascript/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/code/list_by_javascript/","excerpt":"","text":"动态列表 * { margin: 0; padding: 0; } body { background-color: #000; color: #fff; } #app { width: 40%; margin: 20px auto; } #fruits>li { width: 90%; height: 50px; background-color: #6ca; margin: 4px 0; text-align: center; font-size: 20px; list-style-type: none; line-height: 50px; } #fruits>li>a { float: right; color: #fff; text-decoration: none; margin-right: 10px; } #fruits+div { margin-top: 20px; } #fname { width: 70%; height: 40px; color: #fff; border-radius: 8px; border: none; outline: none; font-size: 20px; text-align: center; vertical-align: middle; background-color: #999; } #ok { width: 19%; height: 40px; color: #fff; background-color: #a45; border: none; outline: none; font-size: 16px; vertical-align: middle; } 苹果× 香蕉× 榴莲× 火龙果× 确定 const ul = document.querySelector('#fruits') const fnameInput = document.querySelector('#fname') const okBtn = document.querySelector('#ok') const anchors = document.querySelectorAll('#fruits a') function removeItem(evt) { evt.preventDefault() let li = evt.target.parentNode li.parentNode.removeChild(li) } function addItem(evt) { let fname = fnameInput.value.trim() if (fname.length > 0) { let li = document.createElement('li') li.textContent = fname let a = document.createElement('a') a.setAttribute('href', '') a.textContent = '×' a.addEventListener('click', removeItem) li.appendChild(a) ul.insertBefore(li, ul.firstElementChild) } fnameInput.value = '' fnameInput.focus() } window.addEventListener('load', (evt) => { for (let i = 0; i < anchors.length; i += 1) { anchors[i].addEventListener('click', removeItem) } fnameInput.addEventListener('keydown', (evt) => { let code = evt.keyCode || evt.which if (code == 13) { addItem() } }) okBtn.addEventListener('click', addItem) })","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day21-30/21-30.Web前端概述","date":"2024-12-12T08:38:01.112Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day21-30/21-30.Web前端概述/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day21-30/21-30.Web%E5%89%8D%E7%AB%AF%E6%A6%82%E8%BF%B0/","excerpt":"","text":"Web前端概述 说明：本文使用的部分插图来自Jon Duckett先生的*HTML and CSS: Design and Build Websites*一书，这是一本非常棒的前端入门书，有兴趣的读者可以在亚马逊或者其他网站上找到该书的购买链接。 HTML 是用来描述网页的一种语言，全称是 Hyper-Text Markup Language，即超文本标记语言。我们浏览网页时看到的文字、按钮、图片、视频等元素，它们都是通过 HTML 书写并通过浏览器来呈现的。 HTML简史 1991年10月：一个非正式CERN（欧洲核子研究中心）文件首次公开18个HTML标签，这个文件的作者是物理学家蒂姆·伯纳斯-李，因此他是万维网的发明者，也是万维网联盟的主席。 1995年11月：HTML 2.0标准发布（RFC 1866）。 1997年1月：HTML 3.2作为W3C推荐标准发布。 1997年12月：HTML 4.0作为W3C推荐标准发布。 1999年12月：HTML4.01作为W3C推荐标准发布。 2008年1月：HTML5由W3C作为工作草案发布。 2011年5月：W3C将HTML5推进至“最终征求”（Last Call）阶段。 2012年12月：W3C指定HTML5作为“候选推荐”阶段。 2014年10月：HTML5作为稳定W3C推荐标准发布，这意味着HTML5的标准化已经完成。 HTML5新特性 引入原生多媒体支持（audio和video标签） 引入可编程内容（canvas标签） 引入语义Web（article、aside、details、figure、footer、header、nav、section、summary等标签） 引入新的表单控件（日历、邮箱、搜索、滑条等） 引入对离线存储更好的支持（localStorage和sessionStorage） 引入对定位、拖放、WebSocket、后台任务等的支持 使用标签承载内容 结构 html head title meta body 文本 标题（heading）和段落（paragraph） h1 ~ h6 p 上标（superscript）和下标（subscript） sup sub 空白（白色空间折叠） 折行（break）和水平标尺（horizontal ruler） br hr 语义化标签 加粗和强调 - strong 引用 - blockquote 缩写词和首字母缩写词 - abbr &#x2F; acronym 引文 - cite 所有者联系信息 - address 内容的修改 - ins &#x2F; del 列表（list） 有序列表（ordered list）- ol &#x2F; li 无序列表（unordered list）- ul &#x2F; li 定义列表（definition list）- dl &#x2F; dt &#x2F; dd 链接（anchor） 页面链接 锚链接 功能链接 图像（image） 图像存储位置 图像及其宽高 选择正确的图像格式 JPEG GIF PNG 矢量图 语义化标签 - figure &#x2F; figcaption 表格（table） 基本的表格结构 - table &#x2F; tr &#x2F; td &#x2F; th 表格的标题 - caption 跨行和跨列 - rowspan属性 &#x2F; colspan属性 长表格 - thead &#x2F; tbody &#x2F; tfoot 表单（form） 重要属性 - action &#x2F; method &#x2F; enctype 表单控件（input）- type属性 文本框 - text &#x2F; 密码框 - password &#x2F; 数字框 - number 邮箱 - email &#x2F; 电话 - tel &#x2F; 日期 - date &#x2F; 滑条 - range &#x2F; URL - url &#x2F; 搜索 - search 单选按钮 - radio &#x2F; 复选按钮 - checkbox 文件上传 - file &#x2F; 隐藏域 - hidden 提交按钮 - submit &#x2F; 图像按钮 - image &#x2F; 重置按钮 - reset 下拉列表 - select &#x2F; option 文本域（多行文本）- textarea 组合表单元素 - fieldset &#x2F; legend 音视频（audio &#x2F; video） 视频格式和播放器 视频托管服务 添加视频的准备工作 video标签和属性 - autoplay &#x2F; controls &#x2F; loop &#x2F; muted &#x2F; preload &#x2F; src audio标签和属性 - autoplay &#x2F; controls &#x2F; loop &#x2F; muted &#x2F; preload &#x2F; src &#x2F; width &#x2F; height &#x2F; poster 窗口（frame） 框架集（过时，不建议使用） - frameset &#x2F; frame 内嵌窗口 - iframe 其他 文档类型 1&lt;!doctype html&gt; 1&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot; &quot;http://www.w3.org/TR/html4/strict.dtd&quot;&gt; 1&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt; 注释 1&lt;!-- 这是一段注释，注释不能够嵌套 --&gt; 属性 id：唯一标识 class：元素所属的类，用于区分不同的元素 title：元素的额外信息（鼠标悬浮时会显示工具提示文本） tabindex：Tab键切换顺序 contenteditable：元素是否可编辑 draggable：元素是否可拖拽 块级元素 &#x2F; 行级元素 字符实体（实体替换符） 使用CSS渲染页面简介 CSS的作用 CSS的工作原理 规则、属性和值 常用选择器 颜色（color） 如何指定颜色 颜色术语和颜色对比 背景色 文本（text &#x2F; font） 文本的大小和字型(font-size &#x2F; font-family) 粗细、样式、拉伸和装饰(font-weight &#x2F; font-style &#x2F; font-stretch &#x2F; text-decoration) 行间距(line-height)、字母间距(letter-spacing)和单词间距(word-spacing) 对齐(text-align)方式和缩进(text-ident) 链接样式（:link &#x2F; :visited &#x2F; :active &#x2F; :hover） CSS3新属性 阴影效果 - text-shadow 首字母和首行文本(:first-letter &#x2F; :first-line) 响应用户 盒子（box model） 盒子大小的控制（width &#x2F; height） 盒子的边框、外边距和内边距（border &#x2F; margin &#x2F; padding） 盒子的显示和隐藏（display &#x2F; visibility） CSS3新属性 边框图像（border-image） 投影（border-shadow） 圆角（border-radius） 列表、表格和表单 列表的项目符号（list-style） 表格的边框和背景（border-collapse） 表单控件的外观 表单控件的对齐 浏览器的开发者工具 图像 控制图像的大小（display: inline-block） 对齐图像 背景图像（background &#x2F; background-image &#x2F; background-repeat &#x2F; background-position） 布局 控制元素的位置（position &#x2F; z-index） 普通流 相对定位 绝对定位 固定定位 浮动元素（float &#x2F; clear） 网站布局 HTML5布局 适配屏幕尺寸 固定宽度布局 流体布局 布局网格 使用JavaScript控制行为JavaScript基本语法 语句和注释 变量和数据类型 声明和赋值 简单数据类型和复杂数据类型 变量的命名规则 表达式和运算符 赋值运算符 算术运算符 比较运算符 逻辑运算符：&amp;&amp;、||、! 分支结构 if...else... switch...cas...default... 循环结构 for循环 while循环 do...while循环 数组 创建数组 操作数组中的元素 函数 声明函数 调用函数 参数和返回值 匿名函数 立即调用函数 面向对象 对象的概念 创建对象的字面量语法 访问成员运算符 创建对象的构造函数语法 this关键字 添加和删除属性 delete关键字 标准对象 Number &#x2F; String &#x2F; Boolean &#x2F; Symbol &#x2F; Array &#x2F; Function Date &#x2F; Error &#x2F; Math &#x2F; RegExp &#x2F; Object &#x2F; Map &#x2F; Set JSON &#x2F; Promise &#x2F; Generator &#x2F; Reflect &#x2F; Proxy BOM window对象的属性和方法 history对象 forward() &#x2F; back() &#x2F; go() location对象 navigator对象 screen对象 DOM DOM树 访问元素 getElementById() &#x2F; querySelector() getElementsByClassName() &#x2F; getElementsByTagName() &#x2F; querySelectorAll() parentNode &#x2F; previousSibling &#x2F; nextSibling &#x2F; children &#x2F; firstChild &#x2F; lastChild 操作元素 nodeValue innerHTML &#x2F; textContent &#x2F; createElement() &#x2F; createTextNode() &#x2F; appendChild() &#x2F; insertBefore() &#x2F; removeChild() className &#x2F; id &#x2F; hasAttribute() &#x2F; getAttribute() &#x2F; setAttribute() &#x2F; removeAttribute() 事件处理 事件类型 UI事件：load &#x2F; unload &#x2F; error &#x2F; resize &#x2F; scroll 键盘事件：keydown &#x2F; keyup &#x2F; keypress 鼠标事件：click &#x2F; dbclick &#x2F; mousedown &#x2F; mouseup &#x2F; mousemove &#x2F; mouseover &#x2F; mouseout 焦点事件：focus &#x2F; blur 表单事件：input &#x2F; change &#x2F; submit &#x2F; reset &#x2F; cut &#x2F; copy &#x2F; paste &#x2F; select 事件绑定 HTML事件处理程序（不推荐使用，因为要做到标签与代码分离） 传统的DOM事件处理程序（只能附加一个回调函数） 事件监听器（旧的浏览器中不被支持） 事件流：事件捕获 &#x2F; 事件冒泡 事件对象（低版本IE中的window.event） target（有些浏览器使用srcElement） type cancelable preventDefault() stopPropagation()（低版本IE中的cancelBubble） 鼠标事件 - 事件发生的位置 屏幕位置：screenX和screenY 页面位置：pageX和pageY 客户端位置：clientX和clientY 键盘事件 - 哪个键被按下了 keyCode属性（有些浏览器使用which） String.fromCharCode(event.keyCode) HTML5事件 DOMContentLoaded hashchange beforeunload JavaScript API 客户端存储 - localStorage和sessionStorage 123localStorage.colorSetting = &#x27;#a4509b&#x27;;localStorage[&#x27;colorSetting&#x27;] = &#x27;#a4509b&#x27;;localStorage.setItem(&#x27;colorSetting&#x27;, &#x27;#a4509b&#x27;); 获取位置信息 - geolocation 1234navigator.geolocation.getCurrentPosition(function(pos) &#123; console.log(pos.coords.latitude) console.log(pos.coords.longitude)&#125;) 从服务器获取数据 - Fetch API 绘制图形 - &lt;canvas&gt;的API 音视频 - &lt;audio&gt;和&lt;video&gt;的API 使用jQueryjQuery概述 Write Less Do More（用更少的代码来完成更多的工作） 使用CSS选择器来查找元素（更简单更方便） 使用jQuery方法来操作元素（解决浏览器兼容性问题、应用于所有元素并施加多个方法） 引入jQuery 下载jQuery的开发版和压缩版 从CDN加载jQuery 12345&lt;script src=&quot;https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; window.jQuery || document.write(&#x27;&lt;script src=&quot;js/jquery-3.3.1.min.js&quot;&gt;&lt;/script&gt;&#x27;)&lt;/script&gt; 查找元素 选择器 * &#x2F; element &#x2F; #id &#x2F; .class &#x2F; selector1, selector2 ancestor descendant &#x2F; parent&gt;child &#x2F; previous+next &#x2F; previous~siblings 筛选器 基本筛选器：:not(selector) &#x2F; :first &#x2F; :last &#x2F; :even &#x2F; :odd &#x2F; :eq(index) &#x2F; :gt(index) &#x2F; :lt(index) &#x2F; :animated &#x2F; :focus 内容筛选器：:contains(‘…’) &#x2F; :empty &#x2F; :parent &#x2F; :has(selector) 可见性筛选器：:hidden &#x2F; :visible 子节点筛选器：:nth-child(expr) &#x2F; :first-child &#x2F; :last-child &#x2F; :only-child 属性筛选器：[attribute] &#x2F; [attribute&#x3D;’value’] &#x2F; [attribute!&#x3D;’value’] &#x2F; [attribute^&#x3D;’value’] &#x2F; [attribute$&#x3D;’value’] &#x2F; [attribute|&#x3D;’value’] &#x2F; [attribute~&#x3D;’value’] 表单：:input &#x2F; :text &#x2F; :password &#x2F; :radio &#x2F; :checkbox &#x2F; :submit &#x2F; :image &#x2F; :reset &#x2F; :button &#x2F; :file &#x2F; :selected &#x2F; :enabled &#x2F; :disabled &#x2F; :checked 执行操作 内容操作 获取&#x2F;修改内容：html() &#x2F; text() &#x2F; replaceWith() &#x2F; remove() 获取&#x2F;设置元素：before() &#x2F; after() &#x2F; prepend() &#x2F; append() &#x2F; remove() &#x2F; clone() &#x2F; unwrap() &#x2F; detach() &#x2F; empty() &#x2F; add() 获取&#x2F;修改属性：attr() &#x2F; removeAttr() &#x2F; addClass() &#x2F; removeClass() &#x2F; css() 获取&#x2F;设置表单值：val() 查找操作 查找方法：find() &#x2F; parent() &#x2F; children() &#x2F; siblings() &#x2F; next() &#x2F; nextAll() &#x2F; prev() &#x2F; prevAll() 筛选器：filter() &#x2F; not() &#x2F; has() &#x2F; is() &#x2F; contains() 索引编号：eq() 尺寸和位置 尺寸相关：height() &#x2F; width() &#x2F; innerHeight() &#x2F; innerWidth() &#x2F; outerWidth() &#x2F; outerHeight() 位置相关：offset() &#x2F; position() &#x2F; scrollLeft() &#x2F; scrollTop() 特效和动画 基本动画：show() &#x2F; hide() &#x2F; toggle() 消失出现：fadeIn() &#x2F; fadeOut() &#x2F; fadeTo() &#x2F; fadeToggle() 滑动效果：slideDown() &#x2F; slideUp() &#x2F; slideToggle() 自定义：delay() &#x2F; stop() &#x2F; animate() 事件 文档加载：ready() &#x2F; load() 用户交互：on() &#x2F; off() 链式操作检测页面是否可用12345&lt;script&gt; $(document).ready(function() &#123; &#125;);&lt;/script&gt; 12345&lt;script&gt; $(function() &#123; &#125;);&lt;/script&gt; jQuery插件 jQuery Validation jQuery Treeview jQuery Autocomplete jQuery UI 避免和其他库的冲突先引入其他库再引入jQuery的情况。 12345678&lt;script src=&quot;other.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;jquery.js&quot;&gt;&lt;/script&gt;&lt;script&gt; jQuery.noConflict(); jQuery(function() &#123; jQuery(&#x27;div&#x27;).hide(); &#125;);&lt;/script&gt; 先引入jQuery再引入其他库的情况。 12345678&lt;script src=&quot;jquery.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;other.js&quot;&gt;&lt;/script&gt;&lt;script&gt; jQuery(function() &#123; jQuery(&#x27;div&#x27;).hide(); &#125;);&lt;/script&gt; 使用AjaxAjax是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 原生的Ajax 基于jQuery的Ajax 加载内容 提交表单 前端框架渐进式框架 - Vue.js前后端分离开发（前端渲染）必选框架。 快速上手 引入Vue的JavaScript文件，我们仍然推荐从CDN服务器加载它。 1&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt; 数据绑定（声明式渲染 ）。 12345678910111213&lt;div id=&quot;app&quot;&gt; &lt;h1&gt;&#123;&#123; product &#125;&#125;库存信息&lt;/h1&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; product: &#x27;iPhone X&#x27; &#125; &#125;);&lt;/script&gt; 条件与循环。 1234567891011121314151617181920212223242526&lt;div id=&quot;app&quot;&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if=&quot;product.quantity === 0&quot;&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [ &#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;iPhone X&quot;, &quot;quantity&quot;: 20&#125;, &#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;华为 Mate20&quot;, &quot;quantity&quot;: 0&#125;, &#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;小米 Mix3&quot;, &quot;quantity&quot;: 50&#125; ] &#125; &#125;);&lt;/script&gt; 计算属性。 12345678910111213141516171819202122232425262728293031323334&lt;div id=&quot;app&quot;&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if=&quot;product.quantity === 0&quot;&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [ &#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;iPhone X&quot;, &quot;quantity&quot;: 20&#125;, &#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;华为 Mate20&quot;, &quot;quantity&quot;: 0&#125;, &#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;小米 Mix3&quot;, &quot;quantity&quot;: 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 处理事件。 12345678910111213141516171819202122232425262728293031323334353637&lt;div id=&quot;app&quot;&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if=&quot;product.quantity === 0&quot;&gt; 已经售罄 &lt;/span&gt; &lt;button @click=&quot;product.quantity += 1&quot;&gt; 增加库存 &lt;/button&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [ &#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;iPhone X&quot;, &quot;quantity&quot;: 20&#125;, &#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;华为 Mate20&quot;, &quot;quantity&quot;: 0&#125;, &#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;小米 Mix3&quot;, &quot;quantity&quot;: 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 用户输入。 1234567891011121314151617181920212223242526272829303132333435363738&lt;div id=&quot;app&quot;&gt; &lt;h1&gt;库存信息&lt;/h1&gt; &lt;hr&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt; &#123;&#123; product.name &#125;&#125; - &lt;input type=&quot;number&quot; v-model.number=&quot;product.quantity&quot; min=&quot;0&quot;&gt; &lt;span v-if=&quot;product.quantity === 0&quot;&gt; 已经售罄 &lt;/span&gt; &lt;button @click=&quot;product.quantity += 1&quot;&gt; 增加库存 &lt;/button&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;库存总量：&#123;&#123; totalQuantity &#125;&#125;台&lt;/h2&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [ &#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;iPhone X&quot;, &quot;quantity&quot;: 20&#125;, &#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;华为 Mate20&quot;, &quot;quantity&quot;: 0&#125;, &#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;小米 Mix3&quot;, &quot;quantity&quot;: 50&#125; ] &#125;, computed: &#123; totalQuantity() &#123; return this.products.reduce((sum, product) =&gt; &#123; return sum + product.quantity &#125;, 0); &#125; &#125; &#125;);&lt;/script&gt; 通过网络加载JSON数据。 12345678910111213141516171819202122232425262728&lt;div id=&quot;app&quot;&gt; &lt;h2&gt;库存信息&lt;/h2&gt; &lt;ul&gt; &lt;li v-for=&quot;product in products&quot;&gt; &#123;&#123; product.name &#125;&#125; - &#123;&#123; product.quantity &#125;&#125; &lt;span v-if=&quot;product.quantity === 0&quot;&gt; 已经售罄 &lt;/span&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue&quot;&gt;&lt;/script&gt;&lt;script&gt; const app = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; products: [] &#125;， created() &#123; fetch(&#x27;https://jackfrued.top/api/products&#x27;) .then(response =&gt; response.json()) .then(json =&gt; &#123; this.products = json &#125;); &#125; &#125;);&lt;/script&gt; 使用脚手架 - vue-cliVue为商业项目开发提供了非常便捷的脚手架工具vue-cli，通过工具可以省去手工配置开发环境、测试环境和运行环境的步骤，让开发者只需要关注要解决的问题。 安装脚手架。 创建项目。 安装依赖包。 运行项目。 UI框架 - Element基于Vue 2.0的桌面端组件库，用于构造用户界面，支持响应式布局。 引入Element的CSS和JavaScript文件。 1234&lt;!-- 引入样式 --&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/element-ui/lib/theme-chalk/index.css&quot;&gt;&lt;!-- 引入组件库 --&gt;&lt;script src=&quot;https://unpkg.com/element-ui/lib/index.js&quot;&gt;&lt;/script&gt; 一个简单的例子。 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/element-ui/lib/theme-chalk/index.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt; &lt;el-button @click=&quot;visible = true&quot;&gt;点我&lt;/el-button&gt; &lt;el-dialog :visible.sync=&quot;visible&quot; title=&quot;Hello world&quot;&gt; &lt;p&gt;开始使用Element吧&lt;/p&gt; &lt;/el-dialog&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src=&quot;https://unpkg.com/vue/dist/vue.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/element-ui/lib/index.js&quot;&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; visible: false, &#125; &#125;) &lt;/script&gt;&lt;/html&gt; 使用组件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/element-ui/lib/theme-chalk/index.css&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;app&quot;&gt; &lt;el-table :data=&quot;tableData&quot; stripe style=&quot;width: 100%&quot;&gt; &lt;el-table-column prop=&quot;date&quot; label=&quot;日期&quot; width=&quot;180&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;name&quot; label=&quot;姓名&quot; width=&quot;180&quot;&gt; &lt;/el-table-column&gt; &lt;el-table-column prop=&quot;address&quot; label=&quot;地址&quot;&gt; &lt;/el-table-column&gt; &lt;/el-table&gt; &lt;/div&gt; &lt;/body&gt; &lt;script src=&quot;https://unpkg.com/vue/dist/vue.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/element-ui/lib/index.js&quot;&gt;&lt;/script&gt; &lt;script&gt; new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; tableData: [ &#123; date: &#x27;2016-05-02&#x27;, name: &#x27;王一霸&#x27;, address: &#x27;上海市普陀区金沙江路 1518 弄&#x27; &#125;, &#123; date: &#x27;2016-05-04&#x27;, name: &#x27;刘二狗&#x27;, address: &#x27;上海市普陀区金沙江路 1517 弄&#x27; &#125;, &#123; date: &#x27;2016-05-01&#x27;, name: &#x27;杨三萌&#x27;, address: &#x27;上海市普陀区金沙江路 1519 弄&#x27; &#125;, &#123; date: &#x27;2016-05-03&#x27;, name: &#x27;陈四吹&#x27;, address: &#x27;上海市普陀区金沙江路 1516 弄&#x27; &#125; ] &#125; &#125;) &lt;/script&gt;&lt;/html&gt; 报表框架 - ECharts百度出品的开源可视化库，常用于生成各种类型的报表。 基于弹性盒子的CSS框架 - BulmaBulma是一个基于Flexbox的现代化的CSS框架，其初衷就是移动优先（Mobile First），模块化设计，可以轻松用来实现各种简单或者复杂的内容布局，即使不懂CSS的开发者也能够使用它定制出漂亮的页面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Bulma&lt;/title&gt; &lt;link href=&quot;https://cdn.bootcss.com/bulma/0.7.4/css/bulma.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;style type=&quot;text/css&quot;&gt; div &#123; margin-top: 10px; &#125; .column &#123; color: #fff; background-color: #063; margin: 10px 10px; text-align: center; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class=&quot;columns&quot;&gt; &lt;div class=&quot;column&quot;&gt;1&lt;/div&gt; &lt;div class=&quot;column&quot;&gt;2&lt;/div&gt; &lt;div class=&quot;column&quot;&gt;3&lt;/div&gt; &lt;div class=&quot;column&quot;&gt;4&lt;/div&gt; &lt;/div&gt; &lt;div&gt; &lt;a class=&quot;button is-primary&quot;&gt;Primary&lt;/a&gt; &lt;a class=&quot;button is-link&quot;&gt;Link&lt;/a&gt; &lt;a class=&quot;button is-info&quot;&gt;Info&lt;/a&gt; &lt;a class=&quot;button is-success&quot;&gt;Success&lt;/a&gt; &lt;a class=&quot;button is-warning&quot;&gt;Warning&lt;/a&gt; &lt;a class=&quot;button is-danger&quot;&gt;Danger&lt;/a&gt; &lt;/div&gt; &lt;div&gt; &lt;progress class=&quot;progress is-danger is-medium&quot; max=&quot;100&quot;&gt;60%&lt;/progress&gt; &lt;/div&gt; &lt;div&gt; &lt;table class=&quot;table is-hoverable&quot;&gt; &lt;tr&gt; &lt;th&gt;One&lt;/th&gt; &lt;th&gt;Two&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Three&lt;/td&gt; &lt;td&gt;Four&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Five&lt;/td&gt; &lt;td&gt;Six&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Seven&lt;/td&gt; &lt;td&gt;Eight&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Nine&lt;/td&gt; &lt;td&gt;Ten&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Eleven&lt;/td&gt; &lt;td&gt;Twelve&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 响应式布局框架 - Bootstrap用于快速开发Web应用程序的前端框架，支持响应式布局。 特点 支持主流的浏览器和移动设备 容易上手 响应式设计 内容 网格系统 封装的CSS 现成的组件 JavaScript插件 可视化","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day16-20/16-20.Python语言进阶","date":"2024-12-12T08:38:01.035Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day16-20/16-20.Python语言进阶/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day16-20/16-20.Python%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6/","excerpt":"","text":"Python语言进阶重要知识点 生成式（推导式）的用法 123456789101112prices = &#123; &#x27;AAPL&#x27;: 191.88, &#x27;GOOG&#x27;: 1186.96, &#x27;IBM&#x27;: 149.24, &#x27;ORCL&#x27;: 48.44, &#x27;ACN&#x27;: 166.89, &#x27;FB&#x27;: 208.09, &#x27;SYMC&#x27;: 21.29&#125;# 用股票价格大于100元的股票构造一个新的字典prices2 = &#123;key: value for key, value in prices.items() if value &gt; 100&#125;print(prices2) 说明：生成式（推导式）可以用来生成列表、集合和字典。 嵌套的列表的坑 12345678910names = [&#x27;关羽&#x27;, &#x27;张飞&#x27;, &#x27;赵云&#x27;, &#x27;马超&#x27;, &#x27;黄忠&#x27;]courses = [&#x27;语文&#x27;, &#x27;数学&#x27;, &#x27;英语&#x27;]# 录入五个学生三门课程的成绩# 错误 - 参考http://pythontutor.com/visualize.html#mode=edit# scores = [[None] * len(courses)] * len(names)scores = [[None] * len(courses) for _ in range(len(names))]for row, name in enumerate(names): for col, course in enumerate(courses): scores[row][col] = float(input(f&#x27;请输入&#123;name&#125;的&#123;course&#125;成绩: &#x27;)) print(scores) Python Tutor - VISUALIZE CODE AND GET LIVE HELP heapq模块（堆排序） 12345678910111213141516171819&quot;&quot;&quot;从列表中找出最大的或最小的N个元素堆结构(大根堆/小根堆)&quot;&quot;&quot;import heapqlist1 = [34, 25, 12, 99, 87, 63, 58, 78, 88, 92]list2 = [ &#123;&#x27;name&#x27;: &#x27;IBM&#x27;, &#x27;shares&#x27;: 100, &#x27;price&#x27;: 91.1&#125;, &#123;&#x27;name&#x27;: &#x27;AAPL&#x27;, &#x27;shares&#x27;: 50, &#x27;price&#x27;: 543.22&#125;, &#123;&#x27;name&#x27;: &#x27;FB&#x27;, &#x27;shares&#x27;: 200, &#x27;price&#x27;: 21.09&#125;, &#123;&#x27;name&#x27;: &#x27;HPQ&#x27;, &#x27;shares&#x27;: 35, &#x27;price&#x27;: 31.75&#125;, &#123;&#x27;name&#x27;: &#x27;YHOO&#x27;, &#x27;shares&#x27;: 45, &#x27;price&#x27;: 16.35&#125;, &#123;&#x27;name&#x27;: &#x27;ACME&#x27;, &#x27;shares&#x27;: 75, &#x27;price&#x27;: 115.65&#125;]print(heapq.nlargest(3, list1))print(heapq.nsmallest(3, list1))print(heapq.nlargest(2, list2, key=lambda x: x[&#x27;price&#x27;]))print(heapq.nlargest(2, list2, key=lambda x: x[&#x27;shares&#x27;])) itertools模块 12345678910111213&quot;&quot;&quot;迭代工具模块&quot;&quot;&quot;import itertools# 产生ABCD的全排列itertools.permutations(&#x27;ABCD&#x27;)# 产生ABCDE的五选三组合itertools.combinations(&#x27;ABCDE&#x27;, 3)# 产生ABCD和123的笛卡尔积itertools.product(&#x27;ABCD&#x27;, &#x27;123&#x27;)# 产生ABC的无限循环序列itertools.cycle((&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;)) collections模块 常用的工具类： namedtuple：命令元组，它是一个类工厂，接受类型的名称和属性列表来创建一个类。 deque：双端队列，是列表的替代实现。Python中的列表底层是基于数组来实现的，而deque底层是双向链表，因此当你需要在头尾添加和删除元素时，deque会表现出更好的性能，渐近时间复杂度为$O(1)$。 Counter：dict的子类，键是元素，值是元素的计数，它的most_common()方法可以帮助我们获取出现频率最高的元素。Counter和dict的继承关系我认为是值得商榷的，按照CARP原则，Counter跟dict的关系应该设计为关联关系更为合理。 OrderedDict：dict的子类，它记录了键值对插入的顺序，看起来既有字典的行为，也有链表的行为。 defaultdict：类似于字典类型，但是可以通过默认的工厂函数来获得键对应的默认值，相比字典中的setdefault()方法，这种做法更加高效。 12345678910111213&quot;&quot;&quot;找出序列中出现次数最多的元素&quot;&quot;&quot;from collections import Counterwords = [ &#x27;look&#x27;, &#x27;into&#x27;, &#x27;my&#x27;, &#x27;eyes&#x27;, &#x27;look&#x27;, &#x27;into&#x27;, &#x27;my&#x27;, &#x27;eyes&#x27;, &#x27;the&#x27;, &#x27;eyes&#x27;, &#x27;the&#x27;, &#x27;eyes&#x27;, &#x27;the&#x27;, &#x27;eyes&#x27;, &#x27;not&#x27;, &#x27;around&#x27;, &#x27;the&#x27;, &#x27;eyes&#x27;, &quot;don&#x27;t&quot;, &#x27;look&#x27;, &#x27;around&#x27;, &#x27;the&#x27;, &#x27;eyes&#x27;, &#x27;look&#x27;, &#x27;into&#x27;, &#x27;my&#x27;, &#x27;eyes&#x27;, &quot;you&#x27;re&quot;, &#x27;under&#x27;]counter = Counter(words)print(counter.most_common(3)) 数据结构和算法 算法：解决问题的方法和步骤 评价算法的好坏：渐近时间复杂度和渐近空间复杂度。 渐近时间复杂度的大O标记： - 常量时间复杂度 - 布隆过滤器 &#x2F; 哈希存储 - 对数时间复杂度 - 折半查找（二分查找） - 线性时间复杂度 - 顺序查找 &#x2F; 计数排序 - 对数线性时间复杂度 - 高级排序算法（归并排序、快速排序） - 平方时间复杂度 - 简单排序算法（选择排序、插入排序、冒泡排序） - 立方时间复杂度 - Floyd算法 &#x2F; 矩阵乘法运算 - 几何级数时间复杂度 - 汉诺塔 - 阶乘时间复杂度 - 旅行经销商问题 - NPC 排序算法（选择、冒泡和归并）和查找算法（顺序和折半） 12345678910def select_sort(items, comp=lambda x, y: x &lt; y): &quot;&quot;&quot;简单选择排序&quot;&quot;&quot; items = items[:] for i in range(len(items) - 1): min_index = i for j in range(i + 1, len(items)): if comp(items[j], items[min_index]): min_index = j items[i], items[min_index] = items[min_index], items[i] return items 123456789101112def bubble_sort(items, comp=lambda x, y: x &gt; y): &quot;&quot;&quot;冒泡排序&quot;&quot;&quot; items = items[:] for i in range(len(items) - 1): swapped = False for j in range(len(items) - 1 - i): if comp(items[j], items[j + 1]): items[j], items[j + 1] = items[j + 1], items[j] swapped = True if not swapped: break return items 123456789101112131415161718def bubble_sort(items, comp=lambda x, y: x &gt; y): &quot;&quot;&quot;搅拌排序(冒泡排序升级版)&quot;&quot;&quot; items = items[:] for i in range(len(items) - 1): swapped = False for j in range(len(items) - 1 - i): if comp(items[j], items[j + 1]): items[j], items[j + 1] = items[j + 1], items[j] swapped = True if swapped: swapped = False for j in range(len(items) - 2 - i, i, -1): if comp(items[j - 1], items[j]): items[j], items[j - 1] = items[j - 1], items[j] swapped = True if not swapped: break return items 12345678910111213141516171819202122232425262728def merge(items1, items2, comp=lambda x, y: x &lt; y): &quot;&quot;&quot;合并(将两个有序的列表合并成一个有序的列表)&quot;&quot;&quot; items = [] index1, index2 = 0, 0 while index1 &lt; len(items1) and index2 &lt; len(items2): if comp(items1[index1], items2[index2]): items.append(items1[index1]) index1 += 1 else: items.append(items2[index2]) index2 += 1 items += items1[index1:] items += items2[index2:] return itemsdef merge_sort(items, comp=lambda x, y: x &lt; y): return _merge_sort(list(items), comp)def _merge_sort(items, comp): &quot;&quot;&quot;归并排序&quot;&quot;&quot; if len(items) &lt; 2: return items mid = len(items) // 2 left = _merge_sort(items[:mid], comp) right = _merge_sort(items[mid:], comp) return merge(left, right, comp) 123456def seq_search(items, key): &quot;&quot;&quot;顺序查找&quot;&quot;&quot; for index, item in enumerate(items): if item == key: return index return -1 123456789101112def bin_search(items, key): &quot;&quot;&quot;折半查找&quot;&quot;&quot; start, end = 0, len(items) - 1 while start &lt;= end: mid = (start + end) // 2 if key &gt; items[mid]: start = mid + 1 elif key &lt; items[mid]: end = mid - 1 else: return mid return -1 常用算法： 穷举法 - 又称为暴力破解法，对所有的可能性进行验证，直到找到正确答案。 贪婪法 - 在对问题求解时，总是做出在当前看来 最好的选择，不追求最优解，快速找到满意解。 分治法 - 把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题，直到可以直接求解的程度，最后将子问题的解进行合并得到原问题的解。 回溯法 - 回溯法又称为试探法，按选优条件向前搜索，当搜索到某一步发现原先选择并不优或达不到目标时，就退回一步重新选择。 动态规划 - 基本思想也是将待求解问题分解成若干个子问题，先求解并保存这些子问题的解，避免产生大量的重复运算。 穷举法例子：百钱百鸡和五人分鱼。 1234567891011121314151617181920212223242526# 公鸡5元一只 母鸡3元一只 小鸡1元三只# 用100元买100只鸡 问公鸡/母鸡/小鸡各多少只for x in range(20): for y in range(33): z = 100 - x - y if 5 * x + 3 * y + z // 3 == 100 and z % 3 == 0: print(x, y, z)# A、B、C、D、E五人在某天夜里合伙捕鱼 最后疲惫不堪各自睡觉# 第二天A第一个醒来 他将鱼分为5份 扔掉多余的1条 拿走自己的一份# B第二个醒来 也将鱼分为5份 扔掉多余的1条 拿走自己的一份# 然后C、D、E依次醒来也按同样的方式分鱼 问他们至少捕了多少条鱼fish = 6while True: total = fish enough = True for _ in range(5): if (total - 1) % 5 == 0: total = (total - 1) // 5 * 4 else: enough = False break if enough: print(fish) break fish += 5 贪婪法例子：假设小偷有一个背包，最多能装20公斤赃物，他闯入一户人家，发现如下表所示的物品。很显然，他不能把所有物品都装进背包，所以必须确定拿走哪些物品，留下哪些物品。 名称 价格（美元） 重量（kg） 电脑 200 20 收音机 20 4 钟 175 10 花瓶 50 2 书 10 1 油画 90 9 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&quot;&quot;&quot;贪婪法：在对问题求解时，总是做出在当前看来是最好的选择，不追求最优解，快速找到满意解。输入：20 6电脑 200 20收音机 20 4钟 175 10花瓶 50 2书 10 1油画 90 9&quot;&quot;&quot;class Thing(object): &quot;&quot;&quot;物品&quot;&quot;&quot; def __init__(self, name, price, weight): self.name = name self.price = price self.weight = weight @property def value(self): &quot;&quot;&quot;价格重量比&quot;&quot;&quot; return self.price / self.weightdef input_thing(): &quot;&quot;&quot;输入物品信息&quot;&quot;&quot; name_str, price_str, weight_str = input().split() return name_str, int(price_str), int(weight_str)def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; max_weight, num_of_things = map(int, input().split()) all_things = [] for _ in range(num_of_things): all_things.append(Thing(*input_thing())) all_things.sort(key=lambda x: x.value, reverse=True) total_weight = 0 total_price = 0 for thing in all_things: if total_weight + thing.weight &lt;= max_weight: print(f&#x27;小偷拿走了&#123;thing.name&#125;&#x27;) total_weight += thing.weight total_price += thing.price print(f&#x27;总价值: &#123;total_price&#125;美元&#x27;)if __name__ == &#x27;__main__&#x27;: main() 分治法例子：快速排序。 12345678910111213141516171819202122232425&quot;&quot;&quot;快速排序 - 选择枢轴对元素进行划分，左边都比枢轴小右边都比枢轴大&quot;&quot;&quot;def quick_sort(items, comp=lambda x, y: x &lt;= y): items = list(items)[:] _quick_sort(items, 0, len(items) - 1, comp) return itemsdef _quick_sort(items, start, end, comp): if start &lt; end: pos = _partition(items, start, end, comp) _quick_sort(items, start, pos - 1, comp) _quick_sort(items, pos + 1, end, comp)def _partition(items, start, end, comp): pivot = items[end] i = start - 1 for j in range(start, end): if comp(items[j], pivot): i += 1 items[i], items[j] = items[j], items[i] items[i + 1], items[end] = items[end], items[i + 1] return i + 1 回溯法例子：骑士巡逻。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&quot;&quot;&quot;递归回溯法：叫称为试探法，按选优条件向前搜索，当搜索到某一步，发现原先选择并不优或达不到目标时，就退回一步重新选择，比较经典的问题包括骑士巡逻、八皇后和迷宫寻路等。&quot;&quot;&quot;import sysimport timeSIZE = 5total = 0def print_board(board): for row in board: for col in row: print(str(col).center(4), end=&#x27;&#x27;) print()def patrol(board, row, col, step=1): if row &gt;= 0 and row &lt; SIZE and \\ col &gt;= 0 and col &lt; SIZE and \\ board[row][col] == 0: board[row][col] = step if step == SIZE * SIZE: global total total += 1 print(f&#x27;第&#123;total&#125;种走法: &#x27;) print_board(board) patrol(board, row - 2, col - 1, step + 1) patrol(board, row - 1, col - 2, step + 1) patrol(board, row + 1, col - 2, step + 1) patrol(board, row + 2, col - 1, step + 1) patrol(board, row + 2, col + 1, step + 1) patrol(board, row + 1, col + 2, step + 1) patrol(board, row - 1, col + 2, step + 1) patrol(board, row - 2, col + 1, step + 1) board[row][col] = 0def main(): board = [[0] * SIZE for _ in range(SIZE)] patrol(board, SIZE - 1, SIZE - 1)if __name__ == &#x27;__main__&#x27;: main() 动态规划例子：子列表元素之和的最大值。 说明：子列表指的是列表中索引（下标）连续的元素构成的列表；列表中的元素是int类型，可能包含正整数、0、负整数；程序输入列表中的元素，输出子列表元素求和的最大值，例如： 输入：1 -2 3 5 -3 2 输出：8 输入：0 -2 3 5 -1 2 输出：9 输入：-9 -2 -3 -5 -3 输出：-2 1234567891011def main(): items = list(map(int, input().split())) overall = partial = items[0] for i in range(1, len(items)): partial = max(items[i], partial + items[i]) overall = max(partial, overall) print(overall)if __name__ == &#x27;__main__&#x27;: main() 说明：这个题目最容易想到的解法是使用二重循环，但是代码的时间性能将会变得非常的糟糕。使用动态规划的思想，仅仅是多用了两个变量，就将原来$O(N^2)$复杂度的问题变成了$O(N)$。 函数的使用方式 将函数视为“一等公民” 函数可以赋值给变量 函数可以作为函数的参数 函数可以作为函数的返回值 高阶函数的用法（filter、map以及它们的替代品） 12items1 = list(map(lambda x: x ** 2, filter(lambda x: x % 2, range(1, 10))))items2 = [x ** 2 for x in range(1, 10) if x % 2] 位置参数、可变参数、关键字参数、命名关键字参数 参数的元信息（代码可读性问题） 匿名函数和内联函数的用法（lambda函数） 闭包和作用域问题 Python搜索变量的LEGB顺序（Local &gt;&gt;&gt; Embedded &gt;&gt;&gt; Global &gt;&gt;&gt; Built-in） global和nonlocal关键字的作用 global：声明或定义全局变量（要么直接使用现有的全局作用域的变量，要么定义一个变量放到全局作用域）。 nonlocal：声明使用嵌套作用域的变量（嵌套作用域必须存在该变量，否则报错）。 装饰器函数（使用装饰器和取消装饰器） 例子：输出函数执行时间的装饰器。 1234567891011def record_time(func): &quot;&quot;&quot;自定义装饰函数的装饰器&quot;&quot;&quot; @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) print(f&#x27;&#123;func.__name__&#125;: &#123;time() - start&#125;秒&#x27;) return result return wrapper 如果装饰器不希望跟print函数耦合，可以编写可以参数化的装饰器。 12345678910111213141516171819from functools import wrapsfrom time import timedef record(output): &quot;&quot;&quot;可以参数化的装饰器&quot;&quot;&quot; def decorate(func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) output(func.__name__, time() - start) return result return wrapper return decorate 1234567891011121314151617181920from functools import wrapsfrom time import timeclass Record(): &quot;&quot;&quot;通过定义类的方式定义装饰器&quot;&quot;&quot; def __init__(self, output): self.output = output def __call__(self, func): @wraps(func) def wrapper(*args, **kwargs): start = time() result = func(*args, **kwargs) self.output(func.__name__, time() - start) return result return wrapper 说明：由于对带装饰功能的函数添加了@wraps装饰器，可以通过func.__wrapped__方式获得被装饰之前的函数或类来取消装饰器的作用。 例子：用装饰器来实现单例模式。 1234567891011121314151617181920from functools import wrapsdef singleton(cls): &quot;&quot;&quot;装饰类的装饰器&quot;&quot;&quot; instances = &#123;&#125; @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper@singletonclass President: &quot;&quot;&quot;总统(单例类)&quot;&quot;&quot; pass 提示：上面的代码中用到了闭包（closure），不知道你是否已经意识到了。还没有一个小问题就是，上面的代码并没有实现线程安全的单例，如果要实现线程安全的单例应该怎么做呢？ 线程安全的单例装饰器。 123456789101112131415161718from functools import wrapsfrom threading import RLockdef singleton(cls): &quot;&quot;&quot;线程安全的单例装饰器&quot;&quot;&quot; instances = &#123;&#125; locker = RLock() @wraps(cls) def wrapper(*args, **kwargs): if cls not in instances: with locker: if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return wrapper 提示：上面的代码用到了with上下文语法来进行锁操作，因为锁对象本身就是上下文管理器对象（支持__enter__和__exit__魔术方法）。在wrapper函数中，我们先做了一次不带锁的检查，然后再做带锁的检查，这样做比直接加锁检查性能要更好，如果对象已经创建就没有必须再去加锁而是直接返回该对象就可以了。 面向对象相关知识 三大支柱：封装、继承、多态 例子：工资结算系统。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&quot;&quot;&quot;月薪结算系统 - 部门经理每月15000 程序员每小时200 销售员1800底薪加销售额5%提成&quot;&quot;&quot;from abc import ABCMeta, abstractmethodclass Employee(metaclass=ABCMeta): &quot;&quot;&quot;员工(抽象类)&quot;&quot;&quot; def __init__(self, name): self.name = name @abstractmethod def get_salary(self): &quot;&quot;&quot;结算月薪(抽象方法)&quot;&quot;&quot; passclass Manager(Employee): &quot;&quot;&quot;部门经理&quot;&quot;&quot; def get_salary(self): return 15000.0class Programmer(Employee): &quot;&quot;&quot;程序员&quot;&quot;&quot; def __init__(self, name, working_hour=0): self.working_hour = working_hour super().__init__(name) def get_salary(self): return 200.0 * self.working_hourclass Salesman(Employee): &quot;&quot;&quot;销售员&quot;&quot;&quot; def __init__(self, name, sales=0.0): self.sales = sales super().__init__(name) def get_salary(self): return 1800.0 + self.sales * 0.05class EmployeeFactory: &quot;&quot;&quot;创建员工的工厂（工厂模式 - 通过工厂实现对象使用者和对象之间的解耦合）&quot;&quot;&quot; @staticmethod def create(emp_type, *args, **kwargs): &quot;&quot;&quot;创建员工&quot;&quot;&quot; all_emp_types = &#123;&#x27;M&#x27;: Manager, &#x27;P&#x27;: Programmer, &#x27;S&#x27;: Salesman&#125; cls = all_emp_types[emp_type.upper()] return cls(*args, **kwargs) if cls else Nonedef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; emps = [ EmployeeFactory.create(&#x27;M&#x27;, &#x27;曹操&#x27;), EmployeeFactory.create(&#x27;P&#x27;, &#x27;荀彧&#x27;, 120), EmployeeFactory.create(&#x27;P&#x27;, &#x27;郭嘉&#x27;, 85), EmployeeFactory.create(&#x27;S&#x27;, &#x27;典韦&#x27;, 123000), ] for emp in emps: print(f&#x27;&#123;emp.name&#125;: &#123;emp.get_salary():.2f&#125;元&#x27;)if __name__ == &#x27;__main__&#x27;: main() 类与类之间的关系 is-a关系：继承 has-a关系：关联 &#x2F; 聚合 &#x2F; 合成 use-a关系：依赖 例子：扑克游戏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&quot;&quot;&quot;经验：符号常量总是优于字面常量，枚举类型是定义符号常量的最佳选择&quot;&quot;&quot;from enum import Enum, uniqueimport random@uniqueclass Suite(Enum): &quot;&quot;&quot;花色&quot;&quot;&quot; SPADE, HEART, CLUB, DIAMOND = range(4) def __lt__(self, other): return self.value &lt; other.valueclass Card: &quot;&quot;&quot;牌&quot;&quot;&quot; def __init__(self, suite, face): &quot;&quot;&quot;初始化方法&quot;&quot;&quot; self.suite = suite self.face = face def show(self): &quot;&quot;&quot;显示牌面&quot;&quot;&quot; suites = [&#x27;♠︎&#x27;, &#x27;♥︎&#x27;, &#x27;♣︎&#x27;, &#x27;♦︎&#x27;] faces = [&#x27;&#x27;, &#x27;A&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;, &#x27;J&#x27;, &#x27;Q&#x27;, &#x27;K&#x27;] return f&#x27;&#123;suites[self.suite.value]&#125;&#123;faces[self.face]&#125;&#x27; def __repr__(self): return self.show()class Poker: &quot;&quot;&quot;扑克&quot;&quot;&quot; def __init__(self): self.index = 0 self.cards = [Card(suite, face) for suite in Suite for face in range(1, 14)] def shuffle(self): &quot;&quot;&quot;洗牌（随机乱序）&quot;&quot;&quot; random.shuffle(self.cards) self.index = 0 def deal(self): &quot;&quot;&quot;发牌&quot;&quot;&quot; card = self.cards[self.index] self.index += 1 return card @property def has_more(self): return self.index &lt; len(self.cards)class Player: &quot;&quot;&quot;玩家&quot;&quot;&quot; def __init__(self, name): self.name = name self.cards = [] def get_one(self, card): &quot;&quot;&quot;摸一张牌&quot;&quot;&quot; self.cards.append(card) def sort(self, comp=lambda card: (card.suite, card.face)): &quot;&quot;&quot;整理手上的牌&quot;&quot;&quot; self.cards.sort(key=comp)def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; poker = Poker() poker.shuffle() players = [Player(&#x27;东邪&#x27;), Player(&#x27;西毒&#x27;), Player(&#x27;南帝&#x27;), Player(&#x27;北丐&#x27;)] while poker.has_more: for player in players: player.get_one(poker.deal()) for player in players: player.sort() print(player.name, end=&#x27;: &#x27;) print(player.cards)if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码中使用了Emoji字符来表示扑克牌的四种花色，在某些不支持Emoji字符的系统上可能无法显示。 对象的复制（深复制&#x2F;深拷贝&#x2F;深度克隆和浅复制&#x2F;浅拷贝&#x2F;影子克隆） 垃圾回收、循环引用和弱引用 Python使用了自动化内存管理，这种管理机制以引用计数为基础，同时也引入了标记-清除和分代收集两种机制为辅的策略。 123456typedef struct _object &#123; /* 引用计数 */ int ob_refcnt; /* 对象指针 */ struct _typeobject *ob_type;&#125; PyObject; 12345678/* 增加引用计数的宏定义 */#define Py_INCREF(op) ((op)-&gt;ob_refcnt++)/* 减少引用计数的宏定义 */#define Py_DECREF(op) \\ //减少计数 if (--(op)-&gt;ob_refcnt != 0) \\ ; \\ else \\ __Py_Dealloc((PyObject *)(op)) 导致引用计数+1的情况： 对象被创建，例如a = 23 对象被引用，例如b = a 对象被作为参数，传入到一个函数中，例如f(a) 对象作为一个元素，存储在容器中，例如list1 = [a, a] 导致引用计数-1的情况： 对象的别名被显式销毁，例如del a 对象的别名被赋予新的对象，例如a = 24 一个对象离开它的作用域，例如f函数执行完毕时，f函数中的局部变量（全局变量不会） 对象所在的容器被销毁，或从容器中删除对象 引用计数可能会导致循环引用问题，而循环引用会导致内存泄露，如下面的代码所示。为了解决这个问题，Python中引入了“标记-清除”和“分代收集”。在创建一个对象的时候，对象被放在第一代中，如果在第一代的垃圾检查中对象存活了下来，该对象就会被放到第二代中，同理在第二代的垃圾检查中对象存活下来，该对象就会被放到第三代中。 1234567# 循环引用会导致内存泄露 - Python除了引用技术还引入了标记清理和分代回收# 在Python 3.6以前如果重写__del__魔术方法会导致循环引用处理失效# 如果不想造成循环引用可以使用弱引用list1 = []list2 = [] list1.append(list2)list2.append(list1) 以下情况会导致垃圾回收： 调用gc.collect() gc模块的计数器达到阀值 程序退出 如果循环引用中两个对象都定义了__del__方法，gc模块不会销毁这些不可达对象，因为gc模块不知道应该先调用哪个对象的__del__方法，这个问题在Python 3.6中得到了解决。 也可以通过weakref模块构造弱引用的方式来解决循环引用的问题。 魔法属性和方法（请参考《Python魔法方法指南》） 有几个小问题请大家思考： 自定义的对象能不能使用运算符做运算？ 自定义的对象能不能放到set中？能去重吗？ 自定义的对象能不能作为dict的键？ 自定义的对象能不能使用上下文语法？ 混入（Mixin） 例子：自定义字典限制只有在指定的key不存在时才能在字典中设置键值对。 12345678910111213141516171819202122class SetOnceMappingMixin: &quot;&quot;&quot;自定义混入类&quot;&quot;&quot; __slots__ = () def __setitem__(self, key, value): if key in self: raise KeyError(str(key) + &#x27; already set&#x27;) return super().__setitem__(key, value)class SetOnceDict(SetOnceMappingMixin, dict): &quot;&quot;&quot;自定义字典&quot;&quot;&quot; passmy_dict= SetOnceDict()try: my_dict[&#x27;username&#x27;] = &#x27;jackfrued&#x27; my_dict[&#x27;username&#x27;] = &#x27;hellokitty&#x27;except KeyError: passprint(my_dict) 元编程和元类 对象是通过类创建的，类是通过元类创建的，元类提供了创建类的元信息。所有的类都直接或间接的继承自object，所有的元类都直接或间接的继承自type。 例子：用元类实现单例模式。 1234567891011121314151617181920212223import threadingclass SingletonMeta(type): &quot;&quot;&quot;自定义元类&quot;&quot;&quot; def __init__(cls, *args, **kwargs): cls.__instance = None cls.__lock = threading.RLock() super().__init__(*args, **kwargs) def __call__(cls, *args, **kwargs): if cls.__instance is None: with cls.__lock: if cls.__instance is None: cls.__instance = super().__call__(*args, **kwargs) return cls.__instanceclass President(metaclass=SingletonMeta): &quot;&quot;&quot;总统(单例类)&quot;&quot;&quot; pass 面向对象设计原则 单一职责原则 （SRP）- 一个类只做该做的事情（类的设计要高内聚） 开闭原则 （OCP）- 软件实体应该对扩展开发对修改关闭 依赖倒转原则（DIP）- 面向抽象编程（在弱类型语言中已经被弱化） 里氏替换原则（LSP） - 任何时候可以用子类对象替换掉父类对象 接口隔离原则（ISP）- 接口要小而专不要大而全（Python中没有接口的概念） 合成聚合复用原则（CARP） - 优先使用强关联关系而不是继承关系复用代码 最少知识原则（迪米特法则，LoD）- 不要给没有必然联系的对象发消息 说明：上面加粗的字母放在一起称为面向对象的SOLID原则。 GoF设计模式 创建型模式：单例、工厂、建造者、原型 结构型模式：适配器、门面（外观）、代理 行为型模式：迭代器、观察者、状态、策略 例子：可插拔的哈希算法（策略模式）。 1234567891011121314151617181920212223242526272829class StreamHasher(): &quot;&quot;&quot;哈希摘要生成器&quot;&quot;&quot; def __init__(self, alg=&#x27;md5&#x27;, size=4096): self.size = size alg = alg.lower() self.hasher = getattr(__import__(&#x27;hashlib&#x27;), alg.lower())() def __call__(self, stream): return self.to_digest(stream) def to_digest(self, stream): &quot;&quot;&quot;生成十六进制形式的摘要&quot;&quot;&quot; for buf in iter(lambda: stream.read(self.size), b&#x27;&#x27;): self.hasher.update(buf) return self.hasher.hexdigest()def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; hasher1 = StreamHasher() with open(&#x27;Python-3.7.6.tgz&#x27;, &#x27;rb&#x27;) as stream: print(hasher1.to_digest(stream)) hasher2 = StreamHasher(&#x27;sha1&#x27;) with open(&#x27;Python-3.7.6.tgz&#x27;, &#x27;rb&#x27;) as stream: print(hasher2(stream))if __name__ == &#x27;__main__&#x27;: main() 迭代器和生成器 迭代器是实现了迭代器协议的对象。 Python中没有像protocol或interface这样的定义协议的关键字。 Python中用魔术方法表示协议。 __iter__和__next__魔术方法就是迭代器协议。 1234567891011121314151617class Fib(object): &quot;&quot;&quot;迭代器&quot;&quot;&quot; def __init__(self, num): self.num = num self.a, self.b = 0, 1 self.idx = 0 def __iter__(self): return self def __next__(self): if self.idx &lt; self.num: self.a, self.b = self.b, self.a + self.b self.idx += 1 return self.a raise StopIteration() 生成器是语法简化版的迭代器。 123456def fib(num): &quot;&quot;&quot;生成器&quot;&quot;&quot; a, b = 0, 1 for _ in range(num): a, b = b, a + b yield a 生成器进化为协程。 生成器对象可以使用send()方法发送数据，发送的数据会成为生成器函数中通过yield表达式获得的值。这样，生成器就可以作为协程使用，协程简单的说就是可以相互协作的子程序。 123456789101112131415def calc_avg(): &quot;&quot;&quot;流式计算平均值&quot;&quot;&quot; total, counter = 0, 0 avg_value = None while True: value = yield avg_value total, counter = total + value, counter + 1 avg_value = total / countergen = calc_avg()next(gen)print(gen.send(10))print(gen.send(20))print(gen.send(30)) 并发编程Python中实现并发编程的三种方案：多线程、多进程和异步I&#x2F;O。并发编程的好处在于可以提升程序的执行效率以及改善用户体验；坏处在于并发的程序不容易开发和调试，同时对其他程序来说它并不友好。 多线程：Python中提供了Thread类并辅以Lock、Condition、Event、Semaphore和Barrier。Python中有GIL来防止多个线程同时执行本地字节码，这个锁对于CPython是必须的，因为CPython的内存管理并不是线程安全的，因为GIL的存在多线程并不能发挥CPU的多核特性。 123456789101112131415161718192021222324252627282930313233343536373839404142&quot;&quot;&quot;面试题：进程和线程的区别和联系？进程 - 操作系统分配内存的基本单位 - 一个进程可以包含一个或多个线程线程 - 操作系统分配CPU的基本单位并发编程（concurrent programming）1. 提升执行性能 - 让程序中没有因果关系的部分可以并发的执行2. 改善用户体验 - 让耗时间的操作不会造成程序的假死&quot;&quot;&quot;import globimport osimport threadingfrom PIL import ImagePREFIX = &#x27;thumbnails&#x27;def generate_thumbnail(infile, size, format=&#x27;PNG&#x27;): &quot;&quot;&quot;生成指定图片文件的缩略图&quot;&quot;&quot; file, ext = os.path.splitext(infile) file = file[file.rfind(&#x27;/&#x27;) + 1:] outfile = f&#x27;&#123;PREFIX&#125;/&#123;file&#125;_&#123;size[0]&#125;_&#123;size[1]&#125;.&#123;ext&#125;&#x27; img = Image.open(infile) img.thumbnail(size, Image.ANTIALIAS) img.save(outfile, format)def main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; if not os.path.exists(PREFIX): os.mkdir(PREFIX) for infile in glob.glob(&#x27;images/*.png&#x27;): for size in (32, 64, 128): # 创建并启动线程 threading.Thread( target=generate_thumbnail, args=(infile, (size, size)) ).start() if __name__ == &#x27;__main__&#x27;: main() 多个线程竞争资源的情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344&quot;&quot;&quot;多线程程序如果没有竞争资源处理起来通常也比较简单当多个线程竞争临界资源的时候如果缺乏必要的保护措施就会导致数据错乱说明：临界资源就是被多个线程竞争的资源&quot;&quot;&quot;import timeimport threadingfrom concurrent.futures import ThreadPoolExecutorclass Account(object): &quot;&quot;&quot;银行账户&quot;&quot;&quot; def __init__(self): self.balance = 0.0 self.lock = threading.Lock() def deposit(self, money): # 通过锁保护临界资源 with self.lock: new_balance = self.balance + money time.sleep(0.001) self.balance = new_balancedef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; account = Account() # 创建线程池 pool = ThreadPoolExecutor(max_workers=10) futures = [] for _ in range(100): future = pool.submit(account.deposit, 1) futures.append(future) # 关闭线程池 pool.shutdown() for future in futures: future.result() print(account.balance)if __name__ == &#x27;__main__&#x27;: main() 修改上面的程序，启动5个线程向账户中存钱，5个线程从账户中取钱，取钱时如果余额不足就暂停线程进行等待。为了达到上述目标，需要对存钱和取钱的线程进行调度，在余额不足时取钱的线程暂停并释放锁，而存钱的线程将钱存入后要通知取钱的线程，使其从暂停状态被唤醒。可以使用threading模块的Condition来实现线程调度，该对象也是基于锁来创建的，代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&quot;&quot;&quot;多个线程竞争一个资源 - 保护临界资源 - 锁（Lock/RLock）多个线程竞争多个资源（线程数&gt;资源数） - 信号量（Semaphore）多个线程的调度 - 暂停线程执行/唤醒等待中的线程 - Condition&quot;&quot;&quot;from concurrent.futures import ThreadPoolExecutorfrom random import randintfrom time import sleepimport threadingclass Account: &quot;&quot;&quot;银行账户&quot;&quot;&quot; def __init__(self, balance=0): self.balance = balance lock = threading.RLock() self.condition = threading.Condition(lock) def withdraw(self, money): &quot;&quot;&quot;取钱&quot;&quot;&quot; with self.condition: while money &gt; self.balance: self.condition.wait() new_balance = self.balance - money sleep(0.001) self.balance = new_balance def deposit(self, money): &quot;&quot;&quot;存钱&quot;&quot;&quot; with self.condition: new_balance = self.balance + money sleep(0.001) self.balance = new_balance self.condition.notify_all()def add_money(account): while True: money = randint(5, 10) account.deposit(money) print(threading.current_thread().name, &#x27;:&#x27;, money, &#x27;====&gt;&#x27;, account.balance) sleep(0.5)def sub_money(account): while True: money = randint(10, 30) account.withdraw(money) print(threading.current_thread().name, &#x27;:&#x27;, money, &#x27;&lt;====&#x27;, account.balance) sleep(1)def main(): account = Account() with ThreadPoolExecutor(max_workers=15) as pool: for _ in range(5): pool.submit(add_money, account) for _ in range(10): pool.submit(sub_money, account)if __name__ == &#x27;__main__&#x27;: main() 多进程：多进程可以有效的解决GIL的问题，实现多进程主要的类是Process，其他辅助的类跟threading模块中的类似，进程间共享数据可以使用管道、套接字等，在multiprocessing模块中有一个Queue类，它基于管道和锁机制提供了多个进程共享的队列。下面是官方文档上关于多进程和进程池的一个示例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&quot;&quot;&quot;多进程和进程池的使用多线程因为GIL的存在不能够发挥CPU的多核特性对于计算密集型任务应该考虑使用多进程time python3 example22.pyreal 0m11.512suser 0m39.319ssys 0m0.169s使用多进程后实际执行时间为11.512秒，而用户时间39.319秒约为实际执行时间的4倍这就证明我们的程序通过多进程使用了CPU的多核特性，而且这台计算机配置了4核的CPU&quot;&quot;&quot;import concurrent.futuresimport mathPRIMES = [ 1116281, 1297337, 104395303, 472882027, 533000389, 817504243, 982451653, 112272535095293, 112582705942171, 112272535095293, 115280095190773, 115797848077099, 1099726899285419] * 5def is_prime(n): &quot;&quot;&quot;判断素数&quot;&quot;&quot; if n % 2 == 0: return False sqrt_n = int(math.floor(math.sqrt(n))) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return Truedef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; with concurrent.futures.ProcessPoolExecutor() as executor: for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)): print(&#x27;%d is prime: %s&#x27; % (number, prime))if __name__ == &#x27;__main__&#x27;: main() 重点：多线程和多进程的比较。 以下情况需要使用多线程： 程序需要维护许多共享的状态（尤其是可变状态），Python中的列表、字典、集合都是线程安全的，所以使用线程而不是进程维护共享状态的代价相对较小。 程序会花费大量时间在I&#x2F;O操作上，没有太多并行计算的需求且不需占用太多的内存。 以下情况需要使用多进程： 程序执行计算密集型任务（如：字节码操作、数据处理、科学计算）。 程序的输入可以并行的分成块，并且可以将运算结果合并。 程序在内存使用方面没有任何限制且不强依赖于I&#x2F;O操作（如：读写文件、套接字等）。 异步处理：从调度程序的任务队列中挑选任务，该调度程序以交叉的形式执行这些任务，我们并不能保证任务将以某种顺序去执行，因为执行顺序取决于队列中的一项任务是否愿意将CPU处理时间让位给另一项任务。异步任务通常通过多任务协作处理的方式来实现，由于执行时间和顺序的不确定，因此需要通过回调式编程或者future对象来获取任务执行的结果。Python 3通过asyncio模块和await和async关键字（在Python 3.7中正式被列为关键字）来支持异步处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&quot;&quot;&quot;异步I/O - async / await&quot;&quot;&quot;import asynciodef num_generator(m, n): &quot;&quot;&quot;指定范围的数字生成器&quot;&quot;&quot; yield from range(m, n + 1)async def prime_filter(m, n): &quot;&quot;&quot;素数过滤器&quot;&quot;&quot; primes = [] for i in num_generator(m, n): flag = True for j in range(2, int(i ** 0.5 + 1)): if i % j == 0: flag = False break if flag: print(&#x27;Prime =&gt;&#x27;, i) primes.append(i) await asyncio.sleep(0.001) return tuple(primes)async def square_mapper(m, n): &quot;&quot;&quot;平方映射器&quot;&quot;&quot; squares = [] for i in num_generator(m, n): print(&#x27;Square =&gt;&#x27;, i * i) squares.append(i * i) await asyncio.sleep(0.001) return squaresdef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; loop = asyncio.get_event_loop() future = asyncio.gather(prime_filter(2, 100), square_mapper(1, 100)) future.add_done_callback(lambda x: print(x.result())) loop.run_until_complete(future) loop.close()if __name__ == &#x27;__main__&#x27;: main() 说明：上面的代码使用get_event_loop函数获得系统默认的事件循环，通过gather函数可以获得一个future对象，future对象的add_done_callback可以添加执行完成时的回调函数，loop对象的run_until_complete方法可以等待通过future对象获得协程执行结果。 Python中有一个名为aiohttp的三方库，它提供了异步的HTTP客户端和服务器，这个三方库可以跟asyncio模块一起工作，并提供了对Future对象的支持。Python 3.6中引入了async和await来定义异步执行的函数以及创建异步上下文，在Python 3.7中它们正式成为了关键字。下面的代码异步的从5个URL中获取页面并通过正则表达式的命名捕获组提取了网站的标题。 123456789101112131415161718192021222324252627282930313233import asyncioimport reimport aiohttpPATTERN = re.compile(r&#x27;\\&lt;title\\&gt;(?P&lt;title&gt;.*)\\&lt;\\/title\\&gt;&#x27;)async def fetch_page(session, url): async with session.get(url, ssl=False) as resp: return await resp.text()async def show_title(url): async with aiohttp.ClientSession() as session: html = await fetch_page(session, url) print(PATTERN.search(html).group(&#x27;title&#x27;))def main(): urls = (&#x27;https://www.python.org/&#x27;, &#x27;https://git-scm.com/&#x27;, &#x27;https://www.jd.com/&#x27;, &#x27;https://www.taobao.com/&#x27;, &#x27;https://www.douban.com/&#x27;) loop = asyncio.get_event_loop() cos = [show_title(url) for url in urls] loop.run_until_complete(asyncio.wait(cos)) loop.close()if __name__ == &#x27;__main__&#x27;: main() 重点：异步I&#x2F;O与多进程的比较。 当程序不需要真正的并发性或并行性，而是更多的依赖于异步处理和回调时，asyncio就是一种很好的选择。如果程序中有大量的等待与休眠时，也应该考虑asyncio，它很适合编写没有实时数据处理需求的Web应用服务器。 Python还有很多用于处理并行任务的三方库，例如：joblib、PyMP等。实际开发中，要提升系统的可扩展性和并发性通常有垂直扩展（增加单个节点的处理能力）和水平扩展（将单个节点变成多个节点）两种做法。可以通过消息队列来实现应用程序的解耦合，消息队列相当于是多线程同步队列的扩展版本，不同机器上的应用程序相当于就是线程，而共享的分布式消息队列就是原来程序中的Queue。消息队列（面向消息的中间件）的最流行和最标准化的实现是AMQP（高级消息队列协议），AMQP源于金融行业，提供了排队、路由、可靠传输、安全等功能，最著名的实现包括：Apache的ActiveMQ、RabbitMQ等。 要实现任务的异步化，可以使用名为Celery的三方库。Celery是Python编写的分布式任务队列，它使用分布式消息进行工作，可以基于RabbitMQ或Redis来作为后端的消息代理。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/15.图像和办公文档处理","date":"2024-12-12T08:38:00.429Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/15.图像和办公文档处理/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/15.%E5%9B%BE%E5%83%8F%E5%92%8C%E5%8A%9E%E5%85%AC%E6%96%87%E6%A1%A3%E5%A4%84%E7%90%86/","excerpt":"","text":"图像和办公文档处理用程序来处理图像和办公文档经常出现在实际开发中，Python的标准库中虽然没有直接支持这些操作的模块，但我们可以通过Python生态圈中的第三方模块来完成这些操作。 操作图像计算机图像相关知识 颜色。如果你有使用颜料画画的经历，那么一定知道混合红、黄、蓝三种颜料可以得到其他的颜色，事实上这三种颜色就是被我们称为美术三原色的东西，它们是不能再分解的基本颜色。在计算机中，我们可以将红、绿、蓝三种色光以不同的比例叠加来组合成其他的颜色，因此这三种颜色就是色光三原色，所以我们通常会将一个颜色表示为一个RGB值或RGBA值（其中的A表示Alpha通道，它决定了透过这个图像的像素，也就是透明度）。 名称 RGBA值 名称 RGBA值 White (255, 255, 255, 255) Red (255, 0, 0, 255) Green (0, 255, 0, 255) Blue (0, 0, 255, 255) Gray (128, 128, 128, 255) Yellow (255, 255, 0, 255) Black (0, 0, 0, 255) Purple (128, 0, 128, 255) 像素。对于一个由数字序列表示的图像来说，最小的单位就是图像上单一颜色的小方格，这些小方块都有一个明确的位置和被分配的色彩数值，而这些一小方格的颜色和位置决定了该图像最终呈现出来的样子，它们是不可分割的单位，我们通常称之为像素（pixel）。每一个图像都包含了一定量的像素，这些像素决定图像在屏幕上所呈现的大小。 用Pillow操作图像Pillow是由从著名的Python图像处理库PIL发展出来的一个分支，通过Pillow可以实现图像压缩和图像处理等各种操作。可以使用下面的命令来安装Pillow。 1pip install pillow Pillow中最为重要的是Image类，读取和处理图像都要通过这个类来完成。 123456&gt;&gt;&gt; from PIL import Image&gt;&gt;&gt;&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; image.format, image.size, image.mode(&#x27;JPEG&#x27;, (500, 750), &#x27;RGB&#x27;)&gt;&gt;&gt; image.show() 剪裁图像 123&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; rect = 80, 20, 310, 360&gt;&gt;&gt; image.crop(rect).show() 生成缩略图 1234&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; size = 128, 128&gt;&gt;&gt; image.thumbnail(size)&gt;&gt;&gt; image.show() 缩放和黏贴图像 123456&gt;&gt;&gt; image1 = Image.open(&#x27;./res/luohao.png&#x27;)&gt;&gt;&gt; image2 = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; rect = 80, 20, 310, 360&gt;&gt;&gt; guido_head = image2.crop(rect)&gt;&gt;&gt; width, height = guido_head.size&gt;&gt;&gt; image1.paste(guido_head.resize((int(width / 1.5), int(height / 1.5))), (172, 40)) 旋转和翻转 123&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.png&#x27;)&gt;&gt;&gt; image.rotate(180).show()&gt;&gt;&gt; image.transpose(Image.FLIP_LEFT_RIGHT).show() 操作像素 123456&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; for x in range(80, 310):... for y in range(20, 360):... image.putpixel((x, y), (128, 128, 128))... &gt;&gt;&gt; image.show() 滤镜效果 1234&gt;&gt;&gt; from PIL import Image, ImageFilter&gt;&gt;&gt;&gt;&gt;&gt; image = Image.open(&#x27;./res/guido.jpg&#x27;)&gt;&gt;&gt; image.filter(ImageFilter.CONTOUR).show() 处理Excel电子表格Python的openpyxl模块让我们可以在Python程序中读取和修改Excel电子表格，由于微软从Office 2007开始使用了新的文件格式，这使得Office Excel和LibreOffice Calc、OpenOffice Calc是完全兼容的，这就意味着openpyxl模块也能处理来自这些软件生成的电子表格。 123456789101112import datetimefrom openpyxl import Workbookwb = Workbook()ws = wb.activews[&#x27;A1&#x27;] = 42ws.append([1, 2, 3])ws[&#x27;A2&#x27;] = datetime.datetime.now()wb.save(&quot;sample.xlsx&quot;) 处理Word文档利用python-docx模块，Python可以创建和修改Word文档，当然这里的Word文档不仅仅是指通过微软的Office软件创建的扩展名为docx的文档，LibreOffice Writer和OpenOffice Writer都是免费的字处理软件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344from docx import Documentfrom docx.shared import Inchesdocument = Document()document.add_heading(&#x27;Document Title&#x27;, 0)p = document.add_paragraph(&#x27;A plain paragraph having some &#x27;)p.add_run(&#x27;bold&#x27;).bold = Truep.add_run(&#x27; and some &#x27;)p.add_run(&#x27;italic.&#x27;).italic = Truedocument.add_heading(&#x27;Heading, level 1&#x27;, level=1)document.add_paragraph(&#x27;Intense quote&#x27;, style=&#x27;Intense Quote&#x27;)document.add_paragraph( &#x27;first item in unordered list&#x27;, style=&#x27;List Bullet&#x27;)document.add_paragraph( &#x27;first item in ordered list&#x27;, style=&#x27;List Number&#x27;)document.add_picture(&#x27;monty-truth.png&#x27;, width=Inches(1.25))records = ( (3, &#x27;101&#x27;, &#x27;Spam&#x27;), (7, &#x27;422&#x27;, &#x27;Eggs&#x27;), (4, &#x27;631&#x27;, &#x27;Spam, spam, eggs, and spam&#x27;))table = document.add_table(rows=1, cols=3)hdr_cells = table.rows[0].cellshdr_cells[0].text = &#x27;Qty&#x27;hdr_cells[1].text = &#x27;Id&#x27;hdr_cells[2].text = &#x27;Desc&#x27;for qty, id, desc in records: row_cells = table.add_row().cells row_cells[0].text = str(qty) row_cells[1].text = id row_cells[2].text = descdocument.add_page_break()document.save(&#x27;demo.docx&#x27;)","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/14.网络编程入门和网络应用开发","date":"2024-12-12T08:38:00.426Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/14.网络编程入门和网络应用开发/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/14.%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8%E5%92%8C%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/","excerpt":"","text":"网络编程入门计算机网络基础计算机网络是独立自主的计算机互联而成的系统的总称，组建计算机网络最主要的目的是实现多台计算机之间的通信和资源共享。今天计算机网络中的设备和计算机网络的用户已经多得不可计数，而计算机网络也可以称得上是一个“复杂巨系统”，对于这样的系统，我们不可能用一两篇文章把它讲清楚，有兴趣的读者可以自行阅读Andrew S.Tanenbaum老师的经典之作《计算机网络》或Kurose和Ross老师合著的《计算机网络:自顶向下方法》来了解计算机网络的相关知识。 计算机网络发展史 1960s - 美国国防部ARPANET项目问世，奠定了分组交换网络的基础。 1980s - 国际标准化组织（ISO）发布OSI&#x2F;RM，奠定了网络技术标准化的基础。 1990s - 英国人蒂姆·伯纳斯-李发明了图形化的浏览器，浏览器的简单易用性使得计算机网络迅速被普及。 在没有浏览器的年代，上网是这样的。 有了浏览器以后，上网是这样的。 TCP&#x2F;IP模型实现网络通信的基础是网络通信协议，这些协议通常是由互联网工程任务组 （IETF）制定的。所谓“协议”就是通信计算机双方必须共同遵从的一组约定，例如怎样建立连接、怎样互相识别等，网络协议的三要素是：语法、语义和时序。构成我们今天使用的Internet的基础的是TCP&#x2F;IP协议族，所谓协议族就是一系列的协议及其构成的通信模型，我们通常也把这套东西称为TCP&#x2F;IP模型。与国际标准化组织发布的OSI&#x2F;RM这个七层模型不同，TCP&#x2F;IP是一个四层模型，也就是说，该模型将我们使用的网络从逻辑上分解为四个层次，自底向上依次是：网络接口层、网络层、传输层和应用层，如下图所示。 IP通常被翻译为网际协议，它服务于网络层，主要实现了寻址和路由的功能。接入网络的每一台主机都需要有自己的IP地址，IP地址就是主机在计算机网络上的身份标识。当然由于IPv4地址的匮乏，我们平常在家里、办公室以及其他可以接入网络的公共区域上网时获得的IP地址并不是全球唯一的IP地址，而是一个局域网（LAN）中的内部IP地址，通过网络地址转换（NAT）服务我们也可以实现对网络的访问。计算机网络上有大量的被我们称为“路由器”的网络中继设备，它们会存储转发我们发送到网络上的数据分组，让从源头发出的数据最终能够找到传送到目的地通路，这项功能就是所谓的路由。 TCP全称传输控制协议，它是基于IP提供的寻址和路由服务而建立起来的负责实现端到端可靠传输的协议，之所以将TCP称为可靠的传输协议是因为TCP向调用者承诺了三件事情： 数据不传丢不传错（利用握手、校验和重传机制可以实现）。 流量控制（通过滑动窗口匹配数据发送者和接收者之间的传输速度）。 拥塞控制（通过RTT时间以及对滑动窗口的控制缓解网络拥堵）。 网络应用模式 C&#x2F;S模式和B&#x2F;S模式。这里的C指的是Client（客户端），通常是一个需要安装到某个宿主操作系统上的应用程序；而B指的是Browser（浏览器），它几乎是所有图形化操作系统都默认安装了的一个应用软件；通过C或B都可以实现对S（服务器）的访问。关于二者的比较和讨论在网络上有一大堆的文章，在此我们就不再浪费笔墨了。 去中心化的网络应用模式。不管是B&#x2F;S还是C&#x2F;S都需要服务器的存在，服务器就是整个应用模式的中心，而去中心化的网络应用通常没有固定的服务器或者固定的客户端，所有应用的使用者既可以作为资源的提供者也可以作为资源的访问者。 基于HTTP协议的网络资源访问HTTP（超文本传输协议）HTTP是超文本传输协议（Hyper-Text Transfer Proctol）的简称，维基百科上对HTTP的解释是：超文本传输协议是一种用于分布式、协作式和超媒体信息系统的应用层协议，它是万维网数据通信的基础，设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法，通过HTTP或者HTTPS（超文本传输安全协议）请求的资源由URI（统一资源标识符）来标识。关于HTTP的更多内容，我们推荐阅读阮一峰老师的《HTTP 协议入门》，简单的说，通过HTTP我们可以获取网络上的（基于字符的）资源，开发中经常会用到的网络API（有的地方也称之为网络数据接口）就是基于HTTP来实现数据传输的。 JSON格式JSON（JavaScript Object Notation）是一种轻量级的数据交换语言，该语言以易于让人阅读的文字（纯文本）为基础，用来传输由属性值或者序列性的值组成的数据对象。尽管JSON是最初只是Javascript中一种创建对象的字面量语法，但它在当下更是一种独立于语言的数据格式，很多编程语言都支持JSON格式数据的生成和解析，Python内置的json模块也提供了这方面的功能。由于JSON是纯文本，它和XML一样都适用于异构系统之间的数据交换，而相较于XML，JSON显得更加的轻便和优雅。下面是表达同样信息的XML和JSON，而JSON的优势是相当直观的。 XML的例子： 123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;message&gt; &lt;from&gt;Alice&lt;/from&gt; &lt;to&gt;Bob&lt;/to&gt; &lt;content&gt;Will you marry me?&lt;/content&gt;&lt;/message&gt; JSON的例子： 12345&#123; &quot;from&quot;: &quot;Alice&quot;, &quot;to&quot;: &quot;Bob&quot;, &quot;content&quot;: &quot;Will you marry me?&quot;&#125; requests库requests是一个基于HTTP协议来使用网络的第三库，其官方网站有这样的一句介绍它的话：“Requests是唯一的一个非转基因的Python HTTP库，人类可以安全享用。”简单的说，使用requests库可以非常方便的使用HTTP，避免安全缺陷、冗余代码以及“重复发明轮子”（行业黑话，通常用在软件工程领域表示重新创造一个已有的或是早已被优化過的基本方法）。前面的文章中我们已经使用过这个库，下面我们还是通过requests来实现一个访问网络数据接口并从中获取美女图片下载链接然后下载美女图片到本地的例子程序，程序中使用了天行数据提供的网络API。 我们可以先通过pip安装requests及其依赖库。 1pip install requests 如果使用PyCharm作为开发工具，可以直接在代码中书写import requests，然后通过代码修复功能来自动下载安装requests。 12345678910111213141516171819202122232425262728293031323334353637from time import timefrom threading import Threadimport requests# 继承Thread类创建自定义的线程类class DownloadHanlder(Thread): def __init__(self, url): super().__init__() self.url = url def run(self): filename = self.url[self.url.rfind(&#x27;/&#x27;) + 1:] resp = requests.get(self.url) with open(&#x27;/Users/Hao/&#x27; + filename, &#x27;wb&#x27;) as f: f.write(resp.content)def main(): # 通过requests模块的get函数获取网络资源 # 下面的代码中使用了天行数据接口提供的网络API # 要使用该数据接口需要在天行数据的网站上注册 # 然后用自己的Key替换掉下面代码的中APIKey即可 resp = requests.get( &#x27;http://api.tianapi.com/meinv/?key=APIKey&amp;num=10&#x27;) # 将服务器返回的JSON格式的数据解析为字典 data_model = resp.json() for mm_dict in data_model[&#x27;newslist&#x27;]: url = mm_dict[&#x27;picUrl&#x27;] # 通过多线程的方式实现图片下载 DownloadHanlder(url).start()if __name__ == &#x27;__main__&#x27;: main() 基于传输层协议的套接字编程套接字这个词对很多不了解网络编程的人来说显得非常晦涩和陌生，其实说得通俗点，套接字就是一套用C语言写成的应用程序开发库，主要用于实现进程间通信和网络编程，在网络应用开发中被广泛使用。在Python中也可以基于套接字来使用传输层提供的传输服务，并基于此开发自己的网络应用。实际开发中使用的套接字可以分为三类：流套接字（TCP套接字）、数据报套接字和原始套接字。 TCP套接字所谓TCP套接字就是使用TCP协议提供的传输服务来实现网络通信的编程接口。在Python中可以通过创建socket对象并指定type属性为SOCK_STREAM来使用TCP套接字。由于一台主机可能拥有多个IP地址，而且很有可能会配置多个不同的服务，所以作为服务器端的程序，需要在创建套接字对象后将其绑定到指定的IP地址和端口上。这里的端口并不是物理设备而是对IP地址的扩展，用于区分不同的服务，例如我们通常将HTTP服务跟80端口绑定，而MySQL数据库服务默认绑定在3306端口，这样当服务器收到用户请求时就可以根据端口号来确定到底用户请求的是HTTP服务器还是数据库服务器提供的服务。端口的取值范围是0~65535，而1024以下的端口我们通常称之为“著名端口”（留给像FTP、HTTP、SMTP等“著名服务”使用的端口，有的地方也称之为“周知端口”），自定义的服务通常不使用这些端口，除非自定义的是HTTP或FTP这样的著名服务。 下面的代码实现了一个提供时间日期的服务器。 12345678910111213141516171819202122232425262728293031323334from socket import socket, SOCK_STREAM, AF_INETfrom datetime import datetimedef main(): # 1.创建套接字对象并指定使用哪种传输服务 # family=AF_INET - IPv4地址 # family=AF_INET6 - IPv6地址 # type=SOCK_STREAM - TCP套接字 # type=SOCK_DGRAM - UDP套接字 # type=SOCK_RAW - 原始套接字 server = socket(family=AF_INET, type=SOCK_STREAM) # 2.绑定IP地址和端口(端口用于区分不同的服务) # 同一时间在同一个端口上只能绑定一个服务否则报错 server.bind((&#x27;192.168.1.2&#x27;, 6789)) # 3.开启监听 - 监听客户端连接到服务器 # 参数512可以理解为连接队列的大小 server.listen(512) print(&#x27;服务器启动开始监听...&#x27;) while True: # 4.通过循环接收客户端的连接并作出相应的处理(提供服务) # accept方法是一个阻塞方法如果没有客户端连接到服务器代码不会向下执行 # accept方法返回一个元组其中的第一个元素是客户端对象 # 第二个元素是连接到服务器的客户端的地址(由IP和端口两部分构成) client, addr = server.accept() print(str(addr) + &#x27;连接到了服务器.&#x27;) # 5.发送数据 client.send(str(datetime.now()).encode(&#x27;utf-8&#x27;)) # 6.断开连接 client.close()if __name__ == &#x27;__main__&#x27;: main() 运行服务器程序后我们可以通过Windows系统的telnet来访问该服务器，结果如下图所示。 1telnet 192.168.1.2 6789 当然我们也可以通过Python的程序来实现TCP客户端的功能，相较于实现服务器程序，实现客户端程序就简单多了，代码如下所示。 123456789101112131415from socket import socketdef main(): # 1.创建套接字对象默认使用IPv4和TCP协议 client = socket() # 2.连接到服务器(需要指定IP地址和端口) client.connect((&#x27;192.168.1.2&#x27;, 6789)) # 3.从服务器接收数据 print(client.recv(1024).decode(&#x27;utf-8&#x27;)) client.close()if __name__ == &#x27;__main__&#x27;: main() 需要注意的是，上面的服务器并没有使用多线程或者异步I&#x2F;O的处理方式，这也就意味着当服务器与一个客户端处于通信状态时，其他的客户端只能排队等待。很显然，这样的服务器并不能满足我们的需求，我们需要的服务器是能够同时接纳和处理多个用户请求的。下面我们来设计一个使用多线程技术处理多个用户请求的服务器，该服务器会向连接到服务器的客户端发送一张图片。 服务器端代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from socket import socket, SOCK_STREAM, AF_INETfrom base64 import b64encodefrom json import dumpsfrom threading import Threaddef main(): # 自定义线程类 class FileTransferHandler(Thread): def __init__(self, cclient): super().__init__() self.cclient = cclient def run(self): my_dict = &#123;&#125; my_dict[&#x27;filename&#x27;] = &#x27;guido.jpg&#x27; # JSON是纯文本不能携带二进制数据 # 所以图片的二进制数据要处理成base64编码 my_dict[&#x27;filedata&#x27;] = data # 通过dumps函数将字典处理成JSON字符串 json_str = dumps(my_dict) # 发送JSON字符串 self.cclient.send(json_str.encode(&#x27;utf-8&#x27;)) self.cclient.close() # 1.创建套接字对象并指定使用哪种传输服务 server = socket() # 2.绑定IP地址和端口(区分不同的服务) server.bind((&#x27;192.168.1.2&#x27;, 5566)) # 3.开启监听 - 监听客户端连接到服务器 server.listen(512) print(&#x27;服务器启动开始监听...&#x27;) with open(&#x27;guido.jpg&#x27;, &#x27;rb&#x27;) as f: # 将二进制数据处理成base64再解码成字符串 data = b64encode(f.read()).decode(&#x27;utf-8&#x27;) while True: client, addr = server.accept() # 启动一个线程来处理客户端的请求 FileTransferHandler(client).start()if __name__ == &#x27;__main__&#x27;: main() 客户端代码： 1234567891011121314151617181920212223242526272829from socket import socketfrom json import loadsfrom base64 import b64decodedef main(): client = socket() client.connect((&#x27;192.168.1.2&#x27;, 5566)) # 定义一个保存二进制数据的对象 in_data = bytes() # 由于不知道服务器发送的数据有多大每次接收1024字节 data = client.recv(1024) while data: # 将收到的数据拼接起来 in_data += data data = client.recv(1024) # 将收到的二进制数据解码成JSON字符串并转换成字典 # loads函数的作用就是将JSON字符串转成字典对象 my_dict = loads(in_data.decode(&#x27;utf-8&#x27;)) filename = my_dict[&#x27;filename&#x27;] filedata = my_dict[&#x27;filedata&#x27;].encode(&#x27;utf-8&#x27;) with open(&#x27;/Users/Hao/&#x27; + filename, &#x27;wb&#x27;) as f: # 将base64格式的数据解码成二进制数据并写入文件 f.write(b64decode(filedata)) print(&#x27;图片已保存.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 在这个案例中，我们使用了JSON作为数据传输的格式（通过JSON格式对传输的数据进行了序列化和反序列化的操作），但是JSON并不能携带二进制数据，因此对图片的二进制数据进行了Base64编码的处理。Base64是一种用64个字符表示所有二进制数据的编码方式，通过将二进制数据每6位一组的方式重新组织，刚好可以使用0~9的数字、大小写字母以及“+”和“&#x2F;”总共64个字符表示从000000到111111的64种状态。维基百科上有关于Base64编码的详细讲解，不熟悉Base64的读者可以自行阅读。 说明： 上面的代码主要为了讲解网络编程的相关内容因此并没有对异常状况进行处理，请读者自行添加异常处理代码来增强程序的健壮性。 UDP套接字传输层除了有可靠的传输协议TCP之外，还有一种非常轻便的传输协议叫做用户数据报协议，简称UDP。TCP和UDP都是提供端到端传输服务的协议，二者的差别就如同打电话和发短信的区别，后者不对传输的可靠性和可达性做出任何承诺从而避免了TCP中握手和重传的开销，所以在强调性能和而不是数据完整性的场景中（例如传输网络音视频数据），UDP可能是更好的选择。可能大家会注意到一个现象，就是在观看网络视频时，有时会出现卡顿，有时会出现花屏，这无非就是部分数据传丢或传错造成的。在Python中也可以使用UDP套接字来创建网络应用，对此我们不进行赘述，有兴趣的读者可以自行研究。 网络应用开发发送电子邮件在即时通信软件如此发达的今天，电子邮件仍然是互联网上使用最为广泛的应用之一，公司向应聘者发出录用通知、网站向用户发送一个激活账号的链接、银行向客户推广它们的理财产品等几乎都是通过电子邮件来完成的，而这些任务应该都是由程序自动完成的。 就像我们可以用HTTP（超文本传输协议）来访问一个网站一样，发送邮件要使用SMTP（简单邮件传输协议），SMTP也是一个建立在TCP（传输控制协议）提供的可靠数据传输服务的基础上的应用级协议，它规定了邮件的发送者如何跟发送邮件的服务器进行通信的细节，而Python中的smtplib模块将这些操作简化成了几个简单的函数。 下面的代码演示了如何在Python发送邮件。 12345678910111213141516171819202122from smtplib import SMTPfrom email.header import Headerfrom email.mime.text import MIMETextdef main(): # 请自行修改下面的邮件发送者和接收者 sender = &#x27;abcdefg@126.com&#x27; receivers = [&#x27;uvwxyz@qq.com&#x27;, &#x27;uvwxyz@126.com&#x27;] message = MIMEText(&#x27;用Python发送邮件的示例代码.&#x27;, &#x27;plain&#x27;, &#x27;utf-8&#x27;) message[&#x27;From&#x27;] = Header(&#x27;王大锤&#x27;, &#x27;utf-8&#x27;) message[&#x27;To&#x27;] = Header(&#x27;骆昊&#x27;, &#x27;utf-8&#x27;) message[&#x27;Subject&#x27;] = Header(&#x27;示例代码实验邮件&#x27;, &#x27;utf-8&#x27;) smtper = SMTP(&#x27;smtp.126.com&#x27;) # 请自行修改下面的登录口令 smtper.login(sender, &#x27;secretpass&#x27;) smtper.sendmail(sender, receivers, message.as_string()) print(&#x27;邮件发送完成!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 如果要发送带有附件的邮件，那么可以按照下面的方式进行操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from smtplib import SMTPfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.mime.image import MIMEImagefrom email.mime.multipart import MIMEMultipartimport urllibdef main(): # 创建一个带附件的邮件消息对象 message = MIMEMultipart() # 创建文本内容 text_content = MIMEText(&#x27;附件中有本月数据请查收&#x27;, &#x27;plain&#x27;, &#x27;utf-8&#x27;) message[&#x27;Subject&#x27;] = Header(&#x27;本月数据&#x27;, &#x27;utf-8&#x27;) # 将文本内容添加到邮件消息对象中 message.attach(text_content) # 读取文件并将文件作为附件添加到邮件消息对象中 with open(&#x27;/Users/Hao/Desktop/hello.txt&#x27;, &#x27;rb&#x27;) as f: txt = MIMEText(f.read(), &#x27;base64&#x27;, &#x27;utf-8&#x27;) txt[&#x27;Content-Type&#x27;] = &#x27;text/plain&#x27; txt[&#x27;Content-Disposition&#x27;] = &#x27;attachment; filename=hello.txt&#x27; message.attach(txt) # 读取文件并将文件作为附件添加到邮件消息对象中 with open(&#x27;/Users/Hao/Desktop/汇总数据.xlsx&#x27;, &#x27;rb&#x27;) as f: xls = MIMEText(f.read(), &#x27;base64&#x27;, &#x27;utf-8&#x27;) xls[&#x27;Content-Type&#x27;] = &#x27;application/vnd.ms-excel&#x27; xls[&#x27;Content-Disposition&#x27;] = &#x27;attachment; filename=month-data.xlsx&#x27; message.attach(xls) # 创建SMTP对象 smtper = SMTP(&#x27;smtp.126.com&#x27;) # 开启安全连接 # smtper.starttls() sender = &#x27;abcdefg@126.com&#x27; receivers = [&#x27;uvwxyz@qq.com&#x27;] # 登录到SMTP服务器 # 请注意此处不是使用密码而是邮件客户端授权码进行登录 # 对此有疑问的读者可以联系自己使用的邮件服务器客服 smtper.login(sender, &#x27;secretpass&#x27;) # 发送邮件 smtper.sendmail(sender, receivers, message.as_string()) # 与邮件服务器断开连接 smtper.quit() print(&#x27;发送完成!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 发送短信发送短信也是项目中常见的功能，网站的注册码、验证码、营销信息基本上都是通过短信来发送给用户的。在下面的代码中我们使用了互亿无线短信平台（该平台为注册用户提供了50条免费短信以及常用开发语言发送短信的demo，可以登录该网站并在用户自服务页面中对短信进行配置）提供的API接口实现了发送短信的服务，当然国内的短信平台很多，读者可以根据自己的需要进行选择（通常会考虑费用预算、短信达到率、使用的难易程度等指标），如果需要在商业项目中使用短信服务建议购买短信平台提供的套餐服务。 1234567891011121314151617181920212223import urllib.parseimport http.clientimport jsondef main(): host = &quot;106.ihuyi.com&quot; sms_send_uri = &quot;/webservice/sms.php?method=Submit&quot; # 下面的参数需要填入自己注册的账号和对应的密码 params = urllib.parse.urlencode(&#123;&#x27;account&#x27;: &#x27;你自己的账号&#x27;, &#x27;password&#x27; : &#x27;你自己的密码&#x27;, &#x27;content&#x27;: &#x27;您的验证码是：147258。请不要把验证码泄露给其他人。&#x27;, &#x27;mobile&#x27;: &#x27;接收者的手机号&#x27;, &#x27;format&#x27;:&#x27;json&#x27; &#125;) print(params) headers = &#123;&#x27;Content-type&#x27;: &#x27;application/x-www-form-urlencoded&#x27;, &#x27;Accept&#x27;: &#x27;text/plain&#x27;&#125; conn = http.client.HTTPConnection(host, port=80, timeout=30) conn.request(&#x27;POST&#x27;, sms_send_uri, params, headers) response = conn.getresponse() response_str = response.read() jsonstr = response_str.decode(&#x27;utf-8&#x27;) print(json.loads(jsonstr)) conn.close()if __name__ == &#x27;__main__&#x27;: main()","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/13.进程和线程","date":"2024-12-12T08:38:00.423Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/13.进程和线程/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/13.%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"进程和线程今天我们使用的计算机早已进入多CPU或多核时代，而我们使用的操作系统都是支持“多任务”的操作系统，这使得我们可以同时运行多个程序，也可以将一个程序分解为若干个相对独立的子任务，让多个子任务并发的执行，从而缩短程序的执行时间，同时也让用户获得更好的体验。因此在当下不管是用什么编程语言进行开发，实现让程序同时执行多个任务也就是常说的“并发编程”，应该是程序员必备技能之一。为此，我们需要先讨论两个概念，一个叫进程，一个叫线程。 概念进程就是操作系统中执行的一个程序，操作系统以进程为单位分配存储空间，每个进程都有自己的地址空间、数据栈以及其他用于跟踪进程执行的辅助数据，操作系统管理所有进程的执行，为它们合理的分配资源。进程可以通过fork或spawn的方式来创建新的进程来执行其他的任务，不过新的进程也有自己独立的内存空间，因此必须通过进程间通信机制（IPC，Inter-Process Communication）来实现数据共享，具体的方式包括管道、信号、套接字、共享内存区等。 一个进程还可以拥有多个并发的执行线索，简单的说就是拥有多个可以获得CPU调度的执行单元，这就是所谓的线程。由于线程在同一个进程下，它们可以共享相同的上下文，因此相对于进程而言，线程间的信息共享和通信更加容易。当然在单核CPU系统中，真正的并发是不可能的，因为在某个时刻能够获得CPU的只有唯一的一个线程，多个线程共享了CPU的执行时间。使用多线程实现并发编程为程序带来的好处是不言而喻的，最主要的体现在提升程序的性能和改善用户体验，今天我们使用的软件几乎都用到了多线程技术，这一点可以利用系统自带的进程监控工具（如macOS中的“活动监视器”、Windows中的“任务管理器”）来证实，如下图所示。 当然多线程也并不是没有坏处，站在其他进程的角度，多线程的程序对其他程序并不友好，因为它占用了更多的CPU执行时间，导致其他程序无法获得足够的CPU执行时间；另一方面，站在开发者的角度，编写和调试多线程的程序都对开发者有较高的要求，对于初学者来说更加困难。 Python既支持多进程又支持多线程，因此使用Python实现并发编程主要有3种方式：多进程、多线程、多进程+多线程。 Python中的多进程Unix和Linux操作系统上提供了fork()系统调用来创建进程，调用fork()函数的是父进程，创建出的是子进程，子进程是父进程的一个拷贝，但是子进程拥有自己的PID。fork()函数非常特殊它会返回两次，父进程中可以通过fork()函数的返回值得到子进程的PID，而子进程中的返回值永远都是0。Python的os模块提供了fork()函数。由于Windows系统没有fork()调用，因此要实现跨平台的多进程编程，可以使用multiprocessing模块的Process类来创建子进程，而且该模块还提供了更高级的封装，例如批量启动进程的进程池（Pool）、用于进程间通信的队列（Queue）和管道（Pipe）等。 下面用一个下载文件的例子来说明使用多进程和不使用多进程到底有什么差别，先看看下面的代码。 123456789101112131415161718192021from random import randintfrom time import time, sleepdef download_task(filename): print(&#x27;开始下载%s...&#x27; % filename) time_to_download = randint(5, 10) sleep(time_to_download) print(&#x27;%s下载完成! 耗费了%d秒&#x27; % (filename, time_to_download))def main(): start = time() download_task(&#x27;Python从入门到住院.pdf&#x27;) download_task(&#x27;Peking Hot.avi&#x27;) end = time() print(&#x27;总共耗费了%.2f秒.&#x27; % (end - start))if __name__ == &#x27;__main__&#x27;: main() 下面是运行程序得到的一次运行结果。 12345开始下载Python从入门到住院.pdf...Python从入门到住院.pdf下载完成! 耗费了6秒开始下载Peking Hot.avi...Peking Hot.avi下载完成! 耗费了7秒总共耗费了13.01秒. 从上面的例子可以看出，如果程序中的代码只能按顺序一点点的往下执行，那么即使执行两个毫不相关的下载任务，也需要先等待一个文件下载完成后才能开始下一个下载任务，很显然这并不合理也没有效率。接下来我们使用多进程的方式将两个下载任务放到不同的进程中，代码如下所示。 12345678910111213141516171819202122232425262728from multiprocessing import Processfrom os import getpidfrom random import randintfrom time import time, sleepdef download_task(filename): print(&#x27;启动下载进程，进程号[%d].&#x27; % getpid()) print(&#x27;开始下载%s...&#x27; % filename) time_to_download = randint(5, 10) sleep(time_to_download) print(&#x27;%s下载完成! 耗费了%d秒&#x27; % (filename, time_to_download))def main(): start = time() p1 = Process(target=download_task, args=(&#x27;Python从入门到住院.pdf&#x27;, )) p1.start() p2 = Process(target=download_task, args=(&#x27;Peking Hot.avi&#x27;, )) p2.start() p1.join() p2.join() end = time() print(&#x27;总共耗费了%.2f秒.&#x27; % (end - start))if __name__ == &#x27;__main__&#x27;: main() 在上面的代码中，我们通过Process类创建了进程对象，通过target参数我们传入一个函数来表示进程启动后要执行的代码，后面的args是一个元组，它代表了传递给函数的参数。Process对象的start方法用来启动进程，而join方法表示等待进程执行结束。运行上面的代码可以明显发现两个下载任务“同时”启动了，而且程序的执行时间将大大缩短，不再是两个任务的时间总和。下面是程序的一次执行结果。 1234567启动下载进程，进程号[1530].开始下载Python从入门到住院.pdf...启动下载进程，进程号[1531].开始下载Peking Hot.avi...Peking Hot.avi下载完成! 耗费了7秒Python从入门到住院.pdf下载完成! 耗费了10秒总共耗费了10.01秒. 我们也可以使用subprocess模块中的类和函数来创建和启动子进程，然后通过管道来和子进程通信，这些内容我们不在此进行讲解，有兴趣的读者可以自己了解这些知识。接下来我们将重点放在如何实现两个进程间的通信。我们启动两个进程，一个输出Ping，一个输出Pong，两个进程输出的Ping和Pong加起来一共10个。听起来很简单吧，但是如果这样写可是错的哦。 123456789101112131415161718192021from multiprocessing import Processfrom time import sleepcounter = 0def sub_task(string): global counter while counter &lt; 10: print(string, end=&#x27;&#x27;, flush=True) counter += 1 sleep(0.01) def main(): Process(target=sub_task, args=(&#x27;Ping&#x27;, )).start() Process(target=sub_task, args=(&#x27;Pong&#x27;, )).start()if __name__ == &#x27;__main__&#x27;: main() 看起来没毛病，但是最后的结果是Ping和Pong各输出了10个，Why？当我们在程序中创建进程的时候，子进程复制了父进程及其所有的数据结构，每个子进程有自己独立的内存空间，这也就意味着两个子进程中各有一个counter变量，所以结果也就可想而知了。要解决这个问题比较简单的办法是使用multiprocessing模块中的Queue类，它是可以被多个进程共享的队列，底层是通过管道和信号量（semaphore）机制来实现的，有兴趣的读者可以自己尝试一下。 Python中的多线程在Python早期的版本中就引入了thread模块（现在名为_thread）来实现多线程编程，然而该模块过于底层，而且很多功能都没有提供，因此目前的多线程开发我们推荐使用threading模块，该模块对多线程编程提供了更好的面向对象的封装。我们把刚才下载文件的例子用多线程的方式来实现一遍。 1234567891011121314151617181920212223242526from random import randintfrom threading import Threadfrom time import time, sleepdef download(filename): print(&#x27;开始下载%s...&#x27; % filename) time_to_download = randint(5, 10) sleep(time_to_download) print(&#x27;%s下载完成! 耗费了%d秒&#x27; % (filename, time_to_download))def main(): start = time() t1 = Thread(target=download, args=(&#x27;Python从入门到住院.pdf&#x27;,)) t1.start() t2 = Thread(target=download, args=(&#x27;Peking Hot.avi&#x27;,)) t2.start() t1.join() t2.join() end = time() print(&#x27;总共耗费了%.3f秒&#x27; % (end - start))if __name__ == &#x27;__main__&#x27;: main() 我们可以直接使用threading模块的Thread类来创建线程，但是我们之前讲过一个非常重要的概念叫“继承”，我们可以从已有的类创建新类，因此也可以通过继承Thread类的方式来创建自定义的线程类，然后再创建线程对象并启动线程。代码如下所示。 1234567891011121314151617181920212223242526272829303132from random import randintfrom threading import Threadfrom time import time, sleepclass DownloadTask(Thread): def __init__(self, filename): super().__init__() self._filename = filename def run(self): print(&#x27;开始下载%s...&#x27; % self._filename) time_to_download = randint(5, 10) sleep(time_to_download) print(&#x27;%s下载完成! 耗费了%d秒&#x27; % (self._filename, time_to_download))def main(): start = time() t1 = DownloadTask(&#x27;Python从入门到住院.pdf&#x27;) t1.start() t2 = DownloadTask(&#x27;Peking Hot.avi&#x27;) t2.start() t1.join() t2.join() end = time() print(&#x27;总共耗费了%.2f秒.&#x27; % (end - start))if __name__ == &#x27;__main__&#x27;: main() 因为多个线程可以共享进程的内存空间，因此要实现多个线程间的通信相对简单，大家能想到的最直接的办法就是设置一个全局变量，多个线程共享这个全局变量即可。但是当多个线程共享同一个变量（我们通常称之为“资源”）的时候，很有可能产生不可控的结果从而导致程序失效甚至崩溃。如果一个资源被多个线程竞争使用，那么我们通常称之为“临界资源”，对“临界资源”的访问需要加上保护，否则资源会处于“混乱”的状态。下面的例子演示了100个线程向同一个银行账户转账（转入1元钱）的场景，在这个例子中，银行账户就是一个临界资源，在没有保护的情况下我们很有可能会得到错误的结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from time import sleepfrom threading import Threadclass Account(object): def __init__(self): self._balance = 0 def deposit(self, money): # 计算存款后的余额 new_balance = self._balance + money # 模拟受理存款业务需要0.01秒的时间 sleep(0.01) # 修改账户余额 self._balance = new_balance @property def balance(self): return self._balanceclass AddMoneyThread(Thread): def __init__(self, account, money): super().__init__() self._account = account self._money = money def run(self): self._account.deposit(self._money)def main(): account = Account() threads = [] # 创建100个存款的线程向同一个账户中存钱 for _ in range(100): t = AddMoneyThread(account, 1) threads.append(t) t.start() # 等所有存款的线程都执行完毕 for t in threads: t.join() print(&#x27;账户余额为: ￥%d元&#x27; % account.balance)if __name__ == &#x27;__main__&#x27;: main() 运行上面的程序，结果让人大跌眼镜，100个线程分别向账户中转入1元钱，结果居然远远小于100元。之所以出现这种情况是因为我们没有对银行账户这个“临界资源”加以保护，多个线程同时向账户中存钱时，会一起执行到new_balance = self._balance + money这行代码，多个线程得到的账户余额都是初始状态下的0，所以都是0上面做了+1的操作，因此得到了错误的结果。在这种情况下，“锁”就可以派上用场了。我们可以通过“锁”来保护“临界资源”，只有获得“锁”的线程才能访问“临界资源”，而其他没有得到“锁”的线程只能被阻塞起来，直到获得“锁”的线程释放了“锁”，其他线程才有机会获得“锁”，进而访问被保护的“临界资源”。下面的代码演示了如何使用“锁”来保护对银行账户的操作，从而获得正确的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from time import sleepfrom threading import Thread, Lockclass Account(object): def __init__(self): self._balance = 0 self._lock = Lock() def deposit(self, money): # 先获取锁才能执行后续的代码 self._lock.acquire() try: new_balance = self._balance + money sleep(0.01) self._balance = new_balance finally: # 在finally中执行释放锁的操作保证正常异常锁都能释放 self._lock.release() @property def balance(self): return self._balanceclass AddMoneyThread(Thread): def __init__(self, account, money): super().__init__() self._account = account self._money = money def run(self): self._account.deposit(self._money)def main(): account = Account() threads = [] for _ in range(100): t = AddMoneyThread(account, 1) threads.append(t) t.start() for t in threads: t.join() print(&#x27;账户余额为: ￥%d元&#x27; % account.balance)if __name__ == &#x27;__main__&#x27;: main() 比较遗憾的一件事情是Python的多线程并不能发挥CPU的多核特性，这一点只要启动几个执行死循环的线程就可以得到证实了。之所以如此，是因为Python的解释器有一个“全局解释器锁”（GIL）的东西，任何线程执行前必须先获得GIL锁，然后每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行，这是一个历史遗留问题，但是即便如此，就如我们之前举的例子，使用多线程在提升执行效率和改善用户体验方面仍然是有积极意义的。 多进程还是多线程无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型。如果你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以旁观者的角度来看，你就正在同时写5科作业。 但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。所以，多任务一旦多到一个限度，反而会使得系统性能急剧下降，最终导致所有任务都做不好。 是否采用多任务的第二个考虑是任务的类型，可以把任务分为计算密集型和I&#x2F;O密集型。计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如对视频进行编码解码或者格式转换等等，这种任务全靠CPU的运算能力，虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低。计算密集型任务由于主要消耗CPU资源，这类任务用Python这样的脚本语言去执行效率通常很低，最能胜任这类任务的是C语言，我们之前提到过Python中有嵌入C&#x2F;C++代码的机制。 除了计算密集型任务，其他的涉及到网络、存储介质I&#x2F;O的任务都可以视为I&#x2F;O密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待I&#x2F;O操作完成（因为I&#x2F;O的速度远远低于CPU和内存的速度）。对于I&#x2F;O密集型任务，如果启动多任务，就可以减少I&#x2F;O等待时间从而让CPU高效率的运转。有一大类的任务都属于I&#x2F;O密集型任务，这其中包括了我们很快会涉及到的网络应用和Web应用。 说明： 上面的内容和例子来自于廖雪峰官方网站的《Python教程》，因为对作者文中的某些观点持有不同的看法，对原文的文字描述做了适当的调整。 单线程+异步I&#x2F;O现代操作系统对I&#x2F;O操作的改进中最为重要的就是支持异步I&#x2F;O。如果充分利用操作系统提供的异步I&#x2F;O支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型。Nginx就是支持异步I&#x2F;O的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。用Node.js开发的服务器端程序也使用了这种工作模式，这也是当下并发编程的一种流行方案。 在Python语言中，单线程+异步I&#x2F;O的编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。协程最大的优势就是极高的执行效率，因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销。协程的第二个优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不用加锁，只需要判断状态就好了，所以执行效率比多线程高很多。如果想要充分利用CPU的多核特性，最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。关于这方面的内容，在后续的课程中会进行讲解。 应用案例例子1：将耗时间的任务放到线程中以获得更好的用户体验。如下所示的界面中，有“下载”和“关于”两个按钮，用休眠的方式模拟点击“下载”按钮会联网下载文件需要耗费10秒的时间，如果不使用“多线程”，我们会发现，当点击“下载”按钮后整个程序的其他部分都被这个耗时间的任务阻塞而无法执行了，这显然是非常糟糕的用户体验，代码如下所示。 123456789101112131415161718192021222324252627282930313233import timeimport tkinterimport tkinter.messageboxdef download(): # 模拟下载任务需要花费10秒钟时间 time.sleep(10) tkinter.messagebox.showinfo(&#x27;提示&#x27;, &#x27;下载完成!&#x27;)def show_about(): tkinter.messagebox.showinfo(&#x27;关于&#x27;, &#x27;作者: 骆昊(v1.0)&#x27;)def main(): top = tkinter.Tk() top.title(&#x27;单线程&#x27;) top.geometry(&#x27;200x150&#x27;) top.wm_attributes(&#x27;-topmost&#x27;, True) panel = tkinter.Frame(top) button1 = tkinter.Button(panel, text=&#x27;下载&#x27;, command=download) button1.pack(side=&#x27;left&#x27;) button2 = tkinter.Button(panel, text=&#x27;关于&#x27;, command=show_about) button2.pack(side=&#x27;right&#x27;) panel.pack(side=&#x27;bottom&#x27;) tkinter.mainloop()if __name__ == &#x27;__main__&#x27;: main() 如果使用多线程将耗时间的任务放到一个独立的线程中执行，这样就不会因为执行耗时间的任务而阻塞了主线程，修改后的代码如下所示。 12345678910111213141516171819202122232425262728293031323334353637383940414243import timeimport tkinterimport tkinter.messageboxfrom threading import Threaddef main(): class DownloadTaskHandler(Thread): def run(self): time.sleep(10) tkinter.messagebox.showinfo(&#x27;提示&#x27;, &#x27;下载完成!&#x27;) # 启用下载按钮 button1.config(state=tkinter.NORMAL) def download(): # 禁用下载按钮 button1.config(state=tkinter.DISABLED) # 通过daemon参数将线程设置为守护线程(主程序退出就不再保留执行) # 在线程中处理耗时间的下载任务 DownloadTaskHandler(daemon=True).start() def show_about(): tkinter.messagebox.showinfo(&#x27;关于&#x27;, &#x27;作者: 骆昊(v1.0)&#x27;) top = tkinter.Tk() top.title(&#x27;单线程&#x27;) top.geometry(&#x27;200x150&#x27;) top.wm_attributes(&#x27;-topmost&#x27;, 1) panel = tkinter.Frame(top) button1 = tkinter.Button(panel, text=&#x27;下载&#x27;, command=download) button1.pack(side=&#x27;left&#x27;) button2 = tkinter.Button(panel, text=&#x27;关于&#x27;, command=show_about) button2.pack(side=&#x27;right&#x27;) panel.pack(side=&#x27;bottom&#x27;) tkinter.mainloop()if __name__ == &#x27;__main__&#x27;: main() 例子2：使用多进程对复杂任务进行“分而治之”。我们来完成1~100000000求和的计算密集型任务，这个问题本身非常简单，有点循环的知识就能解决，代码如下所示。 12345678910111213141516from time import timedef main(): total = 0 number_list = [x for x in range(1, 100000001)] start = time() for number in number_list: total += number print(total) end = time() print(&#x27;Execution time: %.3fs&#x27; % (end - start))if __name__ == &#x27;__main__&#x27;: main() 在上面的代码中，我故意先去创建了一个列表容器然后填入了100000000个数，这一步其实是比较耗时间的，所以为了公平起见，当我们将这个任务分解到8个进程中去执行的时候，我们暂时也不考虑列表切片操作花费的时间，只是把做运算和合并运算结果的时间统计出来，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839from multiprocessing import Process, Queuefrom random import randintfrom time import timedef task_handler(curr_list, result_queue): total = 0 for number in curr_list: total += number result_queue.put(total)def main(): processes = [] number_list = [x for x in range(1, 100000001)] result_queue = Queue() index = 0 # 启动8个进程将数据切片后进行运算 for _ in range(8): p = Process(target=task_handler, args=(number_list[index:index + 12500000], result_queue)) index += 12500000 processes.append(p) p.start() # 开始记录所有进程执行完成花费的时间 start = time() for p in processes: p.join() # 合并执行结果 total = 0 while not result_queue.empty(): total += result_queue.get() print(total) end = time() print(&#x27;Execution time: &#x27;, (end - start), &#x27;s&#x27;, sep=&#x27;&#x27;)if __name__ == &#x27;__main__&#x27;: main() 比较两段代码的执行结果（在我目前使用的MacBook上，上面的代码需要大概6秒左右的时间，而下面的代码只需要不到1秒的时间，再强调一次我们只是比较了运算的时间，不考虑列表创建及切片操作花费的时间），使用多进程后由于获得了更多的CPU执行时间以及更好的利用了CPU的多核特性，明显的减少了程序的执行时间，而且计算量越大效果越明显。当然，如果愿意还可以将多个进程部署在不同的计算机上，做成分布式进程，具体的做法就是通过multiprocessing.managers模块中提供的管理器将Queue对象通过网络共享出来（注册到网络上让其他计算机可以访问），这部分内容也留到爬虫的专题再进行讲解。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/12.字符串和正则表达式","date":"2024-12-12T08:38:00.419Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/12.字符串和正则表达式/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/12.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","excerpt":"","text":"使用正则表达式正则表达式相关知识在编写处理字符串的程序或网页时，经常会有查找符合某些复杂规则的字符串的需要，正则表达式就是用于描述这些规则的工具，换句话说正则表达式是一种工具，它定义了字符串的匹配模式（如何检查一个字符串是否有跟某种模式匹配的部分或者从一个字符串中将与模式匹配的部分提取出来或者替换掉）。如果你在Windows操作系统中使用过文件查找并且在指定文件名时使用过通配符（*和?），那么正则表达式也是与之类似的用来进行文本匹配的工具，只不过比起通配符正则表达式更强大，它能更精确地描述你的需求（当然你付出的代价是书写一个正则表达式比打出一个通配符要复杂得多，要知道任何给你带来好处的东西都是有代价的，就如同学习一门编程语言一样），比如你可以编写一个正则表达式，用来查找所有以0开头，后面跟着2-3个数字，然后是一个连字号“-”，最后是7或8位数字的字符串（像028-12345678或0813-7654321），这不就是国内的座机号码吗。最初计算机是为了做数学运算而诞生的，处理的信息基本上都是数值，而今天我们在日常工作中处理的信息基本上都是文本数据，我们希望计算机能够识别和处理符合某些模式的文本，正则表达式就显得非常重要了。今天几乎所有的编程语言都提供了对正则表达式操作的支持，Python通过标准库中的re模块来支持正则表达式操作。 我们可以考虑下面一个问题：我们从某个地方（可能是一个文本文件，也可能是网络上的一则新闻）获得了一个字符串，希望在字符串中找出手机号和座机号。当然我们可以设定手机号是11位的数字（注意并不是随机的11位数字，因为你没有见过“25012345678”这样的手机号吧）而座机号跟上一段中描述的模式相同，如果不使用正则表达式要完成这个任务就会很麻烦。 关于正则表达式的相关知识，大家可以阅读一篇非常有名的博客叫《正则表达式30分钟入门教程》，读完这篇文章后你就可以看懂下面的表格，这是我们对正则表达式中的一些基本符号进行的扼要总结。 符号 解释 示例 说明 . 匹配任意字符 b.t 可以匹配bat &#x2F; but &#x2F; b#t &#x2F; b1t等 \\w 匹配字母&#x2F;数字&#x2F;下划线 b\\wt 可以匹配bat &#x2F; b1t &#x2F; b_t等但不能匹配b#t \\s 匹配空白字符（包括\\r、\\n、\\t等） love\\syou 可以匹配love you \\d 匹配数字 \\d\\d 可以匹配01 &#x2F; 23 &#x2F; 99等 \\b 匹配单词的边界 \\bThe\\b ^ 匹配字符串的开始 ^The 可以匹配The开头的字符串 $ 匹配字符串的结束 .exe$ 可以匹配.exe结尾的字符串 \\W 匹配非字母&#x2F;数字&#x2F;下划线 b\\Wt 可以匹配b#t &#x2F; b@t等但不能匹配but &#x2F; b1t &#x2F; b_t等 \\S 匹配非空白字符 love\\Syou 可以匹配love#you等但不能匹配love you \\D 匹配非数字 \\d\\D 可以匹配9a &#x2F; 3# &#x2F; 0F等 \\B 匹配非单词边界 \\Bio\\B [] 匹配来自字符集的任意单一字符 [aeiou] 可以匹配任一元音字母字符 [^] 匹配不在字符集中的任意单一字符 [^aeiou] 可以匹配任一非元音字母字符 * 匹配0次或多次 \\w* + 匹配1次或多次 \\w+ ? 匹配0次或1次 \\w? {N} 匹配N次 \\w{3} {M,} 匹配至少M次 \\w{3,} {M,N} 匹配至少M次至多N次 \\w{3,6} | 分支 foo|bar 可以匹配foo或者bar (?#) 注释 (exp) 匹配exp并捕获到自动命名的组中 (?&lt;name&gt;exp) 匹配exp并捕获到名为name的组中 (?:exp) 匹配exp但是不捕获匹配的文本 (?&#x3D;exp) 匹配exp前面的位置 \\b\\w+(?&#x3D;ing) 可以匹配I’m dancing中的danc (?&lt;&#x3D;exp) 匹配exp后面的位置 (?&lt;&#x3D;\\bdanc)\\w+\\b 可以匹配I love dancing and reading中的第一个ing (?!exp) 匹配后面不是exp的位置 (?&lt;!exp) 匹配前面不是exp的位置 *? 重复任意次，但尽可能少重复 a.*ba.*?b 将正则表达式应用于aabab，前者会匹配整个字符串aabab，后者会匹配aab和ab两个字符串 +? 重复1次或多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {M,N}? 重复M到N次，但尽可能少重复 {M,}? 重复M次以上，但尽可能少重复 说明： 如果需要匹配的字符是正则表达式中的特殊字符，那么可以使用\\进行转义处理，例如想匹配小数点可以写成\\.就可以了，因为直接写.会匹配任意字符；同理，想匹配圆括号必须写成\\(和\\)，否则圆括号被视为正则表达式中的分组。 Python对正则表达式的支持Python提供了re模块来支持正则表达式相关操作，下面是re模块中的核心函数。 函数 说明 compile(pattern, flags&#x3D;0) 编译正则表达式返回正则表达式对象 match(pattern, string, flags&#x3D;0) 用正则表达式匹配字符串 成功返回匹配对象 否则返回None search(pattern, string, flags&#x3D;0) 搜索字符串中第一次出现正则表达式的模式 成功返回匹配对象 否则返回None split(pattern, string, maxsplit&#x3D;0, flags&#x3D;0) 用正则表达式指定的模式分隔符拆分字符串 返回列表 sub(pattern, repl, string, count&#x3D;0, flags&#x3D;0) 用指定的字符串替换原字符串中与正则表达式匹配的模式 可以用count指定替换的次数 fullmatch(pattern, string, flags&#x3D;0) match函数的完全匹配（从字符串开头到结尾）版本 findall(pattern, string, flags&#x3D;0) 查找字符串所有与正则表达式匹配的模式 返回字符串的列表 finditer(pattern, string, flags&#x3D;0) 查找字符串所有与正则表达式匹配的模式 返回一个迭代器 purge() 清除隐式编译的正则表达式的缓存 re.I &#x2F; re.IGNORECASE 忽略大小写匹配标记 re.M &#x2F; re.MULTILINE 多行匹配标记 说明： 上面提到的re模块中的这些函数，实际开发中也可以用正则表达式对象的方法替代对这些函数的使用，如果一个正则表达式需要重复的使用，那么先通过compile函数编译正则表达式并创建出正则表达式对象无疑是更为明智的选择。 下面我们通过一系列的例子来告诉大家在Python中如何使用正则表达式。 例子1：验证输入用户名和QQ号是否有效并给出对应的提示信息。12345678910111213141516171819202122232425&quot;&quot;&quot;验证输入用户名和QQ号是否有效并给出对应的提示信息要求：用户名必须由字母、数字或下划线构成且长度在6~20个字符之间，QQ号是5~12的数字且首位不能为0&quot;&quot;&quot;import redef main(): username = input(&#x27;请输入用户名: &#x27;) qq = input(&#x27;请输入QQ号: &#x27;) # match函数的第一个参数是正则表达式字符串或正则表达式对象 # 第二个参数是要跟正则表达式做匹配的字符串对象 m1 = re.match(r&#x27;^[0-9a-zA-Z_]&#123;6,20&#125;$&#x27;, username) if not m1: print(&#x27;请输入有效的用户名.&#x27;) m2 = re.match(r&#x27;^[1-9]\\d&#123;4,11&#125;$&#x27;, qq) if not m2: print(&#x27;请输入有效的QQ号.&#x27;) if m1 and m2: print(&#x27;你输入的信息是有效的!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 提示： 上面在书写正则表达式时使用了“原始字符串”的写法（在字符串前面加上了r），所谓“原始字符串”就是字符串中的每个字符都是它原始的意义，说得更直接一点就是字符串中没有所谓的转义字符啦。因为正则表达式中有很多元字符和需要进行转义的地方，如果不使用原始字符串就需要将反斜杠写作\\\\，例如表示数字的\\d得书写成\\\\d，这样不仅写起来不方便，阅读的时候也会很吃力。 例子2：从一段文字中提取出国内手机号码。下面这张图是截止到2017年底，国内三家运营商推出的手机号段。 123456789101112131415161718192021222324252627import redef main(): # 创建正则表达式对象 使用了前瞻和回顾来保证手机号前后不应该出现数字 pattern = re.compile(r&#x27;(?&lt;=\\D)1[34578]\\d&#123;9&#125;(?=\\D)&#x27;) sentence = &#x27;&#x27;&#x27; 重要的事情说8130123456789遍，我的手机号是13512346789这个靓号， 不是15600998765，也是110或119，王大锤的手机号才是15600998765。 &#x27;&#x27;&#x27; # 查找所有匹配并保存到一个列表中 mylist = re.findall(pattern, sentence) print(mylist) print(&#x27;--------华丽的分隔线--------&#x27;) # 通过迭代器取出匹配对象并获得匹配的内容 for temp in pattern.finditer(sentence): print(temp.group()) print(&#x27;--------华丽的分隔线--------&#x27;) # 通过search函数指定搜索位置找出所有匹配 m = pattern.search(sentence) while m: print(m.group()) m = pattern.search(sentence, m.end())if __name__ == &#x27;__main__&#x27;: main() 说明： 上面匹配国内手机号的正则表达式并不够好，因为像14开头的号码只有145或147，而上面的正则表达式并没有考虑这种情况，要匹配国内手机号，更好的正则表达式的写法是：(?&lt;=\\D)(1[38]\\d&#123;9&#125;|14[57]\\d&#123;8&#125;|15[0-35-9]\\d&#123;8&#125;|17[678]\\d&#123;8&#125;)(?=\\D)，国内最近好像有19和16开头的手机号了，但是这个暂时不在我们考虑之列。 例子3：替换字符串中的不良内容123456789101112import redef main(): sentence = &#x27;你丫是傻叉吗? 我操你大爷的. Fuck you.&#x27; purified = re.sub(&#x27;[操肏艹]|fuck|shit|傻[比屄逼叉缺吊屌]|煞笔&#x27;, &#x27;*&#x27;, sentence, flags=re.IGNORECASE) print(purified) # 你丫是*吗? 我*你大爷的. * you.if __name__ == &#x27;__main__&#x27;: main() 说明： re模块的正则表达式相关函数中都有一个flags参数，它代表了正则表达式的匹配标记，可以通过该标记来指定匹配时是否忽略大小写、是否进行多行匹配、是否显示调试信息等。如果需要为flags参数指定多个值，可以使用按位或运算符进行叠加，如flags=re.I | re.M。 例子4：拆分长字符串12345678910111213import redef main(): poem = &#x27;窗前明月光，疑是地上霜。举头望明月，低头思故乡。&#x27; sentence_list = re.split(r&#x27;[，。, .]&#x27;, poem) while &#x27;&#x27; in sentence_list: sentence_list.remove(&#x27;&#x27;) print(sentence_list) # [&#x27;窗前明月光&#x27;, &#x27;疑是地上霜&#x27;, &#x27;举头望明月&#x27;, &#x27;低头思故乡&#x27;]if __name__ == &#x27;__main__&#x27;: main() 后话如果要从事爬虫类应用的开发，那么正则表达式一定是一个非常好的助手，因为它可以帮助我们迅速的从网页代码中发现某种我们指定的模式并提取出我们需要的信息，当然对于初学者来收，要编写一个正确的适当的正则表达式可能并不是一件容易的事情（当然有些常用的正则表达式可以直接在网上找找），所以实际开发爬虫应用的时候，有很多人会选择Beautiful Soup或Lxml来进行匹配和信息的提取，前者简单方便但是性能较差，后者既好用性能也好，但是安装稍嫌麻烦，这些内容我们会在后期的爬虫专题中为大家介绍。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/11.文件和异常","date":"2024-12-12T08:38:00.417Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/11.文件和异常/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/11.%E6%96%87%E4%BB%B6%E5%92%8C%E5%BC%82%E5%B8%B8/","excerpt":"","text":"文件和异常实际开发中常常会遇到对数据进行持久化操作的场景，而实现数据持久化最直接简单的方式就是将数据保存到文件中。说到“文件”这个词，可能需要先科普一下关于文件系统的知识，但是这里我们并不浪费笔墨介绍这个概念，请大家自行通过维基百科进行了解。 在Python中实现文件的读写操作其实非常简单，通过Python内置的open函数，我们可以指定文件名、操作模式、编码信息等来获得操作文件的对象，接下来就可以对文件进行读写操作了。这里所说的操作模式是指要打开什么样的文件（字符文件还是二进制文件）以及做什么样的操作（读、写还是追加），具体的如下表所示。 操作模式 具体含义 &#39;r&#39; 读取 （默认） &#39;w&#39; 写入（会先截断之前的内容） &#39;x&#39; 写入，如果文件已经存在会产生异常 &#39;a&#39; 追加，将内容写入到已有文件的末尾 &#39;b&#39; 二进制模式 &#39;t&#39; 文本模式（默认） &#39;+&#39; 更新（既可以读又可以写） 下面这张图来自于菜鸟教程网站，它展示了如果根据应用程序的需要来设置操作模式。 读写文本文件读取文本文件时，需要在使用open函数时指定好带路径的文件名（可以使用相对路径或绝对路径）并将文件模式设置为&#39;r&#39;（如果不指定，默认值也是&#39;r&#39;），然后通过encoding参数指定编码（如果不指定，默认值是None，那么在读取文件时使用的是操作系统默认的编码），如果不能保证保存文件时使用的编码方式与encoding参数指定的编码方式是一致的，那么就可能因无法解码字符而导致读取失败。下面的例子演示了如何读取一个纯文本文件。 12345678def main(): f = open(&#x27;致橡树.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) print(f.read()) f.close()if __name__ == &#x27;__main__&#x27;: main() 请注意上面的代码，如果open函数指定的文件并不存在或者无法打开，那么将引发异常状况导致程序崩溃。为了让代码有一定的健壮性和容错性，我们可以使用Python的异常机制对可能在运行时发生状况的代码进行适当的处理，如下所示。 123456789101112131415161718def main(): f = None try: f = open(&#x27;致橡树.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) print(f.read()) except FileNotFoundError: print(&#x27;无法打开指定的文件!&#x27;) except LookupError: print(&#x27;指定了未知的编码!&#x27;) except UnicodeDecodeError: print(&#x27;读取文件时解码错误!&#x27;) finally: if f: f.close()if __name__ == &#x27;__main__&#x27;: main() 在Python中，我们可以将那些在运行时可能会出现状况的代码放在try代码块中，在try代码块的后面可以跟上一个或多个except来捕获可能出现的异常状况。例如在上面读取文件的过程中，文件找不到会引发FileNotFoundError，指定了未知的编码会引发LookupError，而如果读取文件时无法按指定方式解码会引发UnicodeDecodeError，我们在try后面跟上了三个except分别处理这三种不同的异常状况。最后我们使用finally代码块来关闭打开的文件，释放掉程序中获取的外部资源，由于finally块的代码不论程序正常还是异常都会执行到（甚至是调用了sys模块的exit函数退出Python环境，finally块都会被执行，因为exit函数实质上是引发了SystemExit异常），因此我们通常把finally块称为“总是执行代码块”，它最适合用来做释放外部资源的操作。如果不愿意在finally代码块中关闭文件对象释放资源，也可以使用上下文语法，通过with关键字指定文件对象的上下文环境并在离开上下文环境时自动释放文件资源，代码如下所示。 1234567891011121314def main(): try: with open(&#x27;致橡树.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f: print(f.read()) except FileNotFoundError: print(&#x27;无法打开指定的文件!&#x27;) except LookupError: print(&#x27;指定了未知的编码!&#x27;) except UnicodeDecodeError: print(&#x27;读取文件时解码错误!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 除了使用文件对象的read方法读取文件之外，还可以使用for-in循环逐行读取或者用readlines方法将文件按行读取到一个列表容器中，代码如下所示。 1234567891011121314151617181920212223import timedef main(): # 一次性读取整个文件内容 with open(&#x27;致橡树.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f: print(f.read()) # 通过for-in循环逐行读取 with open(&#x27;致橡树.txt&#x27;, mode=&#x27;r&#x27;) as f: for line in f: print(line, end=&#x27;&#x27;) time.sleep(0.5) print() # 读取文件按行读取到列表中 with open(&#x27;致橡树.txt&#x27;) as f: lines = f.readlines() print(lines) if __name__ == &#x27;__main__&#x27;: main() 要将文本信息写入文件文件也非常简单，在使用open函数时指定好文件名并将文件模式设置为&#39;w&#39;即可。注意如果需要对文件内容进行追加式写入，应该将模式设置为&#39;a&#39;。如果要写入的文件不存在会自动创建文件而不是引发异常。下面的例子演示了如何将1-9999之间的素数分别写入三个文件中（1-99之间的素数保存在a.txt中，100-999之间的素数保存在b.txt中，1000-9999之间的素数保存在c.txt中）。 12345678910111213141516171819202122232425262728293031323334353637from math import sqrtdef is_prime(n): &quot;&quot;&quot;判断素数的函数&quot;&quot;&quot; assert n &gt; 0 for factor in range(2, int(sqrt(n)) + 1): if n % factor == 0: return False return True if n != 1 else Falsedef main(): filenames = (&#x27;a.txt&#x27;, &#x27;b.txt&#x27;, &#x27;c.txt&#x27;) fs_list = [] try: for filename in filenames: fs_list.append(open(filename, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)) for number in range(1, 10000): if is_prime(number): if number &lt; 100: fs_list[0].write(str(number) + &#x27;\\n&#x27;) elif number &lt; 1000: fs_list[1].write(str(number) + &#x27;\\n&#x27;) else: fs_list[2].write(str(number) + &#x27;\\n&#x27;) except IOError as ex: print(ex) print(&#x27;写文件时发生错误!&#x27;) finally: for fs in fs_list: fs.close() print(&#x27;操作完成!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 读写二进制文件知道了如何读写文本文件要读写二进制文件也就很简单了，下面的代码实现了复制图片文件的功能。 12345678910111213141516def main(): try: with open(&#x27;guido.jpg&#x27;, &#x27;rb&#x27;) as fs1: data = fs1.read() print(type(data)) # &lt;class &#x27;bytes&#x27;&gt; with open(&#x27;吉多.jpg&#x27;, &#x27;wb&#x27;) as fs2: fs2.write(data) except FileNotFoundError as e: print(&#x27;指定的文件无法打开.&#x27;) except IOError as e: print(&#x27;读写文件时出现错误.&#x27;) print(&#x27;程序执行结束.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 读写JSON文件通过上面的讲解，我们已经知道如何将文本数据和二进制数据保存到文件中，那么这里还有一个问题，如果希望把一个列表或者一个字典中的数据保存到文件中又该怎么做呢？答案是将数据以JSON格式进行保存。JSON是“JavaScript Object Notation”的缩写，它本来是JavaScript语言中创建对象的一种字面量语法，现在已经被广泛的应用于跨平台跨语言的数据交换，原因很简单，因为JSON也是纯文本，任何系统任何编程语言处理纯文本都是没有问题的。目前JSON基本上已经取代了XML作为异构系统间交换数据的事实标准。关于JSON的知识，更多的可以参考JSON的官方网站，从这个网站也可以了解到每种语言处理JSON数据格式可以使用的工具或三方库，下面是一个JSON的简单例子。 1234567891011&#123; &quot;name&quot;: &quot;骆昊&quot;, &quot;age&quot;: 38, &quot;qq&quot;: 957658, &quot;friends&quot;: [&quot;王大锤&quot;, &quot;白元芳&quot;], &quot;cars&quot;: [ &#123;&quot;brand&quot;: &quot;BYD&quot;, &quot;max_speed&quot;: 180&#125;, &#123;&quot;brand&quot;: &quot;Audi&quot;, &quot;max_speed&quot;: 280&#125;, &#123;&quot;brand&quot;: &quot;Benz&quot;, &quot;max_speed&quot;: 320&#125; ]&#125; 可能大家已经注意到了，上面的JSON跟Python中的字典其实是一样一样的，事实上JSON的数据类型和Python的数据类型是很容易找到对应关系的，如下面两张表所示。 JSON Python object dict array list string str number (int &#x2F; real) int &#x2F; float true &#x2F; false True &#x2F; False null None Python JSON dict object list, tuple array str string int, float, int- &amp; float-derived Enums number True &#x2F; False true &#x2F; false None null 我们使用Python中的json模块就可以将字典或列表以JSON格式保存到文件中，代码如下所示。 12345678910111213141516171819202122232425import jsondef main(): mydict = &#123; &#x27;name&#x27;: &#x27;骆昊&#x27;, &#x27;age&#x27;: 38, &#x27;qq&#x27;: 957658, &#x27;friends&#x27;: [&#x27;王大锤&#x27;, &#x27;白元芳&#x27;], &#x27;cars&#x27;: [ &#123;&#x27;brand&#x27;: &#x27;BYD&#x27;, &#x27;max_speed&#x27;: 180&#125;, &#123;&#x27;brand&#x27;: &#x27;Audi&#x27;, &#x27;max_speed&#x27;: 280&#125;, &#123;&#x27;brand&#x27;: &#x27;Benz&#x27;, &#x27;max_speed&#x27;: 320&#125; ] &#125; try: with open(&#x27;data.json&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as fs: json.dump(mydict, fs) except IOError as e: print(e) print(&#x27;保存数据完成!&#x27;)if __name__ == &#x27;__main__&#x27;: main() json模块主要有四个比较重要的函数，分别是： dump - 将Python对象按照JSON格式序列化到文件中 dumps - 将Python对象处理成JSON格式的字符串 load - 将文件中的JSON数据反序列化成对象 loads - 将字符串的内容反序列化成Python对象 这里出现了两个概念，一个叫序列化，一个叫反序列化。自由的百科全书维基百科上对这两个概念是这样解释的：“序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换为可以存储或传输的形式，这样在需要的时候能够恢复到原先的状态，而且通过序列化的数据重新获取字节时，可以利用这些字节来产生原始对象的副本（拷贝）。与这个过程相反的动作，即从一系列字节中提取数据结构的操作，就是反序列化（deserialization）”。 目前绝大多数网络数据服务（或称之为网络API）都是基于HTTP协议提供JSON格式的数据，关于HTTP协议的相关知识，可以看看阮一峰老师的《HTTP协议入门》，如果想了解国内的网络数据服务，可以看看聚合数据和阿凡达数据等网站，国外的可以看看{API}Search网站。下面的例子演示了如何使用requests模块（封装得足够好的第三方网络访问模块）访问网络API获取国内新闻，如何通过json模块解析JSON数据并显示新闻标题，这个例子使用了天行数据提供的国内新闻数据接口，其中的APIKey需要自己到该网站申请。 12345678910111213import requestsimport jsondef main(): resp = requests.get(&#x27;http://api.tianapi.com/guonei/?key=APIKey&amp;num=10&#x27;) data_model = json.loads(resp.text) for news in data_model[&#x27;newslist&#x27;]: print(news[&#x27;title&#x27;])if __name__ == &#x27;__main__&#x27;: main() 在Python中要实现序列化和反序列化除了使用json模块之外，还可以使用pickle和shelve模块，但是这两个模块是使用特有的序列化协议来序列化数据，因此序列化后的数据只能被Python识别。关于这两个模块的相关知识可以自己看看网络上的资料。另外，如果要了解更多的关于Python异常机制的知识，可以看看segmentfault上面的文章《总结：Python中的异常处理》，这篇文章不仅介绍了Python中异常机制的使用，还总结了一系列的最佳实践，很值得一读。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/10.图形用户界面和游戏开发","date":"2024-12-12T08:38:00.415Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/10.图形用户界面和游戏开发/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/10.%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E5%92%8C%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/","excerpt":"","text":"图形用户界面和游戏开发基于tkinter模块的GUIGUI是图形用户界面的缩写，图形化的用户界面对使用过计算机的人来说应该都不陌生，在此也无需进行赘述。Python默认的GUI开发模块是tkinter（在Python 3以前的版本中名为Tkinter），从这个名字就可以看出它是基于Tk的，Tk是一个工具包，最初是为Tcl设计的，后来被移植到很多其他的脚本语言中，它提供了跨平台的GUI控件。当然Tk并不是最新和最好的选择，也没有功能特别强大的GUI控件，事实上，开发GUI应用并不是Python最擅长的工作，如果真的需要使用Python开发GUI应用，wxPython、PyQt、PyGTK等模块都是不错的选择。 基本上使用tkinter来开发GUI应用需要以下5个步骤： 导入tkinter模块中我们需要的东西。 创建一个顶层窗口对象并用它来承载整个GUI应用。 在顶层窗口对象上添加GUI组件。 通过代码将这些GUI组件的功能组织起来。 进入主事件循环(main loop)。 下面的代码演示了如何使用tkinter做一个简单的GUI应用。 12345678910111213141516171819202122232425262728293031323334353637383940414243import tkinterimport tkinter.messageboxdef main(): flag = True # 修改标签上的文字 def change_label_text(): nonlocal flag flag = not flag color, msg = (&#x27;red&#x27;, &#x27;Hello, world!&#x27;)\\ if flag else (&#x27;blue&#x27;, &#x27;Goodbye, world!&#x27;) label.config(text=msg, fg=color) # 确认退出 def confirm_to_quit(): if tkinter.messagebox.askokcancel(&#x27;温馨提示&#x27;, &#x27;确定要退出吗?&#x27;): top.quit() # 创建顶层窗口 top = tkinter.Tk() # 设置窗口大小 top.geometry(&#x27;240x160&#x27;) # 设置窗口标题 top.title(&#x27;小游戏&#x27;) # 创建标签对象并添加到顶层窗口 label = tkinter.Label(top, text=&#x27;Hello, world!&#x27;, font=&#x27;Arial -32&#x27;, fg=&#x27;red&#x27;) label.pack(expand=1) # 创建一个装按钮的容器 panel = tkinter.Frame(top) # 创建按钮对象 指定添加到哪个容器中 通过command参数绑定事件回调函数 button1 = tkinter.Button(panel, text=&#x27;修改&#x27;, command=change_label_text) button1.pack(side=&#x27;left&#x27;) button2 = tkinter.Button(panel, text=&#x27;退出&#x27;, command=confirm_to_quit) button2.pack(side=&#x27;right&#x27;) panel.pack(side=&#x27;bottom&#x27;) # 开启主事件循环 tkinter.mainloop()if __name__ == &#x27;__main__&#x27;: main() 需要说明的是，GUI应用通常是事件驱动式的，之所以要进入主事件循环就是要监听鼠标、键盘等各种事件的发生并执行对应的代码对事件进行处理，因为事件会持续的发生，所以需要这样的一个循环一直运行着等待下一个事件的发生。另一方面，Tk为控件的摆放提供了三种布局管理器，通过布局管理器可以对控件进行定位，这三种布局管理器分别是：Placer（开发者提供控件的大小和摆放位置）、Packer（自动将控件填充到合适的位置）和Grid（基于网格坐标来摆放控件），此处不进行赘述。 使用Pygame进行游戏开发Pygame是一个开源的Python模块，专门用于多媒体应用（如电子游戏）的开发，其中包含对图像、声音、视频、事件、碰撞等的支持。Pygame建立在SDL的基础上，SDL是一套跨平台的多媒体开发库，用C语言实现，被广泛的应用于游戏、模拟器、播放器等的开发。而Pygame让游戏开发者不再被底层语言束缚，可以更多的关注游戏的功能和逻辑。 下面我们来完成一个简单的小游戏，游戏的名字叫“大球吃小球”，当然完成这个游戏并不是重点，学会使用Pygame也不是重点，最重要的我们要在这个过程中体会如何使用前面讲解的面向对象程序设计，学会用这种编程思想去解决现实中的问题。 制作游戏窗口123456789101112131415161718192021import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption(&#x27;大球吃小球&#x27;) running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == &#x27;__main__&#x27;: main() 在窗口中绘图可以通过pygame中draw模块的函数在窗口上绘图，可以绘制的图形包括：线条、矩形、多边形、圆、椭圆、圆弧等。需要说明的是，屏幕坐标系是将屏幕左上角设置为坐标原点(0, 0)，向右是x轴的正向，向下是y轴的正向，在表示位置或者设置尺寸的时候，我们默认的单位都是像素。所谓像素就是屏幕上的一个点，你可以用浏览图片的软件试着将一张图片放大若干倍，就可以看到这些点。pygame中表示颜色用的是色光三原色表示法，即通过一个元组或列表来指定颜色的RGB值，每个值都在0~255之间，因为是每种原色都用一个8位（bit）的值来表示，三种颜色相当于一共由24位构成，这也就是常说的“24位颜色表示法”。 123456789101112131415161718192021222324252627import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption(&#x27;大球吃小球&#x27;) # 设置窗口的背景色(颜色是由红绿蓝三原色构成的元组) screen.fill((242, 242, 242)) # 绘制一个圆(参数分别是: 屏幕, 颜色, 圆心位置, 半径, 0表示填充圆) pygame.draw.circle(screen, (255, 0, 0,), (100, 100), 30, 0) # 刷新当前窗口(渲染窗口将绘制的图像呈现出来) pygame.display.flip() running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == &#x27;__main__&#x27;: main() 加载图像如果需要直接加载图像到窗口上，可以使用pygame中image模块的函数来加载图像，再通过之前获得的窗口对象的blit方法渲染图像，代码如下所示。 1234567891011121314151617181920212223242526272829import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption(&#x27;大球吃小球&#x27;) # 设置窗口的背景色(颜色是由红绿蓝三原色构成的元组) screen.fill((255, 255, 255)) # 通过指定的文件名加载图像 ball_image = pygame.image.load(&#x27;./res/ball.png&#x27;) # 在窗口上渲染图像 screen.blit(ball_image, (50, 50)) # 刷新当前窗口(渲染窗口将绘制的图像呈现出来) pygame.display.flip() running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = Falseif __name__ == &#x27;__main__&#x27;: main() 实现动画效果说到动画这个词大家都不会陌生，事实上要实现动画效果，本身的原理也非常简单，就是将不连续的图片连续的播放，只要每秒钟达到了一定的帧数，那么就可以做出比较流畅的动画效果。如果要让上面代码中的小球动起来，可以将小球的位置用变量来表示，并在循环中修改小球的位置再刷新整个窗口即可。 1234567891011121314151617181920212223242526272829import pygamedef main(): # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption(&#x27;大球吃小球&#x27;) # 定义变量来表示小球在屏幕上的位置 x, y = 50, 50 running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = False screen.fill((255, 255, 255)) pygame.draw.circle(screen, (255, 0, 0,), (x, y), 30, 0) pygame.display.flip() # 每隔50毫秒就改变小球的位置再刷新窗口 pygame.time.delay(50) x, y = x + 5, y + 5if __name__ == &#x27;__main__&#x27;: main() 碰撞检测通常一个游戏中会有很多对象出现，而这些对象之间的“碰撞”在所难免，比如炮弹击中了飞机、箱子撞到了地面等。碰撞检测在绝大多数的游戏中都是一个必须得处理的至关重要的问题，pygame的sprite（动画精灵）模块就提供了对碰撞检测的支持，这里我们暂时不介绍sprite模块提供的功能，因为要检测两个小球有没有碰撞其实非常简单，只需要检查球心的距离有没有小于两个球的半径之和。为了制造出更多的小球，我们可以通过对鼠标事件的处理，在点击鼠标的位置创建颜色、大小和移动速度都随机的小球，当然要做到这一点，我们可以把之前学习到的面向对象的知识应用起来。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from enum import Enum, uniquefrom math import sqrtfrom random import randintimport pygame@uniqueclass Color(Enum): &quot;&quot;&quot;颜色&quot;&quot;&quot; RED = (255, 0, 0) GREEN = (0, 255, 0) BLUE = (0, 0, 255) BLACK = (0, 0, 0) WHITE = (255, 255, 255) GRAY = (242, 242, 242) @staticmethod def random_color(): &quot;&quot;&quot;获得随机颜色&quot;&quot;&quot; r = randint(0, 255) g = randint(0, 255) b = randint(0, 255) return (r, g, b)class Ball(object): &quot;&quot;&quot;球&quot;&quot;&quot; def __init__(self, x, y, radius, sx, sy, color=Color.RED): &quot;&quot;&quot;初始化方法&quot;&quot;&quot; self.x = x self.y = y self.radius = radius self.sx = sx self.sy = sy self.color = color self.alive = True def move(self, screen): &quot;&quot;&quot;移动&quot;&quot;&quot; self.x += self.sx self.y += self.sy if self.x - self.radius &lt;= 0 or \\ self.x + self.radius &gt;= screen.get_width(): self.sx = -self.sx if self.y - self.radius &lt;= 0 or \\ self.y + self.radius &gt;= screen.get_height(): self.sy = -self.sy def eat(self, other): &quot;&quot;&quot;吃其他球&quot;&quot;&quot; if self.alive and other.alive and self != other: dx, dy = self.x - other.x, self.y - other.y distance = sqrt(dx ** 2 + dy ** 2) if distance &lt; self.radius + other.radius \\ and self.radius &gt; other.radius: other.alive = False self.radius = self.radius + int(other.radius * 0.146) def draw(self, screen): &quot;&quot;&quot;在窗口上绘制球&quot;&quot;&quot; pygame.draw.circle(screen, self.color, (self.x, self.y), self.radius, 0) 事件处理可以在事件循环中对鼠标事件进行处理，通过事件对象的type属性可以判定事件类型，再通过pos属性就可以获得鼠标点击的位置。如果要处理键盘事件也是在这个地方，做法与处理鼠标事件类似。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(): # 定义用来装所有球的容器 balls = [] # 初始化导入的pygame中的模块 pygame.init() # 初始化用于显示的窗口并设置窗口尺寸 screen = pygame.display.set_mode((800, 600)) # 设置当前窗口的标题 pygame.display.set_caption(&#x27;大球吃小球&#x27;) running = True # 开启一个事件循环处理发生的事件 while running: # 从消息队列中获取事件并对事件进行处理 for event in pygame.event.get(): if event.type == pygame.QUIT: running = False # 处理鼠标事件的代码 if event.type == pygame.MOUSEBUTTONDOWN and event.button == 1: # 获得点击鼠标的位置 x, y = event.pos radius = randint(10, 100) sx, sy = randint(-10, 10), randint(-10, 10) color = Color.random_color() # 在点击鼠标的位置创建一个球(大小、速度和颜色随机) ball = Ball(x, y, radius, sx, sy, color) # 将球添加到列表容器中 balls.append(ball) screen.fill((255, 255, 255)) # 取出容器中的球 如果没被吃掉就绘制 被吃掉了就移除 for ball in balls: if ball.alive: ball.draw(screen) else: balls.remove(ball) pygame.display.flip() # 每隔50毫秒就改变球的位置再刷新窗口 pygame.time.delay(50) for ball in balls: ball.move(screen) # 检查球有没有吃到其他的球 for other in balls: ball.eat(other)if __name__ == &#x27;__main__&#x27;: main() 上面的两段代码合在一起，我们就完成了“大球吃小球”的游戏（如下图所示），准确的说它算不上一个游戏，但是做一个小游戏的基本知识我们已经通过这个例子告诉大家了，有了这些知识已经可以开始你的小游戏开发之旅了。其实上面的代码中还有很多值得改进的地方，比如刷新窗口以及让球移动起来的代码并不应该放在事件循环中，等学习了多线程的知识后，用一个后台线程来处理这些事可能是更好的选择。如果希望获得更好的用户体验，我们还可以在游戏中加入背景音乐以及在球与球发生碰撞时播放音效，利用pygame的mixer和music模块，我们可以很容易的做到这一点，大家可以自行了解这方面的知识。事实上，想了解更多的关于pygame的知识，最好的教程是pygame的官方网站，如果英语没毛病就可以赶紧去看看啦。 如果想开发3D游戏，pygame就显得力不从心了，对3D游戏开发如果有兴趣的读者不妨看看Panda3D。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/09.面向对象进阶","date":"2024-12-12T08:38:00.413Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/09.面向对象进阶/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/09.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%BF%9B%E9%98%B6/","excerpt":"","text":"面向对象进阶在前面的章节我们已经了解了面向对象的入门知识，知道了如何定义类，如何创建对象以及如何给对象发消息。为了能够更好的使用面向对象编程思想进行程序开发，我们还需要对Python中的面向对象编程进行更为深入的了解。 @property装饰器之前我们讨论过Python中属性和方法访问权限的问题，虽然我们不建议将属性设置为私有的，但是如果直接将属性暴露给外界也是有问题的，比如我们没有办法检查赋给属性的值是否有效。我们之前的建议是将属性命名以单下划线开头，通过这种方式来暗示属性是受保护的，不建议外界直接访问，那么如果想访问属性可以通过属性的getter（访问器）和setter（修改器）方法进行对应的操作。如果要做到这点，就可以考虑使用@property包装器来包装getter和setter方法，使得对属性的访问既安全又方便，代码如下所示。 1234567891011121314151617181920212223242526272829303132333435363738class Person(object): def __init__(self, name, age): self._name = name self._age = age # 访问器 - getter方法 @property def name(self): return self._name # 访问器 - getter方法 @property def age(self): return self._age # 修改器 - setter方法 @age.setter def age(self, age): self._age = age def play(self): if self._age &lt;= 16: print(&#x27;%s正在玩飞行棋.&#x27; % self._name) else: print(&#x27;%s正在玩斗地主.&#x27; % self._name)def main(): person = Person(&#x27;王大锤&#x27;, 12) person.play() person.age = 22 person.play() # person.name = &#x27;白元芳&#x27; # AttributeError: can&#x27;t set attributeif __name__ == &#x27;__main__&#x27;: main() __slots__魔法我们讲到这里，不知道大家是否已经意识到，Python是一门动态语言。通常，动态语言允许我们在程序运行时给对象绑定新的属性或方法，当然也可以对已经绑定的属性和方法进行解绑定。但是如果我们需要限定自定义类型的对象只能绑定某些属性，可以通过在类中定义__slots__变量来进行限定。需要注意的是__slots__的限定只对当前类的对象生效，对子类并不起任何作用。 12345678910111213141516171819202122232425262728293031323334class Person(object): # 限定Person对象只能绑定_name, _age和_gender属性 __slots__ = (&#x27;_name&#x27;, &#x27;_age&#x27;, &#x27;_gender&#x27;) def __init__(self, name, age): self._name = name self._age = age @property def name(self): return self._name @property def age(self): return self._age @age.setter def age(self, age): self._age = age def play(self): if self._age &lt;= 16: print(&#x27;%s正在玩飞行棋.&#x27; % self._name) else: print(&#x27;%s正在玩斗地主.&#x27; % self._name)def main(): person = Person(&#x27;王大锤&#x27;, 22) person.play() person._gender = &#x27;男&#x27; # AttributeError: &#x27;Person&#x27; object has no attribute &#x27;_is_gay&#x27; # person._is_gay = True 静态方法和类方法之前，我们在类中定义的方法都是对象方法，也就是说这些方法都是发送给对象的消息。实际上，我们写在类中的方法并不需要都是对象方法，例如我们定义一个“三角形”类，通过传入三条边长来构造三角形，并提供计算周长和面积的方法，但是传入的三条边长未必能构造出三角形对象，因此我们可以先写一个方法来验证三条边长是否可以构成三角形，这个方法很显然就不是对象方法，因为在调用这个方法时三角形对象尚未创建出来（因为都不知道三条边能不能构成三角形），所以这个方法是属于三角形类而并不属于三角形对象的。我们可以使用静态方法来解决这类问题，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839from math import sqrtclass Triangle(object): def __init__(self, a, b, c): self._a = a self._b = b self._c = c @staticmethod def is_valid(a, b, c): return a + b &gt; c and b + c &gt; a and a + c &gt; b def perimeter(self): return self._a + self._b + self._c def area(self): half = self.perimeter() / 2 return sqrt(half * (half - self._a) * (half - self._b) * (half - self._c))def main(): a, b, c = 3, 4, 5 # 静态方法和类方法都是通过给类发消息来调用的 if Triangle.is_valid(a, b, c): t = Triangle(a, b, c) print(t.perimeter()) # 也可以通过给类发消息来调用对象方法但是要传入接收消息的对象作为参数 # print(Triangle.perimeter(t)) print(t.area()) # print(Triangle.area(t)) else: print(&#x27;无法构成三角形.&#x27;)if __name__ == &#x27;__main__&#x27;: main() 和静态方法比较类似，Python还可以在类中定义类方法，类方法的第一个参数约定名为cls，它代表的是当前类相关的信息的对象（类本身也是一个对象，有的地方也称之为类的元数据对象），通过这个参数我们可以获取和类相关的信息并且可以创建出类的对象，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445from time import time, localtime, sleepclass Clock(object): &quot;&quot;&quot;数字时钟&quot;&quot;&quot; def __init__(self, hour=0, minute=0, second=0): self._hour = hour self._minute = minute self._second = second @classmethod def now(cls): ctime = localtime(time()) return cls(ctime.tm_hour, ctime.tm_min, ctime.tm_sec) def run(self): &quot;&quot;&quot;走字&quot;&quot;&quot; self._second += 1 if self._second == 60: self._second = 0 self._minute += 1 if self._minute == 60: self._minute = 0 self._hour += 1 if self._hour == 24: self._hour = 0 def show(self): &quot;&quot;&quot;显示时间&quot;&quot;&quot; return &#x27;%02d:%02d:%02d&#x27; % \\ (self._hour, self._minute, self._second)def main(): # 通过类方法创建对象并获取系统时间 clock = Clock.now() while True: print(clock.show()) sleep(1) clock.run()if __name__ == &#x27;__main__&#x27;: main() 类之间的关系简单的说，类和类之间的关系有三种：is-a、has-a和use-a关系。 is-a关系也叫继承或泛化，比如学生和人的关系、手机和电子产品的关系都属于继承关系。 has-a关系通常称之为关联，比如部门和员工的关系，汽车和引擎的关系都属于关联关系；关联关系如果是整体和部分的关联，那么我们称之为聚合关系；如果整体进一步负责了部分的生命周期（整体和部分是不可分割的，同时同在也同时消亡），那么这种就是最强的关联关系，我们称之为合成关系。 use-a关系通常称之为依赖，比如司机有一个驾驶的行为（方法），其中（的参数）使用到了汽车，那么司机和汽车的关系就是依赖关系。 我们可以使用一种叫做UML（统一建模语言）的东西来进行面向对象建模，其中一项重要的工作就是把类和类之间的关系用标准化的图形符号描述出来。关于UML我们在这里不做详细的介绍，有兴趣的读者可以自行阅读《UML面向对象设计基础》一书。 利用类之间的这些关系，我们可以在已有类的基础上来完成某些操作，也可以在已有类的基础上创建新的类，这些都是实现代码复用的重要手段。复用现有的代码不仅可以减少开发的工作量，也有利于代码的管理和维护，这是我们在日常工作中都会使用到的技术手段。 继承和多态刚才我们提到了，可以在已有类的基础上创建新类，这其中的一种做法就是让一个类从另一个类那里将属性和方法直接继承下来，从而减少重复代码的编写。提供继承信息的我们称之为父类，也叫超类或基类；得到继承信息的我们称之为子类，也叫派生类或衍生类。子类除了继承父类提供的属性和方法，还可以定义自己特有的属性和方法，所以子类比父类拥有的更多的能力，在实际开发中，我们经常会用子类对象去替换掉一个父类对象，这是面向对象编程中一个常见的行为，对应的原则称之为里氏替换原则。下面我们先看一个继承的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Person(object): &quot;&quot;&quot;人&quot;&quot;&quot; def __init__(self, name, age): self._name = name self._age = age @property def name(self): return self._name @property def age(self): return self._age @age.setter def age(self, age): self._age = age def play(self): print(&#x27;%s正在愉快的玩耍.&#x27; % self._name) def watch_av(self): if self._age &gt;= 18: print(&#x27;%s正在观看爱情动作片.&#x27; % self._name) else: print(&#x27;%s只能观看《熊出没》.&#x27; % self._name)class Student(Person): &quot;&quot;&quot;学生&quot;&quot;&quot; def __init__(self, name, age, grade): super().__init__(name, age) self._grade = grade @property def grade(self): return self._grade @grade.setter def grade(self, grade): self._grade = grade def study(self, course): print(&#x27;%s的%s正在学习%s.&#x27; % (self._grade, self._name, course))class Teacher(Person): &quot;&quot;&quot;老师&quot;&quot;&quot; def __init__(self, name, age, title): super().__init__(name, age) self._title = title @property def title(self): return self._title @title.setter def title(self, title): self._title = title def teach(self, course): print(&#x27;%s%s正在讲%s.&#x27; % (self._name, self._title, course))def main(): stu = Student(&#x27;王大锤&#x27;, 15, &#x27;初三&#x27;) stu.study(&#x27;数学&#x27;) stu.watch_av() t = Teacher(&#x27;骆昊&#x27;, 38, &#x27;砖家&#x27;) t.teach(&#x27;Python程序设计&#x27;) t.watch_av()if __name__ == &#x27;__main__&#x27;: main() 子类在继承了父类的方法后，可以对父类已有的方法给出新的实现版本，这个动作称之为方法重写（override）。通过方法重写我们可以让父类的同一个行为在子类中拥有不同的实现版本，当我们调用这个经过子类重写的方法时，不同的子类对象会表现出不同的行为，这个就是多态（poly-morphism）。 12345678910111213141516171819202122232425262728293031323334353637from abc import ABCMeta, abstractmethodclass Pet(object, metaclass=ABCMeta): &quot;&quot;&quot;宠物&quot;&quot;&quot; def __init__(self, nickname): self._nickname = nickname @abstractmethod def make_voice(self): &quot;&quot;&quot;发出声音&quot;&quot;&quot; passclass Dog(Pet): &quot;&quot;&quot;狗&quot;&quot;&quot; def make_voice(self): print(&#x27;%s: 汪汪汪...&#x27; % self._nickname)class Cat(Pet): &quot;&quot;&quot;猫&quot;&quot;&quot; def make_voice(self): print(&#x27;%s: 喵...喵...&#x27; % self._nickname)def main(): pets = [Dog(&#x27;旺财&#x27;), Cat(&#x27;凯蒂&#x27;), Dog(&#x27;大黄&#x27;)] for pet in pets: pet.make_voice()if __name__ == &#x27;__main__&#x27;: main() 在上面的代码中，我们将Pet类处理成了一个抽象类，所谓抽象类就是不能够创建对象的类，这种类的存在就是专门为了让其他类去继承它。Python从语法层面并没有像Java或C#那样提供对抽象类的支持，但是我们可以通过abc模块的ABCMeta元类和abstractmethod包装器来达到抽象类的效果，如果一个类中存在抽象方法那么这个类就不能够实例化（创建对象）。上面的代码中，Dog和Cat两个子类分别对Pet类中的make_voice抽象方法进行了重写并给出了不同的实现版本，当我们在main函数中调用该方法时，这个方法就表现出了多态行为（同样的方法做了不同的事情）。 综合案例案例1：奥特曼打小怪兽。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185from abc import ABCMeta, abstractmethodfrom random import randint, randrangeclass Fighter(object, metaclass=ABCMeta): &quot;&quot;&quot;战斗者&quot;&quot;&quot; # 通过__slots__魔法限定对象可以绑定的成员变量 __slots__ = (&#x27;_name&#x27;, &#x27;_hp&#x27;) def __init__(self, name, hp): &quot;&quot;&quot;初始化方法 :param name: 名字 :param hp: 生命值 &quot;&quot;&quot; self._name = name self._hp = hp @property def name(self): return self._name @property def hp(self): return self._hp @hp.setter def hp(self, hp): self._hp = hp if hp &gt;= 0 else 0 @property def alive(self): return self._hp &gt; 0 @abstractmethod def attack(self, other): &quot;&quot;&quot;攻击 :param other: 被攻击的对象 &quot;&quot;&quot; passclass Ultraman(Fighter): &quot;&quot;&quot;奥特曼&quot;&quot;&quot; __slots__ = (&#x27;_name&#x27;, &#x27;_hp&#x27;, &#x27;_mp&#x27;) def __init__(self, name, hp, mp): &quot;&quot;&quot;初始化方法 :param name: 名字 :param hp: 生命值 :param mp: 魔法值 &quot;&quot;&quot; super().__init__(name, hp) self._mp = mp def attack(self, other): other.hp -= randint(15, 25) def huge_attack(self, other): &quot;&quot;&quot;究极必杀技(打掉对方至少50点或四分之三的血) :param other: 被攻击的对象 :return: 使用成功返回True否则返回False &quot;&quot;&quot; if self._mp &gt;= 50: self._mp -= 50 injury = other.hp * 3 // 4 injury = injury if injury &gt;= 50 else 50 other.hp -= injury return True else: self.attack(other) return False def magic_attack(self, others): &quot;&quot;&quot;魔法攻击 :param others: 被攻击的群体 :return: 使用魔法成功返回True否则返回False &quot;&quot;&quot; if self._mp &gt;= 20: self._mp -= 20 for temp in others: if temp.alive: temp.hp -= randint(10, 15) return True else: return False def resume(self): &quot;&quot;&quot;恢复魔法值&quot;&quot;&quot; incr_point = randint(1, 10) self._mp += incr_point return incr_point def __str__(self): return &#x27;~~~%s奥特曼~~~\\n&#x27; % self._name + \\ &#x27;生命值: %d\\n&#x27; % self._hp + \\ &#x27;魔法值: %d\\n&#x27; % self._mpclass Monster(Fighter): &quot;&quot;&quot;小怪兽&quot;&quot;&quot; __slots__ = (&#x27;_name&#x27;, &#x27;_hp&#x27;) def attack(self, other): other.hp -= randint(10, 20) def __str__(self): return &#x27;~~~%s小怪兽~~~\\n&#x27; % self._name + \\ &#x27;生命值: %d\\n&#x27; % self._hpdef is_any_alive(monsters): &quot;&quot;&quot;判断有没有小怪兽是活着的&quot;&quot;&quot; for monster in monsters: if monster.alive &gt; 0: return True return Falsedef select_alive_one(monsters): &quot;&quot;&quot;选中一只活着的小怪兽&quot;&quot;&quot; monsters_len = len(monsters) while True: index = randrange(monsters_len) monster = monsters[index] if monster.alive &gt; 0: return monsterdef display_info(ultraman, monsters): &quot;&quot;&quot;显示奥特曼和小怪兽的信息&quot;&quot;&quot; print(ultraman) for monster in monsters: print(monster, end=&#x27;&#x27;)def main(): u = Ultraman(&#x27;骆昊&#x27;, 1000, 120) m1 = Monster(&#x27;狄仁杰&#x27;, 250) m2 = Monster(&#x27;白元芳&#x27;, 500) m3 = Monster(&#x27;王大锤&#x27;, 750) ms = [m1, m2, m3] fight_round = 1 while u.alive and is_any_alive(ms): print(&#x27;========第%02d回合========&#x27; % fight_round) m = select_alive_one(ms) # 选中一只小怪兽 skill = randint(1, 10) # 通过随机数选择使用哪种技能 if skill &lt;= 6: # 60%的概率使用普通攻击 print(&#x27;%s使用普通攻击打了%s.&#x27; % (u.name, m.name)) u.attack(m) print(&#x27;%s的魔法值恢复了%d点.&#x27; % (u.name, u.resume())) elif skill &lt;= 9: # 30%的概率使用魔法攻击(可能因魔法值不足而失败) if u.magic_attack(ms): print(&#x27;%s使用了魔法攻击.&#x27; % u.name) else: print(&#x27;%s使用魔法失败.&#x27; % u.name) else: # 10%的概率使用究极必杀技(如果魔法值不足则使用普通攻击) if u.huge_attack(m): print(&#x27;%s使用究极必杀技虐了%s.&#x27; % (u.name, m.name)) else: print(&#x27;%s使用普通攻击打了%s.&#x27; % (u.name, m.name)) print(&#x27;%s的魔法值恢复了%d点.&#x27; % (u.name, u.resume())) if m.alive &gt; 0: # 如果选中的小怪兽没有死就回击奥特曼 print(&#x27;%s回击了%s.&#x27; % (m.name, u.name)) m.attack(u) display_info(u, ms) # 每个回合结束后显示奥特曼和小怪兽的信息 fight_round += 1 print(&#x27;\\n========战斗结束!========\\n&#x27;) if u.alive &gt; 0: print(&#x27;%s奥特曼胜利!&#x27; % u.name) else: print(&#x27;小怪兽胜利!&#x27;)if __name__ == &#x27;__main__&#x27;: main() 案例2：扑克游戏。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110import randomclass Card(object): &quot;&quot;&quot;一张牌&quot;&quot;&quot; def __init__(self, suite, face): self._suite = suite self._face = face @property def face(self): return self._face @property def suite(self): return self._suite def __str__(self): if self._face == 1: face_str = &#x27;A&#x27; elif self._face == 11: face_str = &#x27;J&#x27; elif self._face == 12: face_str = &#x27;Q&#x27; elif self._face == 13: face_str = &#x27;K&#x27; else: face_str = str(self._face) return &#x27;%s%s&#x27; % (self._suite, face_str) def __repr__(self): return self.__str__()class Poker(object): &quot;&quot;&quot;一副牌&quot;&quot;&quot; def __init__(self): self._cards = [Card(suite, face) for suite in &#x27;♠♥♣♦&#x27; for face in range(1, 14)] self._current = 0 @property def cards(self): return self._cards def shuffle(self): &quot;&quot;&quot;洗牌(随机乱序)&quot;&quot;&quot; self._current = 0 random.shuffle(self._cards) @property def next(self): &quot;&quot;&quot;发牌&quot;&quot;&quot; card = self._cards[self._current] self._current += 1 return card @property def has_next(self): &quot;&quot;&quot;还有没有牌&quot;&quot;&quot; return self._current &lt; len(self._cards)class Player(object): &quot;&quot;&quot;玩家&quot;&quot;&quot; def __init__(self, name): self._name = name self._cards_on_hand = [] @property def name(self): return self._name @property def cards_on_hand(self): return self._cards_on_hand def get(self, card): &quot;&quot;&quot;摸牌&quot;&quot;&quot; self._cards_on_hand.append(card) def arrange(self, card_key): &quot;&quot;&quot;玩家整理手上的牌&quot;&quot;&quot; self._cards_on_hand.sort(key=card_key)# 排序规则-先根据花色再根据点数排序def get_key(card): return (card.suite, card.face)def main(): p = Poker() p.shuffle() players = [Player(&#x27;东邪&#x27;), Player(&#x27;西毒&#x27;), Player(&#x27;南帝&#x27;), Player(&#x27;北丐&#x27;)] for _ in range(13): for player in players: player.get(p.next) for player in players: print(player.name + &#x27;:&#x27;, end=&#x27; &#x27;) player.arrange(get_key) print(player.cards_on_hand)if __name__ == &#x27;__main__&#x27;: main() 说明： 大家可以自己尝试在上面代码的基础上写一个简单的扑克游戏，例如21点(Black Jack)，游戏的规则可以自己在网上找一找。 案例3：工资结算系统。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&quot;&quot;&quot;某公司有三种类型的员工 分别是部门经理、程序员和销售员需要设计一个工资结算系统 根据提供的员工信息来计算月薪部门经理的月薪是每月固定15000元程序员的月薪按本月工作时间计算 每小时150元销售员的月薪是1200元的底薪加上销售额5%的提成&quot;&quot;&quot;from abc import ABCMeta, abstractmethodclass Employee(object, metaclass=ABCMeta): &quot;&quot;&quot;员工&quot;&quot;&quot; def __init__(self, name): &quot;&quot;&quot; 初始化方法 :param name: 姓名 &quot;&quot;&quot; self._name = name @property def name(self): return self._name @abstractmethod def get_salary(self): &quot;&quot;&quot; 获得月薪 :return: 月薪 &quot;&quot;&quot; passclass Manager(Employee): &quot;&quot;&quot;部门经理&quot;&quot;&quot; def get_salary(self): return 15000.0class Programmer(Employee): &quot;&quot;&quot;程序员&quot;&quot;&quot; def __init__(self, name, working_hour=0): super().__init__(name) self._working_hour = working_hour @property def working_hour(self): return self._working_hour @working_hour.setter def working_hour(self, working_hour): self._working_hour = working_hour if working_hour &gt; 0 else 0 def get_salary(self): return 150.0 * self._working_hourclass Salesman(Employee): &quot;&quot;&quot;销售员&quot;&quot;&quot; def __init__(self, name, sales=0): super().__init__(name) self._sales = sales @property def sales(self): return self._sales @sales.setter def sales(self, sales): self._sales = sales if sales &gt; 0 else 0 def get_salary(self): return 1200.0 + self._sales * 0.05def main(): emps = [ Manager(&#x27;刘备&#x27;), Programmer(&#x27;诸葛亮&#x27;), Manager(&#x27;曹操&#x27;), Salesman(&#x27;荀彧&#x27;), Salesman(&#x27;吕布&#x27;), Programmer(&#x27;张辽&#x27;), Programmer(&#x27;赵云&#x27;) ] for emp in emps: if isinstance(emp, Programmer): emp.working_hour = int(input(&#x27;请输入%s本月工作时间: &#x27; % emp.name)) elif isinstance(emp, Salesman): emp.sales = float(input(&#x27;请输入%s本月销售额: &#x27; % emp.name)) # 同样是接收get_salary这个消息但是不同的员工表现出了不同的行为(多态) print(&#x27;%s本月工资为: ￥%s元&#x27; % (emp.name, emp.get_salary()))if __name__ == &#x27;__main__&#x27;: main()","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/08.面向对象编程基础","date":"2024-12-12T08:38:00.409Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/08.面向对象编程基础/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/08.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/","excerpt":"","text":"面向对象编程基础活在当下的程序员应该都听过&quot;面向对象编程&quot;一词，也经常有人问能不能用一句话解释下什么是&quot;面向对象编程&quot;，我们先来看看比较正式的说法。 &quot;把一组数据结构和处理它们的方法组成对象（object），把相同行为的对象归纳为类（class），通过类的封装（encapsulation）隐藏内部细节，通过继承（inheritance）实现类的特化（specialization）和泛化（generalization），通过多态（polymorphism）实现基于对象类型的动态分派。&quot; 这样一说是不是更不明白了。所以我们还是看看更通俗易懂的说法，下面这段内容来自于知乎。 说明： 以上的内容来自于网络，不代表作者本人的观点和看法，与作者本人立场无关，相关责任不由作者承担。 之前我们说过&quot;程序是指令的集合&quot;，我们在程序中书写的语句在执行时会变成一条或多条指令然后由CPU去执行。当然为了简化程序的设计，我们引入了函数的概念，把相对独立且经常重复使用的代码放置到函数中，在需要使用这些功能的时候只要调用函数即可；如果一个函数的功能过于复杂和臃肿，我们又可以进一步将函数继续切分为子函数来降低系统的复杂性。但是说了这么多，不知道大家是否发现，所谓编程就是程序员按照计算机的工作方式控制计算机完成各种任务。但是，计算机的工作方式与正常人类的思维模式是不同的，如果编程就必须得抛弃人类正常的思维方式去迎合计算机，编程的乐趣就少了很多，&quot;每个人都应该学习编程&quot;这样的豪言壮语就只能说说而已。当然，这些还不是最重要的，最重要的是当我们需要开发一个复杂的系统时，代码的复杂性会让开发和维护工作都变得举步维艰，所以在上世纪60年代末期，&quot;软件危机&quot;、&quot;软件工程&quot;等一系列的概念开始在行业中出现。 当然，程序员圈子内的人都知道，现实中并没有解决上面所说的这些问题的&quot;银弹&quot;，真正让软件开发者看到希望的是上世纪70年代诞生的Smalltalk编程语言中引入的面向对象的编程思想（面向对象编程的雏形可以追溯到更早期的Simula语言）。按照这种编程理念，程序中的数据和操作数据的函数是一个逻辑上的整体，我们称之为“对象”，而我们解决问题的方式就是创建出需要的对象并向对象发出各种各样的消息，多个对象的协同工作最终可以让我们构造出复杂的系统来解决现实中的问题。 说明： 当然面向对象也不是解决软件开发中所有问题的最后的“银弹”，所以今天的高级程序设计语言几乎都提供了对多种编程范式的支持，Python也不例外。 类和对象简单的说，类是对象的蓝图和模板，而对象是类的实例。这个解释虽然有点像用概念在解释概念，但是从这句话我们至少可以看出，类是抽象的概念，而对象是具体的东西。在面向对象编程的世界中，一切皆为对象，对象都有属性和行为，每个对象都是独一无二的，而且对象一定属于某个类（型）。当我们把一大堆拥有共同特征的对象的静态特征（属性）和动态特征（行为）都抽取出来后，就可以定义出一个叫做“类”的东西。 定义类在Python中可以使用class关键字定义类，然后在类中通过之前学习过的函数来定义方法，这样就可以将对象的动态特征描述出来，代码如下所示。 123456789101112131415161718class Student(object): # __init__是一个特殊方法用于在创建对象时进行初始化操作 # 通过这个方法我们可以为学生对象绑定name和age两个属性 def __init__(self, name, age): self.name = name self.age = age def study(self, course_name): print(&#x27;%s正在学习%s.&#x27; % (self.name, course_name)) # PEP 8要求标识符的名字用全小写多个单词用下划线连接 # 但是部分程序员和公司更倾向于使用驼峰命名法(驼峰标识) def watch_movie(self): if self.age &lt; 18: print(&#x27;%s只能观看《熊出没》.&#x27; % self.name) else: print(&#x27;%s正在观看岛国爱情大电影.&#x27; % self.name) 说明： 写在类中的函数，我们通常称之为（对象的）方法，这些方法就是对象可以接收的消息。 创建和使用对象当我们定义好一个类之后，可以通过下面的方式来创建对象并给对象发消息。 1234567891011121314def main(): # 创建学生对象并指定姓名和年龄 stu1 = Student(&#x27;骆昊&#x27;, 38) # 给对象发study消息 stu1.study(&#x27;Python程序设计&#x27;) # 给对象发watch_av消息 stu1.watch_movie() stu2 = Student(&#x27;王大锤&#x27;, 15) stu2.study(&#x27;思想品德&#x27;) stu2.watch_movie()if __name__ == &#x27;__main__&#x27;: main() 访问可见性问题对于上面的代码，有C++、Java、C#等编程经验的程序员可能会问，我们给Student对象绑定的name和age属性到底具有怎样的访问权限（也称为可见性）。因为在很多面向对象编程语言中，我们通常会将对象的属性设置为私有的（private）或受保护的（protected），简单的说就是不允许外界访问，而对象的方法通常都是公开的（public），因为公开的方法就是对象能够接受的消息。在Python中，属性和方法的访问权限只有两种，也就是公开的和私有的，如果希望属性是私有的，在给属性命名时可以用两个下划线作为开头，下面的代码可以验证这一点。 1234567891011121314151617181920class Test: def __init__(self, foo): self.__foo = foo def __bar(self): print(self.__foo) print(&#x27;__bar&#x27;)def main(): test = Test(&#x27;hello&#x27;) # AttributeError: &#x27;Test&#x27; object has no attribute &#x27;__bar&#x27; test.__bar() # AttributeError: &#x27;Test&#x27; object has no attribute &#x27;__foo&#x27; print(test.__foo)if __name__ == &quot;__main__&quot;: main() 但是，Python并没有从语法上严格保证私有属性或方法的私密性，它只是给私有的属性和方法换了一个名字来妨碍对它们的访问，事实上如果你知道更换名字的规则仍然可以访问到它们，下面的代码就可以验证这一点。之所以这样设定，可以用这样一句名言加以解释，就是&quot;We are all consenting adults here&quot;。因为绝大多数程序员都认为开放比封闭要好，而且程序员要自己为自己的行为负责。 123456789101112131415161718class Test: def __init__(self, foo): self.__foo = foo def __bar(self): print(self.__foo) print(&#x27;__bar&#x27;)def main(): test = Test(&#x27;hello&#x27;) test._Test__bar() print(test._Test__foo)if __name__ == &quot;__main__&quot;: main() 在实际开发中，我们并不建议将属性设置为私有的，因为这会导致子类无法访问（后面会讲到）。所以大多数Python程序员会遵循一种命名惯例就是让属性名以单下划线开头来表示属性是受保护的，本类之外的代码在访问这样的属性时应该要保持慎重。这种做法并不是语法上的规则，单下划线开头的属性和方法外界仍然是可以访问的，所以更多的时候它是一种暗示或隐喻，关于这一点可以看看我的《Python - 那些年我们踩过的那些坑》文章中的讲解。 面向对象的支柱面向对象有三大支柱：封装、继承和多态。后面两个概念在下一个章节中进行详细的说明，这里我们先说一下什么是封装。我自己对封装的理解是&quot;隐藏一切可以隐藏的实现细节，只向外界暴露（提供）简单的编程接口&quot;。我们在类中定义的方法其实就是把数据和对数据的操作封装起来了，在我们创建了对象之后，只需要给对象发送一个消息（调用方法）就可以执行方法中的代码，也就是说我们只需要知道方法的名字和传入的参数（方法的外部视图），而不需要知道方法内部的实现细节（方法的内部视图）。 练习练习1：定义一个类描述数字时钟。参考答案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445from time import sleepclass Clock(object): &quot;&quot;&quot;数字时钟&quot;&quot;&quot; def __init__(self, hour=0, minute=0, second=0): &quot;&quot;&quot;初始化方法 :param hour: 时 :param minute: 分 :param second: 秒 &quot;&quot;&quot; self._hour = hour self._minute = minute self._second = second def run(self): &quot;&quot;&quot;走字&quot;&quot;&quot; self._second += 1 if self._second == 60: self._second = 0 self._minute += 1 if self._minute == 60: self._minute = 0 self._hour += 1 if self._hour == 24: self._hour = 0 def show(self): &quot;&quot;&quot;显示时间&quot;&quot;&quot; return &#x27;%02d:%02d:%02d&#x27; % \\ (self._hour, self._minute, self._second)def main(): clock = Clock(23, 59, 58) while True: print(clock.show()) sleep(1) clock.run()if __name__ == &#x27;__main__&#x27;: main() 练习2：定义一个类描述平面上的点并提供移动点和计算到另一个点距离的方法。参考答案： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from math import sqrtclass Point(object): def __init__(self, x=0, y=0): &quot;&quot;&quot;初始化方法 :param x: 横坐标 :param y: 纵坐标 &quot;&quot;&quot; self.x = x self.y = y def move_to(self, x, y): &quot;&quot;&quot;移动到指定位置 :param x: 新的横坐标 &quot;param y: 新的纵坐标 &quot;&quot;&quot; self.x = x self.y = y def move_by(self, dx, dy): &quot;&quot;&quot;移动指定的增量 :param dx: 横坐标的增量 &quot;param dy: 纵坐标的增量 &quot;&quot;&quot; self.x += dx self.y += dy def distance_to(self, other): &quot;&quot;&quot;计算与另一个点的距离 :param other: 另一个点 &quot;&quot;&quot; dx = self.x - other.x dy = self.y - other.y return sqrt(dx ** 2 + dy ** 2) def __str__(self): return &#x27;(%s, %s)&#x27; % (str(self.x), str(self.y))def main(): p1 = Point(3, 5) p2 = Point() print(p1) print(p2) p2.move_by(-1, 2) print(p2) print(p1.distance_to(p2))if __name__ == &#x27;__main__&#x27;: main() 说明： 本章中的插图来自于Grady Booch等著作的《面向对象分析与设计》一书，该书是讲解面向对象编程的经典著作，有兴趣的读者可以购买和阅读这本书来了解更多的面向对象的相关知识。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/07.字符串和常用数据结构","date":"2024-12-12T08:38:00.406Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/07.字符串和常用数据结构/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/07.%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%92%8C%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"字符串和常用数据结构使用字符串第二次世界大战促使了现代电子计算机的诞生，最初计算机被应用于导弹弹道的计算，而在计算机诞生后的很多年时间里，计算机处理的信息基本上都是数值型的信息。世界上的第一台电子计算机叫ENIAC（电子数值积分计算机），诞生于美国的宾夕法尼亚大学，每秒钟能够完成约5000次浮点运算。随着时间的推移，虽然数值运算仍然是计算机日常工作中最为重要的事情之一，但是今天的计算机处理得更多的数据可能都是以文本的方式存在的，如果我们希望通过Python程序操作这些文本信息，就必须要先了解字符串类型以及与它相关的知识。 所谓字符串，就是由零个或多个字符组成的有限序列，一般记为。在Python程序中，如果我们把单个或多个字符用单引号或者双引号包围起来，就可以表示一个字符串。 12345678s1 = &#x27;hello, world!&#x27;s2 = &quot;hello, world!&quot;# 以三个双引号或单引号开头的字符串可以折行s3 = &quot;&quot;&quot;hello, world!&quot;&quot;&quot;print(s1, s2, s3, end=&#x27;&#x27;) 可以在字符串中使用\\（反斜杠）来表示转义，也就是说\\后面的字符不再是它原来的意义，例如：\\n不是代表反斜杠和字符n，而是表示换行；而\\t也不是代表反斜杠和字符t，而是表示制表符。所以如果想在字符串中表示&#39;要写成\\&#39;，同理想表示\\要写成\\\\。可以运行下面的代码看看会输出什么。 123s1 = &#x27;\\&#x27;hello, world!\\&#x27;&#x27;s2 = &#x27;\\n\\\\hello, world!\\\\\\n&#x27;print(s1, s2, end=&#x27;&#x27;) 在\\后面还可以跟一个八进制或者十六进制数来表示字符，例如\\141和\\x61都代表小写字母a，前者是八进制的表示法，后者是十六进制的表示法。也可以在\\后面跟Unicode字符编码来表示字符，例如\\u9a86\\u660a代表的是中文“骆昊”。运行下面的代码，看看输出了什么。 123s1 = &#x27;\\141\\142\\143\\x61\\x62\\x63&#x27;s2 = &#x27;\\u9a86\\u660a&#x27;print(s1, s2) 如果不希望字符串中的\\表示转义，我们可以通过在字符串的最前面加上字母r来加以说明，再看看下面的代码又会输出什么。 123s1 = r&#x27;\\&#x27;hello, world!\\&#x27;&#x27;s2 = r&#x27;\\n\\\\hello, world!\\\\\\n&#x27;print(s1, s2, end=&#x27;&#x27;) Python为字符串类型提供了非常丰富的运算符，我们可以使用+运算符来实现字符串的拼接，可以使用*运算符来重复一个字符串的内容，可以使用in和not in来判断一个字符串是否包含另外一个字符串（成员运算），我们也可以用[]和[:]运算符从字符串取出某个字符或某些字符（切片运算），代码如下所示。 1234567891011121314151617s1 = &#x27;hello &#x27; * 3print(s1) # hello hello hello s2 = &#x27;world&#x27;s1 += s2print(s1) # hello hello hello worldprint(&#x27;ll&#x27; in s1) # Trueprint(&#x27;good&#x27; in s1) # Falsestr2 = &#x27;abc123456&#x27;# 从字符串中取出指定位置的字符(下标运算)print(str2[2]) # c# 字符串切片(从指定的开始索引到指定的结束索引)print(str2[2:5]) # c12print(str2[2:]) # c123456print(str2[2::2]) # c246print(str2[::2]) # ac246print(str2[::-1]) # 654321cbaprint(str2[-3:-1]) # 45 在Python中，我们还可以通过一系列的方法来完成对字符串的处理，代码如下所示。 1234567891011121314151617181920212223242526272829303132333435str1 = &#x27;hello, world!&#x27;# 通过内置函数len计算字符串的长度print(len(str1)) # 13# 获得字符串首字母大写的拷贝print(str1.capitalize()) # Hello, world!# 获得字符串每个单词首字母大写的拷贝print(str1.title()) # Hello, World!# 获得字符串变大写后的拷贝print(str1.upper()) # HELLO, WORLD!# 从字符串中查找子串所在位置print(str1.find(&#x27;or&#x27;)) # 8print(str1.find(&#x27;shit&#x27;)) # -1# 与find类似但找不到子串时会引发异常# print(str1.index(&#x27;or&#x27;))# print(str1.index(&#x27;shit&#x27;))# 检查字符串是否以指定的字符串开头print(str1.startswith(&#x27;He&#x27;)) # Falseprint(str1.startswith(&#x27;hel&#x27;)) # True# 检查字符串是否以指定的字符串结尾print(str1.endswith(&#x27;!&#x27;)) # True# 将字符串以指定的宽度居中并在两侧填充指定的字符print(str1.center(50, &#x27;*&#x27;))# 将字符串以指定的宽度靠右放置左侧填充指定的字符print(str1.rjust(50, &#x27; &#x27;))str2 = &#x27;abc123456&#x27;# 检查字符串是否由数字构成print(str2.isdigit()) # False# 检查字符串是否以字母构成print(str2.isalpha()) # False# 检查字符串是否以数字和字母构成print(str2.isalnum()) # Truestr3 = &#x27; jackfrued@126.com &#x27;print(str3)# 获得字符串修剪左右两侧空格之后的拷贝print(str3.strip()) 我们之前讲过，可以用下面的方式来格式化输出字符串。 12a, b = 5, 10print(&#x27;%d * %d = %d&#x27; % (a, b, a * b)) 当然，我们也可以用字符串提供的方法来完成字符串的格式，代码如下所示。 12a, b = 5, 10print(&#x27;&#123;0&#125; * &#123;1&#125; = &#123;2&#125;&#x27;.format(a, b, a * b)) Python 3.6以后，格式化字符串还有更为简洁的书写方式，就是在字符串前加上字母f，我们可以使用下面的语法糖来简化上面的代码。 12a, b = 5, 10print(f&#x27;&#123;a&#125; * &#123;b&#125; = &#123;a * b&#125;&#x27;) 除了字符串，Python还内置了多种类型的数据结构，如果要在程序中保存和操作数据，绝大多数时候可以利用现有的数据结构来实现，最常用的包括列表、元组、集合和字典。 使用列表不知道大家是否注意到，刚才我们讲到的字符串类型（str）和之前我们讲到的数值类型（int和float）有一些区别。数值类型是标量类型，也就是说这种类型的对象没有可以访问的内部结构；而字符串类型是一种结构化的、非标量类型，所以才会有一系列的属性和方法。接下来我们要介绍的列表（list），也是一种结构化的、非标量类型，它是值的有序序列，每个值都可以通过索引进行标识，定义列表可以将列表的元素放在[]中，多个元素用,进行分隔，可以使用for循环对列表元素进行遍历，也可以使用[]或[:]运算符取出列表中的一个或多个元素。 下面的代码演示了如何定义列表、如何遍历列表以及列表的下标运算。 123456789101112131415161718192021222324list1 = [1, 3, 5, 7, 100]print(list1) # [1, 3, 5, 7, 100]# 乘号表示列表元素的重复list2 = [&#x27;hello&#x27;] * 3print(list2) # [&#x27;hello&#x27;, &#x27;hello&#x27;, &#x27;hello&#x27;]# 计算列表长度(元素个数)print(len(list1)) # 5# 下标(索引)运算print(list1[0]) # 1print(list1[4]) # 100# print(list1[5]) # IndexError: list index out of rangeprint(list1[-1]) # 100print(list1[-3]) # 5list1[2] = 300print(list1) # [1, 3, 300, 7, 100]# 通过循环用下标遍历列表元素for index in range(len(list1)): print(list1[index])# 通过for循环遍历列表元素for elem in list1: print(elem)# 通过enumerate函数处理列表之后再遍历可以同时获得元素索引和值for index, elem in enumerate(list1): print(index, elem) 下面的代码演示了如何向列表中添加元素以及如何从列表中移除元素。 12345678910111213141516171819202122list1 = [1, 3, 5, 7, 100]# 添加元素list1.append(200)list1.insert(1, 400)# 合并两个列表# list1.extend([1000, 2000])list1 += [1000, 2000]print(list1) # [1, 400, 3, 5, 7, 100, 200, 1000, 2000]print(len(list1)) # 9# 先通过成员运算判断元素是否在列表中，如果存在就删除该元素if 3 in list1: list1.remove(3)if 1234 in list1: list1.remove(1234)print(list1) # [1, 400, 5, 7, 100, 200, 1000, 2000]# 从指定的位置删除元素list1.pop(0)list1.pop(len(list1) - 1)print(list1) # [400, 5, 7, 100, 200, 1000]# 清空列表元素list1.clear()print(list1) # [] 和字符串一样，列表也可以做切片操作，通过切片操作我们可以实现对列表的复制或者将列表中的一部分取出来创建出新的列表，代码如下所示。 12345678910111213fruits = [&#x27;grape&#x27;, &#x27;apple&#x27;, &#x27;strawberry&#x27;, &#x27;waxberry&#x27;]fruits += [&#x27;pitaya&#x27;, &#x27;pear&#x27;, &#x27;mango&#x27;]# 列表切片fruits2 = fruits[1:4]print(fruits2) # apple strawberry waxberry# 可以通过完整切片操作来复制列表fruits3 = fruits[:]print(fruits3) # [&#x27;grape&#x27;, &#x27;apple&#x27;, &#x27;strawberry&#x27;, &#x27;waxberry&#x27;, &#x27;pitaya&#x27;, &#x27;pear&#x27;, &#x27;mango&#x27;]fruits4 = fruits[-3:-1]print(fruits4) # [&#x27;pitaya&#x27;, &#x27;pear&#x27;]# 可以通过反向切片操作来获得倒转后的列表的拷贝fruits5 = fruits[::-1]print(fruits5) # [&#x27;mango&#x27;, &#x27;pear&#x27;, &#x27;pitaya&#x27;, &#x27;waxberry&#x27;, &#x27;strawberry&#x27;, &#x27;apple&#x27;, &#x27;grape&#x27;] 下面的代码实现了对列表的排序操作。 1234567891011121314list1 = [&#x27;orange&#x27;, &#x27;apple&#x27;, &#x27;zoo&#x27;, &#x27;internationalization&#x27;, &#x27;blueberry&#x27;]list2 = sorted(list1)# sorted函数返回列表排序后的拷贝不会修改传入的列表# 函数的设计就应该像sorted函数一样尽可能不产生副作用list3 = sorted(list1, reverse=True)# 通过key关键字参数指定根据字符串长度进行排序而不是默认的字母表顺序list4 = sorted(list1, key=len)print(list1)print(list2)print(list3)print(list4)# 给列表对象发出排序消息直接在列表对象上进行排序list1.sort(reverse=True)print(list1) 生成式和生成器我们还可以使用列表的生成式语法来创建列表，代码如下所示。 1234567891011121314151617f = [x for x in range(1, 10)]print(f)f = [x + y for x in &#x27;ABCDE&#x27; for y in &#x27;1234567&#x27;]print(f)# 用列表的生成表达式语法创建列表容器# 用这种语法创建列表之后元素已经准备就绪所以需要耗费较多的内存空间f = [x ** 2 for x in range(1, 1000)]print(sys.getsizeof(f)) # 查看对象占用内存的字节数print(f)# 请注意下面的代码创建的不是一个列表而是一个生成器对象# 通过生成器可以获取到数据但它不占用额外的空间存储数据# 每次需要数据的时候就通过内部的运算得到数据(需要花费额外的时间)f = (x ** 2 for x in range(1, 1000))print(sys.getsizeof(f)) # 相比生成式生成器不占用存储数据的空间print(f)for val in f: print(val) 除了上面提到的生成器语法，Python中还有另外一种定义生成器的方式，就是通过yield关键字将一个普通函数改造成生成器函数。下面的代码演示了如何实现一个生成斐波拉切数列的生成器。所谓斐波拉切数列可以通过下面递归的方法来进行定义： 1234567891011121314def fib(n): a, b = 0, 1 for _ in range(n): a, b = b, a + b yield adef main(): for val in fib(20): print(val)if __name__ == &#x27;__main__&#x27;: main() 使用元组Python中的元组与列表类似也是一种容器数据类型，可以用一个变量（对象）来存储多个数据，不同之处在于元组的元素不能修改，在前面的代码中我们已经不止一次使用过元组了。顾名思义，我们把多个元素组合到一起就形成了一个元组，所以它和列表一样可以保存多条数据。下面的代码演示了如何定义和使用元组。 12345678910111213141516171819202122232425# 定义元组t = (&#x27;骆昊&#x27;, 38, True, &#x27;四川成都&#x27;)print(t)# 获取元组中的元素print(t[0])print(t[3])# 遍历元组中的值for member in t: print(member)# 重新给元组赋值# t[0] = &#x27;王大锤&#x27; # TypeError# 变量t重新引用了新的元组原来的元组将被垃圾回收t = (&#x27;王大锤&#x27;, 20, True, &#x27;云南昆明&#x27;)print(t)# 将元组转换成列表person = list(t)print(person)# 列表是可以修改它的元素的person[0] = &#x27;李小龙&#x27;person[1] = 25print(person)# 将列表转换成元组fruits_list = [&#x27;apple&#x27;, &#x27;banana&#x27;, &#x27;orange&#x27;]fruits_tuple = tuple(fruits_list)print(fruits_tuple) 这里有一个非常值得探讨的问题，我们已经有了列表这种数据结构，为什么还需要元组这样的类型呢？ 元组中的元素是无法修改的，事实上我们在项目中尤其是多线程环境（后面会讲到）中可能更喜欢使用的是那些不变对象（一方面因为对象状态不能修改，所以可以避免由此引起的不必要的程序错误，简单的说就是一个不变的对象要比可变的对象更加容易维护；另一方面因为没有任何一个线程能够修改不变对象的内部状态，一个不变对象自动就是线程安全的，这样就可以省掉处理同步化的开销。一个不变对象可以方便的被共享访问）。所以结论就是：如果不需要对元素进行添加、删除、修改的时候，可以考虑使用元组，当然如果一个方法要返回多个值，使用元组也是不错的选择。 元组在创建时间和占用的空间上面都优于列表。我们可以使用sys模块的getsizeof函数来检查存储同样的元素的元组和列表各自占用了多少内存空间，这个很容易做到。我们也可以在ipython中使用魔法指令%timeit来分析创建同样内容的元组和列表所花费的时间，下图是我的macOS系统上测试的结果。 使用集合Python中的集合跟数学上的集合是一致的，不允许有重复元素，而且可以进行交集、并集、差集等运算。 可以按照下面代码所示的方式来创建和使用集合。 1234567891011# 创建集合的字面量语法set1 = &#123;1, 2, 3, 3, 3, 2&#125;print(set1)print(&#x27;Length =&#x27;, len(set1))# 创建集合的构造器语法(面向对象部分会进行详细讲解)set2 = set(range(1, 10))set3 = set((1, 2, 3, 3, 2, 1))print(set2, set3)# 创建集合的推导式语法(推导式也可以用于推导集合)set4 = &#123;num for num in range(1, 100) if num % 3 == 0 or num % 5 == 0&#125;print(set4) 向集合添加元素和从集合删除元素。 123456789set1.add(4)set1.add(5)set2.update([11, 12])set2.discard(5)if 4 in set2: set2.remove(4)print(set1, set2)print(set3.pop())print(set3) 集合的成员、交集、并集、差集等运算。 123456789101112131415161718# 集合的交集、并集、差集、对称差运算print(set1 &amp; set2)# print(set1.intersection(set2))print(set1 | set2)# print(set1.union(set2))print(set1 - set2)# print(set1.difference(set2))print(set1 ^ set2)# print(set1.symmetric_difference(set2))# 判断子集和超集print(set2 &lt;= set1)# print(set2.issubset(set1))print(set3 &lt;= set1)# print(set3.issubset(set1))print(set1 &gt;= set2)# print(set1.issuperset(set2))print(set1 &gt;= set3)# print(set1.issuperset(set3)) 说明： Python中允许通过一些特殊的方法来为某种类型或数据结构自定义运算符（后面的章节中会讲到），上面的代码中我们对集合进行运算的时候可以调用集合对象的方法，也可以直接使用对应的运算符，例如&amp;运算符跟intersection方法的作用就是一样的，但是使用运算符让代码更加直观。 使用字典字典是另一种可变容器模型，Python中的字典跟我们生活中使用的字典是一样一样的，它可以存储任意类型对象，与列表、集合不同的是，字典的每个元素都是由一个键和一个值组成的“键值对”，键和值通过冒号分开。下面的代码演示了如何定义和使用字典。 123456789101112131415161718192021222324252627282930313233# 创建字典的字面量语法scores = &#123;&#x27;骆昊&#x27;: 95, &#x27;白元芳&#x27;: 78, &#x27;狄仁杰&#x27;: 82&#125;print(scores)# 创建字典的构造器语法items1 = dict(one=1, two=2, three=3, four=4)# 通过zip函数将两个序列压成字典items2 = dict(zip([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], &#x27;123&#x27;))# 创建字典的推导式语法items3 = &#123;num: num ** 2 for num in range(1, 10)&#125;print(items1, items2, items3)# 通过键可以获取字典中对应的值print(scores[&#x27;骆昊&#x27;])print(scores[&#x27;狄仁杰&#x27;])# 对字典中所有键值对进行遍历for key in scores: print(f&#x27;&#123;key&#125;: &#123;scores[key]&#125;&#x27;)# 更新字典中的元素scores[&#x27;白元芳&#x27;] = 65scores[&#x27;诸葛王朗&#x27;] = 71scores.update(冷面=67, 方启鹤=85)print(scores)if &#x27;武则天&#x27; in scores: print(scores[&#x27;武则天&#x27;])print(scores.get(&#x27;武则天&#x27;))# get方法也是通过键获取对应的值但是可以设置默认值print(scores.get(&#x27;武则天&#x27;, 60))# 删除字典中的元素print(scores.popitem())print(scores.popitem())print(scores.pop(&#x27;骆昊&#x27;, 100))# 清空字典scores.clear()print(scores) 练习练习1：在屏幕上显示跑马灯文字。参考答案： 1234567891011121314151617import osimport timedef main(): content = &#x27;北京欢迎你为你开天辟地…………&#x27; while True: # 清理屏幕上的输出 os.system(&#x27;cls&#x27;) # os.system(&#x27;clear&#x27;) print(content) # 休眠200毫秒 time.sleep(0.2) content = content[1:] + content[0]if __name__ == &#x27;__main__&#x27;: main() 练习2：设计一个函数产生指定长度的验证码，验证码由大小写字母和数字构成。参考答案： 123456789101112131415161718import randomdef generate_code(code_len=4): &quot;&quot;&quot; 生成指定长度的验证码 :param code_len: 验证码的长度(默认4个字符) :return: 由大小写英文字母和数字构成的随机验证码 &quot;&quot;&quot; all_chars = &#x27;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&#x27; last_pos = len(all_chars) - 1 code = &#x27;&#x27; for _ in range(code_len): index = random.randint(0, last_pos) code += all_chars[index] return code 练习3：设计一个函数返回给定文件名的后缀名。参考答案： 1234567891011121314def get_suffix(filename, has_dot=False): &quot;&quot;&quot; 获取文件名的后缀名 :param filename: 文件名 :param has_dot: 返回的后缀名是否需要带点 :return: 文件的后缀名 &quot;&quot;&quot; pos = filename.rfind(&#x27;.&#x27;) if 0 &lt; pos &lt; len(filename) - 1: index = pos if has_dot else pos + 1 return filename[index:] else: return &#x27;&#x27; 练习4：设计一个函数返回传入的列表中最大和第二大的元素的值。参考答案： 123456789def max2(x): m1, m2 = (x[0], x[1]) if x[0] &gt; x[1] else (x[1], x[0]) for index in range(2, len(x)): if x[index] &gt; m1: m2 = m1 m1 = x[index] elif x[index] &gt; m2: m2 = x[index] return m1, m2 练习5：计算指定的年月日是这一年的第几天。参考答案： 1234567891011121314151617181920212223242526272829303132333435363738def is_leap_year(year): &quot;&quot;&quot; 判断指定的年份是不是闰年 :param year: 年份 :return: 闰年返回True平年返回False &quot;&quot;&quot; return year % 4 == 0 and year % 100 != 0 or year % 400 == 0def which_day(year, month, date): &quot;&quot;&quot; 计算传入的日期是这一年的第几天 :param year: 年 :param month: 月 :param date: 日 :return: 第几天 &quot;&quot;&quot; days_of_month = [ [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] ][is_leap_year(year)] total = 0 for index in range(month - 1): total += days_of_month[index] return total + datedef main(): print(which_day(1980, 11, 28)) print(which_day(1981, 12, 31)) print(which_day(2018, 1, 1)) print(which_day(2016, 3, 1))if __name__ == &#x27;__main__&#x27;: main() 练习6：打印杨辉三角。参考答案： 12345678910111213141516def main(): num = int(input(&#x27;Number of rows: &#x27;)) yh = [[]] * num for row in range(len(yh)): yh[row] = [None] * (row + 1) for col in range(len(yh[row])): if col == 0 or col == row: yh[row][col] = 1 else: yh[row][col] = yh[row - 1][col] + yh[row - 1][col - 1] print(yh[row][col], end=&#x27;\\t&#x27;) print()if __name__ == &#x27;__main__&#x27;: main() 综合案例案例1：双色球选号。12345678910111213141516171819202122232425262728293031323334from random import randrange, randint, sampledef display(balls): &quot;&quot;&quot; 输出列表中的双色球号码 &quot;&quot;&quot; for index, ball in enumerate(balls): if index == len(balls) - 1: print(&#x27;|&#x27;, end=&#x27; &#x27;) print(&#x27;%02d&#x27; % ball, end=&#x27; &#x27;) print()def random_select(): &quot;&quot;&quot; 随机选择一组号码 &quot;&quot;&quot; red_balls = [x for x in range(1, 34)] selected_balls = [] selected_balls = sample(red_balls, 6) selected_balls.sort() selected_balls.append(randint(1, 16)) return selected_ballsdef main(): n = int(input(&#x27;机选几注: &#x27;)) for _ in range(n): display(random_select())if __name__ == &#x27;__main__&#x27;: main() 说明： 上面使用random模块的sample函数来实现从列表中选择不重复的n个元素。 综合案例2：约瑟夫环问题。12345678910111213141516171819202122232425&quot;&quot;&quot;《幸运的基督徒》有15个基督徒和15个非基督徒在海上遇险，为了能让一部分人活下来不得不将其中15个人扔到海里面去，有个人想了个办法就是大家围成一个圈，由某个人开始从1报数，报到9的人就扔到海里面，他后面的人接着从1开始报数，报到9的人继续扔到海里面，直到扔掉15个人。由于上帝的保佑，15个基督徒都幸免于难，问这些人最开始是怎么站的，哪些位置是基督徒哪些位置是非基督徒。&quot;&quot;&quot;def main(): persons = [True] * 30 counter, index, number = 0, 0, 0 while counter &lt; 15: if persons[index]: number += 1 if number == 9: persons[index] = False counter += 1 number = 0 index += 1 index %= 30 for person in persons: print(&#x27;基&#x27; if person else &#x27;非&#x27;, end=&#x27;&#x27;)if __name__ == &#x27;__main__&#x27;: main() 综合案例3：井字棋游戏。123456789101112131415161718192021222324252627282930313233343536373839404142import osdef print_board(board): print(board[&#x27;TL&#x27;] + &#x27;|&#x27; + board[&#x27;TM&#x27;] + &#x27;|&#x27; + board[&#x27;TR&#x27;]) print(&#x27;-+-+-&#x27;) print(board[&#x27;ML&#x27;] + &#x27;|&#x27; + board[&#x27;MM&#x27;] + &#x27;|&#x27; + board[&#x27;MR&#x27;]) print(&#x27;-+-+-&#x27;) print(board[&#x27;BL&#x27;] + &#x27;|&#x27; + board[&#x27;BM&#x27;] + &#x27;|&#x27; + board[&#x27;BR&#x27;])def main(): init_board = &#123; &#x27;TL&#x27;: &#x27; &#x27;, &#x27;TM&#x27;: &#x27; &#x27;, &#x27;TR&#x27;: &#x27; &#x27;, &#x27;ML&#x27;: &#x27; &#x27;, &#x27;MM&#x27;: &#x27; &#x27;, &#x27;MR&#x27;: &#x27; &#x27;, &#x27;BL&#x27;: &#x27; &#x27;, &#x27;BM&#x27;: &#x27; &#x27;, &#x27;BR&#x27;: &#x27; &#x27; &#125; begin = True while begin: curr_board = init_board.copy() begin = False turn = &#x27;x&#x27; counter = 0 os.system(&#x27;clear&#x27;) print_board(curr_board) while counter &lt; 9: move = input(&#x27;轮到%s走棋, 请输入位置: &#x27; % turn) if curr_board[move] == &#x27; &#x27;: counter += 1 curr_board[move] = turn if turn == &#x27;x&#x27;: turn = &#x27;o&#x27; else: turn = &#x27;x&#x27; os.system(&#x27;clear&#x27;) print_board(curr_board) choice = input(&#x27;再玩一局?(yes|no)&#x27;) begin = choice == &#x27;yes&#x27;if __name__ == &#x27;__main__&#x27;: main() 说明： 最后这个案例来自《Python编程快速上手:让繁琐工作自动化》一书（这本书对有编程基础想迅速使用Python将日常工作自动化的人来说还是不错的选择），对代码做了一点点的调整。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/06.函数和模块的使用","date":"2024-12-12T08:38:00.402Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/06.函数和模块的使用/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/06.%E5%87%BD%E6%95%B0%E5%92%8C%E6%A8%A1%E5%9D%97%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"函数和模块的使用在讲解本章节的内容之前，我们先来研究一道数学题，请说出下面的方程有多少组正整数解。 事实上，上面的问题等同于将8个苹果分成四组每组至少一个苹果有多少种方案。想到这一点问题的答案就呼之欲出了。 可以用Python的程序来计算出这个值，代码如下所示。 123456789101112131415161718&quot;&quot;&quot;输入M和N计算C(M,N)Version: 0.1Author: 骆昊&quot;&quot;&quot;m = int(input(&#x27;m = &#x27;))n = int(input(&#x27;n = &#x27;))fm = 1for num in range(1, m + 1): fm *= numfn = 1for num in range(1, n + 1): fn *= numfm_n = 1for num in range(1, m - n + 1): fm_n *= numprint(fm // fn // fm_n) 函数的作用不知道大家是否注意到，在上面的代码中，我们做了3次求阶乘，这样的代码实际上就是重复代码。编程大师Martin Fowler先生曾经说过：“代码有很多种坏味道，重复是最坏的一种！”，要写出高质量的代码首先要解决的就是重复代码的问题。对于上面的代码来说，我们可以将计算阶乘的功能封装到一个称之为“函数”的功能模块中，在需要计算阶乘的地方，我们只需要“调用”这个“函数”就可以了。 定义函数在Python中可以使用def关键字来定义函数，和变量一样每个函数也有一个响亮的名字，而且命名规则跟变量的命名规则是一致的。在函数名后面的圆括号中可以放置传递给函数的参数，这一点和数学上的函数非常相似，程序中函数的参数就相当于是数学上说的函数的自变量，而函数执行完成后我们可以通过return关键字来返回一个值，这相当于数学上说的函数的因变量。 在了解了如何定义函数后，我们可以对上面的代码进行重构，所谓重构就是在不影响代码执行结果的前提下对代码的结构进行调整，重构之后的代码如下所示。 123456789101112131415161718&quot;&quot;&quot;输入M和N计算C(M,N)Version: 0.1Author: 骆昊&quot;&quot;&quot;def fac(num): &quot;&quot;&quot;求阶乘&quot;&quot;&quot; result = 1 for n in range(1, num + 1): result *= n return resultm = int(input(&#x27;m = &#x27;))n = int(input(&#x27;n = &#x27;))# 当需要计算阶乘的时候不用再写循环求阶乘而是直接调用已经定义好的函数print(fac(m) // fac(n) // fac(m - n)) 说明： Python的math模块中其实已经有一个名为factorial函数实现了阶乘运算，事实上求阶乘并不用自己定义函数。下面的例子中，我们讲的函数在Python标准库已经实现过了，我们这里是为了讲解函数的定义和使用才把它们又实现了一遍，实际开发中并不建议做这种低级的重复劳动。 函数的参数函数是绝大多数编程语言中都支持的一个代码的&quot;构建块&quot;，但是Python中的函数与其他语言中的函数还是有很多不太相同的地方，其中一个显著的区别就是Python对函数参数的处理。在Python中，函数的参数可以有默认值，也支持使用可变参数，所以Python并不需要像其他语言一样支持函数的重载，因为我们在定义一个函数的时候可以让它有多种不同的使用方式，下面是两个小例子。 1234567891011121314151617181920212223242526from random import randintdef roll_dice(n=2): &quot;&quot;&quot;摇色子&quot;&quot;&quot; total = 0 for _ in range(n): total += randint(1, 6) return totaldef add(a=0, b=0, c=0): &quot;&quot;&quot;三个数相加&quot;&quot;&quot; return a + b + c# 如果没有指定参数那么使用默认值摇两颗色子print(roll_dice())# 摇三颗色子print(roll_dice(3))print(add())print(add(1))print(add(1, 2))print(add(1, 2, 3))# 传递参数时可以不按照设定的顺序进行传递print(add(c=50, a=100, b=200)) 我们给上面两个函数的参数都设定了默认值，这也就意味着如果在调用函数的时候如果没有传入对应参数的值时将使用该参数的默认值，所以在上面的代码中我们可以用各种不同的方式去调用add函数，这跟其他很多语言中函数重载的效果是一致的。 其实上面的add函数还有更好的实现方案，因为我们可能会对0个或多个参数进行加法运算，而具体有多少个参数是由调用者来决定，我们作为函数的设计者对这一点是一无所知的，因此在不确定参数个数的时候，我们可以使用可变参数，代码如下所示。 1234567891011121314# 在参数名前面的*表示args是一个可变参数def add(*args): total = 0 for val in args: total += val return total# 在调用add函数时可以传入0个或多个参数print(add())print(add(1))print(add(1, 2))print(add(1, 2, 3))print(add(1, 3, 5, 7, 9)) 用模块管理函数对于任何一种编程语言来说，给变量、函数这样的标识符起名字都是一个让人头疼的问题，因为我们会遇到命名冲突这种尴尬的情况。最简单的场景就是在同一个.py文件中定义了两个同名函数，由于Python没有函数重载的概念，那么后面的定义会覆盖之前的定义，也就意味着两个函数同名函数实际上只有一个是存在的。 12345678910def foo(): print(&#x27;hello, world!&#x27;)def foo(): print(&#x27;goodbye, world!&#x27;)# 下面的代码会输出什么呢？foo() 当然上面的这种情况我们很容易就能避免，但是如果项目是由多人协作进行团队开发的时候，团队中可能有多个程序员都定义了名为foo的函数，那么怎么解决这种命名冲突呢？答案其实很简单，Python中每个文件就代表了一个模块（module），我们在不同的模块中可以有同名的函数，在使用函数的时候我们通过import关键字导入指定的模块就可以区分到底要使用的是哪个模块中的foo函数，代码如下所示。 module1.py 12def foo(): print(&#x27;hello, world!&#x27;) module2.py 12def foo(): print(&#x27;goodbye, world!&#x27;) test.py 123456789from module1 import foo# 输出hello, world!foo()from module2 import foo# 输出goodbye, world!foo() 也可以按照如下所示的方式来区分到底要使用哪一个foo函数。 test.py 12345import module1 as m1import module2 as m2m1.foo()m2.foo() 但是如果将代码写成了下面的样子，那么程序中调用的是最后导入的那个foo，因为后导入的foo覆盖了之前导入的foo。 test.py 12345from module1 import foofrom module2 import foo# 输出goodbye, world!foo() test.py 12345from module2 import foofrom module1 import foo# 输出hello, world!foo() 需要说明的是，如果我们导入的模块除了定义函数之外还有可以执行代码，那么Python解释器在导入这个模块时就会执行这些代码，事实上我们可能并不希望如此，因此如果我们在模块中编写了执行代码，最好是将这些执行代码放入如下所示的条件中，这样的话除非直接运行该模块，if条件下的这些代码是不会执行的，因为只有直接执行的模块的名字才是&quot;__main__&quot;。 module3.py 123456789101112131415def foo(): passdef bar(): pass# __name__是Python中一个隐含的变量它代表了模块的名字# 只有被Python解释器直接执行的模块的名字才是__main__if __name__ == &#x27;__main__&#x27;: print(&#x27;call foo()&#x27;) foo() print(&#x27;call bar()&#x27;) bar() test.py 123import module3# 导入module3时 不会执行模块中if条件成立时的代码 因为模块的名字是module3而不是__main__ 练习练习1：实现计算求最大公约数和最小公倍数的函数。参考答案： 1234567891011def gcd(x, y): &quot;&quot;&quot;求最大公约数&quot;&quot;&quot; (x, y) = (y, x) if x &gt; y else (x, y) for factor in range(x, 0, -1): if x % factor == 0 and y % factor == 0: return factordef lcm(x, y): &quot;&quot;&quot;求最小公倍数&quot;&quot;&quot; return x * y // gcd(x, y) 练习2：实现判断一个数是不是回文数的函数。参考答案： 12345678def is_palindrome(num): &quot;&quot;&quot;判断一个数是不是回文数&quot;&quot;&quot; temp = num total = 0 while temp &gt; 0: total = total * 10 + temp % 10 temp //= 10 return total == num 练习3：实现判断一个数是不是素数的函数。参考答案： 123456def is_prime(num): &quot;&quot;&quot;判断一个数是不是素数&quot;&quot;&quot; for factor in range(2, int(num ** 0.5) + 1): if num % factor == 0: return False return True if num != 1 else False 练习4：写一个程序判断输入的正整数是不是回文素数。参考答案： 1234if __name__ == &#x27;__main__&#x27;: num = int(input(&#x27;请输入正整数: &#x27;)) if is_palindrome(num) and is_prime(num): print(&#x27;%d是回文素数&#x27; % num) 注意：通过上面的程序可以看出，当我们将代码中重复出现的和相对独立的功能抽取成函数后，我们可以组合使用这些函数来解决更为复杂的问题，这也是我们为什么要定义和使用函数的一个非常重要的原因。 变量的作用域最后，我们来讨论一下Python中有关变量作用域的问题。 123456789101112131415161718def foo(): b = &#x27;hello&#x27; # Python中可以在函数内部再定义函数 def bar(): c = True print(a) print(b) print(c) bar() # print(c) # NameError: name &#x27;c&#x27; is not definedif __name__ == &#x27;__main__&#x27;: a = 100 # print(b) # NameError: name &#x27;b&#x27; is not defined foo() 上面的代码能够顺利的执行并且打印出100、hello和True，但我们注意到了，在bar函数的内部并没有定义a和b两个变量，那么a和b是从哪里来的。我们在上面代码的if分支中定义了一个变量a，这是一个全局变量（global variable），属于全局作用域，因为它没有定义在任何一个函数中。在上面的foo函数中我们定义了变量b，这是一个定义在函数中的局部变量（local variable），属于局部作用域，在foo函数的外部并不能访问到它；但对于foo函数内部的bar函数来说，变量b属于嵌套作用域，在bar函数中我们是可以访问到它的。bar函数中的变量c属于局部作用域，在bar函数之外是无法访问的。事实上，Python查找一个变量时会按照“局部作用域”、“嵌套作用域”、“全局作用域”和“内置作用域”的顺序进行搜索，前三者我们在上面的代码中已经看到了，所谓的“内置作用域”就是Python内置的那些标识符，我们之前用过的input、print、int等都属于内置作用域。 再看看下面这段代码，我们希望通过函数调用修改全局变量a的值，但实际上下面的代码是做不到的。 123456789def foo(): a = 200 print(a) # 200if __name__ == &#x27;__main__&#x27;: a = 100 foo() print(a) # 100 在调用foo函数后，我们发现a的值仍然是100，这是因为当我们在函数foo中写a = 200的时候，是重新定义了一个名字为a的局部变量，它跟全局作用域的a并不是同一个变量，因为局部作用域中有了自己的变量a，因此foo函数不再搜索全局作用域中的a。如果我们希望在foo函数中修改全局作用域中的a，代码如下所示。 12345678910def foo(): global a a = 200 print(a) # 200if __name__ == &#x27;__main__&#x27;: a = 100 foo() print(a) # 200 我们可以使用global关键字来指示foo函数中的变量a来自于全局作用域，如果全局作用域中没有a，那么下面一行的代码就会定义变量a并将其置于全局作用域。同理，如果我们希望函数内部的函数能够修改嵌套作用域中的变量，可以使用nonlocal关键字来指示变量来自于嵌套作用域，请大家自行试验。 在实际开发中，我们应该尽量减少对全局变量的使用，因为全局变量的作用域和影响过于广泛，可能会发生意料之外的修改和使用，除此之外全局变量比局部变量拥有更长的生命周期，可能导致对象占用的内存长时间无法被垃圾回收。事实上，减少对全局变量的使用，也是降低代码之间耦合度的一个重要举措，同时也是对迪米特法则的践行。减少全局变量的使用就意味着我们应该尽量让变量的作用域在函数的内部，但是如果我们希望将一个局部变量的生命周期延长，使其在定义它的函数调用结束后依然可以使用它的值，这时候就需要使用闭包，这个我们在后续的内容中进行讲解。 说明： 很多人经常会将“闭包”和“匿名函数”混为一谈，但实际上它们并不是一回事，如果想了解这个概念，可以看看维基百科的解释或者知乎上对这个概念的讨论。 说了那么多，其实结论很简单，从现在开始我们可以将Python代码按照下面的格式进行书写，这一点点的改进其实就是在我们理解了函数和作用域的基础上跨出的巨大的一步。 1234567def main(): # Todo: Add your code here passif __name__ == &#x27;__main__&#x27;: main()","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/05.构造程序逻辑","date":"2024-12-12T08:38:00.400Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/05.构造程序逻辑/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/05.%E6%9E%84%E9%80%A0%E7%A8%8B%E5%BA%8F%E9%80%BB%E8%BE%91/","excerpt":"","text":"构造程序逻辑学完前面的几个章节后，我觉得有必要在这里带大家做一些练习来巩固之前所学的知识，虽然迄今为止我们学习的内容只是Python的冰山一角，但是这些内容已经足够我们来构建程序中的逻辑。对于编程语言的初学者来说，在学习了Python的核心语言元素（变量、类型、运算符、表达式、分支结构、循环结构等）之后，必须做的一件事情就是尝试用所学知识去解决现实中的问题，换句话说就是锻炼自己把用人类自然语言描述的算法（解决问题的方法和步骤）翻译成Python代码的能力，而这件事情必须通过大量的练习才能达成。 我们在本章为大家整理了一些经典的案例和习题，希望通过这些例子，一方面帮助大家巩固之前所学的Python知识，另一方面帮助大家了解如何建立程序中的逻辑以及如何运用一些简单的算法解决现实中的问题。 经典的例子 寻找水仙花数。 说明：水仙花数也被称为超完全数字不变数、自恋数、自幂数、阿姆斯特朗数，它是一个3位数，该数字每个位上数字的立方之和正好等于它本身，例如：$1^3 + 5^3+ 3^3&#x3D;153$。 12345678910111213&quot;&quot;&quot;找出所有水仙花数Version: 0.1Author: 骆昊&quot;&quot;&quot;for num in range(100, 1000): low = num % 10 mid = num // 10 % 10 high = num // 100 if num == low ** 3 + mid ** 3 + high ** 3: print(num) 在上面的代码中，我们通过整除和求模运算分别找出了一个三位数的个位、十位和百位，这种小技巧在实际开发中还是常用的。用类似的方法，我们还可以实现将一个正整数反转，例如：将12345变成54321，代码如下所示。 12345678910111213&quot;&quot;&quot;正整数的反转Version: 0.1Author: 骆昊&quot;&quot;&quot;num = int(input(&#x27;num = &#x27;))reversed_num = 0while num &gt; 0: reversed_num = reversed_num * 10 + num % 10 num //= 10print(reversed_num) 百钱百鸡问题。 说明：百钱百鸡是我国古代数学家张丘建在《算经》一书中提出的数学问题：鸡翁一值钱五，鸡母一值钱三，鸡雏三值钱一。百钱买百鸡，问鸡翁、鸡母、鸡雏各几何？翻译成现代文是：公鸡5元一只，母鸡3元一只，小鸡1元三只，用100块钱买一百只鸡，问公鸡、母鸡、小鸡各有多少只？ 123456789101112&quot;&quot;&quot;《百钱百鸡》问题Version: 0.1Author: 骆昊&quot;&quot;&quot;for x in range(0, 20): for y in range(0, 33): z = 100 - x - y if 5 * x + 3 * y + z / 3 == 100: print(&#x27;公鸡: %d只, 母鸡: %d只, 小鸡: %d只&#x27; % (x, y, z)) 上面使用的方法叫做穷举法，也称为暴力搜索法，这种方法通过一项一项的列举备选解决方案中所有可能的候选项并检查每个候选项是否符合问题的描述，最终得到问题的解。这种方法看起来比较笨拙，但对于运算能力非常强大的计算机来说，通常都是一个可行的甚至是不错的选择，而且问题的解如果存在，这种方法一定能够找到它。 CRAPS赌博游戏。 说明：CRAPS又称花旗骰，是美国拉斯维加斯非常受欢迎的一种的桌上赌博游戏。该游戏使用两粒骰子，玩家通过摇两粒骰子获得点数进行游戏。简单的规则是：玩家第一次摇骰子如果摇出了7点或11点，玩家胜；玩家第一次如果摇出2点、3点或12点，庄家胜；其他点数玩家继续摇骰子，如果玩家摇出了7点，庄家胜；如果玩家摇出了第一次摇的点数，玩家胜；其他点数，玩家继续要骰子，直到分出胜负。 1234567891011121314151617181920212223242526272829303132333435363738394041&quot;&quot;&quot;Craps赌博游戏我们设定玩家开始游戏时有1000元的赌注游戏结束的条件是玩家输光所有的赌注Version: 0.1Author: 骆昊&quot;&quot;&quot;from random import randintmoney = 1000while money &gt; 0: print(&#x27;你的总资产为:&#x27;, money) needs_go_on = False while True: debt = int(input(&#x27;请下注: &#x27;)) if 0 &lt; debt &lt;= money: break first = randint(1, 6) + randint(1, 6) print(&#x27;玩家摇出了%d点&#x27; % first) if first == 7 or first == 11: print(&#x27;玩家胜!&#x27;) money += debt elif first == 2 or first == 3 or first == 12: print(&#x27;庄家胜!&#x27;) money -= debt else: needs_go_on = True while needs_go_on: needs_go_on = False current = randint(1, 6) + randint(1, 6) print(&#x27;玩家摇出了%d点&#x27; % current) if current == 7: print(&#x27;庄家胜&#x27;) money -= debt elif current == first: print(&#x27;玩家胜&#x27;) money += debt else: needs_go_on = Trueprint(&#x27;你破产了, 游戏结束!&#x27;) ###有用的练习 生成斐波那契数列的前20个数。 说明：斐波那契数列（Fibonacci sequence），又称黄金分割数列，是意大利数学家莱昂纳多·斐波那契（Leonardoda Fibonacci）在《计算之书》中提出一个在理想假设条件下兔子成长率的问题而引入的数列，所以这个数列也被戏称为&quot;兔子数列&quot;。斐波那契数列的特点是数列的前两个数都是1，从第三个数开始，每个数都是它前面两个数的和，形如：1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, …。斐波那契数列在现代物理、准晶体结构、化学等领域都有直接的应用。 找出10000以内的完美数。 说明：完美数又称为完全数或完备数，它的所有的真因子（即除了自身以外的因子）的和（即因子函数）恰好等于它本身。例如：6（$6&#x3D;1+2+3$）和28（$28&#x3D;1+2+4+7+14$）就是完美数。完美数有很多神奇的特性，有兴趣的可以自行了解。 输出100以内所有的素数。 说明：素数指的是只能被1和自身整除的正整数（不包括1）。 上面练习的参考答案在本章对应的代码目录中，如果需要帮助请读者自行查看参考答案。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/04.循环结构","date":"2024-12-12T08:38:00.397Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/04.循环结构/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/04.%E5%BE%AA%E7%8E%AF%E7%BB%93%E6%9E%84/","excerpt":"","text":"循环结构应用场景我们在写程序的时候，一定会遇到需要重复执行某条或某些指令的场景。例如用程序控制机器人踢足球，如果机器人持球而且还没有进入射门范围，那么我们就要一直发出让机器人向球门方向移动的指令。在这个场景中，让机器人向球门方向移动就是一个需要重复的动作，当然这里还会用到上一课讲的分支结构来判断机器人是否持球以及是否进入射门范围。再举一个简单的例子，如果要实现每隔1秒中在屏幕上打印一次“hello, world”并持续打印一个小时，我们肯定不能够直接把print(&#39;hello, world&#39;)这句代码写3600遍，这里同样需要循环结构。 循环结构就是程序中控制某条或某些指令重复执行的结构。在Python中构造循环结构有两种做法，一种是for-in循环，一种是while循环。 for-in循环如果明确的知道循环执行的次数或者要对一个容器进行迭代（后面会讲到），那么我们推荐使用for-in循环，例如下面代码中计算1~100求和的结果（$\\displaystyle \\sum \\limits_{n&#x3D;1}^{100}n$）。 1234567891011&quot;&quot;&quot;用for循环实现1~100求和Version: 0.1Author: 骆昊&quot;&quot;&quot;sum = 0for x in range(101): sum += xprint(sum) 需要说明的是上面代码中的range(1, 101)可以用来构造一个从1到100的范围，当我们把这样一个范围放到for-in循环中，就可以通过前面的循环变量x依次取出从1到100的整数。当然，range的用法非常灵活，下面给出了一个例子： range(101)：可以用来产生0到100范围的整数，需要注意的是取不到101。 range(1, 101)：可以用来产生1到100范围的整数，相当于前面是闭区间后面是开区间。 range(1, 101, 2)：可以用来产生1到100的奇数，其中2是步长，即每次数值递增的值。 range(100, 0, -2)：可以用来产生100到1的偶数，其中-2是步长，即每次数字递减的值。 知道了这一点，我们可以用下面的代码来实现1~100之间的偶数求和。 1234567891011&quot;&quot;&quot;用for循环实现1~100之间的偶数求和Version: 0.1Author: 骆昊&quot;&quot;&quot;sum = 0for x in range(2, 101, 2): sum += xprint(sum) 当然，也可以通过在循环中使用分支结构的方式来实现相同的功能，代码如下所示。 123456789101112&quot;&quot;&quot;用for循环实现1~100之间的偶数求和Version: 0.1Author: 骆昊&quot;&quot;&quot;sum = 0for x in range(1, 101): if x % 2 == 0: sum += xprint(sum) 说明：相较于上面直接跳过奇数的做法，下面这种做法很明显并不是很好的选择。 while循环如果要构造不知道具体循环次数的循环结构，我们推荐使用while循环。while循环通过一个能够产生或转换出bool值的表达式来控制循环，表达式的值为True则继续循环；表达式的值为False则结束循环。 下面我们通过一个“猜数字”的小游戏来看看如何使用while循环。猜数字游戏的规则是：计算机出一个1到100之间的随机数，玩家输入自己猜的数字，计算机给出对应的提示信息（大一点、小一点或猜对了），如果玩家猜中了数字，计算机提示用户一共猜了多少次，游戏结束，否则游戏继续。 1234567891011121314151617181920212223&quot;&quot;&quot;猜数字游戏Version: 0.1Author: 骆昊&quot;&quot;&quot;import randomanswer = random.randint(1, 100)counter = 0while True: counter += 1 number = int(input(&#x27;请输入: &#x27;)) if number &lt; answer: print(&#x27;大一点&#x27;) elif number &gt; answer: print(&#x27;小一点&#x27;) else: print(&#x27;恭喜你猜对了!&#x27;) breakprint(&#x27;你总共猜了%d次&#x27; % counter)if counter &gt; 7: print(&#x27;你的智商余额明显不足&#x27;) 上面的代码中使用了break关键字来提前终止循环，需要注意的是break只能终止它所在的那个循环，这一点在使用嵌套的循环结构（下面会讲到）需要引起注意。除了break之外，还有另一个关键字是continue，它可以用来放弃本次循环后续的代码直接让循环进入下一轮。 和分支结构一样，循环结构也是可以嵌套的，也就是说在循环中还可以构造循环结构。下面的例子演示了如何通过嵌套的循环来输出一个九九乘法表。 1234567891011&quot;&quot;&quot;输出乘法口诀表(九九表)Version: 0.1Author: 骆昊&quot;&quot;&quot;for i in range(1, 10): for j in range(1, i + 1): print(&#x27;%d*%d=%d&#x27; % (i, j, i * j), end=&#x27;\\t&#x27;) print() 练习练习1：输入一个正整数判断是不是素数。 提示：素数指的是只能被1和自身整除的大于1的整数。 参考答案： 1234567891011121314151617181920&quot;&quot;&quot;输入一个正整数判断它是不是素数Version: 0.1Author: 骆昊Date: 2018-03-01&quot;&quot;&quot;from math import sqrtnum = int(input(&#x27;请输入一个正整数: &#x27;))end = int(sqrt(num))is_prime = Truefor x in range(2, end + 1): if num % x == 0: is_prime = False breakif is_prime and num != 1: print(&#x27;%d是素数&#x27; % num)else: print(&#x27;%d不是素数&#x27; % num) 练习2：输入两个正整数，计算它们的最大公约数和最小公倍数。 提示：两个数的最大公约数是两个数的公共因子中最大的那个数；两个数的最小公倍数则是能够同时被两个数整除的最小的那个数。 参考答案： 1234567891011121314151617181920&quot;&quot;&quot;输入两个正整数计算它们的最大公约数和最小公倍数Version: 0.1Author: 骆昊Date: 2018-03-01&quot;&quot;&quot;x = int(input(&#x27;x = &#x27;))y = int(input(&#x27;y = &#x27;))# 如果x大于y就交换x和y的值if x &gt; y: # 通过下面的操作将y的值赋给x, 将x的值赋给y x, y = y, x# 从两个数中较小的数开始做递减的循环for factor in range(x, 0, -1): if x % factor == 0 and y % factor == 0: print(&#x27;%d和%d的最大公约数是%d&#x27; % (x, y, factor)) print(&#x27;%d和%d的最小公倍数是%d&#x27; % (x, y, x * y // factor)) break 练习3：打印如下所示的三角形图案。12345*************** 12345 * ** *** ********* 12345 * *** ***** **************** 参考答案： 12345678910111213141516171819202122232425262728&quot;&quot;&quot;打印三角形图案Version: 0.1Author: 骆昊&quot;&quot;&quot;row = int(input(&#x27;请输入行数: &#x27;))for i in range(row): for _ in range(i + 1): print(&#x27;*&#x27;, end=&#x27;&#x27;) print()for i in range(row): for j in range(row): if j &lt; row - i - 1: print(&#x27; &#x27;, end=&#x27;&#x27;) else: print(&#x27;*&#x27;, end=&#x27;&#x27;) print()for i in range(row): for _ in range(row - i - 1): print(&#x27; &#x27;, end=&#x27;&#x27;) for _ in range(2 * i + 1): print(&#x27;*&#x27;, end=&#x27;&#x27;) print()","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/03.分支结构","date":"2024-12-12T08:38:00.395Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/03.分支结构/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/03.%E5%88%86%E6%94%AF%E7%BB%93%E6%9E%84/","excerpt":"","text":"分支结构应用场景迄今为止，我们写的Python代码都是一条一条语句顺序执行，这种代码结构通常称之为顺序结构。然而仅有顺序结构并不能解决所有的问题，比如我们设计一个游戏，游戏第一关的通关条件是玩家获得1000分，那么在完成本局游戏后，我们要根据玩家得到分数来决定究竟是进入第二关，还是告诉玩家“Game Over”，这里就会产生两个分支，而且这两个分支只有一个会被执行。类似的场景还有很多，我们将这种结构称之为“分支结构”或“选择结构”。给大家一分钟的时间，你应该可以想到至少5个以上这样的例子，赶紧试一试。 if语句的使用在Python中，要构造分支结构可以使用if、elif和else关键字。所谓关键字就是有特殊含义的单词，像if和else就是专门用于构造分支结构的关键字，很显然你不能够使用它作为变量名（事实上，用作其他的标识符也是不可以）。下面的例子中演示了如何构造一个分支结构。 12345678910111213&quot;&quot;&quot;用户身份验证Version: 0.1Author: 骆昊&quot;&quot;&quot;username = input(&#x27;请输入用户名: &#x27;)password = input(&#x27;请输入口令: &#x27;)# 用户名是admin且密码是123456则身份验证成功否则身份验证失败if username == &#x27;admin&#x27; and password == &#x27;123456&#x27;: print(&#x27;身份验证成功!&#x27;)else: print(&#x27;身份验证失败!&#x27;) 需要说明的是和C&#x2F;C++、Java等语言不同，Python中没有用花括号来构造代码块而是使用了缩进的方式来表示代码的层次结构，如果if条件成立的情况下需要执行多条语句，只要保持多条语句具有相同的缩进就可以了。换句话说连续的代码如果又保持了相同的缩进那么它们属于同一个代码块，相当于是一个执行的整体。缩进可以使用任意数量的空格，但通常使用4个空格，建议大家不要使用制表键或者设置你的代码编辑工具自动将制表键变成4个空格。 当然如果要构造出更多的分支，可以使用if...elif...else...结构或者嵌套的if...else...结构，下面的代码演示了如何利用多分支结构实现分段函数求值。 12345678910111213141516171819&quot;&quot;&quot;分段函数求值 3x - 5 (x &gt; 1)f(x) = x + 2 (-1 &lt;= x &lt;= 1) 5x + 3 (x &lt; -1)Version: 0.1Author: 骆昊&quot;&quot;&quot;x = float(input(&#x27;x = &#x27;))if x &gt; 1: y = 3 * x - 5elif x &gt;= -1: y = x + 2else: y = 5 * x + 3print(&#x27;f(%.2f) = %.2f&#x27; % (x, y)) 当然根据实际开发的需要，分支结构是可以嵌套的，例如判断是否通关以后还要根据你获得的宝物或者道具的数量对你的表现给出等级（比如点亮两颗或三颗星星），那么我们就需要在if的内部构造出一个新的分支结构，同理elif和else中也可以再构造新的分支，我们称之为嵌套的分支结构，也就是说上面的代码也可以写成下面的样子。 12345678910111213141516171819&quot;&quot;&quot;分段函数求值 3x - 5 (x &gt; 1)f(x) = x + 2 (-1 &lt;= x &lt;= 1) 5x + 3 (x &lt; -1)Version: 0.1Author: 骆昊&quot;&quot;&quot;x = float(input(&#x27;x = &#x27;))if x &gt; 1: y = 3 * x - 5else: if x &gt;= -1: y = x + 2 else: y = 5 * x + 3print(&#x27;f(%.2f) = %.2f&#x27; % (x, y)) 说明： 大家可以自己感受一下这两种写法到底是哪一种更好。在之前我们提到的Python之禅中有这么一句话“Flat is better than nested.”，之所以提倡代码“扁平化”是因为嵌套结构的嵌套层次多了之后会严重的影响代码的可读性，所以能使用扁平化的结构时就不要使用嵌套。 练习练习1：英制单位英寸与公制单位厘米互换。参考答案： 1234567891011121314&quot;&quot;&quot;英制单位英寸和公制单位厘米互换Version: 0.1Author: 骆昊&quot;&quot;&quot;value = float(input(&#x27;请输入长度: &#x27;))unit = input(&#x27;请输入单位: &#x27;)if unit == &#x27;in&#x27; or unit == &#x27;英寸&#x27;: print(&#x27;%f英寸 = %f厘米&#x27; % (value, value * 2.54))elif unit == &#x27;cm&#x27; or unit == &#x27;厘米&#x27;: print(&#x27;%f厘米 = %f英寸&#x27; % (value, value / 2.54))else: print(&#x27;请输入有效的单位&#x27;) 练习2：百分制成绩转换为等级制成绩。 要求：如果输入的成绩在90分以上（含90分）输出A；80分-90分（不含90分）输出B；70分-80分（不含80分）输出C；60分-70分（不含70分）输出D；60分以下输出E。 参考答案： 123456789101112131415161718&quot;&quot;&quot;百分制成绩转换为等级制成绩Version: 0.1Author: 骆昊&quot;&quot;&quot;score = float(input(&#x27;请输入成绩: &#x27;))if score &gt;= 90: grade = &#x27;A&#x27;elif score &gt;= 80: grade = &#x27;B&#x27;elif score &gt;= 70: grade = &#x27;C&#x27;elif score &gt;= 60: grade = &#x27;D&#x27;else: grade = &#x27;E&#x27;print(&#x27;对应的等级是:&#x27;, grade) 练习3：输入三条边长，如果能构成三角形就计算周长和面积。参考答案： 12345678910111213141516&quot;&quot;&quot;判断输入的边长能否构成三角形，如果能则计算出三角形的周长和面积Version: 0.1Author: 骆昊&quot;&quot;&quot;a = float(input(&#x27;a = &#x27;))b = float(input(&#x27;b = &#x27;))c = float(input(&#x27;c = &#x27;))if a + b &gt; c and a + c &gt; b and b + c &gt; a: print(&#x27;周长: %f&#x27; % (a + b + c)) p = (a + b + c) / 2 area = (p * (p - a) * (p - b) * (p - c)) ** 0.5 print(&#x27;面积: %f&#x27; % (area))else: print(&#x27;不能构成三角形&#x27;) 说明： 上面使用的通过边长计算三角形面积的公式叫做海伦公式。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/Day01-15/02.语言元素","date":"2024-12-12T08:38:00.392Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/Day01-15/02.语言元素/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/Day01-15/02.%E8%AF%AD%E8%A8%80%E5%85%83%E7%B4%A0/","excerpt":"","text":"语言元素指令和程序计算机的硬件系统通常由五大部件构成，包括：运算器、控制器、存储器、输入设备和输出设备。其中，运算器和控制器放在一起就是我们通常所说的中央处理器，它的功能是执行各种运算和控制指令以及处理计算机软件中的数据。我们通常所说的程序实际上就是指令的集合，我们程序就是将一系列的指令按照某种方式组织到一起，然后通过这些指令去控制计算机做我们想让它做的事情。今天我们大多数时候使用的计算机，虽然它们的元器件做工越来越精密，处理能力越来越强大，但究其本质来说仍然属于“冯·诺依曼结构”的计算机。“冯·诺依曼结构”有两个关键点，一是指出要将存储设备与中央处理器分开，二是提出了将数据以二进制方式编码。二进制是一种“逢二进一”的计数法，跟我们人类使用的“逢十进一”的计数法没有实质性的区别，人类因为有十根手指所以使用了十进制（因为在数数时十根手指用完之后就只能进位了，当然凡事都有例外，玛雅人可能是因为长年光着脚的原因把脚趾头也算上了，于是他们使用了二十进制的计数法，在这种计数法的指导下玛雅人的历法就与我们平常使用的历法不一样，而按照玛雅人的历法，2012年是上一个所谓的“太阳纪”的最后一年，而2013年则是新的“太阳纪”的开始，后来这件事情被以讹传讹的方式误传为”2012年是玛雅人预言的世界末日“这种荒诞的说法，今天我们可以大胆的猜测，玛雅文明之所以发展缓慢估计也与使用了二十进制有关）。对于计算机来说，二进制在物理器件上来说是最容易实现的（高电压表示1，低电压表示0），于是在“冯·诺依曼结构”的计算机都使用了二进制。虽然我们并不需要每个程序员都能够使用二进制的思维方式来工作，但是了解二进制以及它与我们生活中的十进制之间的转换关系，以及二进制与八进制和十六进制的转换关系还是有必要的。如果你对这一点不熟悉，可以自行使用维基百科或者百度百科科普一下。 说明：近期关于量子计算机的研究已经被推倒了风口浪尖，量子计算机基于量子力学进行运算，使用量子瞬移的方式来传递信息。2018年6月，Intel宣布开发出新款量子芯片并通过了在接近绝对零度环境下的测试；2019年，IBM和Google都推出了自己的量子计算机。 变量和类型在程序设计中，变量是一种存储数据的载体。计算机中的变量是实际存在的数据或者说是存储器中存储数据的一块内存空间，变量的值可以被读取和修改，这是所有计算和控制的基础。计算机能处理的数据有很多种类型，除了数值之外还可以处理文本、图形、音频、视频等各种各样的数据，那么不同的数据就需要定义不同的存储类型。Python中的数据类型很多，而且也允许我们自定义新的数据类型（这一点在后面会讲到），我们先介绍几种常用的数据类型。 整型：Python中可以处理任意大小的整数（Python 2.x中有int和long两种类型的整数，但这种区分对Python来说意义不大，因此在Python 3.x中整数只有int这一种了），而且支持二进制（如0b100，换算成十进制是4）、八进制（如0o100，换算成十进制是64）、十进制（100）和十六进制（0x100，换算成十进制是256）的表示法。 浮点型：浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，浮点数除了数学写法（如123.456）之外还支持科学计数法（如1.23456e2）。 字符串型：字符串是以单引号或双引号括起来的任意文本，比如&#39;hello&#39;和&quot;hello&quot;,字符串还有原始字符串表示法、字节字符串表示法、Unicode字符串表示法，而且可以书写成多行的形式（用三个单引号或三个双引号开头，三个单引号或三个双引号结尾）。 布尔型：布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来（例如3 &lt; 5会产生布尔值True，而2 == 1会产生布尔值False）。 复数型：形如3+5j，跟数学上的复数表示一样，唯一不同的是虚部的i换成了j。实际上，这个类型并不常用，大家了解一下就可以了。 变量命名对于每个变量我们需要给它取一个名字，就如同我们每个人都有属于自己的响亮的名字一样。在Python中，变量命名需要遵循以下这些必须遵守硬性规则和强烈建议遵守的非硬性规则。 硬性规则： 变量名由字母（广义的Unicode字符，不包括特殊字符）、数字和下划线构成，数字不能开头。 大小写敏感（大写的a和小写的A是两个不同的变量）。 不要跟关键字（有特殊含义的单词，后面会讲到）和系统保留字（如函数、模块等的名字）冲突。 PEP 8要求： 用小写字母拼写，多个单词用下划线连接。 受保护的实例属性用单个下划线开头（后面会讲到）。 私有的实例属性用两个下划线开头（后面会讲到）。 当然，作为一个专业的程序员，给变量（事实上应该是所有的标识符）命名时做到见名知意也是非常重要的。 变量的使用下面通过几个例子来说明变量的类型和变量使用。 123456789101112&quot;&quot;&quot;使用变量保存数据并进行加减乘除运算Version: 0.1Author: 骆昊&quot;&quot;&quot;a = 321b = 12print(a + b) # 333print(a - b) # 309print(a * b) # 3852print(a / b) # 26.75 在Python中可以使用type函数对变量的类型进行检查。程序设计中函数的概念跟数学上函数的概念是一致的，数学上的函数相信大家并不陌生，它包括了函数名、自变量和因变量。如果暂时不理解这个概念也不要紧，我们会在后续的章节中专门讲解函数的定义和使用。 12345678910111213141516&quot;&quot;&quot;使用type()检查变量的类型Version: 0.1Author: 骆昊&quot;&quot;&quot;a = 100b = 12.345c = 1 + 5jd = &#x27;hello, world&#x27;e = Trueprint(type(a)) # &lt;class &#x27;int&#x27;&gt;print(type(b)) # &lt;class &#x27;float&#x27;&gt;print(type(c)) # &lt;class &#x27;complex&#x27;&gt;print(type(d)) # &lt;class &#x27;str&#x27;&gt;print(type(e)) # &lt;class &#x27;bool&#x27;&gt; 可以使用Python中内置的函数对变量类型进行转换。 int()：将一个数值或字符串转换成整数，可以指定进制。 float()：将一个字符串转换成浮点数。 str()：将指定的对象转换成字符串形式，可以指定编码。 chr()：将整数转换成该编码对应的字符串（一个字符）。 ord()：将字符串（一个字符）转换成对应的编码（整数）。 下面的代码通过键盘输入两个整数来实现对两个整数的算术运算。 1234567891011121314151617&quot;&quot;&quot;使用input()函数获取键盘输入(字符串)使用int()函数将输入的字符串转换成整数使用print()函数输出带占位符的字符串Version: 0.1Author: 骆昊&quot;&quot;&quot;a = int(input(&#x27;a = &#x27;))b = int(input(&#x27;b = &#x27;))print(&#x27;%d + %d = %d&#x27; % (a, b, a + b))print(&#x27;%d - %d = %d&#x27; % (a, b, a - b))print(&#x27;%d * %d = %d&#x27; % (a, b, a * b))print(&#x27;%d / %d = %f&#x27; % (a, b, a / b))print(&#x27;%d // %d = %d&#x27; % (a, b, a // b))print(&#x27;%d %% %d = %d&#x27; % (a, b, a % b))print(&#x27;%d ** %d = %d&#x27; % (a, b, a ** b)) 说明：上面的print函数中输出的字符串使用了占位符语法，其中%d是整数的占位符，%f是小数的占位符，%%表示百分号（因为百分号代表了占位符，所以带占位符的字符串中要表示百分号必须写成%%），字符串之后的%后面跟的变量值会替换掉占位符然后输出到终端中，运行上面的程序，看看程序执行结果就明白啦。 运算符Python支持多种运算符，下表大致按照优先级从高到低的顺序列出了所有的运算符，运算符的优先级指的是多个运算符同时出现时，先做什么运算然后再做什么运算。除了我们之前已经用过的赋值运算符和算术运算符，我们稍后会陆续讲到其他运算符的使用。 运算符 描述 [] [:] 下标，切片 ** 指数 ~ + - 按位取反, 正负号 * / % // 乘，除，模，整除 + - 加，减 &gt;&gt; &lt;&lt; 右移，左移 &amp; 按位与 ^ | 按位异或，按位或 &lt;= &lt; &gt; &gt;= 小于等于，小于，大于，大于等于 == != 等于，不等于 is is not 身份运算符 in not in 成员运算符 not or and 逻辑运算符 = += -= *= /= %= //= **= &amp;= &#96; &#x3D; ^&#x3D; &gt;&gt;&#x3D; &lt;&lt;&#x3D;&#96; 说明： 在实际开发中，如果搞不清楚运算符的优先级，可以使用括号来确保运算的执行顺序。 赋值运算符赋值运算符应该是最为常见的运算符，它的作用是将右边的值赋给左边的变量。下面的例子演示了赋值运算符和复合赋值运算符的使用。 1234567891011&quot;&quot;&quot;赋值运算符和复合赋值运算符Version: 0.1Author: 骆昊&quot;&quot;&quot;a = 10b = 3a += b # 相当于：a = a + ba *= a + 2 # 相当于：a = a * (a + 2)print(a) # 算一下这里会输出什么 比较运算符和逻辑运算符比较运算符有的地方也称为关系运算符，包括==、!=、&lt;、&gt;、&lt;=、&gt;=，我相信没有什么好解释的，大家一看就能懂，唯一需要提醒的是比较相等用的是==，请注意这个地方是两个等号，因为=是赋值运算符，我们在上面刚刚讲到过，==才是比较相等的比较运算符。比较运算符会产生布尔值，要么是True要么是False。 逻辑运算符有三个，分别是and、or和not。and字面意思是“而且”，所以and运算符会连接两个布尔值，如果两个布尔值都是True，那么运算的结果就是True；左右两边的布尔值有一个是False，最终的运算结果就是False。相信大家已经想到了，如果and左边的布尔值是False，不管右边的布尔值是什么，最终的结果都是False，所以在做运算的时候右边的值会被跳过（短路处理），这也就意味着在and运算符左边为False的情况下，右边的表达式根本不会执行。or字面意思是“或者”，所以or运算符也会连接两个布尔值，如果两个布尔值有任意一个是True，那么最终的结果就是True。当然，or运算符也是有短路功能的，在它左边的布尔值为True的情况下，右边的表达式根本不会执行。not运算符的后面会跟上一个布尔值，它的作用是得到与该布尔值相反的值，也就是说，后面的布尔值如果是True运算结果就是False，而后面的布尔值如果是False则运算结果就是True。 123456789101112131415161718&quot;&quot;&quot;比较运算符和逻辑运算符的使用Version: 0.1Author: 骆昊&quot;&quot;&quot;flag0 = 1 == 1flag1 = 3 &gt; 2flag2 = 2 &lt; 1flag3 = flag1 and flag2flag4 = flag1 or flag2flag5 = not (1 != 2)print(&#x27;flag0 =&#x27;, flag0) # flag0 = Trueprint(&#x27;flag1 =&#x27;, flag1) # flag1 = Trueprint(&#x27;flag2 =&#x27;, flag2) # flag2 = Falseprint(&#x27;flag3 =&#x27;, flag3) # flag3 = Falseprint(&#x27;flag4 =&#x27;, flag4) # flag4 = Trueprint(&#x27;flag5 =&#x27;, flag5) # flag5 = False 说明：比较运算符的优先级高于赋值运算符，所以flag0 = 1 == 1先做1 == 1产生布尔值True，再将这个值赋值给变量flag0。print函数可以输出多个值，多个值之间可以用,进行分隔，输出的内容之间默认以空格分开。 练习练习1：华氏温度转换为摄氏温度。 提示：华氏温度到摄氏温度的转换公式为：$C&#x3D;(F - 32) \\div 1.8$。 参考答案： 123456789&quot;&quot;&quot;将华氏温度转换为摄氏温度Version: 0.1Author: 骆昊&quot;&quot;&quot;f = float(input(&#x27;请输入华氏温度: &#x27;))c = (f - 32) / 1.8print(&#x27;%.1f华氏度 = %.1f摄氏度&#x27; % (f, c)) 说明：在使用print函数输出时，也可以对字符串内容进行格式化处理，上面print函数中的字符串%.1f是一个占位符，稍后会由一个float类型的变量值替换掉它。同理，如果字符串中有%d，后面可以用一个int类型的变量值替换掉它，而%s会被字符串的值替换掉。除了这种格式化字符串的方式外，还可以用下面的方式来格式化字符串，其中&#123;f:.1f&#125;和&#123;c:.1f&#125;可以先看成是&#123;f&#125;和&#123;c&#125;，表示输出时会用变量f和变量c的值替换掉这两个占位符，后面的:.1f表示这是一个浮点数，小数点后保留1位有效数字。 1print(f&#x27;&#123;f:.1f&#125;华氏度 = &#123;c:.1f&#125;摄氏度&#x27;) 练习2：输入圆的半径计算计算周长和面积。参考答案： 1234567891011&quot;&quot;&quot;输入半径计算圆的周长和面积Version: 0.1Author: 骆昊&quot;&quot;&quot;radius = float(input(&#x27;请输入圆的半径: &#x27;))perimeter = 2 * 3.1416 * radiusarea = 3.1416 * radius * radiusprint(&#x27;周长: %.2f&#x27; % perimeter)print(&#x27;面积: %.2f&#x27; % area) 练习3：输入年份判断是不是闰年。参考答案： 1234567891011&quot;&quot;&quot;输入年份 如果是闰年输出True 否则输出FalseVersion: 0.1Author: 骆昊&quot;&quot;&quot;year = int(input(&#x27;请输入年份: &#x27;))# 如果代码太长写成一行不便于阅读 可以使用\\对代码进行折行is_leap = year % 4 == 0 and year % 100 != 0 or \\ year % 400 == 0print(is_leap) 说明：比较运算符会产生布尔值，而逻辑运算符and和or会对这些布尔值进行组合，最终也是得到一个布尔值，闰年输出True，平年输出False。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/更新日志","date":"2024-12-12T08:38:00.379Z","updated":"2024-05-30T10:30:37.000Z","comments":true,"path":"2024/12/12/Python-100-Days-master/更新日志/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","excerpt":"","text":"更新日志2024年4月18日 重新整理了项目结构，补充了一部分文档。 准备开始撰写《机器学习和深度学习》部分的内容。 2022年1月11日 将近期收到的打赏通过水滴筹和众缘普济公益慈善促进会捐赠给有需要的人。 2021年11月22日 更新了数据库部分的文档和代码。 修正了网友们指出的文档和代码中的bug。 更新了README文件中的群信息。 2021年10月7日 调整了项目目录结构。 更新了数据分析部分的内容。 2021年3月28日 最近把数据挖掘的东西梳理了一遍，准备开始写文档啦。 近期收到的打赏通过美团公益捐赠给有需要的人。 ###2020年10月3日 调整项目目录结构。 开始补全前面缺失内容和更新最后部分的工作。 将近期收到的打赏通过水滴筹等平台捐助给需要帮助的人。 2020年7月12日 修正了部分文档中的bug。 更新了Django部分的文档。 将近期收到的打赏捐赠给陕西回归儿童救助中心。 2020年4月8日 将基础部分（第1天到第15天）内容重新创建了一个名为“Python-Core-50-Courses”的仓库，更新了部分内容。 更新了README.md文件。 将近期收到的打赏通过腾讯公益捐出，助力“儿童健康课堂”。 2020年3月8日 更新了最后10天的部分文档。 通过美团公益将近期打赏捐助给受疫情影响儿童。 2020年3月1日 优化了项目中的部分图片资源。 更新了部分文档。 2019年9月23日 计划在国庆长假结束前，完成第91天到第100天内容的更新，包括最新的Python面试题集。 修改了第7天《字符串和常用数据结构》一文的内容。 2019年9月15日 将微信打赏所得收入通过腾讯公益捐赠给国家级贫困大学生。 开始更新和调整第1天到第15天的内容。 开始整理大家期待已久的《Python面试题大全及参考答案》。 2019年8月12日 发布了《使用Hexo搭建自己的博客》一文。 2019年8月8日 最近公司给安排了很多任务，所以好久都没有更新这个项目，今天终于把一直计划更新的《关系型数据库MySQL》更新完了。 昨天微信收款提示我已经连续有48天收到了打赏，非常感谢大家持续的支持。 最近一直在计划为这个项目录制一个配套的视频，当然这个工作量的巨大是可想而知的，但是我还是决定要在年底之前开始做这件事情，这样才能不辜负那么多希望通过这个项目学习和了解Python的人。 2019年7月11日 今天终于结束了出差的日子，回来先把最近收到的打赏通过腾讯公益平台全部捐赠给了绿之叶，总共捐出了111笔打赏。 2019年7月9日 最近出差，项目一直处于停更状态。交流群的很多初学者反馈从第8天开始内容就有一定难度，最近本来也打算把语言基础部分和爬虫的部门重新整理一次，这次争取将文字和案例做得更加通俗易懂，而且更具实用性，这个事情从今天已然开始了，最终的目标是要将这部分的内容整理成一本书，希望大家到时候能够一如既往的支持。 最近1周多的时间一共收到60笔以上的打赏，最多的一天收到了14笔打赏，还是要再次谢谢大家为知识付费的行为，当然加交流群是不用付费的，您支付的费用会用于支持山区儿童教育。 今天重新翻译了Zen of Python，这次的版本我自己都比较喜欢，所以也分享给大家。 2019年6月30日 最近2天一共收到11笔打赏。 终于将第48天《前后端分离开发》一文更新完，但是自己都感觉有点凑数的嫌疑，文字描述没怎么花心思去写，大家可以参考项目的代码来了解前后端分离开发。项目中使用了Vue.js，但是没有使用脚手架工具，也没有配置前端路由，仅仅使用了Vue.js来渲染页面，毕竟我自己也不是专业的前端。 2019年6月27日 最近3天一共收到35笔打赏，感谢大家持续关注。 近期事情较多，更新速度可能会放缓，请大家谅解。 今晚公开课相关的资料已经更新到公开课目录中。 2019年6月23日 最近几天一共收到25笔打赏，感谢大家的支持。 更新了QQ交流群，重新创建了一个2000人群。 2019年6月18日 在朋友的建议下，给首页加了一个打赏的二维码，看看有多少愿意为知识付费。今天一共收到了7位小伙伴的打赏，在此表示感谢，打赏获得的收入将通过腾讯公益平台全部捐出。 Django部分（第41天到第55天）更新到第47天，最新上线的部分包括报表、日志、ORM查询优化以及中间件相关的内容，并将投票应用的完成代码同步到github。","categories":[],"tags":[]},{"title":"","slug":"Python-100-Days-master/README","date":"2024-12-12T08:38:00.376Z","updated":"2024-12-12T10:37:51.038Z","comments":true,"path":"2024/12/12/Python-100-Days-master/README/","permalink":"http://bingfly.top/2024/12/12/Python-100-Days-master/README/","excerpt":"","text":"Python - 100天从新手到大师Python应用领域和职业发展分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。 学习曲线低，非专业人士也能上手 开源系统，拥有强大的生态圈 解释型语言，完美的平台可移植性 动态类型语言，支持面向对象和函数式编程 代码规范程度高，可读性强 Python在以下领域都有用武之地。 后端开发 - Python &#x2F; Java &#x2F; Go &#x2F; PHP DevOps - Python &#x2F; Shell &#x2F; Ruby 数据采集 - Python &#x2F; C++ &#x2F; Java 量化交易 - Python &#x2F; C++ &#x2F; R 数据科学 - Python &#x2F; R &#x2F; Julia &#x2F; Matlab 机器学习 - Python &#x2F; R &#x2F; C++ &#x2F; Julia 自动化测试 - Python &#x2F; Shell 作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。 Python后端开发工程师（服务器、云平台、数据接口） Python运维工程师（自动化运维、SRE、DevOps） Python数据分析师（数据分析、商业智能、数字化运营） Python数据挖掘工程师（机器学习、深度学习、算法专家） Python爬虫工程师 Python测试工程师（自动化测试、测试开发） 说明：目前，数据分析和数据挖掘是非常热门的方向，因为不管是互联网行业还是传统行业都已经积累了大量的数据，各行各业都需要数据分析师从已有的数据中发现更多的商业价值，从而为企业的决策提供数据的支撑，这就是所谓的数据驱动决策。 给初学者的几个建议： Make English as your working language. （让英语成为你的工作语言） Practice makes perfect. （熟能生巧） All experience comes from mistakes. （所有的经验都源于你犯过的错误） Don’t be one of the leeches. （不要当伸手党） Either outstanding or out. （要么出众，要么出局） Day01~15 - Python语言基础Day01 - 初识Python Python简介 - Python的历史 &#x2F; Python的优缺点 &#x2F; Python的应用领域 搭建编程环境 - Windows环境 &#x2F; Linux环境 &#x2F; MacOS环境 从终端运行Python程序 - Hello, world &#x2F; print函数 &#x2F; 运行程序 使用IDLE - 交互式环境(REPL) &#x2F; 编写多行代码 &#x2F; 运行程序 &#x2F; 退出IDLE 注释 - 注释的作用 &#x2F; 单行注释 &#x2F; 多行注释 Day02 - 语言元素 程序和进制 - 指令和程序 &#x2F; 冯诺依曼机 &#x2F; 二进制和十进制 &#x2F; 八进制和十六进制 变量和类型 - 变量的命名 &#x2F; 变量的使用 &#x2F; input函数 &#x2F; 检查变量类型 &#x2F; 类型转换 数字和字符串 - 整数 &#x2F; 浮点数 &#x2F; 复数 &#x2F; 字符串 &#x2F; 字符串基本操作 &#x2F; 字符编码 运算符 - 数学运算符 &#x2F; 赋值运算符 &#x2F; 比较运算符 &#x2F; 逻辑运算符 &#x2F; 身份运算符 &#x2F; 运算符的优先级 应用案例 - 华氏温度转换成摄氏温度 &#x2F; 输入圆的半径计算周长和面积 &#x2F; 输入年份判断是否是闰年 Day03 - 分支结构 分支结构的应用场景 - 条件 &#x2F; 缩进 &#x2F; 代码块 &#x2F; 流程图 if语句 - 简单的if &#x2F; if-else结构 &#x2F; if-elif-else结构 &#x2F; 嵌套的if 应用案例 - 用户身份验证 &#x2F; 英制单位与公制单位互换 &#x2F; 掷骰子决定做什么 &#x2F; 百分制成绩转等级制 &#x2F; 分段函数求值 &#x2F; 输入三条边的长度如果能构成三角形就计算周长和面积 Day04 - 循环结构 循环结构的应用场景 - 条件 &#x2F; 缩进 &#x2F; 代码块 &#x2F; 流程图 while循环 - 基本结构 &#x2F; break语句 &#x2F; continue语句 for循环 - 基本结构 &#x2F; range类型 &#x2F; 循环中的分支结构 &#x2F; 嵌套的循环 &#x2F; 提前结束程序 应用案例 - 1~100求和 &#x2F; 判断素数 &#x2F; 猜数字游戏 &#x2F; 打印九九表 &#x2F; 打印三角形图案 &#x2F; 猴子吃桃 &#x2F; 百钱百鸡 Day05 - 构造程序逻辑 经典案例：水仙花数 &#x2F; 百钱百鸡 &#x2F; Craps赌博游戏 练习题目：斐波那契数列 &#x2F; 完美数 &#x2F; 素数 Day06 - 函数和模块的使用 函数的作用 - 代码的坏味道 &#x2F; 用函数封装功能模块 定义函数 - def关键字 &#x2F; 函数名 &#x2F; 参数列表 &#x2F; return语句 &#x2F; 调用自定义函数 调用函数 - Python内置函数 &#x2F; 导入模块和函数 函数的参数 - 默认参数 &#x2F; 可变参数 &#x2F; 关键字参数 &#x2F; 命名关键字参数 函数的返回值 - 没有返回值 &#x2F; 返回单个值 &#x2F; 返回多个值 作用域问题 - 局部作用域 &#x2F; 嵌套作用域 &#x2F; 全局作用域 &#x2F; 内置作用域 &#x2F; 和作用域相关的关键字 用模块管理函数 - 模块的概念 &#x2F; 用自定义模块管理函数 &#x2F; 命名冲突的时候会怎样（同一个模块和不同的模块） Day07 - 字符串和常用数据结构 字符串的使用 - 计算长度 &#x2F; 下标运算 &#x2F; 切片 &#x2F; 常用方法 列表基本用法 - 定义列表 &#x2F; 用下表访问元素 &#x2F; 下标越界 &#x2F; 添加元素 &#x2F; 删除元素 &#x2F; 修改元素 &#x2F; 切片 &#x2F; 循环遍历 列表常用操作 - 连接 &#x2F; 复制(复制元素和复制数组) &#x2F; 长度 &#x2F; 排序 &#x2F; 倒转 &#x2F; 查找 生成列表 - 使用range创建数字列表 &#x2F; 生成表达式 &#x2F; 生成器 元组的使用 - 定义元组 &#x2F; 使用元组中的值 &#x2F; 修改元组变量 &#x2F; 元组和列表转换 集合基本用法 - 集合和列表的区别 &#x2F; 创建集合 &#x2F; 添加元素 &#x2F; 删除元素 &#x2F; 清空 集合常用操作 - 交集 &#x2F; 并集 &#x2F; 差集 &#x2F; 对称差 &#x2F; 子集 &#x2F; 超集 字典的基本用法 - 字典的特点 &#x2F; 创建字典 &#x2F; 添加元素 &#x2F; 删除元素 &#x2F; 取值 &#x2F; 清空 字典常用操作 - keys方法 &#x2F; values方法 &#x2F; items方法 &#x2F; setdefault方法 基础练习 - 跑马灯效果 &#x2F; 列表找最大元素 &#x2F; 统计考试成绩的平均分 &#x2F; Fibonacci数列 &#x2F; 杨辉三角 综合案例 - 双色球选号 &#x2F; 井字棋 Day08 - 面向对象编程基础 类和对象 - 什么是类 &#x2F; 什么是对象 &#x2F; 面向对象其他相关概念 定义类 - 基本结构 &#x2F; 属性和方法 &#x2F; 构造器 &#x2F; 析构器 &#x2F; __str__方法 使用对象 - 创建对象 &#x2F; 给对象发消息 面向对象的四大支柱 - 抽象 &#x2F; 封装 &#x2F; 继承 &#x2F; 多态 基础练习 - 定义学生类 &#x2F; 定义时钟类 &#x2F; 定义图形类 &#x2F; 定义汽车类 Day09 - 面向对象进阶 属性 - 类属性 &#x2F; 实例属性 &#x2F; 属性访问器 &#x2F; 属性修改器 &#x2F; 属性删除器 &#x2F; 使用__slots__ 类中的方法 - 实例方法 &#x2F; 类方法 &#x2F; 静态方法 运算符重载 - __add__ &#x2F; __sub__ &#x2F; __or__ &#x2F;__getitem__ &#x2F; __setitem__ &#x2F; __len__ &#x2F; __repr__ &#x2F; __gt__ &#x2F; __lt__ &#x2F; __le__ &#x2F; __ge__ &#x2F; __eq__ &#x2F; __ne__ &#x2F; __contains__ 类(的对象)之间的关系 - 关联 &#x2F; 继承 &#x2F; 依赖 继承和多态 - 什么是继承 &#x2F; 继承的语法 &#x2F; 调用父类方法 &#x2F; 方法重写 &#x2F; 类型判定 &#x2F; 多重继承 &#x2F; 菱形继承(钻石继承)和C3算法 综合案例 - 工资结算系统 &#x2F; 图书自动折扣系统 &#x2F; 自定义分数类 Day10 - 图形用户界面和游戏开发 使用tkinter开发GUI程序 使用pygame三方库开发游戏应用 “大球吃小球”游戏 Day11 - 文件和异常 读文件 - 读取整个文件 &#x2F; 逐行读取 &#x2F; 文件路径 写文件 - 覆盖写入 &#x2F; 追加写入 &#x2F; 文本文件 &#x2F; 二进制文件 异常处理 - 异常机制的重要性 &#x2F; try-except代码块 &#x2F; else代码块 &#x2F; finally代码块 &#x2F; 内置异常类型 &#x2F; 异常栈 &#x2F; raise语句 数据持久化 - CSV文件概述 &#x2F; csv模块的应用 &#x2F; JSON数据格式 &#x2F; json模块的应用 Day12 - 字符串和正则表达式 字符串高级操作 - 转义字符 &#x2F; 原始字符串 &#x2F; 多行字符串 &#x2F; in和not in运算符 &#x2F; is_xxx方法 &#x2F; join和split方法 &#x2F; strip相关方法 &#x2F; pyperclip模块 &#x2F; 不变字符串和可变字符串 &#x2F; StringIO的使用 正则表达式入门 - 正则表达式的作用 &#x2F; 元字符 &#x2F; 转义 &#x2F; 量词 &#x2F; 分组 &#x2F; 零宽断言 &#x2F;贪婪匹配与惰性匹配懒惰 &#x2F; 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获） 使用正则表达式 - re模块 &#x2F; compile函数 &#x2F; group和groups方法 &#x2F; match方法 &#x2F; search方法 &#x2F; findall和finditer方法 &#x2F; sub和subn方法 &#x2F; split方法 应用案例 - 使用正则表达式验证输入的字符串 Day13 - 进程和线程 进程和线程的概念 - 什么是进程 &#x2F; 什么是线程 &#x2F; 多线程的应用场景 使用进程 - fork函数 &#x2F; multiprocessing模块 &#x2F; 进程池 &#x2F; 进程间通信 使用线程 - threading模块 &#x2F; Thread类 &#x2F; RLock类 &#x2F; Condition类 &#x2F; 线程池 Day14 - 网络编程入门和网络应用开发 计算机网络基础 - 计算机网络发展史 &#x2F; “TCP-IP”模型 &#x2F; IP地址 &#x2F; 端口 &#x2F; 协议 &#x2F; 其他相关概念 网络应用模式 - “客户端-服务器”模式 &#x2F; “浏览器-服务器”模式 基于HTTP协议访问网络资源 - 网络API概述 &#x2F; 访问URL &#x2F; requests三方库 &#x2F; 解析JSON格式数据 Python网络编程 - 套接字的概念 &#x2F; socket模块 &#x2F; socket函数 &#x2F; 创建TCP服务器 &#x2F; 创建TCP客户端 &#x2F; 创建UDP服务器 &#x2F; 创建UDP客户端 电子邮件 - SMTP协议 &#x2F; POP3协议 &#x2F; IMAP协议 &#x2F; smtplib模块 &#x2F; poplib模块 &#x2F; imaplib模块 短信服务 - 调用短信服务网关 Day15 - 图像和文档处理 用Pillow处理图片 - 图片读写 &#x2F; 图片合成 &#x2F; 几何变换 &#x2F; 色彩转换 &#x2F; 滤镜效果 读写Word文档 - 文本内容的处理 &#x2F; 段落 &#x2F; 页眉和页脚 &#x2F; 样式的处理 读写Excel文件 - xlrd &#x2F; xlwt &#x2F; openpyxl Day16~Day20 - Python语言进阶 常用数据结构 函数的高级用法 - “一等公民” &#x2F; 高阶函数 &#x2F; Lambda函数 &#x2F; 作用域和闭包 &#x2F; 装饰器 面向对象高级知识 - “三大支柱” &#x2F; 类与类之间的关系 &#x2F; 垃圾回收 &#x2F; 魔术属性和方法 &#x2F; 混入 &#x2F; 元类 &#x2F; 面向对象设计原则 &#x2F; GoF设计模式 迭代器和生成器 - 相关魔术方法 &#x2F; 创建生成器的两种方式 &#x2F; 并发和异步编程 - 多线程 &#x2F; 多进程 &#x2F; 异步IO &#x2F; async和await Day21~30 - Web前端入门 用HTML标签承载页面内容 用CSS渲染页面 用JavaScript处理交互式行为 jQuery入门和提高 Vue.js入门 Element的使用 Bootstrap的使用 Day31~35 - 玩转Linux操作系统 操作系统发展史和Linux概述 Linux基础命令 Linux中的实用程序 Linux的文件系统 Vim编辑器的应用 环境变量和Shell编程 软件的安装和服务的配置 网络访问和管理 其他相关内容 Day36~45 - 数据库基础和进阶Day36 - 关系型数据库和MySQL概述 关系型数据库概述 MySQL简介 安装MySQL MySQL基本命令 Day37 - SQL详解之DDL 建库建表 删除表和修改表 Day38 - SQL详解之DML insert操作 delete操作 update操作 Day39 - SQL详解之DQL 投影和别名 筛选数据 空值处理 去重 排序 聚合函数 嵌套查询 分组 表连接 笛卡尔积 内连接 自然连接 外连接 窗口函数 定义窗口 排名函数 取数函数 Day40 - SQL详解之DCL 创建用户 授予权限 召回权限 Day41 - MySQL新特性 JSON类型 窗口函数 公共表表达式 Day42 - 视图、函数和过程 视图 使用场景 创建视图 使用限制 函数 内置函数 用户自定义函数（UDF） 过程 创建过程 调用过程 Day43 - 索引 执行计划 索引的原理 创建索引 普通索引 唯一索引 前缀索引 复合索引 注意事项 Day44 - Python接入MySQL数据库 安装三方库 创建连接 获取游标 执行SQL语句 通过游标抓取数据 事务提交和回滚 释放连接 编写ETL脚本 Day45 - 大数据平台和HiveSQL Hadoop生态圈 Hive概述 准备工作 数据类型 DDL操作 DML操作 数据查询 Day46~60 - 实战DjangoDay46 - Django快速上手 Web应用工作机制 HTTP请求和响应 Django框架概述 5分钟快速上手 Day47 - 深入模型 关系型数据库配置 使用ORM完成对模型的CRUD操作 管理后台的使用 Django模型最佳实践 模型定义参考 Day48 - 静态资源和Ajax请求 加载静态资源 Ajax概述 用Ajax实现投票功能 Day49 - Cookie和Session 实现用户跟踪 cookie和session的关系 Django框架对session的支持 视图函数中的cookie读写操作 Day50 - 报表和日志 通过HttpResponse修改响应头 使用StreamingHttpResponse处理大文件 使用xlwt生成Excel报表 使用reportlab生成PDF报表 使用ECharts生成前端图表 Day51 - 日志和调试工具栏 配置日志 配置Django-Debug-Toolbar 优化ORM代码 Day52 - 中间件的应用 什么是中间件 Django框架内置的中间件 自定义中间件及其应用场景 Day53 - 前后端分离开发入门 返回JSON格式的数据 用Vue.js渲染页面 Day54 - RESTful架构和DRF入门 REST概述 DRF库使用入门 前后端分离开发 JWT的应用 Day55 - RESTful架构和DRF进阶 使用CBV 数据分页 数据筛选 Day56 - 使用缓存 网站优化第一定律 在Django项目中使用Redis提供缓存服务 在视图函数中读写缓存 使用装饰器实现页面缓存 为数据接口提供缓存服务 Day57 - 接入三方平台 文件上传表单控件和图片文件预览 服务器端如何处理上传的文件 Day58 - 异步任务和定时任务 网站优化第二定律 配置消息队列服务 在项目中使用Celery实现任务异步化 在项目中使用Celery实现定时任务 Day59 - 单元测试Day60 - 项目上线 Python中的单元测试 Django框架对单元测试的支持 使用版本控制系统 配置和使用uWSGI 动静分离和Nginx配置 配置HTTPS 配置域名解析 Day61~65 - 爬虫开发Day61 - 网络数据采集概述 网络爬虫的概念及其应用领域 网络爬虫的合法性探讨 开发网络爬虫的相关工具 一个爬虫程序的构成 Day62 - 数据抓取和解析 使用requests三方库实现数据抓取 页面解析的三种方式 正则表达式解析 XPath解析 CSS选择器解析 Day63 - Python中的并发编程 多线程 多进程 异步I&#x2F;O Day64 - 使用Selenium抓取网页动态内容 安装Selenium 加载页面 查找元素和模拟用户行为 隐式等待和显示等待 执行JavaScript代码 Selenium反爬破解 设置无头浏览器 Day65 - 爬虫框架Scrapy简介 Scrapy核心组件 Scrapy工作流程 安装Scrapy和创建项目 编写蜘蛛程序 编写中间件和管道程序 Scrapy配置文件 Day66~80 - 数据分析Day66 - 数据分析概述 数据分析师的职责 数据分析师的技能栈 数据分析相关库 Day67 - 环境准备 安装和使用anaconda conda相关命令 安装和使用jupyter-lab 安装和启动 使用小技巧 Day68 - NumPy的应用-1 创建数组对象 数组对象的属性 数组对象的索引运算 普通索引 花式索引 布尔索引 切片索引 案例：使用数组处理图像 Day69 - NumPy的应用-2 数组对象的相关方法 获取描述性统计信息 其他相关方法 Day70 - NumPy的应用-3 数组的运算 数组跟标量的运算 数组跟数组的运算 通用一元函数 通用二元函数 广播机制 Numpy常用函数 Day71 - NumPy的应用-4 向量 行列式 矩阵 多项式 Day72 - 深入浅出pandas-1 创建Series对象 Series对象的运算 Series对象的属性和方法 Day73 - 深入浅出pandas-2 创建DataFrame对象 DataFrame对象的属性和方法 读写DataFrame中的数据 Day74 - 深入浅出pandas-3 数据重塑 数据拼接 数据合并 数据清洗 缺失值 重复值 异常值 预处理 Day75 - 深入浅出pandas-4 数据透视 获取描述性统计信息 排序和头部值 分组聚合 透视表和交叉表 数据呈现 Day76 - 深入浅出pandas-5 计算同比环比 窗口计算 相关性判定 Day77 - 深入浅出pandas-6 索引的使用 范围索引 分类索引 多级索引 间隔索引 日期时间索引 Day78 - 数据可视化-1 安装和导入matplotlib 创建画布 创建坐标系 绘制图表 折线图 散点图 柱状图 饼状图 直方图 箱线图 显示和保存图表 Day79 - 数据可视化-2 高阶图表 气泡图 面积图 雷达图 玫瑰图 3D图表 Day80 - 数据可视化-3 Seaborn Pyecharts Day81~90 - 机器学习和深度学习Day81 - 机器学习基础Day82 - k最近邻分类Day83 - 决策树Day84 - 贝叶斯分类Day85 - 支持向量机Day86 - K-均值聚类Day87 - 回归分析Day88 - 深度学习入门Day89 - PyTorch概述Day90 - PyTorch实战Day91~100 - 团队项目开发第91天：团队项目开发的问题和解决方案 软件过程模型 经典过程模型（瀑布模型） 可行性分析（研究做还是不做），输出《可行性分析报告》。 需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。 概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。 编码 &#x2F; 测试。 上线 &#x2F; 维护。 瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。 敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint 产品的Backlog（用户故事、产品原型）。 计划会议（评估和预算）。 日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。 修复bug（问题描述、重现步骤、测试人员、被指派人）。 发布版本。 评审会议（Showcase，用户需要参与）。 回顾会议（对当前迭代周期做一个总结）。 补充：敏捷软件开发宣言 个体和互动 高于 流程和工具 工作的软件 高于 详尽的文档 客户合作 高于 合同谈判 响应变化 高于 遵循计划 角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。 准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。 敏捷团队通常人数为8-10人。 工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。 项目团队组建 团队的构成和角色 说明：感谢付祥英女士帮助我绘制了下面这张精美的公司组织架构图。 编程规范和代码审查（flake8、pylint） Python中的一些“惯例”（请参考《Python惯例-如何编写Pythonic的代码》） 影响代码可读性的原因： 代码注释太少或者没有注释 代码破坏了语言的最佳实践 反模式编程（意大利面代码、复制-黏贴编程、自负编程、……） 团队开发工具介绍 版本控制：Git、Mercury 缺陷管理：Gitlab、Redmine 敏捷闭环工具：禅道、JIRA 持续集成：Jenkins、Travis-CI 请参考《团队项目开发的问题和解决方案》。 项目选题和理解业务 选题范围设定 CMS（用户端）：新闻聚合网站、问答&#x2F;分享社区、影评&#x2F;书评网站等。 MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。 App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。 其他类型：自身行业背景和工作经验、业务容易理解和把控。 需求理解、模块划分和任务分配 需求理解：头脑风暴和竞品分析。 模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。 任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。 制定项目进度表（每日更新） 模块 功能 人员 状态 完成 工时 计划开始 实际开始 计划结束 实际结束 备注 评论 添加评论 王大锤 正在进行 50% 4 2018&#x2F;8&#x2F;7 2018&#x2F;8&#x2F;7 删除评论 王大锤 等待 0% 2 2018&#x2F;8&#x2F;7 2018&#x2F;8&#x2F;7 查看评论 白元芳 正在进行 20% 4 2018&#x2F;8&#x2F;7 2018&#x2F;8&#x2F;7 需要进行代码审查 评论投票 白元芳 等待 0% 4 2018&#x2F;8&#x2F;8 2018&#x2F;8&#x2F;8 OOAD和数据库设计 UML（统一建模语言）的类图 通过模型创建表（正向工程），例如在Django项目中可以通过下面的命令创建二维表。 12python manage.py makemigrations apppython manage.py migrate 使用PowerDesigner绘制物理模型图。 通过数据表创建模型（反向工程），例如在Django项目中可以通过下面的命令生成模型。 1python manage.py inspectdb &gt; app/models.py 第92天：Docker容器技术详解 Docker简介 安装Docker 使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins） 构建Docker镜像（Dockerfile的编写和相关指令） 容器编排（Docker-compose） 集群管理（Kubernetes） 第93天：MySQL性能优化 基本原则 InnoDB引擎 索引的使用和注意事项 数据分区 SQL优化 配置优化 架构优化 第94天：网络API接口设计 设计原则 关键问题 其他问题 文档撰写 第95天：[使用Django开发商业项目](.&#x2F;Day91-100&#x2F;95.使用Django开发商业项 目.md)项目开发中的公共问题 数据库的配置（多数据库、主从复制、数据库路由） 缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵）） 日志的配置 分析和调试（Django-Debug-ToolBar） 好用的Python模块（日期计算、图像处理、数据加密、三方API） REST API设计 RESTful架构 理解RESTful架构 RESTful API设计指南 RESTful API最佳实践 API接口文档的撰写 RAP2 YAPI django-REST-framework的应用 项目中的重点难点剖析 使用缓存缓解数据库压力 - Redis 使用消息队列做解耦合和削峰 - Celery + RabbitMQ 第96天：软件测试和自动化测试单元测试 测试的种类 编写单元测试（unittest、pytest、nose2、tox、ddt、……） 测试覆盖率（coverage） Django项目部署 部署前的准备工作 关键设置（SECRET_KEY &#x2F; DEBUG &#x2F; ALLOWED_HOSTS &#x2F; 缓存 &#x2F; 数据库） HTTPS &#x2F; CSRF_COOKIE_SECUR &#x2F; SESSION_COOKIE_SECURE 日志相关配置 Linux常用命令回顾 Linux常用服务的安装和配置 uWSGI&#x2F;Gunicorn和Nginx的使用 Gunicorn和uWSGI的比较 对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。 uWSGI支持异构部署。 由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。 在性能上，Gunicorn和uWSGI其实表现相当。 使用虚拟化技术（Docker）部署测试环境和生产环境 性能测试 AB的使用 SQLslap的使用 sysbench的使用 自动化测试 使用Shell和Python进行自动化测试 使用Selenium实现自动化测试 Selenium IDE Selenium WebDriver Selenium Remote Control 测试工具Robot Framework介绍 第97天：电商网站技术要点剖析 商业模式和需求要点 物理模型设计 第三方登录 缓存预热和查询缓存 购物车的实现 支付功能集成 秒杀和超卖问题 静态资源管理 全文检索方案 第98天：项目部署上线和性能调优 MySQL数据库调优 Web服务器性能优化 Nginx负载均衡配置 Keepalived实现高可用 代码性能调优 多线程 异步化 静态资源访问优化 云存储 CDN 第99天：面试中的公共问题 计算机基础 Python基础 Web框架相关 爬虫相关问题 数据分析 项目相关 第100天：Python面试题实录","categories":[],"tags":[]},{"title":"初识Python","slug":"Python-100-Days-master/Day01-15/01.初识Python","date":"2024-06-20T16:00:00.000Z","updated":"2024-12-14T22:09:31.166Z","comments":true,"path":"2024/06/21/Python-100-Days-master/Day01-15/01.初识Python/","permalink":"http://bingfly.top/2024/06/21/Python-100-Days-master/Day01-15/01.%E5%88%9D%E8%AF%86Python/","excerpt":"","text":"初识PythonPython简介Python的历史 1989年圣诞节：Guido von Rossum开始写Python语言的编译器。 1991年2月：第一个Python编译器（同时也是解释器）诞生，它是用C语言实现的（后面），可以调用C语言的库函数。在最早的版本中，Python已经提供了对“类”，“函数”，“异常处理”等构造块的支持，还有对列表、字典等核心数据类型，同时支持以模块为基础来构造应用程序。 1994年1月：Python 1.0正式发布。 2000年10月16日：Python 2.0发布，增加了完整的垃圾回收，提供了对Unicode的支持。与此同时，Python的整个开发过程更加透明，社区对开发进度的影响逐渐扩大，生态圈开始慢慢形成。 2008年12月3日：Python 3.0发布，它并不完全兼容之前的Python代码，不过因为目前还有不少公司在项目和运维中使用Python 2.x版本，所以Python 3.x的很多新特性后来也被移植到Python 2.6&#x2F;2.7版本中。 目前我使用的Python 3.7.x的版本是在2018年发布的，Python的版本号分为三段，形如A.B.C。其中A表示大版本号，一般当整体重写，或出现不向后兼容的改变时，增加A；B表示功能更新，出现新功能时增加B；C表示小的改动（例如：修复了某个Bug），只要有修改就增加C。如果对Python的历史感兴趣，可以阅读名为《Python简史》的网络文章。 Python的优缺点Python的优点很多，简单的可以总结为以下几点。 简单明了，学习曲线低，比很多编程语言都容易上手。 开放源代码，拥有强大的社区和生态圈，尤其是在数据分析和机器学习领域。 解释型语言，天生具有平台可移植性，代码可以工作于不同的操作系统。 对两种主流的编程范式（面向对象编程和函数式编程）都提供了支持。 代码规范程度高，可读性强，适合有代码洁癖和强迫症的人群。 Python的缺点主要集中在以下几点。 执行效率稍低，对执行效率要求高的部分可以由其他语言（如：C、C++）编写。 代码无法加密，但是现在很多公司都不销售卖软件而是销售服务，这个问题会被弱化。 在开发时可以选择的框架太多（如Web框架就有100多个），有选择的地方就有错误。 Python的应用领域目前Python在Web应用后端开发、云基础设施建设、DevOps、网络数据采集（爬虫）、自动化测试、数据分析、机器学习等领域都有着广泛的应用。 安装Python解释器想要开始Python编程之旅，首先得在自己使用的计算机上安装Python解释器环境，下面将以安装官方的Python解释器为例，讲解如何在不同的操作系统上安装Python环境。官方的Python解释器是用C语言实现的，也是使用最为广泛的Python解释器，通常称之为CPython。除此之外，Python解释器还有Java语言实现的Jython、C#语言实现的IronPython以及PyPy、Brython、Pyston等版本，有兴趣的读者可以自行了解。 Windows环境可以在Python官方网站下载到Python的Windows安装程序（exe文件），需要注意的是如果在Windows 7环境下安装Python 3.x，需要先安装Service Pack 1补丁包（可以通过一些工具软件自动安装系统补丁的功能来安装），安装过程建议勾选“Add Python 3.x to PATH”（将Python 3.x添加到PATH环境变量）并选择自定义安装，在设置“Optional Features”界面最好将“pip”、“tcl&#x2F;tk”、“Python test suite”等项全部勾选上。强烈建议选择自定义的安装路径并保证路径中没有中文。安装完成会看到“Setup was successful”的提示。如果稍后运行Python程序时，出现因为缺失一些动态链接库文件而导致Python解释器无法工作的问题，可以按照下面的方法加以解决。 如果系统显示api-ms-win-crt*.dll文件缺失，可以参照《api-ms-win-crt*.dll缺失原因分析和解决方法》一文讲解的方法进行处理或者直接在微软官网下载Visual C++ Redistributable for Visual Studio 2015文件进行修复；如果是因为更新Windows的DirectX之后导致某些动态链接库文件缺失问题，可以下载一个DirectX修复工具进行修复。 Linux环境Linux环境自带了Python 2.x版本，但是如果要更新到3.x的版本，可以在Python的官方网站下载Python的源代码并通过源代码构建安装的方式进行安装，具体的步骤如下所示（以CentOS为例）。 安装依赖库（因为没有这些依赖库可能在源代码构件安装时因为缺失底层依赖库而失败）。 1yum -y install wget gcc zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel libffi-devel 下载Python源代码并解压缩到指定目录。 123wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tar.xzxz -d Python-3.7.6.tar.xztar -xvf Python-3.7.6.tar 切换至Python源代码目录并执行下面的命令进行配置和安装。 123cd Python-3.7.6./configure --prefix=/usr/local/python37 --enable-optimizationsmake &amp;&amp; make install 修改用户主目录下名为.bash_profile的文件，配置PATH环境变量并使其生效。 12cd ~vim .bash_profile 12345# ... 此处省略上面的代码 ...export PATH=$PATH:/usr/local/python37/bin# ... 此处省略下面的代码 ... 激活环境变量。 1source .bash_profile macOS环境macOS也自带了Python 2.x版本，可以通过Python的官方网站提供的安装文件（pkg文件）安装Python 3.x的版本。默认安装完成后，可以通过在终端执行python命令来启动2.x版本的Python解释器，启动3.x版本的Python解释器需要执行python3命令。 运行Python程序确认Python的版本可以Windows的命令行提示符中键入下面的命令。 1python --version 在Linux或macOS系统的终端中键入下面的命令。 1python3 --version 当然也可以先输入python或python3进入交互式环境，再执行以下的代码检查Python的版本。 1234import sysprint(sys.version_info)print(sys.version) 编写Python源代码可以用文本编辑工具（推荐使用Sublime、Visual Studio Code等高级文本编辑工具）编写Python源代码并用py作为后缀名保存该文件，代码内容如下所示。 1print(&#x27;hello, world!&#x27;) 运行程序切换到源代码所在的目录并执行下面的命令，看看屏幕上是否输出了”hello, world!”。 1python hello.py 或 1python3 hello.py 代码中的注释注释是编程语言的一个重要组成部分，用于在源代码中解释代码的作用从而增强程序的可读性和可维护性，当然也可以将源代码中不需要参与运行的代码段通过注释来去掉，这一点在调试程序的时候经常用到。注释在随源代码进入预处理器或编译时会被移除，不会在目标代码中保留也不会影响程序的执行结果。 单行注释 - 以#和空格开头的部分 多行注释 - 三个引号开头，三个引号结尾 123456789&quot;&quot;&quot;第一个Python程序 - hello, world!向伟大的Dennis M. Ritchie先生致敬Version: 0.1Author: 骆昊&quot;&quot;&quot;print(&#x27;hello, world!&#x27;)# print(&quot;你好, 世界！&quot;) Python开发工具IDLE - 自带的集成开发工具IDLE是安装Python环境时自带的集成开发工具，如下图所示。但是由于IDLE的用户体验并不是那么好所以很少在实际开发中被采用。 IPython - 更好的交互式编程工具IPython是一种基于Python的交互式解释器。相较于原生的Python交互式环境，IPython提供了更为强大的编辑和交互功能。可以通过Python的包管理工具pip安装IPython，具体的操作如下所示。 1pip install ipython 或 1pip3 install ipython 安装成功后，可以通过下面的ipython命令启动IPython，如下图所示。 Sublime Text - 高级文本编辑器 首先可以通过官方网站下载安装程序安装Sublime Text 3或Sublime Text 2。 安装包管理工具。 通过快捷键Ctrl+&#96;或者在View菜单中选择Show Console打开控制台，输入下面的代码。 Sublime 3 1import urllib.request,os;pf=&#x27;Package Control.sublime-package&#x27;;ipp=sublime.installed_packages_path();urllib.request.install_opener(urllib.request.build_opener(urllib.request.ProxyHandler()));open(os.path.join(ipp,pf),&#x27;wb&#x27;).write(urllib.request.urlopen(&#x27;http://sublime.wbond.net/&#x27;+pf.replace(&#x27; &#x27;,&#x27;%20&#x27;)).read()) Sublime 2 1import urllib2,os;pf=&#x27;Package Control.sublime-package&#x27;;ipp=sublime.installed_packages_path();os.makedirs(ipp)ifnotos.path.exists(ipp)elseNone;urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler()));open(os.path.join(ipp,pf),&#x27;wb&#x27;).write(urllib2.urlopen(&#x27;http://sublime.wbond.net/&#x27;+pf.replace(&#x27; &#x27;,&#x27;%20&#x27;)).read());print(&#x27;Please restart Sublime Text to finish installation&#x27;) 在浏览器中输入 https://sublime.wbond.net/Package%20Control.sublime-package 下载包管理工具的安装包，并找到安装Sublime目录下名为&quot;Installed Packages&quot;的目录，把刚才下载的文件放到这个文件加下，然后重启Sublime Text就搞定了。 安装插件。通过Preference菜单的Package Control或快捷键Ctrl+Shift+P打开命令面板，在面板中输入Install Package就可以找到安装插件的工具，然后再查找需要的插件。我们推荐大家安装以下几个插件： SublimeCodeIntel - 代码自动补全工具插件。 Emmet - 前端开发代码模板插件。 Git - 版本控制工具插件。 Python PEP8 Autoformat - PEP8规范自动格式化插件。 ConvertToUTF8 - 将本地编码转换为UTF-8。 说明：事实上Visual Studio Code可能是更好的选择，它不用花钱并提供了更为完整和强大的功能，有兴趣的读者可以自行研究。 PyCharm - Python开发神器PyCharm的安装、配置和使用在《玩转PyCharm》进行了介绍，有兴趣的读者可以选择阅读。 练习 在Python交互式环境中输入下面的代码并查看结果，请尝试将看到的内容翻译成中文。 1import this 说明：输入上面的代码，在Python的交互式环境中可以看到Tim Peter撰写的“Python之禅”，里面讲述的道理不仅仅适用于Python，也适用于其他编程语言。 学习使用turtle在屏幕上绘制图形。 说明：turtle是Python内置的一个非常有趣的模块，特别适合对计算机程序设计进行初体验的小伙伴，它最早是Logo语言的一部分，Logo语言是Wally Feurzig和Seymour Papert在1966发明的编程语言。 1234567891011121314import turtleturtle.pensize(4)turtle.pencolor(&#x27;red&#x27;)turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.right(90)turtle.forward(100)turtle.mainloop() 提示：本章提供的代码中还有画国旗和画小猪佩奇的代码，有兴趣的读者请自行研究。","categories":[{"name":"Python基础","slug":"Python基础","permalink":"http://bingfly.top/categories/Python%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://bingfly.top/tags/Python/"}]},{"title":"ELK+Kafka+ZooKeeper","slug":"总结/3、ELK Kafka  ZooKeeper","date":"2024-06-13T16:00:00.000Z","updated":"2024-12-14T22:23:02.000Z","comments":true,"path":"2024/06/14/总结/3、ELK Kafka  ZooKeeper/","permalink":"http://bingfly.top/2024/06/14/%E6%80%BB%E7%BB%93/3%E3%80%81ELK%20Kafka%20%20ZooKeeper/","excerpt":"","text":"[TOC] 一、ELK+Kafka+ZooKeeper分布式日志数据&#x3D;&#x3D;》集中式 &#x3D;&#x3D;》查询和管理，故障排查 1、各组件及作用Elasticsearch：日志信息、日志信息搜索（全文搜索引擎）（外部端口9200；内部端口9300） Logstash：数据输入、输出、数据传输、数据处理、格式化 Kibana:针对Elasticsearch分析及可视化平台，数据整合、数据分析，简单数据导出 （端口5601） Filebeat：轻量级日志收集器（Logstash比较占用资源但有格式转换功能） kafka：中间件，消息队列，有效处理数据保证数据不丢失。 消息队列：处理高并发、高并列，kafka用的最多，，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统（也可以当做MQ系统） 一般用的基础架构通过logstash拉取业务服务信息，然后传给elasticsearch，通过kibana在网站页面展现 一般会使用filebeat替代logstash，主要logstash太占资源 加入中间件的架构（kafka+zookeeper） 第一层、数据采集层 最左边的是业务服务器集群，上面安装了filebeat做日志采集，同时把采集的日志分别发送给两个logstash服务。 第二层、数据处理层，数据缓存层 logstash服务把接受到的日志经过格式处理，转存到本地的kafka broker+zookeeper 集群中。 第三层、数据转发层 这个单独的Logstash节点会实时去kafka broker集群拉数据，转发至ES DataNode。 第四层、数据持久化存储 ES DataNode 会把收到的数据，写磁盘，建索引库。 第五层、数据检索，数据展示 ES Master + Kibana 主要 协调 ES集群，处理数据检索请求，数据展示。 注1：【Kafka的加入原因与作用】 整个架构加入Kafka，是为了让整个系统更好的分层，Kafka作为一个消息流处理与持久化存储软件，能够帮助我们在主节点上屏蔽掉多个从节点之间不同日志文件的差异，负责管理日志端（从节点）的人可以专注于向 Kafka里生产数据，而负责数据分析聚合端的人则可以专注于从 Kafka内消费数据。所以部署时要把Kafka加进去。 而且使用Kafka进行日志传输的原因还在于其有数据缓存的能力，并且它的数据可重复消费，Kafka本身具有高可用性，能够很好的防止数据丢失，它的吞吐量相对来说比较好并且使用广泛。可以有效防止日志丢失和防止logsthash挂掉。综合来说：它均衡了网络传输，从而降低了网络闭塞，尤其是丢失数据的可能性， 注2：【双层的Logstash作用】 这里为什么要在Kafka前面增加二台logstash呢？是因为在大量的日志数据写入时，容易导致数据的丢失和混乱，为了解决这一问题，增加二台logstash可以通过类型进行汇总分类，降低数据传输的臃肿。 如果只有一层的Logstash，它将处理来自不同客户端Filebeat收集的日志信息汇总，并且进行处理分析，在一定程度上会造成在大规模日志数据下信息的处理混乱，并严重加深负载，所以有二层的结构进行负载均衡处理，并且职责分工，一层汇聚简单分流，一层分析过滤处理信息，并且内层都有二台Logstash来保障服务的高可用性，以此提升整个架构的稳定性。 2、ElasticsearchElasticsearch，基于RESTful web接口。 Elasticsearch作为数据持久化存储环节，主要就是接受采集端发送过来的数据，执行写磁盘，建立索引库，最后将结构化的数据存储到ES集群上 Elasticsearch是用Java开发的，提供了一个分布式多用户能力的全文搜索引擎，设计用于云计算中，能够达到 实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch的基础核心概念： 1、接近实时（NRT） lasticsearch是一个接近实时的搜索平台，这意味着，从索引一个文档直到这个文档能够被搜索到有一个轻微的延迟（通常是1秒） 2、集群（cluster） 一个集群就是由一个或多个节点组织在一起，它们共同持有你整个的数据，并一起提供索引和搜索功能。其中一个节点为主节点，这个主节点是可以通过选举产生的，并提供跨节点的联合索引和搜索的功能。集群有一个唯一性标示的名字，默认是elasticsearch，集群名字很重要，每个节点是基于集群名字加入到其集群中的。因此，确保在不同环境中使用不同的集群名字。 一个集群可以只有一个节点。强烈建议在配置elasticsearch时，配置成集群模式。 3、节点（node） 节点就是一台单一的服务器，是集群的一部分，存储数据并参与集群的索引和搜索功能。像集群一样，节点也是通过名字来标识，默认是在节点启动时随机分配的字符名。当然，你可以自己定义。该名字也很重要，在集群中用于识别服务器对应的节点。 节点可以通过指定集群名字来加入到集群中。默认情况，每个节点被设置成加入到elasticsearch集群。如果启动了多个节点，假设能自动发现对方，他们将会自动组建一个名为elasticsearch的集群。 4、索引（index） 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，如果你想，可以定义任意多的索引。 ●索引相对于关系型数据库的库。 5、类型（type） 在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类&#x2F;分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。 ●类型相对于关系型数据库的表 3、ES的数据备份和恢复Elasticsearch 5.x 数据备份和恢复可由 snapshot 模块来完成，snapshot模块可以通过文件共享系统为单个索引或整个集群远程创建快照和进行数据恢复。 索引快照时增量的。在创建快照前es会分析已有快照仓库，只对上次备份后更改的内容进行增量备份。在创建备份时同一个集群中只能运行一个es snapshot进程。 Es 基础命令创建快照仓库123456curl -X PUT &quot;node1:9200/_snapshot/my_backup&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;sys_backup&quot; &#125;&#125;&#x27; 查看已注册的快照仓库1curl -X GET &quot;node1:9200/_snapshot/my_backup&quot; 可以使用逗号间隔多个仓库，星号通配符匹配所有仓库名字，下面示例返回仓库名以repo开头的和包含backup的仓库信息： 1curl -X GET &quot;node1:9200/_snapshot/repo*,*backup*&quot; 获取所有已注册快照仓库，省略仓库名或者使用_all 1curl -X GET &quot;node1:9200/_snapshot&quot; 或者 1curl -X GET &quot;node1:9200/_snapshot/_all&quot; 查看快照仓库列表1curl -X GET &quot;node1:9200/_cat/repositories?v&quot; 准备工作文件共享系统nfs、hdfs？ 共享文件系统仓库（“type”: “fs”）使用共享文件系统存快照，如果要注册共享文件系统仓库，必须在所有master和data节点挂载相同的共享文件系统到同一个路径位置。这个路径位置（或者它的一个父目录）必须在所有master和data节点的path.repo设置上注册。 假设共享文件系统挂载到 /data/backups/es_backup ，应该在elasticsearch.yml文件中添加如下配置： 1path.repo: [&quot;/data/backups&quot;, &quot;/data/longterm_backups&quot;] 创建快照仓库所有节点重启之后，执行下面的命令注册名字为 es_backup 的共享文件系统仓库： 123456789curl -X PUT &#x27;node1:9200/_snapshot/es_backup?verify=false&#x27; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/mount/backups/es_backup&quot;, &quot;compress&quot;: true, &quot;max_restore_bytes_per_sec&quot;: 50m, &quot;max_snapshot_bytes_per_sec&quot;: 30m &#125;&#125;&#x27; 如果使用相对路径，该路径将根据在path.repo中定义的第一个路径决定： 123456789curl -XPUT &#x27;http://node1:9200/_snapshot/es_backup?verify=false&#x27; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;es_backup&quot;, &quot;compress&quot;: true, &quot;max_restore_bytes_per_sec&quot;: 50m, &quot;max_snapshot_bytes_per_sec&quot;: 30m &#125;&#125;&#x27; 参数说明 参数 含义 location 快照存储位置 compress 是否压缩源文件，默认为true chunk_size 如果有需要，可以将大文件分解为多个小文件，默认不开启 max_restore_bytes_per_sec 指定数据恢复速度，默认为 40m&#x2F;s max_snapshot_bytes_per_sec 指定创建快照时的速度，默认为 40m&#x2F;s readonly 设置为只读仓库，默认为false Repository Verification在创建一个仓库时，会即刻在集群所有节点验证确保其功能在所有节点可用，verify 参数可以用来取消该验证（如果想使用验证功能，创建仓库时去掉 ?verify=false 参数即可）： 1234curl -XPUT &#x27;http://node1:9200/_snapshot/es_backup?verify=false&#x27; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;type&quot;: fs ... ...&#125;&#x27; 验证过程可以通过下面命令手动执行： 1curl -X POST &quot;node1:9200/_snapshot/es_backup/_verify&quot; Snapshot创建快照一个仓库可以拥有同一个集群的多个快照。在一个集群中快照拥有一个唯一名字作为标识。 示例： 在仓库 es_backup 中创建名字为 test_snapshot 的快照，可以通过执行下面的命令来实现。 1curl -X PUT &quot;node1:9200/_snapshot/es_backup/test_snapshot?wait_for_completion=true&quot; 参数 wait_for_completion 决定请求是在快照初始化后立即返回（默认），还是等快照创建完成之后再返回。快照初始化时，所有之前的快照信息会被加载到内存，所以在一个大的仓库中改请求需要若干秒（甚至分钟）才能返回，即使参数 wait_for_completion 的值设置为 false。 默认情况下，创建一个快照会包含集群中所有打开和启动状态的索引。可以通过在创建快照的请求体中定义索引列表来改变这个默认处理： 123456curl -X PUT &quot;node1:9200/_snapshot/es_backup/test_snapshot_2?wait_for_completion=true&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;indices&quot;: &quot;index_1,index_2&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false&#125; 要包含到快照中索引列表可以使用支持多个索引语法的 indices 参数来指定。快照请求也支持 ignore_unavailable 选项，该选项设置为 true 时，在创建快照时会忽略不存在的索引。默认情况下，如果选项 ignore_unavailable 没有设值，一个索引缺失，快照请求会失败。 通过设置 include_global_state 为 false，可以阻止集群全局状态信息被保存为快照的一部分。默认情况下，如果如果一个快照中的一个或者多个索引没有所有主分片可用，整个快照创建会失败，该情况可以通过设置 partial 为 true 来改变。 快照名可以通过使用 date_math_expressions 来自动获得，和创建新索引时类似。注意特殊字符需要 URI 转码处理。 例如，在名字中使用当前日期，比如 snapshot-2018.05.11，来创建快照，可以使用如下命令完成： 12# PUT /_snapshot/es_backup/&lt;snapshot-&#123;now/d&#125;&gt;curl -X PUT &quot;node1:9200/_snapshot/es_backup/%3Csnapshot-%7Bnow%2Fd%7D%3E&quot; 创建快照： 123456curl -X PUT &quot;node1:9200/_snapshot/es_backup/syslog?wait_for_completion=true&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;indices&quot;: &quot;bash_history.log*,secure.log*,cron.log*&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false&#125; 查看快照信息1curl -X GET &quot;node1:9200/_snapshot/es_backup/syslog&quot; 这个命令返回快照的基本信息，包括开始合结束时间、创建快照的 ElasticSearch 版本、包含的索引列表、快照当前状态和快照期间产生的失败索引列表。快照的状态有： 状态 含义 IN_PROGRESS 正在创建快照 SUCCESS 快照创建成功 FAILED 快照创建完成，但是有错误，数据不会保存 PARTIAL 整个集群备份完成，但是至少有一个shard数据存贮失败，会有更具体报错信息 INCOMPATIBLE 创建快照的es版本和当前集群es版本不一致 查看某仓库下所有快照信息： 1curl -X GET &quot;node1:9200/_snapshot/es_backup/_all&quot; 查看当前正在运行的快照： 1curl -X GET &quot;localhost:9200/_snapshot/my_backup/_current&quot; 删除快照从仓库中删除一个快照，使用如下命令： 1curl -X DELETE &quot;node1:9200/_snapshot/es_backup/test_snapshot_2&quot; 当一个快照从仓库中删除，ElasticSearch 将删除该快照关联的但不被其他快照使用的所有文件。如果在快照创建的时候执行快照删除操作，此快照创建进程将终止且所有该进程已创建的文件也将被清理。所以，快照删除操作可以用来取消错误启动的长时间运行的快照操作。 删除仓库可以使用下面命令注销仓库： 1curl -X DELETE &quot;node1:9200/_snapshot/es_backup&quot; 数据恢复全量恢复快照可以通过执行以下命令恢复 1curl -X POST &quot;node1:9200/_snapshot/es_backup/syslog/_restore&quot; 默认情况下，快照中的所有索引将被恢复，集群状态不被恢复。可以通过在恢复请求中使用 indices 和 include_global_state 选项来指定要恢复的索引和允许恢复集群全局状态。索引列表支持多索引语法。rename_pattern 和 rename_replacement 选项在恢复时通过正则表达式来重命名索引。设置 include_aliases 为 false 可以防止与索引关联的别名被一起恢复。 12345678curl -X POST &quot;localhost:9200/_snapshot/my_backup/snapshot_1/_restore&quot; -H &#x27;Content-Type: application/json&#x27; -d&#x27;&#123; &quot;indices&quot;: &quot;index_1,index_2&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: true, &quot;rename_pattern&quot;: &quot;index_(.+)&quot;, &quot;rename_replacement&quot;: &quot;restored_index_$1&quot;&#125;&#x27; 恢复操作可以在正常运行的集群上执行。已存在的索引只能在关闭状态下才能恢复，并且要跟快照中索引拥有相同数目的分片。还原操作自动打开关闭状态的索引，如果被还原索引在集群不存在，将创建新索引。如果集群状态通过 include_global_state （默认是 false）选项被还原，在集群中不存在的模板会被新增，已存在的同名模板会被快照中的模板替换。持久化设置会被添加到现有的持久化设置中。 4、Logstash缺点：很消耗内存和CPU 1）、 logstash 介绍LogStash由JRuby语言编写，基于消息（message-based）的简单架构，并运行在Java虚拟机（JVM）上。不同于分离的代理端（agent）或主机端（server），LogStash可配置单一的代理端（agent）与其它开源软件结合，以实现不同的功能。 2）、logStash的四大组件 Shipper：发送事件（events）至LogStash；通常，远程代理端（agent）只需要运行这个组件即可； Broker and Indexer：接收并索引化事件； Search and Storage：允许对事件进行搜索和存储； Web Interface：基于Web的展示界面正是由于以上组件在LogStash架构中可独立部署，才提供了更好的集群扩展性。 3）、LogStash主机分类 代理主机（agent host）：作为事件的传递者（shipper），将各种日志数据发送至中心主机；只需运行Logstash 代理（agent）程序； 中心主机（central host）：可运行包括中间转发器（Broker）、索引器（Indexer）、搜索和存储器（Search and Storage）、Web界面端（Web Interface）在内的各个组件，以实现对日志数据的接收、处理和存储。 4）、Logstash工作原理：Logstash事件处理有三个阶段：inputs → filters → outputs。是一个接收，处理，转发日志的工具。支持系统日志，webserver日志，错误日志，应用日志，总之包括所有可以抛出来的日志类型。 Input：输入数据到logstash。 一些常用的输入为： 1234567file：从文件系统的文件中读取，类似于tail -f命令syslog：在514端口上监听系统日志消息，并根据RFC3164标准进行解析redis：从redis service中读取beats：从filebeat中读取 Filters：数据中间处理，对数据进行操作。 一些常用的过滤器为： 123456789grok：解析任意文本数据，Grok 是 Logstash 最重要的插件。它的主要作用就是将文本格式的字符串，转换成为具体的结构化的数据，配合正则表达式使用。内置120多个解析语法。mutate：对字段进行转换。例如对字段进行删除、替换、修改、重命名等。drop：丢弃一部分events不进行处理。clone：拷贝 event，这个过程中也可以添加或移除字段。geoip：添加地理信息(为前台kibana图形化展示使用) Outputs：outputs是logstash处理管道的最末端组件。一个event可以在处理过程中经过多重输出，但是一旦所有的outputs都执行结束，这个event也就完成生命周期。 一些常见的outputs为： 12345678910111213elasticsearch：可以高效的保存数据，并且能够方便和简单的进行查询。file：将event数据保存到文件中。graphite：将event数据发送到图形化组件中，一个很流行的开源存储图形化展示的组件。**Codecs：codecs 是基于数据流的过滤器，它可以作为input，output的一部分配置**。Codecs可以帮助你轻松的分割发送过来已经被序列化的数据。一些常见的codecs：json：使用json格式对数据进行编码/解码。multiline：将汇多个事件中数据汇总为一个单一的行。比如：java异常信息和堆栈信息。 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354input &#123; syslog &#123; type =&gt; &quot;system-syslog&quot; host =&gt; &quot;192.168.56.11&quot; port =&gt; &quot;514&quot; &#125; file &#123; path =&gt; &quot;/var/log/messages&quot; type =&gt; &quot;system&quot; start_position =&gt; &quot;beginning&quot; &#125; file &#123; path =&gt; &quot;/var/log/nginx/access_json.log&quot; codec =&gt; json start_position =&gt; &quot;beginning&quot; type =&gt; &quot;nginx-log&quot; &#125; file &#123; path =&gt; &quot;/var/log/elasticsearch/chuck-cluster.log&quot; type =&gt; &quot;es-error&quot; start_position =&gt; &quot;beginning&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^\\[&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125; &#125;&#125;output &#123; if [type] == &quot;system&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.56.11:9200&quot;] index =&gt; &quot;system-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125; if [type] == &quot;es-error&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.56.11:9200&quot;] index =&gt; &quot;es-error-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125; if [type] == &quot;nginx-log&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.56.11:9200&quot;] index =&gt; &quot;nginx-log-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125; if [type] == &quot;system-syslog&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;192.168.56.11:9200&quot;] index =&gt; &quot;system-syslog-%&#123;+YYYY.MM.dd&#125;&quot; &#125; &#125;&#125; 5、FilebeatFilebeat工作原理：Filebeat由两个主要组件组成：prospectors 和 harvesters。这两个组件协同工作将文件变动发送到指定的输出中。 Harvester（收割机）： 负责读取单个文件内容。每个文件会启动一个Harvester，每个Harvester会逐行读取各个文件，并将文件内容发送到制定输出中。Harvester负责打开和关闭文件，意味在Harvester运行的时候，文件描述符处于打开状态，如果文件在收集中被重命名或者被删除，Filebeat会继续读取此文件。所以在Harvester关闭之前，磁盘不会被释放。默认情况filebeat会保持文件打开的状态，直到达到close_inactive（如果此选项开启，filebeat会在指定时间内将不再更新的文件句柄关闭，时间从harvester读取最后一行的时间开始计时。若文件句柄被关闭后，文件发生变化，则会启动一个新的harvester。关闭文件句柄的时间不取决于文件的修改时间，若此参数配置不当，则可能发生日志不实时的情况，由scan_frequency参数决定，默认10s。Harvester使用内部时间戳来记录文件最后被收集的时间。例如：设置5m，则在Harvester读取文件的最后一行之后，开始倒计时5分钟，若5分钟内文件无变化，则关闭文件句柄。默认5m）。 Prospector（勘测者）：负责管理Harvester并找到所有读取源。 1`filebeat.prospectors:``- input_type: log`` ``paths:`` ``- /apps/logs/*/info.log` Prospector会找到&#x2F;apps&#x2F;logs&#x2F;*目录下的所有info.log文件，并为每个文件启动一个Harvester。Prospector会检查每个文件，看Harvester是否已经启动，是否需要启动，或者文件是否可以忽略。若Harvester关闭，只有在文件大小发生变化的时候Prospector才会执行检查。只能检测本地的文件。 Filebeat如何记录文件状态： 将文件状态记录在文件中（默认在&#x2F;var&#x2F;lib&#x2F;filebeat&#x2F;registry）。此状态可以记住Harvester收集文件的偏移量。若连接不上输出设备，如ES等，filebeat会记录发送前的最后一行，并再可以连接的时候继续发送。Filebeat在运行的时候，Prospector状态会被记录在内存中。Filebeat重启的时候，利用registry记录的状态来进行重建，用来还原到重启之前的状态。每个Prospector会为每个找到的文件记录一个状态，对于每个文件，Filebeat存储唯一标识符以检测文件是否先前被收集。 Filebeat如何保证事件至少被输出一次： Filebeat之所以能保证事件至少被传递到配置的输出一次，没有数据丢失，是因为filebeat将每个事件的传递状态保存在文件中。在未得到输出方确认时，filebeat会尝试一直发送，直到得到回应。若filebeat在传输过程中被关闭，则不会再关闭之前确认所有时事件。任何在filebeat关闭之前为确认的时间，都会在filebeat重启之后重新发送。这可确保至少发送一次，但有可能会重复。可通过设置shutdown_timeout 参数来设置关闭之前的等待事件回应的时间（默认禁用）。 Filebeat输出到logstash123456789101112131415161718192021222324252627[admin@ris-1 filebeat]$ sudo cat filebeat.ymlfilebeat.inputs:- type: log enabled: true paths: - /var/log/*.log - /var/log/messages fields: service : &quot;ris-1-systemlog-filebeat&quot;filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 1setup.kibana:output.logstash: hosts: [&quot;10.6.75.171:5044&quot;] #logstash地址可以是多个，我这里是本机，其实没必要 #hosts:[&quot;192.168.108.191:5044&quot;, &quot;192.168.108.87:5044&quot;] # 发往二台Logstash-collect loadbalance: true #loadbalance: false # 消息只是往一个logstash里发，如果这个logstash挂了，就会自动将数据发到另一个logstash中。（主备模式） #loadbalance: true # 如果为true，则将数据均分到各个logstash中，挂了就不发了，往存活的logstash里面发送。 worker: 2 #线程数 #compression_level: 3 #压缩级别#output.file:# path: &quot;/tmp/&quot;# filename: filebeat 二、Kafka和ZooKeeper1、架构图 一个典型的 Kafka 体系架构包括若干 Producer（消息生产者），若干 broker（作为 Kafka 节点的服务器），若干 Consumer（Group），以及一个 ZooKeeper 集群。Kafka通过 ZooKeeper 管理集群配置、选举 Leader 以及在 consumer group 发生变化时进行 Rebalance（即消费者负载均衡，在下一课介绍）。Producer 使用 push（推）模式将消息发布到 broker，Consumer 使用 pull（拉）模式从 broker 订阅并消费消息。 2、Kafka中的术语 broker：中间的kafka cluster，存储消息，是由多个server组成的集群。 topic：kafka给消息提供的分类方式。broker用来存储不同topic的消息数据。 producer：往broker中某个topic里面生产数据。 consumer：从broker中某个topic获取数据。 3、topic与消息kafka将所有消息组织成多个topic的形式存储，而每个topic又可以拆分成多个partition，每个partition又由一个一个消息组成。每个消息都被标识了一个递增序列号代表其进来的先后顺序，并按顺序存储在partition中。 这样，消息就以一个个id的方式，组织起来。 producer选择一个topic，生产消息，消息会通过分配策略append到某个partition末尾。 consumer选择一个topic，通过id指定从哪个位置开始消费消息。消费完成之后保留id，下次可以从这个位置开始继续消费，也可以从其他任意位置开始消费。 上面的id在kafka中称为offset，这种组织和处理策略提供了如下好处： 消费者可以根据需求，灵活指定offset消费。 保证了消息不变性，为并发消费提供了线程安全的保证。 每个consumer都保留自己的offset，互相之间不干扰，不存在线程安全问题。 消息访问的并行高效性。 每个topic中的消息被组织成多个partition，partition均匀分配到集群server中。生产、消费消息的时候，会被路由到指定partition，减少竞争，增加了程序的并行能力。 增加消息系统的可伸缩性。 每个topic中保留的消息可能非常庞大，通过partition将消息切分成多个子消息，并通过负责均衡策略将partition分配到不同server。这样当机器负载满的时候，通过扩容可以将消息重新均匀分配。 保证消息可靠性。 消息消费完成之后不会删除，可以通过重置offset重新消费，保证了消息不会丢失。 灵活的持久化策略。 可以通过指定时间段（如最近一天）来保存消息，节省broker存储空间。 备份高可用性。 消息以partition为单位分配到多个server，并以partition为单位进行备份。备份策略为：1个leader和N个followers，leader接受读写请求，followers被动复制leader。leader和followers会在集群中打散，保证partition高可用。 4、Partitions​ 每个Topics划分为一个或者多个Partition,并且Partition中的每条消息都被标记了一个sequential id ,也就是offset,并且存储的数据是可配置存储时间的 5、producerproducer生产消息需要如下参数： topic：往哪个topic生产消息。 partition：往哪个partition生产消息。 key：根据该key将消息分区到不同partition。 message：消息。 6、consumer传统消息系统有两种模式： 队列 发布订阅 kafka通过consumer group将两种模式统一处理：每个consumer将自己标记consumer group名称，之后系统会将consumer group按名称分组，将消息复制并分发给所有分组，每个分组只有一个consumer能消费这条消息。如下图： 于是推理出两个极端情况： 当所有consumer的consumer group相同时，系统变成队列模式 当每个consumer的consumer group都不相同时，系统变成发布订阅 注意： ​ 1、Consumer Groups 提供了topics和partitions的隔离， 如上图Consumer Group A中的consumer-C2挂掉，consumer-C1会接收P1,P2，即一个consumer Group中有其他consumer挂掉后能够重新平衡。如下图： ​ 2、多consumer并发消费消息时，容易导致消息乱序，通过限制消费者为同步，可以保证消息有序，但是这大大降低了程序的并发性。 kafka通过partition的概念，保证了partition内消息有序性，缓解了上面的问题。partition内消息会复制分发给所有分组，每个分组只有一个consumer能消费这条消息。这个语义保证了某个分组消费某个分区的消息，是同步而非并发的。如果一个topic只有一个partition，那么这个topic并发消费有序，否则只是单个partition有序。 一般消息系统，consumer存在两种消费模型： push：优势在于消息实时性高。劣势在于没有考虑consumer消费能力和饱和情况，容易导致producer压垮consumer。 pull：优势在可以控制消费速度和消费数量，保证consumer不会出现饱和。劣势在于当没有数据，会出现空轮询，消耗cpu。 kafka采用pull，并采用可配置化参数保证当存在数据并且数据量达到一定量的时候，consumer端才进行pull操作，否则一直处于block状态。kakfa采用整数值consumer position来记录单个分区的消费状态，并且单个分区单个消息只能被consumer group内的一个consumer消费，维护简单开销小。消费完成，broker收到确认，position指向下次消费的offset。由于消息不会删除，在完成消费，position更新之后，consumer依然可以重置offset重新消费历史消息。 消息发送语义 producer视角 消息最多发送一次：producer异步发送消息，或者同步发消息但重试次数为0。 消息至少发送一次：producer同步发送消息，失败、超时都会重试。 消息发且仅发一次：后续版本支持。 consumer视角 消息最多消费一次：consumer先读取消息，再确认position，最后处理消息。 消息至少消费一次：consumer先读取消息，再处理消息，最后确认position。 消息消费且仅消费一次。 注意： 如果消息处理后的输出端（如db）能保证消息更新幂等性，则多次消费也能保证exactly once语义。 如果输出端能支持两阶段提交协议，则能保证确认position和处理输出消息同时成功或者同时失败。 在消息处理的输出端存储更新后的position，保证了确认position和处理输出消息的原子性（简单、通用）。 可用性 在kafka中，正常情况下所有node处于同步中状态，当某个node处于非同步中状态，也就意味着整个系统出问题，需要做容错处理。 同步中代表了： 该node与zookeeper能连通。 该node如果是follower，那么consumer position与leader不能差距太大（差额可配置）。 某个分区内同步中的node组成一个集合，即该分区的ISR。 kafka通过两个手段容错： 数据备份：以partition为单位备份，副本数可设置。当副本数为N时，代表1个leader，N-1个followers，followers可以视为leader的consumer，拉取leader的消息，append到自己的系统中 failover： 当leader处于非同步中时，系统从followers中选举新leader 当某个follower状态变为非同步中时，leader会将此follower剔除ISR，当此follower恢复并完成数据同步之后再次进入 ISR。 另外，kafka有个保障：当producer生产消息时，只有当消息被所有ISR确认时，才表示该消息提交成功。只有提交成功的消息，才能被consumer消费。 因此，当有N个副本时，N个副本都在ISR中，N-1个副本都出现异常时，系统依然能提供服务。 持久化 基于以下几点事实，kafka重度依赖磁盘而非内存来存储消息。 硬盘便宜，内存贵 顺序读+预读取操作，能提高缓存命中率 在持久化数据结构的选择上，kafka采用了queue而不是Btree kafka只有简单的根据offset读和append操作，所以基于queue操作的时间复杂度为O(1),而基于Btree操作的时间复杂度为O(logN) 7、解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。 不，不可能越过Zookeeper，直接联系Kafka broker。一旦Zookeeper停止工作，它就不能服务客户端请求。 Zookeeper主要用于在集群中不同节点之间进行通信 在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取 除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。 2181 对Client端提供服务的端口。 3888 选举Leader。 2888 集群内的机器通讯使用。（Leader使用此端口） 8、kafka如何彻底删除topic及数据？删除kafka topic及其数据，严格来说并不是很难的操作。但是，往往给kafka 用者带来诸多问题。项目组之前接触过多个开发者，发现都会偶然出现无法彻底删除kafka的情况。本文总结多个删除kafka topic的应用场景，总结一套删除kafka topic的标准操作方法。 step1： 如果需要被删除topic 此时正在被程序 produce和consume，则这些生产和消费程序需要停止。 因为如果有程序正在生产或者消费该topic，则该topic的offset信息会一直在broker更新。调用kafka delete命令则无法删除该topic。 同时，需要设置 auto.create.topics.enable &#x3D; false，默认设置为true。如果设置为true，则produce或者fetch 不存在的topic也会自动创建这个topic。这样会给删除topic带来很多意向不到的问题。 所以，这一步很重要，必须设置auto.create.topics.enable &#x3D; false，并认真把生产和消费程序彻底全部停止。 step2： server.properties配置文件 设置 delete.topic.enable&#x3D;true 如果没有设置 delete.topic.enable&#x3D;true，则调用kafka 的delete命令无法真正将topic删除，而是显示（marked for deletion） step3： 调用命令删除topic： .&#x2F;bin&#x2F;kafka-topics.sh –delete –zookeeper 【zookeeper server:port】 –topic 【topic name】 step4： 删除kafka存储目录（server.properties文件log.dirs配置，默认为”&#x2F;data&#x2F;kafka-logs”）相关topic的数据目录。 注意：如果kafka有多个 broker，且每个broker配置了多个数据盘（比如 &#x2F;data&#x2F;kafka-logs,&#x2F;data1&#x2F;kafka-logs …），且topic也有多个分区和replica，则需要对所有broker的所有数据盘进行扫描，删除该topic的所有分区数据。 step5： 完成之后，调用命令： .&#x2F;bin&#x2F;kafka-topics.sh –list –zookeeper 【zookeeper server:port】 查看现在kafka的topic信息。正常情况下删除的topic就不会再显示。 但是，如果还能够查询到删除的topic，则重启zk和kafka即可。 三、优化：1、为什么要消息队列数据首先通过Filebeat来收集数据，然后经过 Output 插件将数据投递到 Kafka 集群中，这样当遇到 Filebeat接收数据的能力超过 Elasticsearch 集群处理能力的时候，就可以通过队列起到削峰填谷的作用， Elasticsearch 集群就不存在丢失数据的问题。 这种架构适合较大集群的应用部署，通过消息队列解决了消息丢失、数据堆积的问题。 2、kafka和redis对比目前业界在日志服务场景中，使用比较多的两种消息队列为 ：Kafka VS Redis。尽管 ELK Stack 官网建议使用 Redis 来做消息队列，但是我们建议采用 Kafka 。主要从下面两个方面考虑: 1.数据丢失：Redis 队列多用于实时性较高的消息推送，并不保证可靠。Kafka保证可靠但有点延时 2.数据堆积：Redis 队列容量取决于机器内存大小，如果超过设置的Max memory，数据就会抛弃。Kafka 的堆积能力取决于机器硬盘大小 综合上述的理由，我们决定采用 Kafka 来缓冲队列。 3、RabbitMQ和kafka对比：RabbitMQ单机吞吐量：2.6w&#x2F;s；kafka单机吞吐量：10多w&#x2F;s； RabbitMQ支持简单集群，’复制’模式，对高级集群模式支持不好；kafka天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave Kafka的优势在于专为超高吞吐量的实时日志采集； 4、ELK优化Elasticsearch很耗内存、cpu。 标准的建议是把50%的内存给elasticsearch，剩下的50%给Lucene(官方建议:https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html)。 单个es实例内存分配不要超过32G。jdk1.8配置成-Xms32766m -Xmx32766m ES的jvm堆内存不足会有怎样的错误？ 内存的使用和同一时间内的请求数量有关系的，尤其需要评分和排序的时候。 比如说，我分配了10G内存，当请求量不大的时候不会有什么问题，但当请求量大的时候，会怎样呢？ oom，进程直接挂掉；ES节点 JVM挂掉。 ES所在操作系统的内存优化优化设置：禁用swap·分区 通过减低swap分区的使用积极性，永久生效； &#x2F;dev&#x2F;mapper&#x2F;centos-swap swap swap defaults 0 0 #进入&#x2F;etc&#x2F;fstab&#x2F;将其注释 临时生效直接swapoff -a即可 降低swap分区使用积极性，可以控制系统的内存使用空间的阀值；swappiness&#x3D;0表示最大限度使用物理内存，也就是说，当物理内存使用100%之后，才去使用swap交换分区； 比如说，我们现在需要设置系统内存大小阀值，当物理内存使用90%的时候，只剩10%的物理内存，再去使用swap空间 100-10&#x3D;90% # vim &#x2F;etc&#x2F;sysctl.conf vm.swappiness &#x3D; 10 最近发现kibana的日志传的很慢，常常查不到日志，由于所有的日志收集都只传输到了一个logstash进行收集和过滤，所以需要提高logstash的吞吐量。 Logstash性能优化（logstash的吞吐量存在瓶颈）结果：ES的吞吐由每秒9817&#x2F;s提升到41183&#x2F;s logstash.yml配置文件优化： 1、pipline.workers：因为logstash中的grok正则及其消耗系统计算字眼，同时filter也会存在瓶颈，此时增加工作线程，以提高性能 #工作线程数，官方建议是等于CPU内核数 pipeline.workers: 24 # 实际output时的线程数 pipeline.output.workers: 24 #查询一下ES当前的线程情况： GET _nodes&#x2F;stats&#x2F;thread_pool?pretty 其中：”bulk”模板的线程数24，当前活跃的线程数24，证明所有的线程是busy的状态，queue队列214，rejected为30804543。那么问题就找到了，所有的线程都在忙，队列堵满后再有进程写入就会被拒绝，而当前拒绝数为30804543。 优化方案 ●问题找到了，如何优化呢。官方的建议是提高每次批处理的数量，调节传输间歇时间。当batch.size增大，es处理的事件数就会变少，写入也就顺畅了 2、pipeline.batch.size：批量执行event的最大值，该值用于input批量处理事件值， 再打包发送给filter和output.可以提高性能，但是会增加额外的内存开销 # 每次发送的事件数 pipeline.batch.size: 3000 #后来最优的值为10000 3、pipeline.batch.delay:批量处理事件的最大等待值 （input需要按照batch处理的最大发送到消息队列，需要设置一个超时事件） # 发送延时 pipeline.batch.delay: 5 #改成10 Filebeat优化 还记得我们为什么要使用filebeat采集日志数据吗？那是因为Logstash功能虽然强大，但是它依赖于java，在海量日志环境中，logstash进程会消耗更多的系统资源，这将严重的影响业务系统的性能， 而我们说的filebeat是基于go语言，没有任何依赖，配置简单，占用系统资源少， 比logstash更加的轻量级；但是有点还是需要注意。在日志量比较大的情况下或者日志异常突发时， filebeat也会占用大量的系统内存开销，所以说这方面的优化，也是至关重要的 内存优化，Filebeat内存受到两种模式的限制，一种是内存模式，第二种是文件缓存模式，任选其一即可 Kafka的性能优化##每天8个G日志量## 1.Kafka提供两种策略删除旧数据，否则磁盘不够用。 一是基于时间，二是基于Partition文件大小。 可以通过配置server.properties，让Kafka删除一周前的数据，也可在Partition文件超过1GB时删除旧数据，配置如下所示： # The minimum age of a log file to be eligible for deletion log.retention.hours&#x3D;96 （168小时&#x3D;一周，默认是一周，可以调小一点） log.retention.bytes&#x3D;1GB 2.borker进行I&#x2F;O处理的线程数；一般为磁盘个数（默认为8，根据情况调大）num.io.threads&#x3D;8 3.borker进行网络处理的线程数；默认为3，可以调为cpu核数+1；num.network.threads&#x3D;3 4.follow从leader拉取消息进行同步数据的最大字节数：replica.fetch.max.bytes&#x3D;5242880（默认1MB，太小了，改为5MB） 既然我们在ELK中用到了Kafka，那么优化也是必须的，先来回顾一下，kafka是一个高吞吐分布式消息系统，并且提供了持久化，高性能主要表现在以下两点： 第一，磁盘的连续读写性能远远高于随机读写； 第二：拆分一个topic主题分配多个partition分区，这样可以提供并发和吞吐量； 另外，我们的kafka消息读写为什么这么高效？原因何在？ 我们要知道linux系统内核为文件设置一个缓存机制，所有对文件读写的数据内容都会存在着缓存中，称之为：page cache（页缓存） 缓存机制： 当一个文件发生读操作时，系统会先去page cache页缓存中读取，如果找到，便会直接返回，缓存中没有需要读取的数据内容，那么会去磁盘中读取，此时系统写入一份到缓存中。最终返回数据； 当有写操作时，亦是如此，数据会首先写入缓存并进行标识， 等待批量保存到文件系统，减少了磁盘的操作次数和系统额外开销 我们的kafka就是依赖于这种机制，数据的读写交互便是在缓存中完成接力， 不会因为kafka写入磁盘数据影响吞吐量，这就是为什么kafka非常高效的根本原因 topic的拆分： kafka读写单位是partition，将一个topic分配到多个partition可以提高系统的吞吐量，但前提是将不同的partition分配到不同的磁盘上，如果多个partition位于一个磁盘上就会出现多个进程同时对磁盘上多个文件进行读写，这样造成了磁盘的频繁调度，破坏了磁盘读写的连续性 如何实现将不同的partition分配到不同的磁盘上呢？ 我们可以将磁盘上的多个目录配置到broker的log.dirs上 # vim &#x2F;usr&#x2F;local&#x2F;kafka&#x2F;config&#x2F;server.properties log.dirs&#x3D;&#x2F;disk1&#x2F;logs,&#x2F;disk2&#x2F;logs&#x2F;,&#x2F;disk3&#x2F;logs #kafaka在新建partition时，会将partition分布在paritition最少的目录上面，因此，不能将同一个磁盘上的多个目录设置到logs.dirs上 (1) kafka配置参数优化： num.network.threads&#x3D;3 #broker处理消息的最大线程数 num.io.threads&#x3D;8 #broker处理磁盘IO的线程数 一般num.network.threads主要就是处理网络IO，读写缓冲区数据，基本没有IO等待，配置线程数量为CPU核数n+1 num.io.threads主要进行磁盘IO操作，高峰期可以能有些等待，因此配置较大一点，配置线程数量为CPU核数的2~3倍即可 (2) 日志保留策略优化： kafka被大量的写入日志消息后，会生成大量的数据文件，也就是日志消息，这样会占用大量的磁盘空间。 减少日志保留时间，通过log.retention.hours设置，单位是小时 log.retention.hours&#x3D;72 #保留日志数据的时间范围，过后便会删除 段文件大小优化： 段文件配置大小为1GB，这样有利于快速的回收磁盘空间，重启kafka加载也会更快，如果说文件过小，那么文件数量就会较多，kafka启动的时候回单线扫描（log.dir）下的所有文件，文件较多启动较慢，会影响性能， log.segment.bytes&#x3D;1073741824 #段文件最大大小，超过该阀值，会自动创建新的日志段 (3) Logs数据文件写盘策略优化 为了大幅度提高producer写入吞吐量，需要制定定期批量写入文件磁盘的计划 每当producer写入10000条消息事，便会将数据写入磁盘， #log.flush.interval.messages&#x3D;10000 #强行将数据刷新到磁盘之前所能接受的消息数 #log.flush.interval.ms&#x3D;1000 #在强制刷新之前，消息可以停留在日志中最长的时间（单位毫秒，每间隔1秒时间，刷数据到磁盘中） ZooKeeper的配置优化：目的：实现日志自动清理 这两个参数都是在zoo.cfg中配置的： autopurge.purgeInterval 这个参数指定了清理频率，单位是小时，可以填小一点，比如1个小时 四、Logstash过滤+Kafka为什么这么快Java堆栈日志怎么过滤？我看我同事写的，用grok插件正则匹配，日志的第一个字段是”@timestamp”，从这一个时间点开始，到下一个时间点结束。 multiline 插件，匹配多行日志， codec&#x3D;&gt; multiline { ​ pattern &#x3D;&gt; “^[“ ​ negate &#x3D;&gt; true ​ what &#x3D;&gt; “previous” ​ } 对 multiline 插件来说，有三个设置比较重要：negate、pattern 和 what。 negate 默认为 false 否定正则表达式（如果没有匹配的话）。 pattern 类型为 string 没有默认值 要匹配的正则表达式。 what 可以为 previous 或 next 没有默认值 kafka为什么那么快？写入数据： Kafka会把收到的消息都写入到硬盘中，它绝对不会丢失数据。为了优化写入速度Kafka采用了两个技术， 顺序写入 和 MMFile 。 顺序写入： 磁盘读写的快慢取决于你怎么使用它，也就是顺序读写或者随机读写。在顺序读写的情况下，某些优化场景磁盘的读写速度可以和内存持平。 因为硬盘是机械结构，每次读写都会寻址-&gt;写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最讨厌随机I&#x2F;O，最喜欢顺序I&#x2F;O。为了提高读写硬盘的速度，Kafka就是使用顺序I&#x2F;O。 而且Linux对于磁盘的读写优化也比较多，包括read-ahead和write-behind，磁盘缓存等。如果在内存做这些操作的时候，一个是JAVA对象的内存开销很大，另一个是随着堆内存数据的增多，JAVA的GC时间会变得很长，使用磁盘操作有以下几个好处： 磁盘顺序读写速度超过内存随机读写； JVM的GC效率低，内存占用大。使用磁盘可以避免这一问题； 系统冷启动后，磁盘缓存依然可用 五、故障及解决方案：1、启动filebeat将nginx日志文件信息传递给kafka集群时出现如下报错： [root@localhost filebeat-6.5.4-linux-x86_64]# .&#x2F;filebeat -e -c filebeat.yml filebeat先连接14.0.0.20:9092，再连接node2:9092，如果未做本地解析就会无法连接 在kafka的配置文件&#x2F;usr&#x2F;local&#x2F;kafka&#x2F;config&#x2F;server.properties中找到了答案： &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 2、kafka数据堆积： （消费超时死循环）监控kafka的topic消息，里面的消息没有被及时消费，出现报警信息，怎么解决？ 看官方文档上写的kafkaConsumer调用一次轮询方法只是拉取一次消息。客户端为了不断拉取消息，会用一个外部循环不断调用消费者的轮询方法。每次轮询到消息，在处理完这一批消息后，才会继续下一次轮询。但如果一次轮询返回的结构没办法及时处理完成，会有什么后果呢？服务端约定了和客户端max.poll.interval.ms，两次poll最大间隔。如果客户端处理一批消息花费的时间超过了这个限制时间，服务端可能就会把消费者客户端移除掉，并触发rebalance。 解决方案： 1、kafka消费者 默认此间隔时长为300s，本次故障是600s都没处理完成，于是改成3600s max.poll.interval.ms&#x3D;3600000 2、根据逻辑，当处理数据失败后，进行rebalance，跳出该轮回，进行下一项任务，这样也可以解决该问题， 但遗留部分数据异常可能性。 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 3、kafka之消费超时死循环遇见该报错信息及解决 07-28 14:34:46.111 -ERROR 279920[skynet.stream.kfk.consumer-2] skynet.boot.stream.kafka.MyConsumer [188]: kafka consumer poll msg error:org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records. 前提背景： 服务正常输出结果，但是后续任务无法输出结果 根据服务日志发现该报错信息： 这个错误的意思是，消费者在处理完一批poll的消息后，在同步提交偏移量给broker时报的错。初步分析日志是由于当前消费者线程消费的分区已经被broker给回收了，因为kafka认为这个消费者死去。 原因分析问题： 这里就涉及到问题是消费者在创建时会有一个属性max.poll.interval.ms， 该属性意思为kafka消费者在每一轮poll()调用之间的最大延迟,消费者在获取更多记录之前可以空闲的时间量的上限。如果此超时时间期满之前poll()没有被再次调用，则消费者被视为失败，并且分组将重新平衡，以便将分区重新分配给别的成员。 kafkaConsumer调用一次轮询方法只是拉取一次消息。客户端为了不断拉取消息，会用一个外部循环不断调用消费者的轮询方法。每次轮询到消息，在处理完这一批消息后，才会继续下一次轮询。但如果一次轮询返回的结构没办法及时处理完成，会有什么后果呢？服务端约定了和客户端max.poll.interval.ms，两次poll最大间隔。如果客户端处理一批消息花费的时间超过了这个限制时间，服务端可能就会把消费者客户端移除掉，并触发rebalance。 kafka的偏移量(offset)是由消费者进行管理的，偏移量有两种，拉取偏移量(position)与提交偏移量(committed)。拉取偏移量代表当前消费者分区消费进度。每次消息消费后，需要提交偏移量。在提交偏移量时，kafka会使用拉取偏移量的值作为分区的提交偏移量发送给协调者。 如果没有提交偏移量，下一次消费者重新与broker连接后，会从当前消费者group已提交到broker的偏移量处开始消费。 所以，问题就在这里，当我们处理消息时间太长时,已经被broker剔除，提交偏移量又会报错。所以拉取偏移量没有提交到broker，分区又rebalance。下一次重新分配分区时，消费者会从最新的已提交偏移量处开始消费。这里就出现了重复消费的问题。 解决方案： 1、kafka消费者默认此间隔时长为300s，本次故障是600s都没处理完成，于是改成3600s max.poll.interval.ms&#x3D;3600000 2、根据逻辑，当处理数据失败后，进行rebalance，跳出该轮回，进行下一项任务，这样也可以解决该问题，但遗留部分数据异常可能性。 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; 4、如何创建topic？ ###创建topic (只写一个节点就可以,集群之间会自动同步) # 1/usr/local/kafka/bin/kafka-topics.sh --create --zookeeper 14.0.0.10:2181 --replication-factor 3 --partitions 3 --topic topicTest Created topic &quot;topicTest&quot; ###参数解释 12345--topic : 创建topic名为: topicTest --replication-factor : 复制因子为3 --partitions : 3个分区 查看现有的topic： 123/usr/local/kafka/bin/kafka-topics.sh --list --zookeeper 14.0.0.10:2181,14.0.0.20:2181,14.0.0.30:2181 topicTest 查看topic详细信息 123 /usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper 14.0.0.10:2181,14.0.0.20:2181,14.0.0.30:2181 --topic topicTest Topic:topicTest PartitionCount:3 ReplicationFactor:3 Configs: Topic: topicTest Partition: 0 Leader: 2 Replicas: 2,3,1 Isr: 2,3,1 Topic: topicTest Partition: 1 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2 Topic: topicTest Partition: 2 Leader: 1 Replicas: 1,2,3 Isr: 1 返回值解释 # Partition： 分区 # Leader ： 负责读写指定分区的节点 # Replicas ： 复制该分区log的节点列表 # Isr ： 当前活跃的副本列表 5、如何生产消息？1[root@node1 ~]# /usr/local/kafka/bin/kafka-console-producer.sh --broker-list 14.0.0.10:9092,14.0.0.20:9092,14.0.0.30:9092 --topic ConsumerTest &gt;hello kafka And zookeeper &gt;My name is baiyongjie &gt;My blog address is baiyongjie.com &gt;Welcome to visit! 6、如何消费消息？1[root@node2 ~]# /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server 14.0.0.10:9092,14.0.0.20:9092,14.0.0.30:9092 --topic ConsumerTest --from-beginning hello kafka And zookeeper My name is baiyongjie My blog address is baiyongjie.com Welcome to visit !","categories":[{"name":"日志分析","slug":"日志分析","permalink":"http://bingfly.top/categories/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://bingfly.top/tags/ELK/"}]},{"title":"ELK：收集Docker容器日志","slug":"总结/ELK：收集Docker容器日志","date":"2024-06-13T16:00:00.000Z","updated":"2024-12-14T22:25:02.192Z","comments":true,"path":"2024/06/14/总结/ELK：收集Docker容器日志/","permalink":"http://bingfly.top/2024/06/14/%E6%80%BB%E7%BB%93/ELK%EF%BC%9A%E6%94%B6%E9%9B%86Docker%E5%AE%B9%E5%99%A8%E6%97%A5%E5%BF%97/","excerpt":"","text":"ELK：收集Docker容器日志简介 随着容器化时代的到来，容器化部署成为一种很方便的部署方式，收集容器日志也成为刚需。本篇文档从 容器化部署ELK系统，收集容器日志，自动建立项目索引，ElastAlert日志监控报警，定时删除过期日志索引文件 这几个方面来介绍ELK。 大部分配置方法多是看官方文档，理解很辛苦，查了很多文章，走了很多弯路，分享出来，希望让有此需求的朋友少走弯路，如有错误或理解不当的地方，请批评指正。 逻辑结构如下图 ELK之容器日志收集及索引建立 Docker环境下，拉取官方镜像，官方镜像地址 Docker @ Elastic 1234docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.2docker pull docker.elastic.co/kibana/kibana:6.6.2docker pull docker.elastic.co/beats/filebeat:6.6.2docker pull docker.elastic.co/logstash/logstash:6.6.2 Elasticsearch 容器部署Elasticsearch 启动命令说明 启动两个端口，9200为数据查询和写入端口，也即是业务端口，9300为集群端口，同步集群数据，此处我单节点部署 指定日志输出格式为json格式，默认情况下也为json格式，默认输出路径 /var/lib/docker/containers/*/*.log 日志文件最多保留三个，每个最多10M 容器开机自启 传递参数为单节点部署 数据存储映射至宿主机 需要给 /data/elasticsearch 赋予权限，否则报权限不足的错误 1234567891011121314docker run -d \\ --user root \\ -p 127.0.0.1:9200:9200 \\ -p 9300:9300 \\ --log-driver json-file \\ --log-opt max-size=10m \\ --log-opt max-file=3 \\ --name elasticsearch \\ --restart=always \\ -e &quot;discovery.type=single-node&quot; \\ -v /data/elasticsearch:/usr/share/elasticsearch/data \\ docker.elastic.co/elasticsearch/elasticsearch:6.6.2 chmod 777 -R /data/elasticsearch Logstash 容器部署Logstash 配置文件说明 配置文件映射至至宿主机 match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:log-timestamp&#125; %&#123;LOGLEVEL:log-level&#125; %&#123;JAVALOGMESSAGE:log-msg&#125;&quot; &#125; 将日志做简单拆分, 时间戳重命名为 log-timestamp，日志级别重命名为 log-msg，如果拆分成功输出日志中就会产生这两个字段 通过 remove_field =&gt; [&quot;beat&quot;] 移除无用字段 输出到elasticsearch 索引通过 容器名称-时间 建立 12345678910111213141516171819202122232425mkdir -pv /data/confcat &gt; /data/conf/logstash.conf &lt;&lt; &quot;EOF&quot;input &#123; beats &#123; host =&gt; &quot;0.0.0.0&quot; port =&gt; &quot;5043&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:log-timestamp&#125; %&#123;LOGLEVEL:log-level&#125; %&#123;JAVALOGMESSAGE:log-msg&#125;&quot; &#125; &#125; mutate &#123;# remove_field =&gt; [&quot;message&quot;] remove_field =&gt; [&quot;beat&quot;] &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [ &quot;elasticsearch:9200&quot; ] index =&gt; &quot;%&#123;containername&#125;-%&#123;+YYYY.MM.dd&#125;&quot; &#125;&#125;EOF Logstash 启动命令说明 通过 --link elasticsearch 连接 elasticsearch 容器，并生成环境变量在容器中使用 配置文件映射如容器 12345678910docker run -p 5043:5043 -d \\ --user root \\ --log-driver json-file \\ --log-opt max-size=10m \\ --log-opt max-file=3 \\ --name logstash \\ --link elasticsearch \\ --restart=always \\ -v /data/conf/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\ docker.elastic.co/logstash/logstash:6.6.2 Kibana容器部署 启动参数参考上述组件 123456789101112docker run -p 5601:5601 -d \\ --user root \\ --log-driver json-file \\ --log-opt max-size=10m \\ --log-opt max-file=3 \\ --name kibana \\ --link elasticsearch \\ --restart=always \\ -e ELASTICSEARCH_URL=http://elasticsearch:9200 \\ -v /data/conf/kibana.yml:/usr/share/kibana/config/kibana.yml \\ -v /data/elastalert-kibana-plugin:/usr/share/kibana/plugins/elastalert-kibana-plugin \\ docker.elastic.co/kibana/kibana:6.6.2 通过服务器IP地址即可访问Kibana web http://IP:5601 Filebeat 容器部署Filebeat 配置文件说明 Filebeat是一个轻量级的日志收集工具，是Elastic的Beat组成员，默认情况下，docker输出的日志格式为json格式，需要解码才方便查看日志的路径为 /var/lib/docker/containers/*/*.log - type: log 选择filebeat输入类型选择log，之后通过解码json的配置将docker输出的日志解码，输入类型也可选择docker，但是我试过目前版本该数据类型有些功能受限，比如后面rename时，只能rename一个 json解码： 必须至少指定设置其中之一来启用JSON解析模式，建议三行都加上，不然，有些功能会有问题 json.overwrite_keys #如果启用了keys_under_root和此设置，那么来自解码的JSON对象的值将覆盖Filebeat通常添加的字段（type, source, offset,等）以防冲突。 json.add_error_key #如果启用此设置，则在出现JSON解组错误或配置中定义了message_key但无法使用的情况下，Filebeat将添加“error.message”和“error.type：json”键。 json.message_key #一个可选的配置设置，用于指定应用行筛选和多行设置的JSON密钥。 如果指定，键必须位于JSON对象的顶层，且与键关联的值必须是字符串，否则不会发生过滤或多行聚合。 多行合并： 将多行日志合并成一条，比如java的报错日志，规则为，如果不是以四个数字，即年份开头的日志合并到上一条日志当中 排除或删除通配符行： 即排除含有相关关键字的行，或只收集含有相关关键字的行 排除通配文件： 即不读取该含该字符的文件的日志 添加docker元数据： 将docker容器的相关数据收集起来，方便作为索引的字段， match_source_index: 4 为获取docker数据的索引，其数据为json格式，索引太低可能无法获取关键docker数据 处理器重命名和删除无用字段： - rename: 将上面收集的元数据重命名来提升其索引等级，以便交给logstash来处理，其中 docker.container.name 为docker容器的名字，重命名为 containername字段， - drop_fields: 清除掉无用索引数据，不输出到 output 日志输出：日志输出的几种方式 output.file 输出到文件，output.console 输出到标准输出，通过 docker logs containName 查看，该两种输出方便调试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051cat &gt; /data/conf/filebeat.yml &lt;&lt; &quot;EOF&quot;filebeat.inputs:- type: log paths: - &#x27;/var/lib/docker/containers/*/*.log&#x27;# json解码 json.add_error_key: true json.overwrite_keys: true json.message_key: log# 多行合并# multiline:# pattern: ^\\d&#123;4&#125;# negate: true# match: after## 排除或删除通配符行# exclude_lines: [&#x27;^DBG&#x27;]# include_lines: [&#x27;ERROR&#x27;, &#x27;WARN&#x27;, &#x27;INFO&#x27;]# 排除通配文件# exclude_files: [&#x27;\\.gz$&#x27;]## 添加docker元数据 processors: - add_docker_metadata: match_source: true# 选取添加docker元数据的层级 match_source_index: 4# 处理器重命名和删除无用字段processors:- rename: fields: - from: &quot;json.log&quot; to: &quot;message&quot; - from: &quot;docker.container.name&quot; to: &quot;containername&quot; - from: &quot;log.file.path&quot; to: &quot;filepath&quot;- drop_fields: fields: [&quot;docker&quot;,&quot;metadata&quot;,&quot;beat&quot;,&quot;input&quot;,&quot;prospector&quot;,&quot;host&quot;,&quot;source&quot;,&quot;offset&quot;]# 日志输出output.logstash: # 输出地址 hosts: [&quot;192.168.30.42:5043&quot;]#output.elasticsearch:# hosts: [&quot;192.168.30.42:9200&quot;]# protocol: &quot;http&quot;#output.file:# path: &quot;/tmp&quot;# filename: filebeat.out#output.console:# pretty: trueEOF Filebeat 启动命令 -v /var/lib/docker/containers/:/var/lib/docker/containers/ 映射日志目录 -v /var/run/docker.sock:/var/run/docker.sock:ro 映射docker套接字文件，来收集docker信息，添加docker元数据 1234567891011docker run -d \\ --name filebeat \\ --user root \\ --log-driver json-file \\ --log-opt max-size=10m \\ --log-opt max-file=3 \\ --restart=always \\ -v /data/conf/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro \\ -v /var/lib/docker/containers/:/var/lib/docker/containers/ \\ -v /var/run/docker.sock:/var/run/docker.sock:ro \\ docker.elastic.co/beats/filebeat:6.6.2 Filebeat是日志收集客户端，正常情况下其部署在需要收集日志的主机上 添加kibana索引 通过服务器IP地址访问Kibana web http://IP:5601 同时添加索引 如此便可查看相关日志 ELK之ElastAlert日志告警 ElastAlert分为两部分，后端程序，项目地址 https://github.com/bitsensor/elastalert，Kibana前端页面插件地址 https://github.com/bitsensor/elastalert-kibana-plugin ElastAlert目前支持的报警类型： any：只要有匹配就报警； blacklist：compare_key字段的内容匹配上 blacklist数组里任意内容； whitelist：compare_key字段的内容一个都没能匹配上whitelist数组里内容； change：在相同query_key条件下，compare_key字段的内容，在 timeframe范围内发送变化； frequency：在相同 query_key条件下，timeframe 范围内有num_events个被过滤出来的异常； spike：在相同query_key条件下，前后两个timeframe范围内数据量相差比例超过spike_height。其中可以通过spike_type设置具体涨跌方向是up,down,both 。还可以通过threshold_ref设置要求上一个周期数据量的下限，threshold_cur设置要求当前周期数据量的下限，如果数据量不到下限，也不触发； flatline：timeframe 范围内，数据量小于threshold阈值； new_term：fields字段新出现之前terms_window_size(默认30天)范围内最多的terms_size (默认50)个结果以外的数据； cardinality：在相同 query_key条件下，timeframe范围内cardinality_field的值超过 max_cardinality 或者低于min_cardinality Percentage Match: 在buffer_time 中匹配所设置的字段的百分比高于或低于阈值时，此规则将匹配。默认情况下为全局的buffer_time ElastAlert容器启动 -p 3030:3030 指定映射端口，该端口需要kibana调用 -v config/elastalert.yaml:/opt/elastalert/config.yaml 映射ElastAlert的配置文件 -v rules:/opt/elastalert/rules 映射报警规则文件目录 --link elasticsearch ElastAlert需要连接ElasticSearch 进行数据的查询 rule_templates:/opt/elastalert/rule_templates 该目录下为报警规则的模版，可使用其配置并放入 rules 使其生效，具体配置规则参考 https://elastalert.readthedocs.io/en/latest/ruletypes.html 1234567891011git clone https://github.com/bitsensor/elastalert.git; cd elastalertsed -i -e &#x27;s/localhost/elasticsearch/g&#x27; `pwd`/config/elastalert.yamldocker run -d \\ -p 3030:3030 \\ -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \\ -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \\ -v `pwd`/rules:/opt/elastalert/rules \\ -v `pwd`/rule_templates:/opt/elastalert/rule_templates \\ --link elasticsearch \\ --name elastalert \\ bitsensor/elastalert:latest 可通过 elastalert-test-rule rules/rule1.yaml 在 elastalert 容器内测试报警规则 Kibana 需要更改配置 添加 elastalert 相关的配置，包括主机名和端口 123456789cat &gt; /data/conf/kibana.yml &lt;&lt; &quot;EOF&quot;# Default Kibana configuration from kibana-docker.server.name: kibanaserver.host: &quot;0&quot;elasticsearch.hosts: http://elasticsearch:9200xpack.monitoring.ui.container.elasticsearch.enabled: trueelastalert-kibana-plugin.serverHost: elastalertelastalert-kibana-plugin.serverPort: 3030 Kibana 启动参数更改 配置 Kibana的插件 elastalert-kibana-plugin 需要注意的是，插件的下载地址 https://github.com/bitsensor/elastalert-kibana-plugin/releases 需要找到对应自己 kibana 版本，否则插件无法安装成功 从目前版本来看，插件的使用不是很方便，其相当于在 ElasticAlert 启动时 -v rules:/opt/elastalert/rules 目录下写文件 -v /data/kibana/elastalert-kibana-plugin:/usr/share/kibana/plugins/elastalert-kibana-plugin 映射插件目录 12345678910111213141516cd /datawget https://github.com/bitsensor/elastalert-kibana-plugin/releases/download/1.0.2/elastalert-kibana-plugin-1.0.2-6.6.2.zipunzip elastalert-kibana-plugin-1.0.2-6.6.2.zipdocker run -p 5601:5601 -d \\ --user root \\ --log-driver json-file \\ --log-opt max-size=10m \\ --log-opt max-file=3 \\ --name kibana \\ --link elasticsearch \\ --link elastalert \\ --restart=always \\ -e ELASTICSEARCH_URL=http://elasticsearch:9200 \\ -v /data/conf/kibana.yml:/usr/share/kibana/config/kibana.yml \\ -v /data/kibana/elastalert-kibana-plugin:/usr/share/kibana/plugins/elastalert-kibana-plugin \\ docker.elastic.co/kibana/kibana:6.6.2 启动后，可查看 http://IP:5601，可通过此来创建报警规则 报警规则配置 这里给出一个报警规则的示例，更多请查看文档 https://elastalert.readthedocs.io/en/latest/ruletypes.html es_host: elasticsearch 该配置会使用docker容器中的变量 name: filebeat info frequency rule 报警规则的名称，需要唯一 timeframe: 设定查询的时间尺度，这里指定为5分钟 filter 指定报警条件，示例为日志中message字段还有INFO信息就示作匹配 alert 指定报警方式，报警方式有很多，这里使用slack，可查看文档配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Alert when the rate of events exceeds a threshold# (Optional)# Elasticsearch host# es_host: elasticsearch.example.comes_host: elasticsearch# (Optional)# Elasticsearch port# es_port: 14900es_port: 9200# (OptionaL) Connect with SSL to Elasticsearch#use_ssl: True# (Optional) basic-auth username and password for Elasticsearch#es_username: someusername#es_password: somepassword# (Required)# Rule name, must be uniquename: filebeat info frequency rule# (Required)# Type of alert.# the frequency rule type alerts when num_events events occur with timeframe timetype: frequency# (Required)# Index to search, wildcard supportedindex: filebeat-*# (Required, frequency specific)# Alert when this many documents matching the query occur within a timeframenum_events: 1# (Required, frequency specific)# num_events must occur within this amount of time to trigger an alerttimeframe:# hours: 1 minutes: 5# (Required)# A list of Elasticsearch filters used for find events# These filters are joined with AND and nested in a filtered query# For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.htmlfilter:- query: query_string: query: &quot;message:INFO&quot;# (Required)# The alert is use when a match is foundalert:- &quot;slack&quot;alert_subject: &quot;&#123;&#125; @&#123;&#125;&quot;alert_subject_args: - containername - log-timestampalert_text_type: alert_text_onlyalert_text: | &gt; 【Name】: &#123;&#125; &gt; 【Log-Msg】: &#123;&#125;alert_text_args: - containername - messageslack_webhook_url: &quot;https://hooks.slack.com/services/xxxx/xxxx/xxxxxx&quot;slack_title_link: &quot;https://elk.glinux.top&quot;slack_title: &quot;ELK URL&quot;slack_username_override: &quot;ELK Alert&quot;slack_channel_override: &quot;#filebeat-test&quot; 报警的效果如图 定时删除过期日志索引文件 该脚本用于删除7天以前的索引文件（过月日志清除），脚本为参考他人作品，做了少许更改，可查看参考文档 curl -s &quot;http://127.0.0.1:9200/_cat/indices?v&quot; 该命令可以查看es中的索引 curl -XDELETE -s &quot;http://127.0.0.1:9200/filebeat-2019.04.27&quot; 该命令用于删除es中的 filebeat-2019.04.27 索引 12345678910111213141516171819202122232425262728293031323334353637383940cat &gt; /data/delete_index.sh &lt;&lt; &quot;EOF&quot;#!/bin/bashelastic_url=127.0.0.1elastic_port=9200user=&quot;username&quot;pass=&quot;password&quot;log=&quot;/data/log/delete_index.log&quot;# 当前日期时间戳减去索引名时间转化时间戳是否大于1dateDiff ()&#123; dte1=$1 dte2=$2 diffSec=$((dte1-dte2)) if ((diffSec &gt; 6)); then echo &quot;1&quot; else echo &quot;0&quot; fi&#125;# 循环获取索引文件，通过正则匹配过滤for index in $(curl -s &quot;$&#123;elastic_url&#125;:$&#123;elastic_port&#125;/_cat/indices?v&quot; | awk -F&#x27; &#x27; &#x27;&#123;print $3&#125;&#x27; | grep -E &quot;[0-9]&#123;4&#125;.[0-9]&#123;2&#125;.[0-9]&#123;2&#125;$&quot;); do# 获取索引文件日期，并转化格式 date=$(echo $index | awk -F&#x27;-&#x27; &#x27;&#123;print $NF&#125;&#x27; | sed -n &#x27;s/\\.//p&#x27; | sed -n &#x27;s/\\.//p&#x27;)# 获取当前日期 cond=$(date &#x27;+%Y%m%d&#x27;)# 根据不同服务器，计算不同数值 diff=$(dateDiff &quot;$&#123;cond&#125;&quot; &quot;$&#123;date&#125;&quot;)# 打印索引名和结果数值 #echo -n &quot;$&#123;index&#125; ($&#123;diff&#125;)&quot;# 判断结果值是否大于等于1 if [ $diff -eq 1 ]; then curl -XDELETE -s &quot;$&#123;elastic_url&#125;:$&#123;elastic_port&#125;/$&#123;index&#125;&quot; &amp;&amp; echo &quot;$&#123;index&#125; 删除成功&quot; &gt;&gt; $log || echo &quot;$&#123;index&#125; 删除失败&quot; &gt;&gt; $logfidoneEOF 该脚本可以做成定时任务，每天都执行一次，来删除过期数据 参考文档 ELK 使用Docker搭建ELK日志系统 https://zhuanlan.zhihu.com/p/32559371 Beats详解（四) http://www.51niux.com/?id=204 Offical document: https://www.elastic.co/guide/index.html 将日志输出到Docker容器外: https://www.jianshu.com/p/bf2eb121ac62 elk日志大盘显示和日志监控报警配置实践: https://blog.csdn.net/yujiak/article/details/79897408 Elastalert elastalert github地址： https://github.com/bitsensor/elastalert elastalert-kibana-plugin github地址: https://github.com/bitsensor/elastalert-kibana-plugin/tree/1.0.2 ElastAlert 官方操作文档：https://elastalert.readthedocs.io/en/latest/ruletypes.html 索引定期清除 历史索引删除 http://www.cenhq.com/2017/11/07/elasticsearch-deletes-index-by-date/ 其他 Centos 7 Docker命令自动补全 https://medium.com/@ismailyenigul/enable-docker-command-line-auto-completion-in-bash-on-centos-ubuntu-5f1ac999a8a6 Searchguard 插件支持OpenLdap：https://github.com/floragunncom/search-guard-kibana-plugin 设置登录认证: https://birdben.github.io/2017/02/08/Kibana/Kibana学习（六）Kibana设置登录认证 SSL免费证书 https://certbot.eff.org/lets-encrypt/ubuntuxenial-nginx","categories":[{"name":"日志分析","slug":"日志分析","permalink":"http://bingfly.top/categories/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"http://bingfly.top/tags/ELK/"}]},{"title":"普罗米修斯","slug":"总结/普罗米修斯","date":"2024-06-13T16:00:00.000Z","updated":"2024-12-14T22:20:08.775Z","comments":true,"path":"2024/06/14/总结/普罗米修斯/","permalink":"http://bingfly.top/2024/06/14/%E6%80%BB%E7%BB%93/%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF/","excerpt":"","text":"Prometheus1、特点Prometheus 相比于其他传统监控工具主要有以下几个特点： 具有由 metric 名称和键值对标识的时间序列数据的多维数据模型 有一个灵活的查询语言 不依赖分布式存储，只和本地磁盘有关 通过 HTTP 的服务拉取时间序列数据 也支持推送的方式来添加时间序列数据 还支持通过服务发现或静态配置发现目标 多种图形和仪表板支持 2、组件 Prometheus Server：用于抓取指标、存储时间序列数据 exporter：暴露指标让任务来抓 pushgateway：push 的方式将指标数据推送到该网关 alertmanager：处理报警的报警组件 adhoc：用于数据查询 3、监控应用的两种方式1、暴露metrics接口给promethus 2、通过exporter 4、对集群节点的监控1、对于集群的监控一般我们需要考虑以下几个方面： Kubernetes 节点的监控：比如节点的 cpu、load、disk、memory 等指标 内部系统组件的状态：比如 kube-scheduler、kube-controller-manager、kubedns&#x2F;coredns 等组件的详细运行状态 编排级的 metrics：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标 2、由于我们要获取到的数据是主机的监控指标数据，而我们的 node-exporter 是运行在容器中的，所以我们在 Pod 中需要配置一些 Pod 的安全策略，这里我们就添加了 hostPID: true、hostIPC: true、hostNetwork: true 3个策略，用来使用主机的 PID namespace、IPC namespace 以及主机网络，这些 namespace 就是用于容器隔离的关键技术，要注意这里的 namespace 和集群中的 namespace 是两个完全不相同的概念。 另外我们还将主机的 /dev、/proc、/sys这些目录挂载到容器中，这些因为我们采集的很多节点数据都是通过这些文件夹下面的文件来获取到的，比如我们在使用 top 命令可以查看当前 cpu 使用情况，数据就来源于文件 /proc/stat，使用 free 命令可以查看当前内存使用情况，其数据来源是来自 /proc/meminfo 文件。 另外由于我们集群使用的是 kubeadm 搭建的，所以如果希望 master 节点也一起被监控，则需要添加相应的容忍 3、让 Prometheus 去自动发现我们节点的 node-exporter 程序，并且按节点进行分组呢？这就是 Prometheus 里面非常重要的服务发现功能了。 在 Kubernetes 下，Promethues 通过与 Kubernetes API 集成，主要支持5中服务发现模式，分别是：Node、Service、Pod、Endpoints、Ingress。 要让 Prometheus 也能够获取到当前集群中的所有节点信息的话，我们就需要利用 Node 的服务发现模式，同样的，在 prometheus.yml 文件中配置如下的 job 任务即可： 1234- job_name: &#x27;kubernetes-nodes&#x27; kubernetes_sd_configs: - role: node 通过指定 kubernetes_sd_configs 的模式为node，Prometheus 就会自动从 Kubernetes 中发现所有的 node 节点并作为当前 job 监控的目标实例，发现的节点 /metrics 接口是默认的 kubelet 的 HTTP 接口。 Prometheus Operator 上图是 Prometheus-Operator 官方提供的架构图，各组件以不同的方式运行在 Kubernetes 集群中，其中 Operator 是最核心的部分，作为一个控制器，他会去创建Prometheus、ServiceMonitor、AlertManager 以及 PrometheusRule 4个 CRD 资源对象，然后会一直监控并维持这4个资源对象的状态。 Operator：根据自定义资源来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。 Prometheus：声明 Prometheus 资源对象期望的状态，Operator 确保这个资源对象运行时一直与定义保持一致。 Prometheus Server：Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。 ServiceMonitor：声明指定监控的服务，描述了一组被 Prometheus 监控的目标列表，就是 exporter 的抽象，用来提供 metrics 数据接口的工具。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。 Service：简单的说就是 Prometheus 监控的对象。 Alertmanager：定义 AlertManager 资源对象期望的状态，Operator 确保这个资源对象运行时一直与定义保持一致。 Promethus(普罗米修斯）监控Mysql数据库prometheus 监控mysql监控什么Mysql所在的节点服务器的Ⅰ&#x2F;O读写压力磁盘使用量、内存使用量（阈值-比如该节点的磁盘使用量是否超过总量的80%)监控mysql的健康状态（是否宕机)Mysql所在的节点,其他的服务组件mysql所在的服务器的网络状态（传输的速率波动) 被监控的mysql服务器安装mysql_exporter，安装MariaDB 数据库，授权为localhost（因为不是prometheus服务器来直接找mariadb 获取数据，而是prometheus服务器找mysql_exporter,mysql_exporter 再找mariadb。所以这个localhost是指的mysql_exporter的IP），创建一个mariadb配置文件，启动mysqld_exporter，确认端口(9104)，在prometheus服务器的配置文件里添加被监控的mariadb的配置段 1234vim /usr/local/prometheus/prometheus.yml - job_name: &#x27;mariadb&#x27; static_configs: - targets: [&#x27;192.168.116.130:9104&#x27;] 重启服务回到web管理界面 –》点Status –》点Targets –》可以看到监控 mariadb了 https://my.oschina.net/u/4314581/blog/3316484","categories":[{"name":"监控","slug":"监控","permalink":"http://bingfly.top/categories/%E7%9B%91%E6%8E%A7/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://bingfly.top/tags/Prometheus/"}]},{"title":"Kubernetes","slug":"总结/1、k8s","date":"2024-06-12T16:00:00.000Z","updated":"2024-12-14T22:27:41.882Z","comments":true,"path":"2024/06/13/总结/1、k8s/","permalink":"http://bingfly.top/2024/06/13/%E6%80%BB%E7%BB%93/1%E3%80%81k8s/","excerpt":"","text":"[TOC] 一、K8S基于Google公司Borg用go语言翻写的 1、k8s特性Kubernetes是自动化容器编排的开源平台，目标是让部署容器化的应用简单并且高效，提供了应用部署，规划，更新，维护的一种机制，轻量级，消耗资源小，开源，，弹性伸缩，负载均衡：IPVS Kubernetes一个核心的特点就是可以让容器按照用户的期望状态运行 关闭虚拟内存，防止容器运行在虚拟内存中 2、K8S各组件功能 1234567etcd：键值对数据库 储存K8S集群所有重要信息（持久化）保存了整个集群的状态kube-apiserver：所有服务访问统一入口，并提供认证、授权、访问控制、API注册和发现等机制kube-controller-manager：是与底层云计算服务商交互的管理控制器，维持副本期望数目kub-scheduler：负责接受任务，调度资源，选择合适的节点进行分配任务，或者说，按照预定的调度策略将Pod调度到相应的机器上kubelet：直接跟容器引擎交互实现容器的生命周期管理，同时也负责Volume和网络的管理kube-proxy：负责为Service提供内部的服务发现和负载均衡，并维护网络规则（写入规则至 IPTABLES、IPVS 实现服务映射访问的）container-runtime：是负责管理运行容器的软件，比如docker 123Node（节点）是k8s集群中相对于Master而言的工作主机。Node可以是一台物理主机，也可以是一台虚拟机（VM）。在每个Node上运行用于启动和管理Pid的服务Kubelet，并能够被Master管理。在Node上运行的服务进行包括Kubelet、kube-proxy和docker daemon。 1Pod：k8s基本管理单元，容器的集合，可以理解为多个linux命名空间的联合，包括PID命名空间同一个Pod中的容器可以互相看到PID、网络命名空间同一个Pod中的容器可以使用同一IP 其他： 123456coreDNS：可以为集群中的SVC创建一个域名IP的对应关系解析，实现负载均衡中的功能dashboard：给 K8S 集群提供一个 B/S 结构访问体系，网页UI界面ingress controller：官方只能实现四层代理，INGRESS 可以实现七层代理federation：提供一个可以跨集群中心多K8S统一管理功能Prometheus：提供K8S集群的监控能力ELK：提供 K8S 集群日志统一分析介入平台 高可用集群副本数量最好是 &gt;&#x3D;3 的奇数个 3、查看日志命令有两种日志方案在工作，默认为rsyslogd和systemd journald的两种方式会占用内存，所以改为使用systemd journald 123456789journalctl -u kube-schedulerjournalctl -xefu kubeletjournalctl -u kube-apiserverjournalctl -u kubelet |tailjournalctl -xe 二、Pod1、概念Pod：k8s基本管理单元，容器的集合，可以理解为多个linux命名空间的联合，包括PID命名空间同一个Pod中的容器可以互相看到PID、网络命名空间同一个Pod中的容器可以使用同一IP（共用pause的网络栈） 自主式Pod： 由控制器管理的Pod： 2、优点Pod中的容器可以共用Pod提供的基础设施 Pod的生命周期与管理器的生命周期的分离 调度和管理的易用性，解偶控制器和服务，后段管理器仅仅监控Pod 3、创建和删除Pod的流程创建： 12345678910111、客户端提交Pod的配置信息（可以是Deployment定义好的信息）到kube-apiserver，kube-apiserve会把Pod信息存储到ETCD当中2、kube-scheduler 检测到Pod信息会开始调度3、kube-scheduler 开始调度预选，主要是过滤掉不符合Pod要求的节点4、kube-scheduler 开始调度调优，主要是会给节点打分以选择更加适合的节点5、kube-scheduler 选择好节点后会把结果存储到ETCD6、kubelet 根据调度结果执行Pod创建操作 删除： 1234567891011kube-apiserver会接受到用户的删除指令，默认有30秒时间等待优雅退出，超过30秒会被标记为死亡状态此时Pod的状态是Terminating，Kubelet看到Pod标记为Terminating开始了关闭Pod的工作1、Pod从service的列表中被删除2、如果该Pod定义了一个停止前的钩子，其会在pod内部被调用，停止钩子一般定义了如何优雅结束进程3、进程被发送TERM信号（kill -14）4、当超过优雅退出时间时，Pod中的所有进程都很被发送SIGKILL信号（kill -9） 4、Pod通讯1）、Pod中容器互相通讯①、localhost 1234pod内部的容器是共享网络名称空间的，所以容器直接可以使用localhost访问其他容器；k8s在启动容器的时候会先启动一个Pause容器，这个容器就是实现这个功能的。pause：每个Pod里运行着一个特殊的被称之为Pause的容器，其他容器则为业务容器，这些业务容器共享Pause容器的网络栈和Volume存储卷，因此他们之间通信和数据交换更为高效，在设计时我们可以充分利用这一特性将一组密切相关的服务进程放入同一个Pod中。 ②、环回网口 ③、挂载的数据卷 port、nodeport、targetport、containerport分别是什么 2）、Pod1与Pod2在同一台机器由Docker0网桥直接转发请求至Pod2，不需要经过Flannel 3）、Pod1与Pod2不在同一台主机 这种情况k8s官方推荐的是使用flannel网络，pod的ip分配由flannel统一分配，通讯过程也是走flannel的网桥方式。 Pod的地址和docker0在同一个网段，但docker网段与宿主机网卡是两个完全不同的IP网段，并且不同Node之间的通信只能通过宿主机的物理网卡进行。将Pod的IP和所在Node的IP关联起来，通过这个关联可以让Pod互相访问 Web app2 —–&gt; Backend 通讯 1数据包源地址写自己的docker0网段的地址，目标写目标的ip地址，因为不在同一网段，所以发到docker0，docker0上有对应的钩子函数把它抓到Flannel0，Flannel由路由表记录（从etcd里获取到的）写入到当前的主机，判断路由到哪一台机器，由Flannel0到Flanneld后对这个数据报文进行封装，由mac到三层，源为本node的IP地址，目为目标node的IP地址，下一层封装时UDP报文（因为flannel使用的是UDP保温去转发这些数据包的，因为更快，毕竟在同一局域网，再下一层，又封装了一层新的三层信息，源为源Pod的docker网段网址，目为目标Pod的docker网段地址，外面封装了一层数据包实体（Payload），然后数据帧被转发到目标node的网卡上，然后到Flanneld上，然后拆封，转发到Flannel0，再转发到docker0，由docker转发到目标Pod 4）、Pod至Service网络由各节点的iptables维护和转发，还有一种方法是LVS进行转发（转发效率更高，上限更高） 5）、Pod至外网Pod向外网发送请求，查找路由表，转发数据包到宿主机的网卡，宿主网卡完成路由选择后，iptables执行Masquerade，把源IP更改为宿主网卡的IP，然后向外网服务器发送请求 6）外网访问Podkubernetes集群上运行的pod，在集群内访问是很容易的，最简单的，可以通过pod的ip来访问，也可以通过对应的svc来访问,但在集群外，由于kubernetes集群的pod ip地址是内部网络地址，因此从集群外是访问不到的。 为了解决这个问题，kubernetes提供了如下几个方法。 hostNetwork hostPort service NodePort 1、hostNetwork: truehostNetwork为true时，容器将使用宿主机node的网络，因此，只要知道容器在哪个node上运行，从集群外以 node-ip + port 的方式就可以访问容器的服务。 123456789apiVersion: v1kind: Podmetadata: name: nginxspec: hostNetwork: true containers: - name: nginx image: nginx pod启动后,如下,可以看到pod的ip地址与node节点的地址是一致的 1234567[root@localhost ~]# kubectl get pods -o wide nginxNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx 1/1 Running 0 8m2s 192.168.10.10 minikube &lt;none&gt;[root@localhost ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEminikube Ready master 11d v1.12.1 192.168.10.10 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://17.12.1-ce 可以通过curl或者浏览器直接访问node节点的地址，即可以访问到nginx的服务 12345678[root@localhost ~]# curl http://192.168.10.10&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;......&lt;/html&gt;[root@localhost ~]# hostNetwork的优点是直接使用宿主机的网络，只要宿主机能访问，Pod就可以访问；但缺点也是很明显的： 易用性：Pod漂移到其他node上，访问时需要更换ip地址。解决方法是将Pod绑定在某几个node上，并在这几个node上运行keepalived以漂移vip，从而客户端可以使用vip+port的方式来访问。 易用性：Pod间可能出现端口冲突，造成Pod无法调度成功。 安全性：Pod可以直接观察到宿主机的网络。 2、hostPorthostPort的效果与hostNetwork类似，hostPort是直接将容器的端口与所调度的节点上的端口进行映射，这样用户就可以通过宿主机的IP加上映射到绑定主机的端口来访问Pod了 1234567891011apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80 #pod容器指定的端口 hostPort: 9090 #映射到node主机的端口 pod启动后如下,可以看到，pod的ip地址是内部网络ip，与宿主机node的ip不同；与hostNetwork一样，也可以通过node ip + pod port访问,此处的访问port地址为，pod容器端口映射到node主机的端口地址 1234[root@localhost ~]# kubectl get pods -o wide nginxNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx 1/1 Running 0 57s 172.17.0.2 minikube &lt;none&gt;[root@localhost ~]# 可以curl访问，也可以浏览器访问 123456789101112[root@localhost ~]# curl -I http://192.168.10.10:9090HTTP/1.1 200 OKServer: nginx/1.19.2Date: Tue, 01 Sep 2020 09:56:16 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 11 Aug 2020 14:50:35 GMTConnection: keep-aliveETag: &quot;5f32b03b-264&quot;Accept-Ranges: bytes[root@localhost ~]# hostPort的优缺点与hostNetwork类似，因为它们都是使用了宿主机的网络资源。hostPort相对hostNetwork的一个优点是，hostPort不需要提供宿主机的网络信息，但其性能不如hostNetwork，因为需要经过iptables的转发才能到达Pod。 3、NodePort与hostPort、hostNetwork只是Pod的配置不同，NodePort是一种service，其使用的是宿主机node上的端口号，从集群外以 任意node的ip + nodePort 来访问Pod的服务。 NodePort 在 kubenretes 里是一个广泛应用的服务暴露方式。Kubernetes中的service默认情况下都是使用的ClusterIP这种类型，这样的service会产生一个ClusterIP，这个IP只能在集群内部访问，要想让外部能够直接访问service，需要将service type修改为 nodePort。 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: nginx labels: name: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80---kind: ServiceapiVersion: v1metadata: name: nginxspec: type: NodePort ports: - name: nginx port: 80 nodePort: 30000 selector: name: nginx svc 配置中的nodePort，即为访问服务时，宿主机的端口号。可以在配置文件中指定（当然不能与其他nodePort类型的svc冲突），也可以不配置，由k8s来分配。 创建上述Pod和service后，如下，查看pod和svc的相关信息，我们可以通过宿主机的ip地址+noePort来访问pod的服务 123456789101112131415161718192021222324[root@localhost ~]# kubectl get pods -o wide nginxNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEnginx 1/1 Running 0 5m47s 172.17.0.2 minikube &lt;none&gt;[root@localhost ~]# [root@localhost ~]# kubectl get svc -o wide nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORnginx NodePort 10.96.116.150 &lt;none&gt; 80:30000/TCP 5m58s name=nginx[root@localhost ~]# [root@localhost ~]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEminikube Ready master 11d v1.12.1 192.168.10.10 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://17.12.1-ce[root@localhost ~]# [root@localhost ~]# curl -I http://192.168.10.10:30000HTTP/1.1 200 OKServer: nginx/1.19.2Date: Tue, 01 Sep 2020 10:14:12 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 11 Aug 2020 14:50:35 GMTConnection: keep-aliveETag: &quot;5f32b03b-264&quot;Accept-Ranges: bytes[root@localhost ~]# 集群外就可以使用kubernetes任意一个节点的IP加上30000端口访问该服务了。kube-proxy会自动将流量以round-robin的方式转发给该service的每一个pod。 这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口 7）kubernetes中的CNI网络插件kubernetes中的网络插件 Kubernetes中常见的网络插件有哪些？ 1.flannel：能提供ip地址，但不支持网络策略 2.calico：既提供ip地址，又支持网络策略 3.canal：flannel和calico结合，通过flannel提供ip地址，calico提供网络策略 什么叫做网络策略？ 网络策略：可以达到多租户网络隔离，可以控制入网和出网流量，入网和出网ip访问的一种策略 8）flannel介绍Flannel是CoreOS团队针对Kubernetes设计的一个网络服务，它的功能是让集群中的不同节点创建的Docker容器都具有全集群唯一的虚拟IP地址，Flannel的设计目的就是为集群中的所有节点重新规划IP地址，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信，Flannel实质上是一种“覆盖网络(overlaynetwork)”，也就是将TCP数据包装在另一种网络包里面进行路由转发和通信，目前已经支持udp、vxlan、host-gw、aws-vpc、gce路由等数据转发方式。 实现扁平网络空间 （1）、flannel支持的路由转发方式 ①、vxlan，包含以下两种模式 123vxlan：叠加网络模式，利用内核级别的VXLAN来封装host之间传送的包；Directrouting：直接路由模式，当主机位于同一子网时，启用直接路由（类似host-gw），通过宿主机的物理网卡通信； ②、host-gw:直接路由模式，要求所有pod在同一个网段中,host-gw性能好，依赖少，并且易于设置 ③、UDP: 不可用于生产环境，仅在内核或网络无法使用VXLAN或 host-gw时，用 UDP 进行调试 （2）、flannel网络原理图 原理图解释说明 ①、网卡和组件说明 安装 flanneld 的守护进程，这个进程会监听一个端口（后期转发和接收数据包的端口）进程开启后会创建一个名为 flannel0 的网桥，这个网桥的一端连接 docker0 的网桥（强行获取 docker0 的数据报文），docker会分配ip到对应的 Pod上 Flanneld 守护进程：它需要连 etcd，利用 etcd 来管理可分配的IP资源，同时监控 etcd 中每个 Pod 的实际地址，将 docker0 发给它的数据包装起来，利用物理网络的连接将数据包投递到目标 flanneld 上，从而完成 pod 到 pod 之间的通信 ②、ping包走向 123Ⅰ、数据从源容器中发出后，经由所在主机的docker0虚拟网卡转发到flannel0虚拟网卡，flanneld服务监听在网卡的另外一端。Ⅱ、源主机的flanneld服务将原本的数据内容封装成一个数据包，然后根据自己的路由表投递给目的节点的flanneld服务，数据到达以后被解包，然后直接进入目的节点的flannel0虚拟网卡，然后被转发到目的主机的docker0虚拟网卡，最后就像本机容器通信一样由docker0路由到达目标容器。 注： flannel通过Etcd分配了每个节点可用的IP地址段后，可以保证每个结点上容器分配的地址不冲突。 （3）、flannel的原理flannel旨在解决不同节点上的容器网络互联问题，大致原理是为每个 host 分配一个子网，容器从此子网中分配IP，这些 IP可以在 host间路由，容器间无需 NAT 转发就可以跨主机通信，为了在各个主机间共享信息，flannel 用 etcd（如果是k8s集群会直接调用k8s api）存放网络配置、已分配的子网、主机ip等信息。 通过具体例子解释容器跨节点通信时数据包走向 假设容器1是nginx，容器2是tomcat 12345①、在发送端的node1节点上，数据请求从nginx容器（10.0.46.2:2379）发出后，首先经由所在主机的docker0虚拟网卡（10.0.46.1）转发到flannel0虚拟网卡（10.0.46.0）。②、接着flannel服务将原本的数据内容UDP封装后根据自己的路由表投递给目的节点的flanneld服务。在此包中，包含有router-ip（宿主机源ip：192.168.8.227 ；宿主机目的ip：192.168.8.228），还有容器ip（source：10.0.46.2:2379dest：10.0.90.2:8080）等数据信息。③、然后在接收端node2节点上，数据到达以后被解包，直接进入目的节点的flannel0虚拟网卡中（10.0.90.0），且被转发到目的主机的docker0虚拟网卡（10.0.90.1），最后就像本机容器通信一样由docker0路由到达目标 tomcat 容器（10.0.90.2:8080）。 （4）、flannel部署及参数配置①、部署 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml kubectl get pods -n kube-system -o wide 显示如下： kubectl get configmap -n kube-system 显示如下： 这个kube-flannel-cfg用来设置上面看到的flannel是怎么运行的，它可以把相关的变量注入到flannel的pod中。 kubectl get configmap kube-flannel-cfg -n kube-system -o yaml可显示configmap的yaml文件内容，具体内容在备注列出 12345678net-conf.json: |&#123;&quot;Network&quot;: &quot;10.244.0.0/16&quot;,&quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;上面可以看到使用的后端网络模型是vxlan，为pod分配的网段是10.244.0.0/16 ②、把flannel后端的通信类型修改成Directrouting 重新修改flannel的yaml文件 cd&#x2F;root&#x2F;manifests&#x2F; mkdir flannel cd flannel 修改kube-flannel.yml文件 kube-flannel.yaml这个文件里在net-conf.json字段增加了”Directrouting”: true，表示是使用直接路由模式，如果没有这个字段就表示使用的是vxlan这种叠加网络模式通信。 kubectl delete -f kube-flannel.yml 等到上面关于flannel的pod都删除之后再重新执行： kubectl apply -f kube-flannel.yml kubectl get pods -n kube-system 显示如下，说明重新生成了flannel这个pod，这时后端通信类型使用的就是directrouting了 验证是否是通过Directrouting（直接路由模式）通信 cd &#x2F;root&#x2F;manifests kubectl apply -f deploy-myapp.yaml cat deploy-myapp.yaml 1234567891011121314151617181920212223apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy namespace: defaultspec: replicas: 3 selector: matchLabels: app: myapp release: canary template: metadata: labels: app: myapp release: canary spec: containers: - name: myapp image: ikubernetes/myapp:v1 ports: - name: http containerPort: 80 kubectl get pods -o wide 显示如下，说明pod创建成功： 窗口1： kubectl exec -itmyapp-deploy-69b47bc96d-7s7zs – &#x2F;bin&#x2F;sh 窗口2： kubectl exec -itmyapp-deploy-69b47bc96d-lm4m4 – &#x2F;bin&#x2F;sh ping10.244.0.6 窗口3：抓包 tcpdump -i ens160 -nnhost 172.21.0.100 显示如下： 上面可以看到两个pod之间的通信是通过ens160，而不是overlay（覆盖网络），这样就提高了网络传输性能；如果两个节点是跨网段的，那么就会降级到vxlan（叠加网络）模式，否则就可以使用直接路由模式。 ③、把flannel的后端通信方式改成host-gw 修改kube-flannel.yml文件的如下部分 1234567net-conf.json: |&#123;&quot;Network&quot;:&quot;10.244.0.0/16&quot;,&quot;Backend&quot;:&#123;&quot;Type&quot;:&quot;host-gw&quot;&#125;&#125; 注：上面修改flannel的网络模式，一般在刚开始安装flannel的时候修改，不要在线上直接修改 ④、flannel中vxlan和host-gw的区别 vxlan模式下的的vxlan是一种覆盖网络；host-gw是直接路由模式，将主机作为网关，依赖于纯三层的ip转发。 不同网络模型的性能分析 121.vxlan的Directrouting模式和host-gw模式性能一样，都是通过宿主机的物理网络进行通信，效率高于vxlan的vxlan模式，但是不能实现跨网段通信；2.如果要跨网段通信vxlan的Directrouting模式会自动降级到vxlan的vxlan这种叠加网络模式 Flannel网络优点：1）集群中的不同Node主机创建的Docker容器都具有全集群唯一的虚拟IP地址。 2）etcd保证了所有node上flanned所看到的配置是一致的，同时每个node上的flanned监听etcd上的数据变化，实时感知集群中node的变化 6.Flannel网络缺点： 不支持pod之间的网络隔离，不支持网路策略 （4）、卸载flannel网络 12345678910111213#第一步，在master节点删除flannelkubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml#第二步，在node节点清理flannel网络留下的文件ifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/rm -f /etc/cni/net.d/*注：执行完上面的操作，重启kubelet#第三步，应用calico相关的yaml文件 9）、Calico实现Pod间的网络隔离(比flannel多的功能)Calico 是一个纯三层的方案，不需要 Overlay，基于 Etcd 维护网络准确性，也基于 Iptables 增加了策略配置 安装Calico，定义网络策略。 123456789101112131415ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend - ports: - protocol: TCP port: 80 这里的 ingress 规则中我们定义了 from 和 ports，表示允许流入的白名单和端口，上面我们也说了 Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 8- 端口。而这个允许流入的白名单中指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector： default 命名空间下面带有 role=frontend 标签的 Pod 带有 project=myproject 标签的 Namespace 下的任何 Pod 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。 egress: 每个 NetworkPolicy 包含一个 egress 规则的白名单列表。每个规则都允许匹配 to 和 port 部分的流量。比如我们这里示例规则的配置： 1234567egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 表示 Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。 4、Pod生命周期状态 挂起（Pending）：Pod 信息已经提交给了集群，但是还没有被调度器调度到合适的节点或者 Pod 里的镜像正在下载，或者是下载镜像慢，调度不成功 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止 未知（Unknown）：因为某些原因apiserver无法取得 Pod 的状态，通常是master与与 Pod 所在主机通信失败导致的 Pending状态：Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 kubectl describe pod 命令查看到当前 Pod 的事件，进而判断为什么没有调度。可能的原因包括：资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 等资源 HostPort 已被占用，通常推荐使用 Service 对外开放服务端口 imagePullBackOff：镜像拉取失败这也是我们测试环境常见的，通常是镜像拉取失败。这种情况可以使用 docker pull 来验证镜像是否可以正常拉取。 Error：通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括： 依赖的 ConfigMap、Secret 或者 PV 等不存在 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等 违反集群的安全策略，比如违反了 PodSecurityPolicy 等 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定 Evicted状态：出现这种情况，多见于系统内存或硬盘资源不足，可df-h查看docker存储所在目录的资源使用情况，如果百分比大于85%，就要及时清理下资源，尤其是一些大文件、docker镜像。 5、Pod的重启策略restartpolicy字段设置Pod中所有容器的重启策略 Always（默认），OnFailure，Nerver 不同类型的的控制器可以控制 Pod 的重启策略： Job：适用于一次性任务如批量计算，任务结束后 Pod 会被此类控制器清除。Job 的重启策略只能是&quot;OnFailure&quot;或者&quot;Never&quot;。 Replication Controller, ReplicaSet, or Deployment，此类控制器希望 Pod 一直运行下去，它们的重启策略只能是&quot;Always&quot;。 DaemonSet：每个节点上启动一个 Pod，很明显此类控制器的重启策略也应该是&quot;Always&quot;。 6、Init容器（初始化容器）1、解决服务间的依赖问题 2、做初始化配置 7、Pod Hook（钩子函数）由kubelet发起的，当容器中的进程启动前或者容器中的进程终止之前运行。，包含在容器的生命周期中，了一通是为 Pod 中的所有容器都配置 hook PostStart：容器创建后立即执行，主要用于资源部署、环境准备等 PreStop： 在容器终止之前立即被调用。 主要用于优雅关闭应用程序、通知其他系统等 Hook 的类型： exec：执行一段命令 HTTP：发送HTTP请求 8、Pod健康检查（探针）liveness probe(存活探针) 和 readiness probe（可读性探针） 三种方式： exec：执行一段命令，返回码为0，诊断成功 http：检测某个http请求，状态码在200~400之间，认为诊断成功 tcpSocket：检查端口，打开就是成功 liveness probe(存活探针)：探测容器是否存活。如果存活探测失败，则kubelet会杀死容器，并且容器将受重启策略影响。如果容器不提供存活探针，则默认状态为Success。 readiness probe（可读性探针）：指容器是否准备好服务请求。如果探测失败，端点控制器从与pod匹配的所有service的端点中删除该pod的IP地址。 initialDelaySeconds：容器启动，探针延后工作，默认是0s periodSeconds 探针探测周期，默认10s timeoutSeconds： 探针工作的超时时间，默认1s successThreshold： 连续几次探测成功，该探针被认为是成功的，默认1次 failureThreshold： 连续几次探测失败，该探针被认为最终失败，对于livenes探针最终失败意味着重启，对于readiness探针意味着该pod Unready, 默认3次。 9、Pod漂移1众所周知Kubernetes具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个Pod可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上;那么自然随着Pod 的创建和销毁，Pod lP肯定会动态变化;那么如何把这个动态的 Pod lP暴露出去?这里借助于Kubernetes的 Service机制，Service可以以标签的形式选定一组带有指定标签的Pod，并监控和自动负载他们的Pod IP，那么我们向外暴露只暴露Service lP就行了;这就是NodePort模式:即在每个节点上开起一个端口，然后转发到内部 Pod iP上 10、pod中的容器可以共享的资源 三、资源清单1、集群资源分类k8s中所有的内容都抽象为资源，资源实例化之后，叫对象 1）名称空间级别 1234567工作负载型资源（workload）：Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、CronJob（ReplicationController在v1.11版本被废弃）服务发现及负载均衡型资源（Service Discovery LoadBalance）：Service、Ingress、...配置与存储型资源：Volume（存储卷）、CSI（容器存储接口，可扩展各种各样的第三方存储卷）特殊类型的存储卷：ConfigMap（当配置中心来使用的资源类型）、Secret（保存敏感数据）、DownwardAPI（把外部环境中的信息输出给容器） 2）集群级别 1Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding 3）元数据型：通过指标进行操作 1Hpa、PodTemplate、LimitRange 2、资源清单k8s中一般使用yaml格式的文件来创建我么不所预期的pod，这样的yaml文件我们一般称为资源清单 3、常用字段解释说明 参数名 字段类型 说明 apiVersion String 这里指的是k8s API的版本，基本上是v1.可以用kubectl api-version命令查询 kind String 这里指的是yaml文件定义的资源类型和角色，比如：Pod metadata Object 元数据对象，固定值就写medata metadata.name String 元数据对象的名字，这里由我们白那些，比如命名Pod的名字 metadata.namespace String 元数据对象的命名空间，由我们自身定义 metadata.labels metadata.labels.app metadata.labels.version spec Object 详细定义对象，固定值就写Spec spec.containers[] List 这里是Spec对象的容器列表定义，是个列表 spec.containers[].name String 这里定义容器的名字 spec.containers[].image String 这里定义要用到镜像名称 spce.containers[].imagepPullPolicy String 定义镜像拉取策略，由Always、Never、IfNotPresent三个值可选（1）Always：意思是每次都尝试拉取镜像（2）Never：表示仅使用本地镜像（3）IfNotPresent：如果本地有镜像就使用本地镜像，没有就在线拉取镜像，上面三个值都没有设置的话，就默认Always spec.containers[].command[] List 指定容器启动命令，因为是数组可以指定多个，不指定则使用镜像打包时使用的启动命令 spec.containers[].arg[] List 指定容器启动命令参数，因为是数组可以指定多个 spec.sontainers[].workingDir String 指定容器的工作目录 spec.containers[].volumeMounts[] List 指定容器内部的存储卷配置 spec.containers[].volumeMounts[].name String 指定可以被容器挂载的存储卷的名称 spec.containers[].volumeMounts[].mountPath String 指定可以被容器挂载的存储卷的路径 spec.containers[].volumeMounts[].readOnly String 设置存储卷路径的读写模式，true或者是false，默认为读写模式 spec.containers[].pots[] List 指定容器需要用到的端口列表 spec.containers[].pots[].name String 指定端口名称 spec.containers[].pots[].containerPots String 指定容器需要监听的端口号 spec.containers[].pots[].hostPort String 指定容器需要监听的端口号，默认跟上面的containerPort相同，注意设置了hostPort同一台主机无法启动该容器的相同副本（因为主机的端口号不能相同，这样会冲突） spec.containers[].ports[].protocol String 指定端口协议，支持TCP和UDP，默认为TCP spec.containers[].env[] List 指定容器运行前需设置的环境变量列表 spec.containers[].env[].name String 指定环境变量名称 spec.containers[].env[].value String 指定环境变量值 spec.containers[].env[].resources Object 指定资源限制和资源请求的值(这里开始就是设置容器的资源上限) spec.containers[].env[].resources.limits Object 指定设置容器运行时资源的运行上限 spec.containers[].env[].resources.cpu String 指定CPU的限制，单位为core数，将用于docker run –cpu-shares参数(这里前面文章Pod资源限制有讲过) spec.containers[].env[].resources.limits.memory String 指定MEM内存的限制，单位为MIB、GiB spec.containers[].env[].resources.requests Object 指定容器启动和调度时的限制设置 spec.containers[].env[].resources.resquests.cpu String CPU请求，单位为core数，容器启动时初始化可用数量 spec.containers[].env[].resources.request.memory String 内存请求，单位为MIB、GiB，容器启动的初始化可用数量 spec.restartPolicy String 定义Pod的重启策略，可选值为Always、OnFailure，默认值为Always 1.Always：Pod一旦终止运行，则无论容器是如何终止的，kubectl服务都将重启它 2.OnFailure：只有Pod以非零退出码终止时，kubectl才会重启该容器。如果以正常结束（退出码为0），则kubectl将不会重启它 3.Never：Pod终止后，kubectl将退出码报告给Master，不会重启该Pod sspec.nodeSelector Object 定义Node的Label过滤标签，以key：value格式指定 sspec.imagePullSecrets Object 定义Pull镜像时使用secret名称，以name:secretkey格式指定 sspec.hostNetwork Boolean 定义是否使用主机网络模式，默认值为false。设置ture表示使用宿主机网络，不使用docker网桥，同时设置了true将无法在同一台宿主机上启动第二个副本 4、容器生命周期 Pod 能够具有多个容器，应用运行在容器里，但是它也有可能有一个或者多个先于容器应用启动的 Init 容器 Init 容器与普通容器非常像，除了如以下两点： Init 容器总是运行到成功完成为止 每个 Init 容器都必须在下一个 Init 容器自动启动之前完成 如果 Pod 的 Init 容器启动失败， Kubernetes 会不断的重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 retartPolicy 为 Never，它不会重新启动 因为 Init 容器具有与应用程序分离的单独镜像，所以他们的启动相关代码具有如下优势 它们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的 它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要FROM另一个镜像，只需要在安装过程中使用类似sed、awk、python或dig这样的工具。 应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。 Init容器使用Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问Secret的权限，而应用程序容器则不能。 它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以 Init容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。 在Pod 启动过程中，Init容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出 如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的restartPolicy 指定的策略进行重试。然而，如果Pod 的 restartPolicy 设置为Always，Init 容器失败时会使用 RestartPolicy 策略 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为true 如果 Pod 重启，所有 Init 容器必须重新执行 #对 Init 容器 spec 的修改被限制在容器 image 字段，修改其他字段都不会生效。更改 Init 容器的 image 字段，等价于重启该 Pod Init 容器具有应用容器的所有字段。除了 readinessProbe，因为 Init 容器无法定义不同于完成(completion）的就绪（readiness）之外的其他状态。这会在验证过程中强制执行 在 Pod 中的每个 app 和 Init 容器的名称必须唯一; 与任何其它容器共享同一个名称，会在验证时抛出错误 四、控制器ReplicaSet跟RelicationController没有什么本质的不同，只是名字不同，并且ReplicaSet支持集合式的selector，虽然RS可以独立使用，但一般还是建议使用Deployment来自动管理RS，这样就无需担心跟其他机制不兼容问题（比如RS不支持rolling-update但Deployment支持） 1、ReplicaSet作用： 维持一组 Pod 副本的运行，保证一定数量的 Pod 在集群中正常运行，ReplicaSet 控制器会持续监听它说控制的这些 Pod 的运行状态，在 Pod 发送故障数量减少或者增加时会触发调谐过程，始终保持副本数量一定。 1234567891011121314151617181920apiVersion: apps/v1kind: ReplicaSet metadata: name: nginx-rs namespace: defaultspec: replicas: 3 # 期望的 Pod 副本数量，默认值为1 selector: # Label Selector，必须匹配 Pod 模板中的标签 matchLabels: app: nginx template: # Pod 模板 metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 RS通过标签找Pod HPA基于RS定义，监控RS中Pod的利用率， 模板 123CPU &gt; 80 ##时进行扩展 Max 10 ##扩展最大值为10Min 2 ##最小值为2 当CPU达到模板中的值时，在RS中新建Pod直到达到max最大值（如果创建第三个Pod时CPU利用率就小于了80，那么也不会继续新建Pod），利用率变低以后Pod就会被回收，删到最小值min为止 2、Deployment（针对无状态服务）1、定义Deployment来创建Pod和RS 2、滚动升级和回滚应用 3、扩容和缩容 4、暂停和继续Deployment 1234567891011121314151617181920apiVersion: apps/v1kind: Deployment ###控制器类型metadata: name: nginx-deploy namespace: defaultspec: replicas: 3 # 期望的 Pod 副本数量，默认值为1 selector: # Label Selector，必须匹配 Pod 模板中的标签 matchLabels: app: nginx template: # Pod 模板 metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 Deployment控制RS，RS控制Pod 1）、扩容水平增加Pod数量，通过管理副本数 1kubectl scale deployment nginx-deployment --replicas 10 设置自动扩展 1kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 2）、滚动升级（更新镜像）1kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 更新一个删除一个 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deployment metadata: name: nginx-deploy namespace: defaultspec: replicas: 3 selector: matchLabels: app: nginx minReadySeconds: 5 strategy: type: RollingUpdate # 指定更新策略：RollingUpdate和Recreate rollingUpdate: maxSurge: 1 maxUnavailable: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 minReadySeconds：表示 Kubernetes 在等待设置的时间后才进行升级，如果没有设置该值，Kubernetes 会假设该容器启动起来后就提供服务了，如果没有设置该值，在某些极端情况下可能会造成服务不正常运行，默认值就是0。 type=RollingUpdate：表示设置更新策略为滚动更新，可以设置为Recreate和RollingUpdate两个值，Recreate表示全部重新创建，默认值就是RollingUpdate。 maxSurge：表示升级过程中最多可以比原先设置多出的 Pod 数量，例如：maxSurage=1，replicas=5，就表示Kubernetes 会先启动一个新的 Pod，然后才删掉一个旧的 Pod，整个升级过程中最多会有5+1个 Pod。 maxUnavaible：表示升级过程中最多有多少个 Pod 处于无法提供服务的状态，当maxSurge不为0时，该值也不能为0，例如：maxUnavaible=1，则表示 Kubernetes 整个升级过程中最多会有1个 Pod 处于无法服务的状态。 我们可以添加了一个额外的 --record 参数来记录下我们的每次操作所执行的命令，以方便后面查看。 查看滚动更新状态 1kubectl rollout status deployment/nginx-deployment 暂停滚动更新 1kubectl rollout pause deployment/nginx-deploy 恢复滚动更新 1kubectl rollout resume deployment/nginx-deployment 查看历史版本 1kubectl rollout history deployment nginx-deployment 3）、回滚1kubectl rollout undo deployment/nginx-deployment 回滚一个删除一个 回退到指定版本 1kubectl rollout undo deployment/nginx-deployment --to-revision=1 3、StatefulSet（针对有状态服务）1）、有状态服务和无状态服务 无状态服务（Stateless Service）：该服务运行的实例不会在本地存储需要持久化的数据，并且多个实例对于同一个请求响应的结果是完全一致的 有状态服务（Stateful Service）：就和上面的概念是对立的了，该服务运行的实例需要在本地存储持久化数据 ：MySQL，mangoDB 2）、功能特性 稳定的、唯一的网络标识符，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 稳定的、持久化的存储，即Pod重新调度后还能访问到相同持久化数据，基于PVC来实现 有序的、优雅的部署和缩放，即Pod时有顺序的在部署或者扩展的时候要有根据定义的顺序依次进行（即从 0 到 N-1 ，在下一个Pod运行之前所有的 Pod 必须都是 Running 和 Ready 状态），基于 initial containers 来实现 有序的、优雅的删除和终止（即 N-1 到 0） 有序的、自动滚动更新 启停顺序： 3）、Headless Service（无头服务）特点：ClusterIP&#x3D;None 创建后不会被分配clusterip，以DNS记录的方式暴露所代理的Pod，所代理的Pod绑定如下DNS记录 1&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local 4）、创建1、先创建PV 12345678910111213141516171819202122232425apiVersion: v1kind: PersistentVolumemetadata: name: pv001spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /tmp/pv001---apiVersion: v1kind: PersistentVolumemetadata: name: pv002spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /tmp/pv002 2、stateful的资源清单 123456789101112131415161718192021222324252627282930313233apiVersion: apps/v1kind: StatefulSetmetadata: name: web namespace: defaultspec: serviceName: &quot;nginx&quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - name: web containerPort: 80 volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 1Gi 上面资源清单中和 volumeMounts 进行关联的不是 volumes 而是一个新的属性：volumeClaimTemplates，该属性会自动创建一个 PVC 对象 ，PVC 被创建后会自动去关联当前系统中和他合适的 PV 进行绑定 serviceName 就是管理当前 StatefulSet 的服务名称，该服务必须在 StatefulSet 之前存在，并且负责该集合的网络标识，Pod 会遵循以下格式获取 DNS&#x2F;主机名：pod-specific-string.serviceName.default.svc.cluster.local，其中 pod-specific-string 由 StatefulSet 控制器管理。 StatefulSet 引入了 PV 和 PVC 对象来持久存储服务产生的状态，这样所有的服务虽然可以被杀掉或者重启，但是其中的数据由于 PV 的原因不会丢失。 Pod 的名称的形式为 1&lt;statefulset name&gt;-&lt;ordinal index&gt; StatefulSet 中 Pod 副本的创建会按照序列号升序处理，副本的更新和删除会按照序列号降序处理。 5）、管理策略对于某些分布式系统来说，StatefulSet 的顺序性保证是不必要和&#x2F;或者不应该的，这些系统仅仅要求唯一性和身份标志。为了解决这个问题，我们只需要在声明 StatefulSet 的时候重新设置 spec.podManagementPolicy 的策略即可。 默认的管理策略是 OrderedReady，表示让 StatefulSet 控制器遵循上文演示的顺序性保证。除此之外，还可以设置为 Parallel 管理模式，表示让 StatefulSet 控制器并行的终止所有 Pod，在启动或终止另一个 Pod 前，不必等待这些 Pod 变成 Running 和 Ready 或者完全终止状态。 6）、更新策略 在 StatefulSet 中同样也支持两种升级策略：onDelete 和 RollingUpdate，同样可以通过设置 .spec.updateStrategy.type 进行指定 OnDelete: 该策略表示当更新了 StatefulSet 的模板后，只有手动删除旧的 Pod 才会创建新的 Pod。 RollingUpdate：该策略表示当更新 StatefulSet 模板后会自动删除旧的 Pod 并创建新的Pod，如果更新发生了错误，这次“滚动更新”就会停止。不过需要注意 StatefulSet 的 Pod 在部署时是顺序从 0n 的，而在滚动更新时，这些 Pod 则是按逆序的方式即 n0 一次删除并创建。 另外SatefulSet 的滚动升级还支持 Partitions的特性，可以通过 过.spec.updateStrategy.rollingUpdate.partition 进行设置，在设置 partition 后，SatefulSet 的 Pod 中序号大于或等于 partition 的 Pod 会在 StatefulSet 的模板更新后进行滚动升级，而其余的 Pod 保持不变 有状态服务一般拿更高级的 Operator 来部署 7）、使用场景 4、DaemonSet确保全部（或一些）Node上运行一个（有且只有一个）Pod副本。当有Node加入集群时，会为他们新增一个Pod。当有Node从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它所创建的所有Pod 典型用法： 集群存储守护程序，如 glusterd、ceph 要部署在每个节点上以提供持久性存储； 节点监控守护进程，如 Prometheus 监控集群，可以在每个节点上运行一个 node-exporter 进程来收集监控节点的信息； 日志收集守护程序，如 fluentd 或 logstash，在每个节点上运行以收集容器的日志 节点网络插件，比如 flannel、calico，在每个节点上运行为 Pod 提供网络服务。 每个节点部署nginx pod 1234567891011121314151617181920apiVersion: apps/v1kind: DaemonSetmetadata: name: nginx-ds namespace: defaultspec: selector: matchLabels: k8s-app: nginx template: metadata: labels: k8s-app: nginx spec: containers: - image: nginx:1.7.9 name: nginx ports: - name: http containerPort: 80 那么，DaemonSet 控制器是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？ 首先控制器从 Etcd 获取到所有的 Node 列表，然后遍历所有的 Node。 根据资源对象定义是否有调度相关的配置，然后分别检查 Node 是否符合要求。 在可运行 Pod 的节点上检查是否已有对应的 Pod，如果没有，则在这个 Node 上创建该 Pod；如果有，并且数量大于 1，那就把多余的 Pod 从这个节点上删除；如果有且只有一个 Pod，那就说明是正常情况。 5、Job及CronJob五、对外暴露集群服务 有状态服务：删除后重新加入，可能不能正常工作 无状态服务：删除后重新加入工作正常进行，无影响 1、Service概念：Pod的逻辑分组，提供可以访问Pod的逻辑策略 通过标签找到对应的Pod 局限：只提供4层负载均衡能力，没有7层功能（ingress） 1）、服务类型： ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的服务类型。 NodePort：通过每个 Node 节点上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 NodeIp:NodePort，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务，这个需要结合具体的云厂商进行操作。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如， foo.bar.example.com）。 2）、IPVS代理模式： kube-proxy会监视Kubernetes Service对象和Endpoints，调用netlink接口以相应地创建ipvs规则并定期与Kubernetes Service对象和Endpoints对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod 3）、iptables代理模式 这种模式，kube-proxy 会监视 apiserver 对 Service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会添加上 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某一个 Pod 上面。我们还可以使用 Pod readiness 探针 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端，这样做意味着可以避免将流量通过 kube-proxy 发送到已知失败的 Pod 中，所以对于线上的应用来说一定要做 readiness 探针。 4）、ClusterIP实现： Headless Server：指定ClusterIP为“none”。不会分配Cluster IP，kube-proxy不会处理它们，平台也不会为它们进行负载均衡和路由 5）、NodePort(重点)如果设置 type 的值为 “NodePort”，Kubernetes master 将从给定的配置范围内（默认：30000-32767）分配端口，每个 Node 将从该端口（每个 Node 上的同一端口）代理到 Service。该端口将通过 Service 的 spec.ports[*].nodePort 字段被指定，如果不指定的话会自动生成一个端口。 需要注意的是，Service 将能够通过 spec.ports[].nodePort 和 spec.clusterIp:spec.ports[].port 而对外可见。 12345678910111213apiVersion: v1kind: Servicemetadata: name: myservicespec: selector: app: myapp type: NodePort ports: - protocol: TCP port: 80 targetPort: 80 name: myapp-http 6）、ExternalNameExternalName 是 Service 的特例，它没有 selector，也没有定义任何的端口和 Endpoint。对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式来提供服务。 12345678kind: ServiceapiVersion: v1metadata: name: my-service namespace: prodspec: type: ExternalName externalName: my.database.example.com 当访问地址 my-service.prod.svc.cluster.local（后面服务发现的时候我们会再深入讲解）时，集群的 DNS 服务将返回一个值为 my.database.example.com 的 CNAME 记录。访问这个服务的工作方式与其它的相同，唯一不同的是重定向发生在 DNS 层，而且不会进行代理或转发。如果后续决定要将数据库迁移到 Kubernetes 集群中，可以启动对应的 Pod，增加合适的 Selector 或 Endpoint，修改 Service 的 type，完全不需要修改调用的代码，这样就完全解耦了。 除了可以直接通过 externalName 指定外部服务的域名之外，我们还可以通过自定义 Endpoints 来创建 Service，前提是 clusterIP=None，名称要和 Service 保持一致，如下所示： 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Servicemetadata: name: etcd-k8s namespace: kube-system labels: k8s-app: etcdspec: type: ClusterIP clusterIP: None ports: - name: port port: 2379---apiVersion: v1kind: Endpointsmetadata: name: etcd-k8s # 名称必须和 Service 一致 namespace: kube-system labels: k8s-app: etcdsubsets:- addresses: - ip: 10.151.30.57 # Service 将连接重定向到 endpoint ports: - name: port port: 2379 # endpoint 的目标端口 上面这个服务就是将外部的 etcd 服务引入到 Kubernetes 集群中来。 7）、Headless Service（无头服务）特点：ClusterIP&#x3D;None 创建后不会被分配clusterip，以DNS记录的方式暴露所代理的Pod，所代理的Pod绑定如下DNS记录 1&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local 8）、生成的域名 普通的 Service：会生成 servicename.namespace.svc.cluster.local 的域名，会解析到 Service 对应的 ClusterIP 上，在 Pod 之间的调用可以简写成 servicename.namespace，如果处于同一个命名空间下面，甚至可以只写成 servicename 即可访问 Headless Service：无头服务，就是把 clusterIP 设置为 None 的，会被解析为指定 Pod 的 IP 列表，同样还可以通过 podname.servicename.namespace.svc.cluster.local 访问到具体的某一个 Pod。 2、Ingress（ingress-nginx） Ingress 其实就是从 Kuberenets 集群外部访问集群的一个入口，将外部的请求转发到集群内不同的 Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器 。 ngress 实际上就是这样实现的，只是服务发现的功能自己实现了，不需要使用第三方的服务了，然后再加上一个域名规则定义，路由信息的刷新依靠 Ingress Controller 来提供 Ingress Controller 可以理解为一个监听器，通过不断地监听 kube-apiserver，实时的感知后端 Service、Pod 的变化，当得到这些信息变化后，Ingress Controller 再结合 Ingress 的配置，更新反向代理负载均衡器，达到服务发现的作用。其实这点和服务发现工具 consul、 consul-template 非常类似。 下图显示了客户端是如果通过 Ingress Controller 连接到其中一个 Pod 的流程，客户端首先对 ngdemo.qikqiak.com 执行 DNS 解析，得到 Ingress Controller 所在节点的 IP，然后客户端向 Ingress Controller 发送 HTTP 请求，然后根据 Ingress 对象里面的描述匹配域名，找到对应的 Service 对象，并获取关联的 Endpoints 列表，将客户端的请求转发给其中一个 Pod。 12345678910111213141516apiVersion: extensions/v1beta1kind: Ingressmetadata: name: my-nginx annotations: kubernetes.io/ingress.class: &quot;nginx&quot; #指定让这个 Ingress 通过 nginx-ingress 来处理spec: rules: - host: ngdemo.qikqiak.com # 将域名映射到 my-nginx 服务 http: paths: - path: / backend: serviceName: my-nginx # 将所有请求发送到 my-nginx 服务的 80 端口 servicePort: 80 # 不过需要注意大部分Ingress controller都不是直接转发到Service # 而是只是通过Service来获取后端的Endpoints列表，直接转发到Pod，这样可以减少网络跳转，提高性能 1）、URL Rewrite 1234567891011121314151617apiVersion: extensions/v1beta1kind: Ingressmetadata: name: fe namespace: default annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: http://foo.bar.com:31795/hostname.htmlspec: rules: - host: foo10.bar.com http: paths: - path backend: serviceName: fe servicePort: 3000 2）、Auth认证1、生成密码 1htpasswd -c auth foo 2、创建secret对象 1kubectl create secret generic basic-auth --from-file=auth 3、创建Ingress对象 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-with-auth annotations: # 认证类型 nginx.ingress.kubernetes.io/auth-type: basic # 包含 user/password 定义的 secret 对象名 nginx.ingress.kubernetes.io/auth-secret: basic-auth # 要显示的带有适当上下文的消息，说明需要身份验证的原因 nginx.ingress.kubernetes.io/auth-realm: &#x27;Authentication Required - foo&#x27;spec: rules: - host: foo.bar.com http: paths: - path: / backend: serviceName: my-nginx servicePort: 80 3）、HTTPS代理访问1、创建证书 1openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/CN=foo.bar.com&quot; 2、 通过 Secret 对象来引用证书文件 12# 要注意证书文件名称必须是 tls.crt 和 tls.key $ kubectl create secret tls foo-tls --cert=tls.crt --key=tls.key 3、创建Ingress对象 123456789101112131415161718192021222324apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-with-auth annotations: # 认证类型 nginx.ingress.kubernetes.io/auth-type: basic # 包含 user/password 定义的 secret 对象名 nginx.ingress.kubernetes.io/auth-secret: basic-auth # 要显示的带有适当上下文的消息，说明需要身份验证的原因 nginx.ingress.kubernetes.io/auth-realm: &#x27;Authentication Required - foo&#x27;spec: rules: - host: foo.bar.com http: paths: - path: / backend: serviceName: my-nginx servicePort: 80 tls: - hosts: - foo.bar.com secretName: foo-tls 六、配置管理1、ConfigMap（可变配置管理）概念：向容器中注入配置信息。ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象 注：一般情况ConfigMap存储一些非安全的配置信息，是明文存储的 ConfigMap创建1）、使用目录创建1234567891011$ ls testcmredis.confmysql.conf$ cat testcm/redis.confhost=127.0.0.1port=6379$ cat testcm/mysql.confhost=127.0.0.1port=3306 然后我们就可以使用 from-file 关键字来创建包含这个目录下面所以配置文件的 ConfigMap 1kubectl create configmap cm-demo1 --from-file=testcm 查看完整键值 1kubectl get configmap cm-demo1 -o yaml 2）、使用文件创建1$ kubectl create configmap cm-demo2 --from-file=testcm/redis.conf 注意： --from-file 这个参数可以使用多次，比如我们这里使用两次分别指定 redis.conf 和 mysql.conf 文件，就和直接指定整个目录是一样的效果了。 3）、使用字符串创建 通过 --from-literal 参数传递配置信息，同样的，这个参数可以使用多次 1kubectl create configmap cm-demo3 --from-literal=db.host=localhost --from-literal=db.port=3306 ConfigMap使用1）、使用ConfigMap代替环境变量1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: testcm1-podspec: containers: - name: testcm1 image: busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] env: - name: DB_HOST valueFrom: configMapKeyRef: name: cm-demo3 key: db.host - name: DB_PORT valueFrom: configMapKeyRef: name: cm-demo3 key: db.port envFrom: - configMapRef: name: cm-demo1 输出效果： 123456789$ kubectl logs testcm1-pod......DB_HOST=localhostDB_PORT=3306mysql.conf=host=127.0.0.1port=3306redis.conf=host=127.0.0.1port=6379...... 2）、使用ConfigMap设置命令行参数 使用 ConfigMap来设置命令行参数，ConfigMap 也可以被用来设置容器中的命令或者参数值 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: testcm2-podspec: containers: - name: testcm2 image: busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo $(DB_HOST) $(DB_PORT)&quot; ] env: - name: DB_HOST valueFrom: configMapKeyRef: name: cm-demo3 key: db.host - name: DB_PORT valueFrom: configMapKeyRef: name: cm-demo3 key: db.port 输出效果： 1$ kubectl logs testcm2-pod localhost 3306 3）、通过数据卷使用 通过数据卷使用，在数据卷里面使用 ConfigMap，就是将文件填入数据卷，在这个文件中，键就是文件名，键值就是文件内容 12345678910111213141516apiVersion: v1kind: Podmetadata: name: testcm3-podspec: volumes: - name: config-volume configMap: name: cm-demo2 containers: - name: testcm3 image: busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cat /etc/config/redis.conf&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config 输出效果： 123$ kubectl logs testcm3-podhost=127.0.0.1port=6379 也可以在 ConfigMap 值被映射的数据卷里去控制路径 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: testcm4-podspec: volumes: - name: config-volume configMap: name: cm-demo1 items: - key: mysql.conf path: path/to/msyql.conf containers: - name: testcm4 image: busybox command: [ &quot;/bin/sh&quot;,&quot;-c&quot;,&quot;cat /etc/config/path/to/msyql.conf&quot; ] volumeMounts: - name: config-volume mountPath: /etc/config 输出效果： 123$ kubectl logs testcm4-podhost=127.0.0.1port=3306 ConfigMap的热更新1、 当 ConfigMap 以数据卷的形式挂载进 Pod 的时，这时更新 ConfigMap（或删掉重建ConfigMap），Pod 内挂载的配置信息会热更新 。 这时可以增加一些监测配置文件变更的脚本，然后重加载对应服务就可以实现应用的热更新。 2、使用ConfigMap挂载的ENV不会同步更新 3、 只有通过 Kubernetes API 创建的 Pod 才能使用 ConfigMap，其他方式创建的（比如静态 Pod）不能使用 4、 ConfigMap 文件大小限制为 1MB（ETCD 的要求） 5、更新ConfigMap目前并不会触发相关Pod 的滚动更新，可以通过修改pod annotations的方式强制触发滚动更新 2、Secret（敏感信息配置管理） Secret用来保存敏感信息，例如密码、OAuth 令牌和 ssh key 等等，将这些信息放在 Secret 中比放在 Pod 的定义中或者 Docker 镜像中要更加安全和灵活。 Secret 主要使用的有以下三种类型： Opaque：base64 编码格式的 Secret，用来存储密码、密钥等；但数据也可以通过base64 –decode解码得到原始数据，所有加密性很弱。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token：用于被 ServiceAccount ServiceAccount 创建时 Kubernetes 会默认创建一个对应的 Secret 对象。Pod 如果使用了 ServiceAccount，对应的 Secret 会自动挂载到 Pod 目录 /run/secrets/kubernetes.io/serviceaccount 中。 bootstrap.kubernetes.io/token：用于节点接入集群的校验的 Secret 1）、Opaque Secret Opaque 类型的数据是一个 map 类型，要求 value 必须是 base64 编码格式，比如我们来创建一个用户名为 admin，密码为 admin321 的 Secret 对象，首先我们需要先把用户名和密码做 base64 编码： 1234$ echo -n &quot;admin&quot; | base64YWRtaW4=$ echo -n &quot;admin321&quot; | base64YWRtaW4zMjE= 编写yaml文件 12345678apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: username: YWRtaW4= password: YWRtaW4zMjE= 发布并查看 123456$ kubectl create -f secret-demo.yaml$ kubectl get secretNAME TYPE DATA AGEdefault-token-n9w2d kubernetes.io/service-account-token 3 33dmysecret Opaque 2 40s default-token-n9w2d 为创建集群时默认创建的 Secret，被 serviceacount/default 引用 两种使用方式 以环境变量的形式 以Volume的形式挂载 环境变量： 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: secret1-podspec: containers: - name: secret1 image: busybox command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] env: - name: USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: PASSWORD valueFrom: secretKeyRef: name: mysecret key: password 创建后查看日志输出： 12345$ kubectl logs secret1-pod...USERNAME=adminPASSWORD=admin321... volume挂载： 12345678910111213141516apiVersion: v1kind: Podmetadata: name: secret2-podspec: containers: - name: secret2 image: busybox command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;ls /etc/secrets&quot;] volumeMounts: - name: secrets mountPath: /etc/secrets volumes: - name: secrets secret: secretName: mysecret 2）、service-accountService Account用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount目录中 有三个文件： ca.crt：用于校验服务端的证书信息 namespace：表示当前管理的命名空间 token：用于 Pod 身份认证的 Token 3）、dockerconfigjson （1）创建用户 docker registry 认证的 Secret，直接使用&#96;&#96;kubectl create&#96; 命令创建即可 12$ kubectl create secret docker-registry myregistry --docker-server=DOCKER_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAILsecret &quot;myregistry&quot; created （2）使用指定文件 1$ kubectl create secret generic myregistry --from-file=.dockerconfigjson=/root/.docker/config.json --type=kubernetes.io/dockerconfigjson 3、Secret和ConfidMap的异同点相同点： key&#x2F;value的形式 属于某个特定的命名空间 可以导出到环境变量 可以通过目录&#x2F;文件形式挂载 通过 volume 挂载的配置信息均可热更新 不同点： Secret 可以被 ServerAccount 关联 Secret 可以存储 docker register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像 Secret 支持 Base64 加密 Secret 分为 kubernetes.io/service-account-token、kubernetes.io/dockerconfigjson、Opaque 三种类型，而 Configmap 不区分类型 七、存储1）、背景首先，当容器崩溃时，kubelet会重启容器，但容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。 其次，在Pod中同时运行多个容器时，这些容器之间通常需要共享文件，K8S中的volume抽象就很好的解决这些问题。 2）、emptyDir当Pod被分配到节点时，首先创建emptyDir卷，并且只要该Pod在该节点上运行，该卷就会存在。它最初是空的，Pod中的容器可以读取和写入emptyDir卷中的相同文件，尽管改卷可以挂载到容器中的相同或不同路径上。当出于任何原因从节点中删除Pod时，emptyDir中的数据讲被永久删除。 emptyDir的用法： 1、暂存空间，例如用于基于磁盘的合并排序； 2、用作长时间计算崩溃恢复时的检查点； 3、Web服务器容器提供数据时，保存内容管理器容器提取的文件。 1234567891011121314apiVersion: v1kind: Podmetadata: name: test-pdspec: containers: - image: k8s.gcr.io/test-webserver name: test-container volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 3）、hostpath将主机节点的文件系统中的文件或目录挂载到集群中 hostPath Volume 的作用是将 Docker Host 文件系统中已经存在的目录 mount 给 Pod 的容器。大部分应用都不会使用 hostPath Volume，因为这实际上增加了 Pod 与节点的耦合，限制了 Pod 的使用。不过那些需要访问 Kubernetes 或 Docker 内部数据（配置文件和二进制库）的应用则需要使用 hostPath 4）、PV和PVC概念 PV 的全称是：PersistentVolume（持久化卷），是对底层共享存储的一种抽象，PV 由管理员进行创建和配置，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS、hostPath 等，都是通过插件机制完成与共享存储的对接。 静态PV 集群管理员创建一些PV。它们带有可供集群用户使用的实际存储细节。它们存在于Kubernetes API中，可用于消费。 动态PV 静态PV都不匹配PVC，集群可能会尝试动态为PVC创建卷。基于StorageClasses。 PVC 的全称是：PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。 但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了，此外 StorageClass 还可以为我们自动生成 PV，免去了每次手动创建的麻烦。 绑定 master中的控制环路监视新的PVC，寻找匹配的PV（如果可能），并将它们绑定在一起。如果为新的PVC 动态调配PV，则该环路将始终将该PV 绑定到PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦PV 和PVC 绑定后PersistentVolumeClaim（PVC）绑定是排他性的，不管它们是如何绑定的。 PVC 跟PV 绑定是一对一的映射。 5）、PV访问模式AccessModes（访问模式）：用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 6）、PV回收策略其中有一项 RECLAIM POLICY 的配置，同样我们可以通过 PV 的 persistentVolumeReclaimPolicy（回收策略）属性来进行配置，目前 PV 支持的策略有三种： Retain（保留）：保留数据，需要管理员手工清理数据 Recycle（回收）：清除 PV 中的数据，效果相当于执行 rm -rf /thevoluem/* Delete（删除）：与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务，比如 ASW EBS。 不过需要注意的是，目前只有 NFS 和 HostPath 两种类型支持回收策略，当然一般来说还是设置为 Retain 这种策略保险一点。 7）、PV状态关于 PV 的状态，实际上描述的是 PV 的生命周期的某个阶段，一个 PV 的生命周期中，可能会处于4种不同的阶段： Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PVC 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 现在我们创建完成了 PV，如果我们需要使用这个 PV 的话，就需要创建一个对应的 PVC 来和他进行绑定了，就类似于我们的服务是通过 Pod 来运行的，而不是 Node，只是 Pod 跑在 Node 上而已。 8）、创建PVC的资源清单 八、kube-scheduler调度器 kube-scheduler 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源 ​ 1、调度流程 首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源 API Server 收到用户请求后，存储相关数据到 etcd 数据库中 调度器监听 API Server 查看到还未被调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段： 预选阶段(Predicates)，过滤节点，调度器用一组规则过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉 优选阶段(Priorities)，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，调度器会考虑一些整体的优化策略，比如把 Deployment 控制的多个 Pod 副本尽量分布到不同的主机上，使用最低负载的主机等等策略 经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 binding 操作，然后将结果存储到 etcd 中 最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作（当然也是 watch APIServer 发现的）。 （首先过滤掉不满足条件的节点，这个过程称为predicate；然后对通过的节点按照优先级排序，这个是priority；最后从中选择优先级最高的节点。如果中间热河一步骤有错误，就直接返回报错。） 1）、预选过程 Predicates 阶段首先遍历全部节点，过滤掉不满足条件的节点，属于强制性规则，这一阶段输出的所有满足要求的节点将被记录并作为第二阶段的输入，如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试 预选过程的调度策略算法 PodFitsResources：节点上剩余的资源（如 CPU 和内存）是否满足 Pod 请求的资源 PodFitsHost：如果 Pod 指定了 NodeName，检查节点名称是否和 NodeName 匹配 PodFitsHostPorts：如果 Pod 中定义了 hostPort 属性，那么需要先检查这个指定端口是否已经被节点上其他服务占用了 PodMatchNodeSelector：检查 Node 的标签是否能匹配 Pod 的 nodeSelector 的标签值 NoDiskConflict：检查 Pod 请求的存储卷在 Node 上是否可用，若不存在冲突则通过检查 NoVolumeZoneConflict：检测 Pod 请求的 Volumes 在节点上是否可用，因为某些存储卷存在区域调度约束 CheckNodeDiskPressure：检查节点磁盘空间是否符合要求 CheckNodeMemoryPressure：检查节点内存是否够用 CheckNodeCondition：Node 可以上报其自身的状态，如磁盘、网络不可用，表明 kubelet 未准备壕运行 Pod，如果节点被设置成这种状态，那么 Pod 不会被调度到这个节点上 PodToleratesNodeTaints：检查 Pod 属性上的 tolerations 能否容忍节点的 taints 污点 CheckVolumeBinding：检查节点上已经绑定的和未绑定的 PVC 能否满足 Pod 的存储卷需求 如果在predicate过程中没有合适的节点，pod会一直在pending状态，不断重试调度，直到有节点满足条件，经过这个步骤，如果有多个节点满足条件，就继续priorities过程，按照优先级大小对节点排序 2）、优选过程 LeastRequestedPriority：通过计算 CPU 和内存的使用率来决定权重，使用率越低权重越高，当然正常肯定也是资源是使用率越低权重越高，能给别的 Pod 运行的可能性就越大 SelectorSpreadPriority：为了更好的高可用，对同属于一个 Deployment 或者 RC 下面的多个 Pod 副本，尽量调度到多个不同的节点上，当一个 Pod 被调度的时候，会先去查找该 Pod 对应的 controller，然后查看该 controller 下面的已存在的 Pod，运行 Pod 越少的节点权重越高 InterPodAffinityPriority：遍历 Pod 的亲和性条目，并将那些能够匹配到给定节点的条目的权重相加，结果值越大的节点得分越高 MostRequestedPriority：空闲资源比例越低的 Node 得分越高，这个调度策略将会把你的所有的工作负载（Pod）调度到尽量少的节点上 RequestedToCapacityRatioPriority：为 Node 上每个资源占用比例设定得分值，给资源打分函数在打分时使用 BalancedResourceAllocation：优选那些使得资源利用率更为均衡的节点。 NodePreferAvoidPodsPriority：这个策略将根据 Node 的注解信息中是否含有 scheduler.alpha.kubernetes.io/preferAvoidPods 来计算其优先级，使用这个策略可以将两个不同 Pod 运行在不同的 Node 上 NodeAffinityPriority：基于 Pod 属性中 PreferredDuringSchedulingIgnoredDuringExecution 来进行 Node 亲和性调度 TaintTolerationPriority：基于 Pod 中对每个 Node 上污点容忍程度进行优先级评估，这个策略能够调整待选 Node 的排名 ImageLocalityPriority：Node 上已经拥有 Pod 需要的容器镜像的 Node 会有较高的优先级 ServiceSpreadingPriority：这个调度策略的主要目的是确保将归属于同一个 Service 的 Pod 调度到不同的 Node 上，如果 Node 上没有归属于同一个 Service 的 Pod，这个策略更倾向于将 Pod 调度到这类 Node 上。最终的目的：即使在一个 Node 宕机之后 Service 也具有很强容灾能力。 CalculateAntiAffinityPriorityMap：这个策略主要是用来实现 Pod 反亲和的 EqualPriorityMap：将所有的 Node 设置成相同的权重为 1 每一个优先级函数会返回一个 0-10 的分数，分数越高表示节点越优，同时每一个函数也会对应一个表示权重的值。最终主机的得分用以下公式计算得出： 1finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn 2、自定义调度器通过 spec:schedulername 参数指定调度器的名字，可以为pod选择某个调度器进行调度 123456789101112apiVersion: v1kind: Podmetadata: name: annotation-second-scheduler labels: name: multischeduler-example spec: schedulername: my-scheduler containers: - name: pod-with-second-annotation-container image: gcr.io/google_containers/pause:2.0 3、节点调度1）、nodeSelector通过节点标签来调度 查看node的标签： 1$ kubectl get nodes --show-labels 给节点打标签： 1$ kubectl label nodes ydzs-node2 com=youdianzhishi 将pod调度到选定节点 12345678910111213141516apiVersion: v1kind: Podmetadata: labels: app: busybox-pod name: test-busyboxspec: containers: - command: - sleep - &quot;3600&quot; image: busybox imagePullPolicy: Always name: test-busybox nodeSelector: com: youdianzhishi nodeSelector属于强制性，如果目标节点没有可用的资源。Pod会一直处于Pending状态 2）、亲和性和反亲和性 实际需求来控制 Pod 的调度，这就需要用到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。 亲和性调度分为软策略和硬策略两种方式 软策略就是如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓 硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然就不干了 节点亲和性 控制 Pod 要部署在哪些节点上，以及不能部署在哪些节点上的，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配 123456789101112131415161718192021222324252627282930313233343536373839apiVersion: apps/v1kind: Deploymentmetadata: name: node-affinity labels: app: node-affinityspec: replicas: 8 selector: matchLabels: app: node-affinity template: metadata: labels: app: node-affinity spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 name: nginxweb affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - ydzs-node3 preferredDuringSchedulingIgnoredDuringExecution: # 软策略 - weight: 1 preference: matchExpressions: - key: com operator: In values: - youdianzhishi 上面这个 Pod 首先是要求不能运行在 ydzs-node3 这个节点上，如果有个节点满足 com=youdianzhishi 的话就优先调度到这个节点上。 现在 Kubernetes 提供的操作符有下面的几种： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 Pod亲和性 Pod 亲和性（podAffinity）主要解决 Pod 可以和哪些 Pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等） Pod 反亲和性（podAntiAffinity）主要是解决 Pod 不能和哪些 Pod 部署在同一个拓扑域中的问题 它们都是处理的 Pod 与 Pod 之间的关系，比如一个 Pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 Pod 在节点上了，那么我就不想和你待在同一个节点上。 污点 对于 nodeAffinity 无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而污点（Taints）恰好与之相反，如果一个节点标记为 污点（Taints） ，除非 Pod 也被标识为可以容忍污点节点，否则该污点（Taints） 节点不会被调度 Pod。 污点的组成： 1key=value:effect 每个污点有一个key和value作为污点的标签，其中value可以为空，effect描述污点的作用。当前taint effect支持三个选项： NoSchedule：表示k8s将不会把Pod调度到具有该污点的Node上 PreferNoSchedule：表示k8s将尽量避免把Pod调度到具有该污点的Node上 NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去 污点的设置、查看和去除 12345678#设置污点kubectl taint nodes node1 key1=value:NoSchedule#节点说明中，查找Taints字段kubectl describe pod pod-name#去除污点kubectl taint nodes node1 key1:NoSchedule- 容忍在Pod上设置容忍，可以容忍污点的存在，可以被调度到存在污点的Node上 1234tolerations:- key: &quot;node-role.kubernetes.io/master&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 其中的 key、value、effect 与 Node 的 Taint 设置需保持一致 如果 operator 的值是 Exists，则 value 属性可省略 如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于) 如果不指定 operator 属性，则默认值为 Equal 另外，还有两个特殊值： 空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也就是是能容忍所有节点的所有 Taints 空的 effect 匹配所有的 effect 九、安全1、机制说明API Server是集群内部各个组件通信的中介，也是外部控制的入口。所以Kubernetes的安全机制基本就是围绕保护API Server来设计的 Kubernetes使用了认证（Authentication）、鉴权（Authorization）、准入控制（Admission Control）三步来保证API Server的安全。 2、Authentication（认证） HTTP Token 认证：通过一个Token来识别合法用户 HTTP Base认证：通过 用户名+密码 的方式认证 最严格的HTTPS证书认证：基于CA根证书签名的客户端身份认证方式 1）、HTTPS证书认证 2）、需要认证的节点两种类型 Kubenetes组件对API Server的访问：kubectl、Controller Manager、Scheduler、kubelete、kube-proxy Kubernetes管理的Pod对容器的访问：Pod（dashborad也是也Pod形式运行） 安全性说明 Controller Manager、Scheduler与API Server在同一台机器，所以直接使用API Server的非安全端口访问，–insecure-bind-address&#x3D;127.0.0.1 kubectl、kubelet、kube-proxy访问API Server就都需要证书进行HTTPS双向认证 证书颁发 手动签发：通过k8s集群的跟ca进行签发HTTPS证书 自动签发：kubelet首次访问API Server时，使用token做认证，通过后，Controller Manager会为kubelet生成一个证书，以后的访问都是用证书做认证了 3）、kubeconfigkubeconfig文件包含集群参数（CA证书、API Server地址），客户端参数（上面生成的证书和私钥），集群context信息（集群名称、用户名）。Kubenetes组件通过启动时指定不同的kubeconfig文件可以 4）、ServiceAccountPod中的容器访问API Server。因为Pod的创建、销毁是动态的，所以要为它手动生成证书就不可行了。Kubenetes使用了Service Account解决Pod访问API Server的认证问题 5）、Secret与SA的关系Kubernetes设计了一种资源对象叫做Secret，分为两类，一种是用于ServiceAccount的service-account-token，另一种是用于保存用户自定义保密信息的Opaque。ServiceAccount中用到包含三个部分：Token、ca.crt、namespace token是使用API Server私钥签名的JWT。用于访问API Server时，Server端认证 ca.crt，根证书。用于Client端验证API Server发送的证书 namespace，表示这个service-account-token的作用域名空间 默认情况下，每个namespace都会有一个ServiceAccount，如果Pod在创建时没有指定ServiceAccount，就会使用Pod所属的namespace的ServiceAccount。 3、Authorization（鉴权）上面认证过程，只是确认通信的双方都确认了对方是可信的，可以相互通信。而鉴权是确定请求方有哪些资源的权限。API Server目前支持以下几种授权策略（通过API Server的启动参数“–anthorization”设置） AlwaysDeny：表示拒绝所有的请求，一般用于测试 AlwaysAllow：允许接收所有请求，如果集群不需要授权流程，则可以采用该策略 （这两种现在不使用） ABAC（Attribute-Based Access Control）：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 Webbook：通过调用外部REST服务对用户进行授权 RBAC（Role-Based Access Control）：基于角色的访问控制，现行默认规则 1）、RBAC授权模式RABC：基于角色的权限控制 RBAC优势： 对集群中的资源和非资源均拥有完整的覆盖 整个RBAC完全由几个API对象完成，同其他API对象一样，可以拥有kubectl或API进行操作 可以在运行时进行调整，无需重启API Server 需要了解的概念： Rule：规则，规则是一组属于不同 API Group 资源上的一组操作的集合 Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的 Rules 元素，二者的区别在于，在 Role 中，定义的规则只适用于单个命名空间，也就是和 namespace 关联的，而 ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外 Role 和 ClusterRole 在Kubernetes 中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、Deployment 这些对象类似，都是我们集群的资源对象，所以同样的可以使用 YAML 文件来描述，用 kubectl 工具来管理 Subject：主题，对应集群中尝试操作的对象，集群中定义了3种类型的主题资源： User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone 或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如 cluster-admin Service Account：服务帐号，通过 Kubernetes API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点 RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程（给某个用户绑定上操作的权限），二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。 2）、Role和ClusterRole Role表示一组规则权限，权限指挥增加（累加权限），不存在一个资源一开始就有很多权限而通过RBAC对其减少的操作。Role可以定义在一个namespace中，如果想要跨namespace则可以创建ClusterRole ClusterRole具有和Role相同的权限角色控制能力，不同的时ClusterRole是集群级别的，ClusterRole可以用于： 集群级别的资源控制（例如node访问权限） 非资源型endpoints（例如&#x2F;healthz访问） 所有命名空间资源控制（例如pods） 3）、RoleBinding和ClusterRoleBindingRoleBinding可以将角色中定义的权限授予用户或用户组，RoleBindbing包含一组权限列表（subjects），权限列表中包含有不同形式的代收与权限资源类型（users,groups,or service accounts）；RoleBinding同样包含对被Bind的Role引用；RoleBinding适用于某个命名空间授权，而ClusterRoleBinding适用于集群范围内的授权 RoleBinding同样可以引用ClusterRole来对当前namespace内用户、用户组或ServiceAccount进行授权，这种操作允许集群管理员在整个集群内定义一些通用的ClusterRole，然后再不同的namespace中使用RoleBinding来引用 4）、只能访问某个namespace的普通用户12username: cnychgroup: youdianzhishi 1、创建用户凭证使用OpenSSL证书来创建一个User 给用户cnych创建一个私钥，命名为cnych.key 1$ openssl genrsa -out cnych.key 2048 使用我们刚刚创建的私钥创建一个证书签名请求文件：cnych.csr，要注意需要确保在-subj参数中指定用户名和组(CN表示用户名，O表示组) 1$ openssl req -new -key cnych.key -out cnych.csr -subj &quot;/CN=cnych/O=youdianzhishi&quot; 通过CA证书（在 &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F; 下的ca.crt和ca.key）批准上面的证书请求，生成最终的证书文件。设置有效期500天。 1234$ openssl x509 -req -in cnych.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out cnych.crt -days 500Signature oksubject=/CN=cnych/O=youdianzhishiGetting CA Private Key 现在我们可以使用刚刚创建的证书文件和私钥文件在集群中创建新的凭证和上下文(Context): 12$ kubectl config set-credentials cnych --client-certificate=cnych.crt --client-key=cnych.keyUser &quot;cnych&quot; set. 我们可以看到一个用户 cnych 创建了，然后为这个用户设置新的 Context，我们这里指定特定的一个 namespace： 12$ kubectl config set-context cnych-context --cluster=kubernetes --namespace=kube-system --user=cnychContext &quot;cnych-context&quot; created. 2、创建角色允许用户操作Deployment，Pod，ReplicaSets 123456789apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cnych-role namespace: kube-systemrules:- apiGroups: [&quot;&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] # 也可以使用[&#x27;*&#x27;] 其中 Pod 属于 core 这个 API Group，在 YAML 中用空字符就可以，而 Deployment 和 ReplicaSet 现在都属于 apps 这个 API Group（如果不知道则可以用 kubectl explain 命令查看），所以 rules 下面的 apiGroups 就综合了这几个资源的 API Group：[“”, “apps”]，其中verbs 就是我们上面提到的可以对这些资源对象执行的操作，我们这里需要所有的操作方法，所以我们也可以使用[‘*’]来代替。然后直接创建这个 Role 3、创建角色权限绑定 在 kube-system 这个命名空间下面将上面的 cnych-role 角色和用户 cnych 进行绑定 12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cnych-rolebinding namespace: kube-systemsubjects:- kind: User name: cnych apiGroup: &quot;&quot;roleRef: kind: Role name: cnych-role apiGroup: rbac.authorization.k8s.io # 留空字符串也可以，则使用当前的apiGroup 4、准入控制准入控制是API Server的插件集合，通过添加不同的插件，实现额外的准入控制规则。甚至于API Server的一些主要的功能都需要通过Admission Controllers实现，比如ServiceAccount。 列举一些插件的功能： NamespaceLifecycle：防止在不存在的namespace上创建对象，防止删除系统预置namespace，删除namespace时，连带删除它的所有资源对象； LimitRanger：确保请求的资源不会超过资源所在Namespace的LimitRange的限制； ServiceAccount：实现自动化添加ServiceAccount； ResourceQuota：确保请求的资源不会超过资源的ResourceQuota限制 十、Helm相当于Linux里的yun 1、用途做为 Kubernetes 的一个包管理工具，Helm具有如下功能： 创建新的 chart chart 打包成 tgz 格式 上传 chart 到 chart 仓库或从仓库中下载 chart 在Kubernetes集群中安装或卸载 chart 管理用Helm安装的 chart 的发布周期 2、重要概念Helm 有三个重要概念： chart：包含了创建Kubernetes的一个应用实例的必要信息 config：包含了应用发布配置信息 release：是一个 chart 及其配置的一个运行实例 3、Helm组件Helm 有以下两个组成部分： Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release 4、Helm命令1、安装应用：helm install 可以指定命名空间、yaml文件 2、查看已安装的release：helm ls 3、删除release：helm uninstall release名字 uninstall 命令会从 Kubernetes 中删除 release，也会删除与 release 相关的所有 Kubernetes 资源以及 release 历史记录。也可以在删除的时候使用 --keep-history 参数，则会保留 release 的历史记录，可以获取该 release 的状态就是 UNINSTALLED，而不是找不到 release了 4、helm show values release名 查看chart包所有可配置的参数选项 5、helm install 命令可以从多个源进行安装： chart 仓库（类似于上面我们提到的） 本地 chart 压缩包（helm install foo-0.1.1.tgz） 本地解压缩的 chart 目录（helm install foo path&#x2F;to&#x2F;foo） 在线的 URL（helm install fool https://example.com/charts/foo-1.2.3.tgz） 6、升级和回滚 升级：helm upgrade 回滚：helm rollback 根据RESISION 十一、kubeadm部署高可用集群k8s集群的高可用实际上是api server的高可用 2种方案： 1、堆叠方案： etcd服务和控制平面被部署在同样的节点中，对基础设施的要求较低，对故障的应对能力也较低 2、 外置etcd方案：etcd和控制平面被分离，需要更多的硬件，也有更好的保障能力 具体过程1、系统设置（&#x2F;etc&#x2F;hosts，依赖环境，关闭防火墙，交换分区等） 2、所有节点安装Docker 3、安装必要工具：kubeadm（所有节点），kubelet（所有节点），kubectl（master节点） 4、安装LVS负载均衡器与keeplived（或haproxy）高可用软件 5、第一台kubeadm进行初始化，生成两个token令牌，一个是master的，一个是slave的 token 过期之后，如何加入集群123456789101112# 创建token$ kubeadm token createll3wpn.pct6tlq66lis3uhk# 查看token$ kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSll3wpn.pct6tlq66lis3uhk 23h 2020-01-17T14:42:50+08:00 authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token# 获取 CA 证书 sha256 编码 hash 值$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed &#x27;s/^.* //&#x27;b5e5c1a000284781677336b00e7345838195ca78af21bddd9defad799243752b 十二、更新10年可用证书kubeadm安装的根证书位于Master节点：&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt 1、查看证书是否过期 1$ kubeadm alpha certs check-expiration 2、部署go语言环境 3、根据版本去Github kubernetes release下载对应的源码并解压 4、修改 cmd&#x2F;kubeadm&#x2F;app&#x2F;util&#x2F;pkiutil&#x2F;pki_helpers.go 文件 修改NotAfter NotAfter: now.Add(duration365d * 10).UTC(), 4、重新编译 make WHAT&#x3D;cmd&#x2F;kubeadm GOFLAGS&#x3D;-v 5、更新kubeadm 123cp /usr/bin/kubeadm /usr/bin/kubeadm.oldcp /root/kubeadm-new /usr/bin/kubeadmchmod a+x /usr/bin/kubeadm 6、更新各节点证书至Master节点 1234cp-r /etc/kubernetes/pki /etc/kubernetes/pki.oldcd /etc/kubernetes/pkikubeadm alpha certs renew all --config=/root/kubeadm-config.yamlopenssl x509 -in apiserver.crt -text-noout | grep Not 7、HA集群其余master节点证书更新 12345678910111213#!/bin/bashmasterNode=&quot;192.168.66.20 192.168.66.21&quot;#for host in $&#123;masterNode&#125;; do# scp /etc/kubernetes/pki/&#123;ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key&#125;&quot;$&#123;USER&#125;&quot;@$host:/etc/kubernetes/pki/# scp /etc/kubernetes/pki/etcd/&#123;ca.crt,ca.key&#125; &quot;root&quot;@$host:/etc/kubernetes/pki/etcd# scp /etc/kubernetes/admin.conf &quot;root&quot;@$host:/etc/kubernetes/#donefor host in$&#123;CONTROL_PLANE_IPS&#125;; doscp /etc/kubernetes/pki/&#123;ca.crt,ca.key,sa.key,sa.pub,front-proxy-ca.crt,front-proxy-ca.key&#125;&quot;$&#123;USER&#125;&quot;@$host:/root/pki/ scp /etc/kubernetes/pki/etcd/&#123;ca.crt,ca.key&#125; &quot;root&quot;@$host:/root/etcd scp /etc/kubernetes/admin.conf &quot;root&quot;@$host:/root/kubernetes/done 故障排查https://bbs.huaweicloud.com/blogs/192473 node节点网络不通","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://bingfly.top/categories/Kubernetes/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://bingfly.top/tags/K8s/"}]},{"title":"K8s基础命令","slug":"总结/K8S基础命令","date":"2024-06-12T16:00:00.000Z","updated":"2024-12-14T22:29:02.599Z","comments":true,"path":"2024/06/13/总结/K8S基础命令/","permalink":"http://bingfly.top/2024/06/13/%E6%80%BB%E7%BB%93/K8S%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/","excerpt":"","text":"kubernetes用到的一些命令kubectl管理工具以及命令 一、基础命令1、create根据文件或者输入来创建资源 123# 创建Deployment和Service资源kubectl create -f javak8s-deployment.yamlkubectl create -f javak8s-service.yaml 2、delete删除资源 12345# 根据yaml文件删除对应的资源，但是yaml文件并不会被删除，这样更加高效kubectl delete -f javak8s-deployment.yaml kubectl delete -f javak8s-service.yaml# 也可以通过具体的资源名称来进行删除，使用这个删除资源，需要同时删除pod和service资源才行kubectl delete 具体的资源名称 3、get获得资源信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 查看所有的资源信息kubectl get all# 查看pod列表kubectl get pod# 显示pod节点的标签信息kubectl get pod --show-labels# 根据指定标签匹配到具体的podkubectl get pods -l app=example# 查看node节点列表kubectl get node # 显示node节点的标签信息kubectl get node --show-labels# 查看pod详细信息，也就是可以查看pod具体运行在哪个节点上（ip地址信息）kubectl get pod -o wide# 查看服务的详细信息，显示了服务名称，类型，集群ip，端口，时间等信息kubectl get svc[root@master ~]# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEgo-service NodePort 10.10.10.247 &lt;none&gt; 8089:33702/TCP 29mjava-service NodePort 10.10.10.248 &lt;none&gt; 8082:32823/TCP 5h17mkubernetes ClusterIP 10.10.10.1 &lt;none&gt; 443/TCP 5d16hnginx-service NodePort 10.10.10.146 &lt;none&gt; 88:34823/TCP 2d19h# 查看命名空间kubectl get ns# 查看所有pod所属的命名空间kubectl get pod --all-namespaces# 查看所有pod所属的命名空间并且查看都在哪些节点上运行kubectl get pod --all-namespaces -o wide# 查看目前所有的replica set，显示了所有的pod的副本数，以及他们的可用数量以及状态等信息kubectl get rs[root@master ~]# kubectl get rsNAME DESIRED CURRENT READY AGEgo-deployment-58c76f7d5c 1 1 1 32mjava-deployment-76889f56c5 1 1 1 5h21mnginx-deployment-58d6d6ccb8 3 3 3 2d19h# 查看目前所有的deploymentkubectl get deployment[root@master ~]# kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEgo-deployment 1/1 1 1 34mjava-deployment 1/1 1 1 5h23mnginx-deployment 3/3 3 3 2d19h# 查看已经部署了的所有应用，可以看到容器，以及容器所用的镜像，标签等信息 kubectl get deploy -o wide[root@master bin]# kubectl get deploy -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx 3/3 3 3 16m nginx nginx:1.10 app=example 4、run在集群中创建并运行一个或多个容器镜像。 12345# 基本语法run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...]# 示例，运行一个名称为nginx，副本数为3，标签为app=example，镜像为nginx:1.10，端口为80的容器实例kubectl run nginx --replicas=3 --labels=&quot;app=example&quot; --image=nginx:1.10 --port=80其他用法参见：http://docs.kubernetes.org.cn/468.html 5、expose创建一个service服务，并且暴露端口让外部可以访问 123# 创建一个nginx服务并且暴露端口让外界可以访问kubectl expose deployment nginx --port=88 --type=NodePort --target-port=80 --name=nginx-service关于expose的详细用法参见：http://docs.kubernetes.org.cn/475.html 6、set配置应用的一些特定资源，也可以修改应用已有的资源 1234# 使用kubectl set --help查看，它的子命令，env，image，resources，selector，serviceaccount，subject。set命令详情参见：http://docs.kubernetes.org.cn/669.html语法：resources (-f FILENAME | TYPE NAME) ([--limits=LIMITS &amp; --requests=REQUESTS] 7、kubectl set resources这个命令用于设置资源的一些范围限制。 资源对象中的Pod可以指定计算资源需求（CPU-单位m、内存-单位Mi），即使用的最小资源请求（Requests），限制（Limits）的最大资源需求，Pod将保证使用在设置的资源数量范围。 对于每个Pod资源，如果指定了Limits（限制）值，并省略了Requests（请求），则Requests默认为Limits的值。 可用资源对象包括(支持大小写)：replicationcontroller、deployment、daemonset、job、replicaset。 例如： 123456# 将deployment的nginx容器cpu限制为“200m”，将内存设置为“512Mi”kubectl set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi# 为nginx中的所有容器设置 Requests和Limitskubectl set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi# 删除nginx中容器的计算资源值kubectl set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0 8、kubectl set selector设置资源的selector（选择器）。如果在调用”set selector”命令之前已经存在选择器，则新创建的选择器将覆盖原来的选择器。 selector必须以字母或数字开头，最多包含63个字符，可使用：字母、数字、连字符” - “ 、点”.”和下划线” _ “。如果指定了–resource-version，则更新将使用此资源版本，否则将使用现有的资源版本。 注意：目前selector命令只能用于Service对象。 12# 语法selector (-f FILENAME | TYPE NAME) EXPRESSIONS [--resource-version=version] 8、kubectl set image 用于更新现有资源的容器镜像。 可用资源对象包括：pod (po)、replicationcontroller (rc)、deployment (deploy)、daemonset (ds)、job、replicaset (rs)。 12345678910# 语法image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N# 将deployment中的nginx容器镜像设置为“nginx：1.9.1”。kubectl set image deployment/nginx busybox=busybox nginx=nginx:1.9.1# 所有deployment和rc的nginx容器镜像更新为“nginx：1.9.1”kubectl set image deployments,rc nginx=nginx:1.9.1 --all# 将daemonset abc的所有容器镜像更新为“nginx：1.9.1”kubectl set image daemonset abc *=nginx:1.9.1# 从本地文件中更新nginx容器镜像kubectl set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml explain命令：用于显示资源文档信息 1kubectl explain rs edit命令:用于编辑资源信息 1234# 编辑Deployment nginx的一些信息kubectl edit deployment nginx# 编辑service类型的nginx的一些信息kubectl edit service/nginx 二、设置命令1、label用于更新（增加、修改或删除）资源上的 label（标签） label 必须以字母或数字开头，可以使用字母、数字、连字符、点和下划线，最长63个字符。 如果–overwrite 为 true，则可以覆盖已有的 label，否则尝试覆盖 label 将会报错。 如果指定了–resource-version，则更新将使用此资源版本，否则将使用现有的资源版本。 123456789101112# 语法label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]# 给名为foo的Pod添加label unhealthy=truekubectl label pods foo unhealthy=true# 给名为foo的Pod修改label 为 &#x27;status&#x27; / value &#x27;unhealthy&#x27;，且覆盖现有的valuekubectl label --overwrite pods foo status=unhealthy# 给 namespace 中的所有 pod 添加 labelkubectl label pods --all status=unhealthy# 仅当resource-version=1时才更新 名为foo的Pod上的labelkubectl label pods foo status=unhealthy --resource-version=1# 删除名为“bar”的label 。（使用“ - ”减号相连）kubectl label pods foo bar- 2、annotate更新一个或多个资源的Annotations信息。也就是注解信息，可以方便的查看做了哪些操作。 Annotations由key&#x2F;value组成。 Annotations的目的是存储辅助数据，特别是通过工具和系统扩展操作的数据，更多介绍在这里。 如果–overwrite为true，现有的annotations可以被覆盖，否则试图覆盖annotations将会报错。 如果设置了–resource-version，则更新将使用此resource version，否则将使用原有的resource version。 1234567891011121314# 语法annotate [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version]# 更新Pod“foo”，设置annotation “description”的value “my frontend”，如果同一个annotation多次设置，则只使用最后设置的value值kubectl annotate pods foo description=&#x27;my frontend&#x27;# 根据“pod.json”中的type和name更新pod的annotationkubectl annotate -f pod.json description=&#x27;my frontend&#x27;# 更新Pod&quot;foo&quot;，设置annotation“description”的value“my frontend running nginx”，覆盖现有的值kubectl annotate --overwrite pods foo description=&#x27;my frontend running nginx&#x27;# 更新 namespace中的所有podkubectl annotate pods --all description=&#x27;my frontend running nginx&#x27;# 只有当resource-version为1时，才更新pod &#x27; foo &#x27;kubectl annotate pods foo description=&#x27;my frontend running nginx&#x27; --resource-version=1# 通过删除名为“description”的annotations来更新pod &#x27; foo &#x27;。#不需要- overwrite flag。kubectl annotate pods foo description- 3、completion用于设置kubectl命令自动补全 12$ source &lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first.$ source &lt;(kubectl completion zsh) # setup autocomplete in zsh 三、kubectl 部署命令1、rollout用于对资源进行管理 可用资源包括：deployments，daemonsets。 子命令： history（查看历史版本） pause（暂停资源） resume（恢复暂停资源） status（查看资源状态） undo（回滚版本） 123456# 语法kubectl rollout SUBCOMMAND# 回滚到之前的deploymentkubectl rollout undo deployment/abc# 查看daemonet的状态kubectl rollout status daemonset/foo 2、rolling-update执行指定ReplicationController的滚动更新。 该命令创建了一个新的RC， 然后一次更新一个pod方式逐步使用新的PodTemplate，最终实现Pod滚动更新，new-controller.json需要与之前RC在相同的namespace下。 123456789101112# 语法rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] --image=NEW_CONTAINER_IMAGE | -f NEW_CONTROLLER_SPEC)# 使用frontend-v2.json中的新RC数据更新frontend-v1的podkubectl rolling-update frontend-v1 -f frontend-v2.json# 使用JSON数据更新frontend-v1的podcat frontend-v2.json | kubectl rolling-update frontend-v1 -f -# 其他的一些滚动更新kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2kubectl rolling-update frontend --image=image:v2kubectl rolling-update frontend-v1 frontend-v2 --rollback 3、scale扩容或缩容 Deployment、ReplicaSet、Replication Controller或 Job 中Pod数量 scale也可以指定多个前提条件，如：当前副本数量或 –resource-version ，进行伸缩比例设置前，系统会先验证前提条件是否成立。这个就是弹性伸缩策略 1234567891011# 语法kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)# 将名为foo中的pod副本数设置为3。kubectl scale --replicas=3 rs/fookubectl scale deploy/nginx --replicas=30# 将由“foo.yaml”配置文件中指定的资源对象和名称标识的Pod资源副本设为3kubectl scale --replicas=3 -f foo.yaml# 如果当前副本数为2，则将其扩展至3。kubectl scale --current-replicas=2 --replicas=3 deployment/mysql# 设置多个RC中Pod副本数量kubectl scale --replicas=5 rc/foo rc/bar rc/baz 4、autoscale这个比scale更加强大，也是弹性伸缩策略 ，它是根据流量的多少来自动进行扩展或者缩容 指定Deployment、ReplicaSet或ReplicationController，并创建已经定义好资源的自动伸缩器。使用自动伸缩器可以根据需要自动增加或减少系统中部署的pod数量。 123456# 语法kubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [flags]# 使用 Deployment “foo”设定，使用默认的自动伸缩策略，指定目标CPU使用率，使其Pod数量在2到10之间kubectl autoscale deployment foo --min=2 --max=10# 使用RC“foo”设定，使其Pod的数量介于1和5之间，CPU使用率维持在80％kubectl autoscale rc foo --max=5 --cpu-percent=80 四、集群管理命令1、certificate用于证书资源管理，授权等 123456789101112131415[root@master ~]# kubectl certificate --helpModify certificate resources.Available Commands: approve Approve a certificate signing request deny Deny a certificate signing requestUsage: kubectl certificate SUBCOMMAND [options]Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).# 例如，当有node节点要向master请求，那么是需要master节点授权的kubectl certificate approve node-csr-81F5uBehyEyLWco5qavBsxc1GzFcZk3aFM3XW5rT3mw node-csr-Ed0kbFhc_q7qx14H3QpqLIUs0uKo036O2SnFpIheM18 2、cluster-info显示集群信息 12345kubectl cluster-info[root@master ~]# kubectl cluster-infoKubernetes master is running at http://localhost:8080To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;. 3、top用于查看资源的cpu，内存磁盘等资源的使用率 12kubectl top pod --all-namespaces它需要heapster运行才行 cordon命令：用于标记某个节点不可调度 uncordon命令：用于标签节点可以调度 drain命令： 用于在维护期间排除节点。 taint命令：参见：https://blog.frognew.com/2018/05/taint-and-toleration.html 五、集群故障排查和调试命令1、describe显示特定资源的详细信息 123456# 语法kubectl describe TYPE NAME_PREFIX（首先检查是否有精确匹配TYPE和NAME_PREFIX的资源，如果没有，将会输出所有名称以NAME_PREFIX开头的资源详细信息）支持的资源包括但不限于（大小写不限）：pods (po)、services (svc)、 replicationcontrollers (rc)、nodes (no)、events (ev)、componentstatuses (cs)、 limitranges (limits)、persistentvolumes (pv)、persistentvolumeclaims (pvc)、 resourcequotas (quota)和secrets。#查看my-nginx pod的详细状态kubectl describe po my-nginx 2、logs用于在一个pod中打印一个容器的日志，如果pod中只有一个容器，可以省略容器名 1234567891011121314151617181920212223# 语法kubectl logs [-f] [-p] POD [-c CONTAINER]# 返回仅包含一个容器的pod nginx的日志快照$ kubectl logs nginx# 返回pod ruby中已经停止的容器web-1的日志快照$ kubectl logs -p -c ruby web-1# 持续输出pod ruby中的容器web-1的日志$ kubectl logs -f -c ruby web-1# 仅输出pod nginx中最近的20条日志$ kubectl logs --tail=20 nginx# 输出pod nginx中最近一小时内产生的所有日志$ kubectl logs --since=1h nginx# 参数选项 -c, --container=&quot;&quot;: 容器名。 -f, --follow[=false]: 指定是否持续输出日志（实时日志）。 --interactive[=true]: 如果为true，当需要时提示用户进行输入。默认为true。 --limit-bytes=0: 输出日志的最大字节数。默认无限制。 -p, --previous[=false]: 如果为true，输出pod中曾经运行过，但目前已终止的容器的日志。 --since=0: 仅返回相对时间范围，如5s、2m或3h，之内的日志。默认返回所有日志。只能同时使用since和since-time中的一种。 --since-time=&quot;&quot;: 仅返回指定时间（RFC3339格式）之后的日志。默认返回所有日志。只能同时使用since和since-time中的一种。 --tail=-1: 要显示的最新的日志条数。默认为-1，显示所有的日志。 --timestamps[=false]: 在日志中包含时间戳。 3、exec进入容器进行交互，在容器中执行命令 123456789# 语法kubectl exec POD [-c CONTAINER] -- COMMAND [args...]#命令选项 -c, --container=&quot;&quot;: 容器名。如果未指定，使用pod中的一个容器。 -p, --pod=&quot;&quot;: Pod名。 -i, --stdin[=false]: 将控制台输入发送到容器。 -t, --tty[=false]: 将标准输入控制台作为容器的控制台输入。# 进入nginx容器，执行一些命令操作kubectl exec -it nginx-deployment-58d6d6ccb8-lc5fp bash 4、attach连接到一个正在运行的容器。 1234567891011121314#语法kubectl attach POD -c CONTAINER# 参数选项-c, --container=&quot;&quot;: 容器名。如果省略，则默认选择第一个 pod -i, --stdin[=false]: 将控制台输入发送到容器。 -t, --tty[=false]: 将标准输入控制台作为容器的控制台输入。 # 获取正在运行中的pod 123456-7890的输出，默认连接到第一个容器kubectl attach 123456-7890# 获取pod 123456-7890中ruby-container的输出kubectl attach 123456-7890 -c ruby-container# 切换到终端模式，将控制台输入发送到pod 123456-7890的ruby-container的“bash”命令，并将其输出到控制台/# 错误控制台的信息发送回客户端。kubectl attach 123456-7890 -c ruby-container -i -t 5、cp拷贝文件或者目录到pod容器中 用于pod和外部的文件交换,类似于docker 的cp，就是将容器中的内容和外部的内容进行交换。 六、其他命令1、api-servions打印受支持的api版本信息 1234567891011121314151617181920212223242526# kubectl api-versions[root@master ~]# kubectl api-versionsadmissionregistration.k8s.io/v1beta1apiextensions.k8s.io/v1beta1apiregistration.k8s.io/v1beta1apps/v1apps/v1beta1apps/v1beta2authentication.k8s.io/v1authentication.k8s.io/v1beta1authorization.k8s.io/v1authorization.k8s.io/v1beta1autoscaling/v1autoscaling/v2beta1batch/v1batch/v1beta1certificates.k8s.io/v1beta1events.k8s.io/v1beta1extensions/v1beta1networking.k8s.io/v1policy/v1beta1rbac.authorization.k8s.io/v1rbac.authorization.k8s.io/v1beta1storage.k8s.io/v1storage.k8s.io/v1beta1v1 2、help用于查看命令帮助 1234# 显示全部的命令帮助提示kubectl --help# 具体的子命令帮助，例如kubectl create --help 3、config用于修改kubeconfig配置文件（用于访问api，例如配置认证信息） 4、version打印客户端和服务端版本信息 123[root@master ~]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;15&quot;, GitVersion:&quot;v1.15.3&quot;, GitCommit:&quot;2d3c76f9091b6bec110a5e63777c332469e0cba2&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2019-08-19T11:13:54Z&quot;, GoVersion:&quot;go1.12.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.0&quot;, GitCommit:&quot;925c127ec6b946659ad0fd596fa959be43f0cc05&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-12-15T20:55:30Z&quot;, GoVersion:&quot;go1.9.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; 5、plugin运行一个命令行插件 七、高级命令1、apply通过文件名或者标准输入对资源应用配置 通过文件名或控制台输入，对资源进行配置。 如果资源不存在，将会新建一个。可以使用 JSON 或者 YAML 格式。 12345678910111213141516# 语法kubectl apply -f FILENAME# 将pod.json中的配置应用到podkubectl apply -f ./pod.json# 将控制台输入的JSON配置应用到Podcat pod.json | kubectl apply -f -选项-f, --filename=[]: 包含配置信息的文件名，目录名或者URL。 --include-extended-apis[=true]: If true, include definitions of new APIs via calls to the API server. [default true] -o, --output=&quot;&quot;: 输出模式。&quot;-o name&quot;为快捷输出(资源/name). --record[=false]: 在资源注释中记录当前 kubectl 命令。 -R, --recursive[=false]: Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests organized within the same directory. --schema-cache-dir=&quot;~/.kube/schema&quot;: 非空则将API schema缓存为指定文件，默认缓存到&#x27;$HOME/.kube/schema&#x27; --validate[=true]: 如果为true，在发送到服务端前先使用schema来验证输入。 2、patch使用补丁修改，更新资源的字段，也就是修改资源的部分内容 1234567# 语法kubectl patch (-f FILENAME | TYPE NAME) -p PATCH# Partially update a node using strategic merge patchkubectl patch node k8s-node-1 -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;unschedulable&quot;:true&#125;&#125;&#x27;# Update a container&#x27;s image; spec.containers[*].name is required because it&#x27;s a merge keykubectl patch pod valid-pod -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;name&quot;:&quot;kubernetes-serve-hostname&quot;,&quot;image&quot;:&quot;new image&quot;&#125;]&#125;&#125;&#x27; 3、replace通过文件或者标准输入替换原有资源 1234567891011# 语法kubectl replace -f FILENAME# Replace a pod using the data in pod.json.kubectl replace -f ./pod.json# Replace a pod based on the JSON passed into stdin.cat pod.json | kubectl replace -f -# Update a single-container pod&#x27;s image version (tag) to v4kubectl get pod mypod -o yaml | sed &#x27;s/\\(image: myimage\\):.*$/\\1:v4/&#x27; | kubectl replace -f -# Force replace, delete and then re-create the resourcekubectl replace --force -f ./pod.json 4、convert不同的版本之间转换配置文件 12345678910# 语法kubectl convert -f FILENAME# Convert &#x27;pod.yaml&#x27; to latest version and print to stdout.kubectl convert -f pod.yaml# Convert the live state of the resource specified by &#x27;pod.yaml&#x27; to the latest version# and print to stdout in json format.kubectl convert -f pod.yaml --local -o json# Convert all files under current directory to latest version and create them all.kubectl convert -f . | kubectl create -f -","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://bingfly.top/categories/Kubernetes/"}],"tags":[{"name":"K8s","slug":"K8s","permalink":"http://bingfly.top/tags/K8s/"}]},{"title":"Hello World","slug":"hello-world","date":"2024-06-04T16:00:00.000Z","updated":"2024-12-14T22:20:06.662Z","comments":true,"path":"2024/06/05/hello-world/","permalink":"http://bingfly.top/2024/06/05/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"hexo+博客","slug":"hexo-博客","permalink":"http://bingfly.top/categories/hexo-%E5%8D%9A%E5%AE%A2/"}],"tags":[]}],"categories":[{"name":"Python面试","slug":"Python面试","permalink":"http://bingfly.top/categories/Python%E9%9D%A2%E8%AF%95/"},{"name":"Python基础","slug":"Python基础","permalink":"http://bingfly.top/categories/Python%E5%9F%BA%E7%A1%80/"},{"name":"日志分析","slug":"日志分析","permalink":"http://bingfly.top/categories/%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/"},{"name":"监控","slug":"监控","permalink":"http://bingfly.top/categories/%E7%9B%91%E6%8E%A7/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://bingfly.top/categories/Kubernetes/"},{"name":"hexo+博客","slug":"hexo-博客","permalink":"http://bingfly.top/categories/hexo-%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://bingfly.top/tags/Python/"},{"name":"ELK","slug":"ELK","permalink":"http://bingfly.top/tags/ELK/"},{"name":"Prometheus","slug":"Prometheus","permalink":"http://bingfly.top/tags/Prometheus/"},{"name":"K8s","slug":"K8s","permalink":"http://bingfly.top/tags/K8s/"}]}